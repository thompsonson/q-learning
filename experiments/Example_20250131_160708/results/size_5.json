[
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.1287760734558105,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 21,
        "policy_stability_history": [
            0.0,
            0.7768595041322314,
            0.9173553719008265,
            0.8429752066115702,
            0.8512396694214877,
            0.9421487603305785,
            0.8512396694214877,
            0.8429752066115702,
            0.9008264462809917,
            0.9256198347107438,
            0.9917355371900827,
            0.9421487603305785,
            0.8842975206611571,
            0.9008264462809917,
            0.9586776859504132,
            0.8512396694214877,
            0.9338842975206612,
            0.9008264462809917,
            0.9173553719008265,
            0.9917355371900827,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -1210,
            -209,
            -489,
            -1210,
            -91,
            -930,
            -1210,
            -568,
            -232,
            -209,
            -182,
            -628,
            -616,
            -244,
            -1210,
            -303,
            -1210,
            -917,
            -109,
            -341,
            -8
        ],
        "steps_history": [
            1210,
            1210,
            310,
            590,
            1210,
            192,
            1031,
            1210,
            669,
            333,
            310,
            283,
            729,
            717,
            345,
            1210,
            404,
            1210,
            1018,
            210,
            442,
            109
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "181/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.141272068023682,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.8677685950413223,
            0.8347107438016529,
            0.9256198347107438,
            0.8677685950413223,
            0.9173553719008265,
            0.9421487603305785,
            0.9504132231404959,
            0.8760330578512396,
            0.9173553719008265,
            0.8264462809917356,
            0.9586776859504132,
            0.9421487603305785,
            0.8842975206611571,
            0.9917355371900827,
            0.9669421487603306,
            0.8677685950413223,
            0.9834710743801653,
            0.9917355371900827,
            0.9752066115702479,
            0.8842975206611571,
            0.9008264462809917,
            1.0
        ],
        "reward_history": [
            -1210,
            -1210,
            -830,
            -22,
            -383,
            -266,
            -235,
            -374,
            -630,
            -499,
            -1210,
            -137,
            -74,
            -1210,
            -158,
            -168,
            -973,
            -372,
            -434,
            -201,
            -1210,
            -1210,
            -247
        ],
        "steps_history": [
            1210,
            1210,
            931,
            123,
            484,
            367,
            336,
            475,
            731,
            600,
            1210,
            238,
            175,
            1210,
            259,
            269,
            1074,
            473,
            535,
            302,
            1210,
            1210,
            348
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "182/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.62497091293335,
        "final_policy_stability": 0.9834710743801653,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.8181818181818182,
            0.8842975206611571,
            0.8925619834710744,
            0.8512396694214877,
            0.8429752066115702,
            0.9504132231404959,
            0.9173553719008265,
            0.9421487603305785,
            0.8429752066115702,
            0.9752066115702479,
            0.9586776859504132,
            0.8925619834710744,
            0.9586776859504132,
            0.9669421487603306,
            0.9834710743801653,
            0.9173553719008265,
            0.9421487603305785,
            0.9752066115702479,
            0.9834710743801653,
            0.9752066115702479,
            0.8512396694214877,
            0.9752066115702479,
            0.8925619834710744,
            0.9586776859504132,
            0.9917355371900827,
            0.9173553719008265,
            0.9917355371900827,
            1.0,
            1.0,
            0.9834710743801653
        ],
        "reward_history": [
            -231,
            -1210,
            -513,
            -204,
            -1210,
            -911,
            -660,
            -100,
            -1210,
            -205,
            -1210,
            -117,
            -86,
            -428,
            -288,
            25,
            -177,
            -647,
            -253,
            -65,
            -158,
            -63,
            -1210,
            -356,
            -1210,
            -366,
            -76,
            -1210,
            -240,
            -139,
            -69,
            -765
        ],
        "steps_history": [
            332,
            1210,
            614,
            305,
            1210,
            1012,
            761,
            201,
            1210,
            306,
            1210,
            218,
            187,
            529,
            389,
            76,
            278,
            748,
            354,
            166,
            259,
            164,
            1210,
            457,
            1210,
            467,
            177,
            1210,
            341,
            240,
            170,
            866
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "183/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.125517129898071,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.8429752066115702,
            0.9421487603305785,
            0.8677685950413223,
            0.9090909090909091,
            0.9669421487603306,
            0.8512396694214877,
            0.9669421487603306,
            0.8512396694214877,
            0.9586776859504132,
            0.9173553719008265,
            0.8347107438016529,
            0.8512396694214877,
            0.8099173553719008,
            0.9586776859504132,
            0.9669421487603306,
            0.8842975206611571,
            0.9752066115702479,
            0.8264462809917356,
            0.9669421487603306,
            0.9090909090909091,
            0.9173553719008265,
            0.9917355371900827,
            0.8760330578512396,
            1.0,
            0.9834710743801653,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -1210,
            -1210,
            -111,
            -1210,
            -1210,
            -268,
            -1210,
            -1,
            -1210,
            -137,
            -146,
            -1210,
            -806,
            -880,
            -124,
            -214,
            -522,
            -208,
            -968,
            -366,
            -316,
            -623,
            -91,
            -1210,
            -20,
            -495,
            -362,
            -314
        ],
        "steps_history": [
            1210,
            1210,
            1210,
            212,
            1210,
            1210,
            369,
            1210,
            102,
            1210,
            238,
            247,
            1210,
            907,
            981,
            225,
            315,
            623,
            309,
            1069,
            467,
            417,
            724,
            192,
            1210,
            121,
            596,
            463,
            415
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "184/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.2460925579071045,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.7355371900826446,
            0.8429752066115702,
            0.8677685950413223,
            0.9256198347107438,
            0.9504132231404959,
            0.9173553719008265,
            0.8842975206611571,
            0.9338842975206612,
            0.8677685950413223,
            0.8347107438016529,
            0.9752066115702479,
            0.8429752066115702,
            0.8677685950413223,
            0.8677685950413223,
            0.9669421487603306,
            0.9834710743801653,
            0.9669421487603306,
            0.9752066115702479,
            0.9256198347107438,
            0.9669421487603306,
            0.9008264462809917,
            0.9917355371900827,
            1.0
        ],
        "reward_history": [
            -155,
            -1096,
            -1210,
            -703,
            -460,
            -126,
            -10,
            -225,
            -140,
            -1210,
            -1210,
            -133,
            -974,
            -793,
            -1210,
            -282,
            -90,
            -370,
            -230,
            -539,
            -254,
            -1210,
            -316,
            -60
        ],
        "steps_history": [
            256,
            1197,
            1210,
            804,
            561,
            227,
            111,
            326,
            241,
            1210,
            1210,
            234,
            1075,
            894,
            1210,
            383,
            191,
            471,
            331,
            640,
            355,
            1210,
            417,
            161
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "185/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.739726781845093,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.7603305785123967,
            0.768595041322314,
            0.8264462809917356,
            0.9256198347107438,
            0.9173553719008265,
            0.859504132231405,
            0.8181818181818182,
            0.9421487603305785,
            0.8842975206611571,
            0.9421487603305785,
            0.9504132231404959,
            0.859504132231405,
            0.9669421487603306,
            0.8760330578512396,
            0.9669421487603306,
            0.9504132231404959,
            0.8842975206611571,
            0.9752066115702479,
            0.9504132231404959,
            0.8677685950413223,
            0.9834710743801653,
            0.9256198347107438,
            0.9421487603305785,
            0.9834710743801653,
            0.9008264462809917,
            0.9504132231404959,
            0.9834710743801653,
            0.9669421487603306,
            0.9752066115702479,
            0.9752066115702479,
            0.9834710743801653,
            1.0
        ],
        "reward_history": [
            -310,
            -1210,
            -1210,
            -1210,
            -309,
            -237,
            -807,
            -1210,
            -257,
            -316,
            -271,
            -174,
            -1210,
            -101,
            -794,
            -262,
            -237,
            -1210,
            -4,
            -342,
            -1210,
            -19,
            -173,
            -138,
            -35,
            -1210,
            -213,
            -238,
            -54,
            -108,
            -491,
            -442,
            -39
        ],
        "steps_history": [
            411,
            1210,
            1210,
            1210,
            410,
            338,
            908,
            1210,
            358,
            417,
            372,
            275,
            1210,
            202,
            895,
            363,
            338,
            1210,
            105,
            443,
            1210,
            120,
            274,
            239,
            136,
            1210,
            314,
            339,
            155,
            209,
            592,
            543,
            140
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "186/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.869625091552734,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.7933884297520661,
            0.8512396694214877,
            0.8347107438016529,
            0.9256198347107438,
            0.9256198347107438,
            0.9338842975206612,
            0.9586776859504132,
            0.8842975206611571,
            0.8677685950413223,
            0.9421487603305785,
            0.8925619834710744,
            0.8925619834710744,
            0.9421487603305785,
            0.8099173553719008,
            0.8925619834710744,
            0.9586776859504132,
            0.9421487603305785,
            0.9338842975206612,
            0.9752066115702479,
            0.9917355371900827,
            0.8512396694214877,
            0.9834710743801653,
            0.8677685950413223,
            0.9669421487603306,
            1.0,
            0.9834710743801653,
            0.9752066115702479,
            1.0,
            1.0,
            0.9008264462809917,
            1.0
        ],
        "reward_history": [
            -545,
            -1210,
            -1210,
            -822,
            -237,
            -212,
            -170,
            -273,
            -550,
            -1210,
            -84,
            -261,
            -450,
            -156,
            -1210,
            -394,
            -315,
            -219,
            -225,
            -163,
            -21,
            -1210,
            -70,
            -1210,
            -129,
            18,
            -41,
            -50,
            -133,
            -15,
            -982,
            -48
        ],
        "steps_history": [
            646,
            1210,
            1210,
            923,
            338,
            313,
            271,
            374,
            651,
            1210,
            185,
            362,
            551,
            257,
            1210,
            495,
            416,
            320,
            326,
            264,
            122,
            1210,
            171,
            1210,
            230,
            83,
            142,
            151,
            234,
            116,
            1083,
            149
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "187/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.5846474170684814,
        "final_policy_stability": 0.8842975206611571,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.859504132231405,
            0.7603305785123967,
            0.8760330578512396,
            0.8760330578512396,
            0.8925619834710744,
            0.9008264462809917,
            0.8264462809917356,
            0.9586776859504132,
            0.8512396694214877,
            0.8429752066115702,
            0.9421487603305785,
            0.9338842975206612,
            0.8842975206611571,
            0.9669421487603306,
            0.9504132231404959,
            0.9504132231404959,
            0.9586776859504132,
            0.9669421487603306,
            0.9586776859504132,
            0.8347107438016529,
            0.9421487603305785,
            0.9338842975206612,
            0.9669421487603306,
            0.9917355371900827,
            0.9917355371900827,
            0.9917355371900827,
            0.9504132231404959,
            0.9917355371900827,
            0.8925619834710744,
            1.0,
            1.0,
            0.9917355371900827,
            0.8842975206611571
        ],
        "reward_history": [
            -1210,
            -1210,
            -1210,
            -341,
            -1210,
            -427,
            -347,
            -1025,
            -87,
            -984,
            -1210,
            -108,
            -138,
            -460,
            18,
            -170,
            -70,
            -103,
            33,
            -57,
            -1210,
            -370,
            -204,
            -266,
            -89,
            -61,
            -214,
            -357,
            -8,
            -578,
            -110,
            26,
            -271,
            -935
        ],
        "steps_history": [
            1210,
            1210,
            1210,
            442,
            1210,
            528,
            448,
            1126,
            188,
            1085,
            1210,
            209,
            239,
            561,
            83,
            271,
            171,
            204,
            68,
            158,
            1210,
            471,
            305,
            367,
            190,
            162,
            315,
            458,
            109,
            679,
            211,
            75,
            372,
            1036
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "188/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.068878889083862,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.7768595041322314,
            0.9256198347107438,
            0.9090909090909091,
            0.8016528925619835,
            0.8842975206611571,
            0.8512396694214877,
            0.8842975206611571,
            0.8264462809917356,
            0.859504132231405,
            0.859504132231405,
            0.8842975206611571,
            0.8760330578512396,
            0.9256198347107438,
            0.9917355371900827,
            0.9752066115702479,
            0.9752066115702479,
            0.9834710743801653,
            0.9917355371900827,
            1.0,
            1.0,
            1.0,
            0.9917355371900827
        ],
        "reward_history": [
            -1210,
            -1098,
            -120,
            -429,
            -620,
            -584,
            -772,
            -582,
            -813,
            -1210,
            -1210,
            -1210,
            -988,
            -444,
            -85,
            -253,
            -126,
            -2,
            -32,
            -31,
            11,
            -36,
            -61
        ],
        "steps_history": [
            1210,
            1199,
            221,
            530,
            721,
            685,
            873,
            683,
            914,
            1210,
            1210,
            1210,
            1089,
            545,
            186,
            354,
            227,
            103,
            133,
            132,
            90,
            137,
            162
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "189/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.909197807312012,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.7768595041322314,
            0.8760330578512396,
            0.8181818181818182,
            0.7851239669421488,
            0.9090909090909091,
            0.8512396694214877,
            0.9256198347107438,
            0.9504132231404959,
            0.8512396694214877,
            0.9090909090909091,
            0.7851239669421488,
            0.8347107438016529,
            0.9586776859504132,
            0.9338842975206612,
            0.9752066115702479,
            0.9834710743801653,
            0.8925619834710744,
            0.9586776859504132,
            0.9917355371900827,
            0.9834710743801653,
            0.9669421487603306,
            0.9834710743801653,
            0.8760330578512396,
            0.9834710743801653,
            0.9669421487603306,
            0.9917355371900827
        ],
        "reward_history": [
            -331,
            -1210,
            -676,
            -1210,
            -956,
            -198,
            -1210,
            -103,
            -165,
            -1210,
            -345,
            -1210,
            -541,
            -276,
            -341,
            -64,
            -151,
            -927,
            -292,
            -36,
            -161,
            -231,
            -233,
            -1210,
            -65,
            -248,
            -276
        ],
        "steps_history": [
            432,
            1210,
            777,
            1210,
            1057,
            299,
            1210,
            204,
            266,
            1210,
            446,
            1210,
            642,
            377,
            442,
            165,
            252,
            1028,
            393,
            137,
            262,
            332,
            334,
            1210,
            166,
            349,
            377
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "190/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.837060928344727,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.9008264462809917,
            0.768595041322314,
            0.9090909090909091,
            0.8099173553719008,
            0.8512396694214877,
            0.9008264462809917,
            0.9008264462809917,
            0.9504132231404959,
            0.9752066115702479,
            0.9752066115702479,
            0.9338842975206612,
            0.9421487603305785,
            0.9504132231404959,
            0.9338842975206612,
            0.859504132231405,
            0.9173553719008265,
            0.9338842975206612,
            0.8760330578512396,
            0.9834710743801653,
            0.9586776859504132,
            0.9669421487603306,
            0.9834710743801653,
            0.9752066115702479,
            0.8677685950413223,
            0.8842975206611571,
            0.9917355371900827,
            0.9834710743801653,
            0.9917355371900827,
            0.9173553719008265,
            1.0,
            0.9421487603305785,
            1.0
        ],
        "reward_history": [
            -1210,
            -303,
            -1052,
            -356,
            -1210,
            -768,
            -430,
            -429,
            -217,
            -229,
            -234,
            -281,
            -123,
            -105,
            -26,
            -1210,
            -304,
            -204,
            -1210,
            -33,
            -119,
            4,
            -14,
            21,
            -1210,
            -482,
            -16,
            -5,
            -96,
            -395,
            -102,
            -289,
            -148
        ],
        "steps_history": [
            1210,
            404,
            1153,
            457,
            1210,
            869,
            531,
            530,
            318,
            330,
            335,
            382,
            224,
            206,
            127,
            1210,
            405,
            305,
            1210,
            134,
            220,
            97,
            115,
            80,
            1210,
            583,
            117,
            106,
            197,
            496,
            203,
            390,
            249
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "191/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.6265869140625,
        "final_policy_stability": 0.9834710743801653,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.7768595041322314,
            0.8181818181818182,
            0.8925619834710744,
            0.7851239669421488,
            0.9256198347107438,
            0.8099173553719008,
            0.9090909090909091,
            0.9338842975206612,
            0.9421487603305785,
            0.9586776859504132,
            0.859504132231405,
            0.9669421487603306,
            0.9669421487603306,
            0.9421487603305785,
            0.9173553719008265,
            0.9421487603305785,
            0.9173553719008265,
            0.9669421487603306,
            0.9917355371900827,
            0.8677685950413223,
            0.9669421487603306,
            0.9586776859504132,
            0.9917355371900827,
            0.9752066115702479,
            0.9834710743801653,
            0.9504132231404959,
            0.9669421487603306,
            0.9917355371900827,
            0.9834710743801653,
            1.0,
            0.8925619834710744,
            0.9917355371900827,
            0.9834710743801653
        ],
        "reward_history": [
            -612,
            -1210,
            -1100,
            -168,
            -1210,
            -28,
            -1210,
            -429,
            -85,
            -69,
            -95,
            -1210,
            -134,
            -65,
            -304,
            -454,
            -48,
            -500,
            -78,
            -72,
            -266,
            -141,
            -121,
            23,
            -18,
            -43,
            -310,
            -67,
            -99,
            -114,
            -154,
            -1210,
            -131,
            -171
        ],
        "steps_history": [
            713,
            1210,
            1201,
            269,
            1210,
            129,
            1210,
            530,
            186,
            170,
            196,
            1210,
            235,
            166,
            405,
            555,
            149,
            601,
            179,
            173,
            367,
            242,
            222,
            78,
            119,
            144,
            411,
            168,
            200,
            215,
            255,
            1210,
            232,
            272
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "192/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.646373748779297,
        "final_policy_stability": 0.8925619834710744,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.8842975206611571,
            0.8264462809917356,
            0.8512396694214877,
            0.8677685950413223,
            0.9256198347107438,
            0.8842975206611571,
            0.8925619834710744,
            0.9669421487603306,
            0.8925619834710744,
            0.8760330578512396,
            0.8429752066115702,
            0.9504132231404959,
            0.9256198347107438,
            0.8512396694214877,
            0.9669421487603306,
            0.9586776859504132,
            0.9008264462809917,
            0.9834710743801653,
            0.8842975206611571,
            0.9917355371900827,
            0.9834710743801653,
            0.9173553719008265,
            0.9834710743801653,
            0.9834710743801653,
            0.9421487603305785,
            0.8677685950413223,
            0.9586776859504132,
            1.0,
            0.9752066115702479,
            0.9917355371900827,
            0.8925619834710744
        ],
        "reward_history": [
            -609,
            -1210,
            -163,
            -1210,
            -610,
            -1210,
            -68,
            -260,
            -209,
            -122,
            -263,
            -1210,
            -692,
            -54,
            -307,
            -823,
            -47,
            -68,
            -295,
            18,
            -589,
            -55,
            -48,
            -372,
            -64,
            -1,
            -549,
            -749,
            -266,
            -159,
            -278,
            -21,
            -1210
        ],
        "steps_history": [
            710,
            1210,
            264,
            1210,
            711,
            1210,
            169,
            361,
            310,
            223,
            364,
            1210,
            793,
            155,
            408,
            924,
            148,
            169,
            396,
            83,
            690,
            156,
            149,
            473,
            165,
            102,
            650,
            850,
            367,
            260,
            379,
            122,
            1210
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "193/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.835353136062622,
        "final_policy_stability": 0.9752066115702479,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.9090909090909091,
            0.8512396694214877,
            0.8925619834710744,
            0.768595041322314,
            0.9421487603305785,
            0.9256198347107438,
            0.8842975206611571,
            0.8429752066115702,
            0.8512396694214877,
            0.9256198347107438,
            0.9917355371900827,
            0.8925619834710744,
            0.9504132231404959,
            0.8842975206611571,
            0.9586776859504132,
            0.8760330578512396,
            0.9421487603305785,
            0.9256198347107438,
            0.9669421487603306,
            0.9586776859504132,
            0.9504132231404959,
            0.9669421487603306,
            0.9752066115702479,
            0.9917355371900827,
            0.9917355371900827,
            1.0,
            0.9752066115702479
        ],
        "reward_history": [
            -1106,
            -289,
            -1210,
            -258,
            -1210,
            -105,
            -142,
            -516,
            -1210,
            -196,
            -216,
            -25,
            -479,
            3,
            -1210,
            -136,
            -1210,
            -186,
            -378,
            -49,
            -83,
            -280,
            -64,
            -9,
            -21,
            6,
            -40,
            -31
        ],
        "steps_history": [
            1207,
            390,
            1210,
            359,
            1210,
            206,
            243,
            617,
            1210,
            297,
            317,
            126,
            580,
            98,
            1210,
            237,
            1210,
            287,
            479,
            150,
            184,
            381,
            165,
            110,
            122,
            95,
            141,
            132
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "194/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.69278359413147,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8760330578512396,
            0.8842975206611571,
            0.8677685950413223,
            0.8099173553719008,
            0.8264462809917356,
            0.9090909090909091,
            0.9752066115702479,
            0.8760330578512396,
            0.9090909090909091,
            0.8925619834710744,
            0.9586776859504132,
            0.9338842975206612,
            0.859504132231405,
            0.9421487603305785,
            0.9338842975206612,
            0.9669421487603306,
            0.8264462809917356,
            0.9504132231404959,
            0.8760330578512396,
            0.9586776859504132,
            0.9752066115702479,
            0.9669421487603306,
            0.9504132231404959,
            0.9917355371900827,
            1.0,
            1.0,
            1.0,
            0.9917355371900827,
            0.9834710743801653,
            1.0,
            1.0,
            0.9917355371900827,
            0.9917355371900827
        ],
        "reward_history": [
            -92,
            -1210,
            -715,
            -1210,
            -554,
            -552,
            -187,
            29,
            -1210,
            -109,
            -672,
            -58,
            -114,
            -565,
            -92,
            -48,
            -66,
            -844,
            -228,
            -1210,
            -232,
            -283,
            -259,
            -262,
            -61,
            -53,
            -137,
            -241,
            -117,
            -2,
            -203,
            -108,
            -110,
            -189
        ],
        "steps_history": [
            193,
            1210,
            816,
            1210,
            655,
            653,
            288,
            72,
            1210,
            210,
            773,
            159,
            215,
            666,
            193,
            149,
            167,
            945,
            329,
            1210,
            333,
            384,
            360,
            363,
            162,
            154,
            238,
            342,
            218,
            103,
            304,
            209,
            211,
            290
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "195/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.182461261749268,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.7768595041322314,
            0.9173553719008265,
            0.8512396694214877,
            0.8429752066115702,
            0.9421487603305785,
            0.8512396694214877,
            0.8429752066115702,
            0.9008264462809917,
            0.9421487603305785,
            0.9669421487603306,
            0.9504132231404959,
            0.8677685950413223,
            0.9421487603305785,
            0.9752066115702479,
            0.9586776859504132,
            0.8677685950413223,
            0.8760330578512396,
            0.9421487603305785,
            0.9504132231404959,
            0.9917355371900827,
            0.9917355371900827,
            1.0
        ],
        "reward_history": [
            -1210,
            -1210,
            -209,
            -489,
            -1210,
            -91,
            -930,
            -1210,
            -568,
            -232,
            -209,
            -182,
            -628,
            -426,
            -89,
            -244,
            -1210,
            -1210,
            -435,
            -785,
            -109,
            -341,
            -8
        ],
        "steps_history": [
            1210,
            1210,
            310,
            590,
            1210,
            192,
            1031,
            1210,
            669,
            333,
            310,
            283,
            729,
            527,
            190,
            345,
            1210,
            1210,
            536,
            886,
            210,
            442,
            109
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "196/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.924487829208374,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8842975206611571,
            0.8512396694214877,
            0.8099173553719008,
            0.9090909090909091,
            0.9173553719008265,
            0.9504132231404959,
            0.9256198347107438,
            0.9586776859504132,
            0.8842975206611571,
            0.9752066115702479,
            0.9669421487603306,
            1.0,
            1.0,
            0.8925619834710744,
            0.8842975206611571,
            1.0,
            0.9504132231404959,
            0.9586776859504132,
            0.8512396694214877,
            0.8429752066115702,
            0.9834710743801653,
            0.9834710743801653,
            0.9752066115702479,
            0.9834710743801653,
            1.0,
            0.9917355371900827,
            1.0,
            1.0,
            0.9917355371900827,
            1.0
        ],
        "reward_history": [
            -1210,
            -1210,
            -830,
            -873,
            -782,
            -328,
            -129,
            -331,
            -214,
            -1210,
            -165,
            -193,
            3,
            39,
            -1210,
            -1210,
            -310,
            -498,
            -137,
            -1210,
            -1210,
            -4,
            -142,
            -226,
            -257,
            1,
            -58,
            -149,
            28,
            -201,
            -207
        ],
        "steps_history": [
            1210,
            1210,
            931,
            974,
            883,
            429,
            230,
            432,
            315,
            1210,
            266,
            294,
            98,
            62,
            1210,
            1210,
            411,
            599,
            238,
            1210,
            1210,
            105,
            243,
            327,
            358,
            100,
            159,
            250,
            73,
            302,
            308
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "197/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.889697074890137,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.8347107438016529,
            0.8677685950413223,
            0.8347107438016529,
            0.9504132231404959,
            0.859504132231405,
            0.8347107438016529,
            0.9421487603305785,
            0.8842975206611571,
            0.8677685950413223,
            0.9421487603305785,
            0.8760330578512396,
            0.9669421487603306,
            0.9752066115702479,
            0.9834710743801653,
            0.8099173553719008,
            0.9917355371900827,
            0.9421487603305785,
            0.9669421487603306,
            0.859504132231405,
            0.9834710743801653,
            0.9008264462809917,
            0.9338842975206612,
            1.0,
            0.8842975206611571,
            0.9917355371900827
        ],
        "reward_history": [
            -231,
            -1210,
            -513,
            -204,
            -1210,
            -345,
            -638,
            -1210,
            -315,
            -1210,
            -446,
            -399,
            -231,
            -133,
            -288,
            25,
            -1210,
            -69,
            -324,
            -63,
            -1210,
            -356,
            -1210,
            -1210,
            -330,
            -1210,
            -519
        ],
        "steps_history": [
            332,
            1210,
            614,
            305,
            1210,
            446,
            739,
            1210,
            416,
            1210,
            547,
            500,
            332,
            234,
            389,
            76,
            1210,
            170,
            425,
            164,
            1210,
            457,
            1210,
            1210,
            431,
            1210,
            620
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "198/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.025716066360474,
        "final_policy_stability": 0.9421487603305785,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.8842975206611571,
            0.8842975206611571,
            0.9338842975206612,
            0.9256198347107438,
            0.9504132231404959,
            0.8181818181818182,
            0.9256198347107438,
            0.8760330578512396,
            0.9669421487603306,
            0.859504132231405,
            0.8760330578512396,
            0.8842975206611571,
            0.9090909090909091,
            0.9752066115702479,
            0.9669421487603306,
            0.9173553719008265,
            0.9834710743801653,
            0.8760330578512396,
            0.9834710743801653,
            0.9504132231404959,
            0.9834710743801653,
            0.9917355371900827,
            1.0,
            0.9834710743801653,
            0.9421487603305785
        ],
        "reward_history": [
            -1210,
            -1210,
            -688,
            -532,
            -123,
            -297,
            -115,
            -1210,
            -1210,
            -1210,
            -198,
            -1210,
            -996,
            -738,
            -667,
            -337,
            -214,
            -522,
            -208,
            -968,
            -366,
            -316,
            -276,
            -246,
            -91,
            -218,
            -983
        ],
        "steps_history": [
            1210,
            1210,
            789,
            633,
            224,
            398,
            216,
            1210,
            1210,
            1210,
            299,
            1210,
            1097,
            839,
            768,
            438,
            315,
            623,
            309,
            1069,
            467,
            417,
            377,
            347,
            192,
            319,
            1084
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "199/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.251083850860596,
        "final_policy_stability": 0.9669421487603306,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.7355371900826446,
            0.8429752066115702,
            0.8677685950413223,
            0.9173553719008265,
            0.9504132231404959,
            0.9173553719008265,
            0.8842975206611571,
            0.9421487603305785,
            0.8677685950413223,
            0.8347107438016529,
            0.9752066115702479,
            0.8429752066115702,
            0.8677685950413223,
            0.8677685950413223,
            0.9669421487603306,
            0.9834710743801653,
            0.9669421487603306,
            0.9752066115702479,
            0.9256198347107438,
            0.9586776859504132,
            0.8925619834710744,
            0.9917355371900827,
            1.0,
            0.9669421487603306
        ],
        "reward_history": [
            -155,
            -1096,
            -1210,
            -703,
            -460,
            -126,
            -10,
            -225,
            -140,
            -1210,
            -1210,
            -133,
            -974,
            -793,
            -1210,
            -282,
            -90,
            -370,
            -230,
            -539,
            -254,
            -1210,
            -316,
            -60,
            -192
        ],
        "steps_history": [
            256,
            1197,
            1210,
            804,
            561,
            227,
            111,
            326,
            241,
            1210,
            1210,
            234,
            1075,
            894,
            1210,
            383,
            191,
            471,
            331,
            640,
            355,
            1210,
            417,
            161,
            293
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "200/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.243988275527954,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.7603305785123967,
            0.768595041322314,
            0.8264462809917356,
            0.9256198347107438,
            0.9173553719008265,
            0.8512396694214877,
            0.9090909090909091,
            0.8181818181818182,
            0.9421487603305785,
            0.9586776859504132,
            0.9338842975206612,
            0.9338842975206612,
            0.8429752066115702,
            0.8677685950413223,
            0.9834710743801653,
            0.9090909090909091,
            0.9008264462809917,
            1.0,
            0.9173553719008265,
            0.9338842975206612,
            0.9917355371900827,
            0.9917355371900827,
            0.9917355371900827
        ],
        "reward_history": [
            -310,
            -1210,
            -1210,
            -1210,
            -309,
            -237,
            -1210,
            -198,
            -1210,
            -73,
            -190,
            -133,
            -465,
            -909,
            -631,
            -20,
            -600,
            -1210,
            -4,
            -612,
            -686,
            -129,
            -72,
            -282
        ],
        "steps_history": [
            411,
            1210,
            1210,
            1210,
            410,
            338,
            1210,
            299,
            1210,
            174,
            291,
            234,
            566,
            1010,
            732,
            121,
            701,
            1210,
            105,
            713,
            787,
            230,
            173,
            383
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "201/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.8681321144104,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.8016528925619835,
            0.7851239669421488,
            0.8264462809917356,
            0.8016528925619835,
            0.9090909090909091,
            0.9008264462809917,
            0.9421487603305785,
            0.9008264462809917,
            0.8842975206611571,
            0.8347107438016529,
            0.8925619834710744,
            0.9669421487603306,
            0.9586776859504132,
            0.8429752066115702,
            0.8842975206611571,
            0.9834710743801653,
            0.9338842975206612,
            0.9586776859504132,
            0.9256198347107438,
            0.9090909090909091,
            0.9669421487603306,
            0.9917355371900827,
            0.8512396694214877,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -545,
            -1210,
            -1210,
            -1210,
            -534,
            -166,
            -342,
            -47,
            -66,
            -605,
            -739,
            -596,
            -65,
            -452,
            -827,
            -438,
            -156,
            -219,
            -225,
            -285,
            -623,
            -180,
            -21,
            -1024,
            -199,
            -21,
            -46,
            18
        ],
        "steps_history": [
            646,
            1210,
            1210,
            1210,
            635,
            267,
            443,
            148,
            167,
            706,
            840,
            697,
            166,
            553,
            928,
            539,
            257,
            320,
            326,
            386,
            724,
            281,
            122,
            1125,
            300,
            122,
            147,
            83
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "202/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.434917688369751,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.859504132231405,
            0.7603305785123967,
            0.8760330578512396,
            0.8760330578512396,
            0.8925619834710744,
            0.9008264462809917,
            0.8264462809917356,
            0.9586776859504132,
            0.8512396694214877,
            0.8429752066115702,
            0.9421487603305785,
            0.9338842975206612,
            0.8925619834710744,
            0.9669421487603306,
            0.9421487603305785,
            0.9586776859504132,
            0.9504132231404959,
            0.9669421487603306,
            0.9586776859504132,
            0.8347107438016529,
            0.9421487603305785,
            0.9338842975206612,
            0.9008264462809917,
            0.9834710743801653,
            0.8512396694214877,
            1.0,
            0.9917355371900827,
            0.9586776859504132,
            0.859504132231405,
            0.9917355371900827,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -1210,
            -1210,
            -341,
            -1210,
            -427,
            -347,
            -1025,
            -87,
            -984,
            -1210,
            -108,
            -138,
            -460,
            18,
            -170,
            -70,
            -103,
            33,
            -57,
            -1210,
            -370,
            -204,
            -1210,
            -41,
            -1038,
            26,
            -271,
            -84,
            -1210,
            -219,
            -316,
            -80,
            -210
        ],
        "steps_history": [
            1210,
            1210,
            1210,
            442,
            1210,
            528,
            448,
            1126,
            188,
            1085,
            1210,
            209,
            239,
            561,
            83,
            271,
            171,
            204,
            68,
            158,
            1210,
            471,
            305,
            1210,
            142,
            1139,
            75,
            372,
            185,
            1210,
            320,
            417,
            181,
            311
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "203/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.080922603607178,
        "final_policy_stability": 0.9173553719008265,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.7768595041322314,
            0.9256198347107438,
            0.9090909090909091,
            0.8016528925619835,
            0.8842975206611571,
            0.8512396694214877,
            0.8925619834710744,
            0.8264462809917356,
            0.859504132231405,
            0.8677685950413223,
            0.8842975206611571,
            0.8760330578512396,
            0.9256198347107438,
            0.9752066115702479,
            0.9669421487603306,
            0.9752066115702479,
            0.9917355371900827,
            0.9917355371900827,
            0.9008264462809917,
            1.0,
            0.9917355371900827,
            0.9173553719008265
        ],
        "reward_history": [
            -1210,
            -1098,
            -120,
            -429,
            -620,
            -584,
            -772,
            -582,
            -813,
            -1210,
            -1210,
            -1210,
            -988,
            -444,
            -85,
            -253,
            -126,
            -2,
            -32,
            -349,
            -13,
            -76,
            -974
        ],
        "steps_history": [
            1210,
            1199,
            221,
            530,
            721,
            685,
            873,
            683,
            914,
            1210,
            1210,
            1210,
            1089,
            545,
            186,
            354,
            227,
            103,
            133,
            450,
            114,
            177,
            1075
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "204/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.9241721630096436,
        "final_policy_stability": 0.9834710743801653,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.7768595041322314,
            0.8760330578512396,
            0.8181818181818182,
            0.7851239669421488,
            0.9090909090909091,
            0.8512396694214877,
            0.9256198347107438,
            0.9504132231404959,
            0.859504132231405,
            0.9090909090909091,
            0.7851239669421488,
            0.8429752066115702,
            0.9586776859504132,
            0.9256198347107438,
            0.9752066115702479,
            0.9834710743801653,
            0.859504132231405,
            0.9586776859504132,
            0.9917355371900827,
            0.9834710743801653,
            0.9752066115702479,
            0.9752066115702479,
            0.9090909090909091,
            0.9834710743801653,
            0.9504132231404959,
            0.9669421487603306,
            1.0,
            1.0,
            0.9834710743801653
        ],
        "reward_history": [
            -331,
            -1210,
            -676,
            -1210,
            -956,
            -198,
            -1210,
            -103,
            -165,
            -1210,
            -345,
            -1210,
            -541,
            -276,
            -341,
            -64,
            -151,
            -927,
            -292,
            -36,
            -161,
            -231,
            -233,
            -1210,
            -65,
            -248,
            -276,
            -203,
            -10,
            -203
        ],
        "steps_history": [
            432,
            1210,
            777,
            1210,
            1057,
            299,
            1210,
            204,
            266,
            1210,
            446,
            1210,
            642,
            377,
            442,
            165,
            252,
            1028,
            393,
            137,
            262,
            332,
            334,
            1210,
            166,
            349,
            377,
            304,
            111,
            304
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "205/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.0927629470825195,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.9008264462809917,
            0.768595041322314,
            0.9090909090909091,
            0.768595041322314,
            0.9256198347107438,
            0.9173553719008265,
            0.9504132231404959,
            0.9256198347107438,
            0.9008264462809917,
            0.9504132231404959,
            0.9090909090909091,
            0.9338842975206612,
            0.8842975206611571,
            0.8016528925619835,
            0.9338842975206612,
            0.8925619834710744,
            0.9834710743801653,
            0.8347107438016529,
            0.9669421487603306,
            0.9834710743801653,
            0.9173553719008265,
            1.0,
            1.0,
            0.9917355371900827
        ],
        "reward_history": [
            -1210,
            -303,
            -1052,
            -356,
            -1029,
            -55,
            -96,
            -191,
            -203,
            -1210,
            -68,
            -564,
            -281,
            -1210,
            -861,
            -204,
            -952,
            -52,
            -1210,
            -130,
            -113,
            -455,
            -80,
            -34,
            -38
        ],
        "steps_history": [
            1210,
            404,
            1153,
            457,
            1130,
            156,
            197,
            292,
            304,
            1210,
            169,
            665,
            382,
            1210,
            962,
            305,
            1053,
            153,
            1210,
            231,
            214,
            556,
            181,
            135,
            139
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "206/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.666499853134155,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.7768595041322314,
            0.8181818181818182,
            0.8925619834710744,
            0.7851239669421488,
            0.9256198347107438,
            0.8181818181818182,
            0.9090909090909091,
            0.9338842975206612,
            0.9421487603305785,
            0.9586776859504132,
            0.859504132231405,
            0.9669421487603306,
            0.9586776859504132,
            0.9504132231404959,
            0.9173553719008265,
            0.9421487603305785,
            0.9173553719008265,
            0.9586776859504132,
            0.9834710743801653,
            0.8842975206611571,
            0.9752066115702479,
            0.9504132231404959,
            0.9917355371900827,
            0.9669421487603306,
            0.9834710743801653,
            0.9504132231404959,
            0.9752066115702479,
            0.9917355371900827,
            0.9834710743801653,
            1.0,
            0.8842975206611571,
            0.9917355371900827,
            0.9917355371900827
        ],
        "reward_history": [
            -612,
            -1210,
            -1100,
            -168,
            -1210,
            -28,
            -1210,
            -429,
            -85,
            -69,
            -95,
            -1210,
            -134,
            -65,
            -304,
            -454,
            -48,
            -500,
            -78,
            -72,
            -266,
            -141,
            -121,
            23,
            -18,
            -43,
            -310,
            -67,
            -99,
            -114,
            -154,
            -1210,
            -131,
            -171
        ],
        "steps_history": [
            713,
            1210,
            1201,
            269,
            1210,
            129,
            1210,
            530,
            186,
            170,
            196,
            1210,
            235,
            166,
            405,
            555,
            149,
            601,
            179,
            173,
            367,
            242,
            222,
            78,
            119,
            144,
            411,
            168,
            200,
            215,
            255,
            1210,
            232,
            272
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "207/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.596434593200684,
        "final_policy_stability": 0.9752066115702479,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.8842975206611571,
            0.8264462809917356,
            0.8512396694214877,
            0.8677685950413223,
            0.9173553719008265,
            0.8760330578512396,
            0.9008264462809917,
            0.9586776859504132,
            0.8925619834710744,
            0.8760330578512396,
            0.8429752066115702,
            0.9504132231404959,
            0.9256198347107438,
            0.8512396694214877,
            0.9669421487603306,
            0.9586776859504132,
            0.9008264462809917,
            0.9834710743801653,
            0.8842975206611571,
            0.9917355371900827,
            0.9834710743801653,
            0.9173553719008265,
            0.9834710743801653,
            1.0,
            0.9338842975206612,
            0.8925619834710744,
            1.0,
            0.9834710743801653,
            1.0,
            0.9752066115702479
        ],
        "reward_history": [
            -609,
            -1210,
            -163,
            -1210,
            -610,
            -1210,
            -68,
            -260,
            -209,
            -122,
            -263,
            -1210,
            -692,
            -54,
            -307,
            -823,
            -47,
            -68,
            -295,
            18,
            -589,
            -55,
            -48,
            -372,
            -64,
            -1,
            -549,
            -1210,
            18,
            -462,
            -21,
            -467
        ],
        "steps_history": [
            710,
            1210,
            264,
            1210,
            711,
            1210,
            169,
            361,
            310,
            223,
            364,
            1210,
            793,
            155,
            408,
            924,
            148,
            169,
            396,
            83,
            690,
            156,
            149,
            473,
            165,
            102,
            650,
            1210,
            83,
            563,
            122,
            568
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "208/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.016805410385132,
        "final_policy_stability": 0.8842975206611571,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.9090909090909091,
            0.8512396694214877,
            0.8925619834710744,
            0.8677685950413223,
            0.8264462809917356,
            0.8677685950413223,
            0.859504132231405,
            0.9173553719008265,
            0.9173553719008265,
            0.9338842975206612,
            0.9173553719008265,
            0.9008264462809917,
            0.9834710743801653,
            0.8760330578512396,
            0.9008264462809917,
            0.9008264462809917,
            0.9834710743801653,
            0.9504132231404959,
            0.9338842975206612,
            0.9586776859504132,
            0.9669421487603306,
            0.9752066115702479,
            1.0,
            1.0,
            0.9669421487603306,
            0.8842975206611571
        ],
        "reward_history": [
            -1106,
            -289,
            -1210,
            -258,
            -507,
            -739,
            -1210,
            -427,
            -267,
            -275,
            -195,
            -479,
            -863,
            -138,
            -498,
            -1210,
            -408,
            -49,
            -83,
            -280,
            -64,
            -9,
            -21,
            6,
            -40,
            -31,
            -762
        ],
        "steps_history": [
            1207,
            390,
            1210,
            359,
            608,
            840,
            1210,
            528,
            368,
            376,
            296,
            580,
            964,
            239,
            599,
            1210,
            509,
            150,
            184,
            381,
            165,
            110,
            122,
            95,
            141,
            132,
            863
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "209/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.685848951339722,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8760330578512396,
            0.8842975206611571,
            0.8677685950413223,
            0.8099173553719008,
            0.8264462809917356,
            0.9173553719008265,
            0.9834710743801653,
            0.8760330578512396,
            0.9090909090909091,
            0.8842975206611571,
            0.9586776859504132,
            0.9338842975206612,
            0.8512396694214877,
            0.9421487603305785,
            0.9338842975206612,
            0.9669421487603306,
            0.8347107438016529,
            0.9504132231404959,
            0.8925619834710744,
            0.9504132231404959,
            0.9752066115702479,
            0.9586776859504132,
            0.9504132231404959,
            0.9917355371900827,
            1.0,
            1.0,
            1.0,
            1.0,
            0.9917355371900827,
            1.0,
            0.9917355371900827,
            0.9256198347107438,
            0.9917355371900827
        ],
        "reward_history": [
            -92,
            -1210,
            -715,
            -1210,
            -554,
            -552,
            -187,
            29,
            -1210,
            -109,
            -672,
            -58,
            -114,
            -565,
            -92,
            -48,
            -66,
            -844,
            -228,
            -1210,
            -232,
            -283,
            -259,
            -262,
            -61,
            -53,
            -137,
            -241,
            -117,
            -2,
            -203,
            -108,
            -457,
            -227
        ],
        "steps_history": [
            193,
            1210,
            816,
            1210,
            655,
            653,
            288,
            72,
            1210,
            210,
            773,
            159,
            215,
            666,
            193,
            149,
            167,
            945,
            329,
            1210,
            333,
            384,
            360,
            363,
            162,
            154,
            238,
            342,
            218,
            103,
            304,
            209,
            558,
            328
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "210/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.2_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.396226167678833,
        "final_policy_stability": 0.9256198347107438,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8842975206611571,
            0.8677685950413223,
            0.9173553719008265,
            0.8429752066115702,
            0.9586776859504132,
            0.9256198347107438,
            0.8760330578512396,
            0.9586776859504132,
            0.9421487603305785,
            0.8842975206611571,
            0.9752066115702479,
            0.9421487603305785,
            0.8512396694214877,
            0.9917355371900827,
            0.9586776859504132,
            0.9917355371900827,
            0.8677685950413223,
            0.9834710743801653,
            0.9256198347107438
        ],
        "reward_history": [
            -1210,
            -280,
            -713,
            -224,
            -1210,
            -89,
            -525,
            -1210,
            -119,
            -820,
            -424,
            -120,
            -321,
            -1210,
            -122,
            -195,
            -244,
            -1210,
            -303,
            -543
        ],
        "steps_history": [
            1210,
            381,
            814,
            325,
            1210,
            190,
            626,
            1210,
            220,
            921,
            525,
            221,
            422,
            1210,
            223,
            296,
            345,
            1210,
            404,
            644
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "211/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.3546764850616455,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8264462809917356,
            0.8429752066115702,
            0.8016528925619835,
            0.9008264462809917,
            0.9090909090909091,
            0.8512396694214877,
            0.9504132231404959,
            0.9504132231404959,
            0.859504132231405,
            0.8760330578512396,
            0.9173553719008265,
            0.9586776859504132,
            0.9834710743801653,
            0.9752066115702479,
            0.9504132231404959,
            0.9917355371900827,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -901,
            -1210,
            -736,
            -244,
            -431,
            -1210,
            -310,
            -398,
            -1210,
            -1210,
            -1210,
            -388,
            -434,
            -201,
            -425,
            -147,
            -246,
            -63
        ],
        "steps_history": [
            1210,
            1002,
            1210,
            837,
            345,
            532,
            1210,
            411,
            499,
            1210,
            1210,
            1210,
            489,
            535,
            302,
            526,
            248,
            347,
            164
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "212/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4535648822784424,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.8099173553719008,
            0.8760330578512396,
            0.9008264462809917,
            0.8512396694214877,
            0.9421487603305785,
            0.8677685950413223,
            0.9173553719008265,
            0.8760330578512396,
            0.8429752066115702,
            0.9586776859504132,
            0.8925619834710744,
            1.0,
            0.8842975206611571,
            0.9834710743801653,
            0.9256198347107438,
            1.0,
            0.9173553719008265,
            1.0
        ],
        "reward_history": [
            -231,
            -1210,
            -818,
            -1210,
            -345,
            -947,
            -178,
            -390,
            -446,
            -1210,
            -1210,
            -302,
            -364,
            -177,
            -733,
            -167,
            -324,
            -63,
            -1210,
            -356
        ],
        "steps_history": [
            332,
            1210,
            919,
            1210,
            446,
            1048,
            279,
            491,
            547,
            1210,
            1210,
            403,
            465,
            278,
            834,
            268,
            425,
            164,
            1210,
            457
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "213/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.156747579574585,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.8264462809917356,
            0.8347107438016529,
            0.8760330578512396,
            0.8677685950413223,
            0.8842975206611571,
            0.9338842975206612,
            0.9917355371900827,
            0.8347107438016529,
            0.9504132231404959,
            0.9834710743801653,
            0.9669421487603306,
            0.9669421487603306,
            0.9669421487603306,
            0.8677685950413223,
            0.9338842975206612,
            0.9917355371900827,
            0.9173553719008265,
            0.9834710743801653,
            0.8429752066115702,
            0.9834710743801653,
            0.9752066115702479,
            1.0
        ],
        "reward_history": [
            -1210,
            -1210,
            -816,
            -1210,
            -753,
            -1210,
            -605,
            -189,
            -1210,
            -260,
            -170,
            -519,
            -160,
            -485,
            -1210,
            -734,
            -214,
            -590,
            -140,
            -968,
            -366,
            -316,
            -276
        ],
        "steps_history": [
            1210,
            1210,
            917,
            1210,
            854,
            1210,
            706,
            290,
            1210,
            361,
            271,
            620,
            261,
            586,
            1210,
            835,
            315,
            691,
            241,
            1069,
            467,
            417,
            377
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "214/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4722931385040283,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.8347107438016529,
            0.7603305785123967,
            0.8925619834710744,
            0.8181818181818182,
            0.9090909090909091,
            0.8512396694214877,
            0.9090909090909091,
            0.9586776859504132,
            0.9338842975206612,
            0.8347107438016529,
            0.9504132231404959,
            0.859504132231405,
            0.9008264462809917,
            1.0,
            0.9834710743801653,
            1.0,
            1.0
        ],
        "reward_history": [
            -155,
            -1210,
            -1210,
            -285,
            -865,
            -237,
            -1077,
            -1210,
            -49,
            -145,
            -1210,
            -201,
            -793,
            -1210,
            -282,
            -561,
            -230,
            -124
        ],
        "steps_history": [
            256,
            1210,
            1210,
            386,
            966,
            338,
            1178,
            1210,
            150,
            246,
            1210,
            302,
            894,
            1210,
            383,
            662,
            331,
            225
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "215/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.201432943344116,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 21,
        "policy_stability_history": [
            0.0,
            0.7933884297520661,
            0.8264462809917356,
            0.9008264462809917,
            0.8347107438016529,
            0.9421487603305785,
            0.859504132231405,
            0.8760330578512396,
            0.9173553719008265,
            0.9090909090909091,
            0.9504132231404959,
            0.9834710743801653,
            0.9421487603305785,
            0.9752066115702479,
            0.9752066115702479,
            0.9834710743801653,
            0.8347107438016529,
            0.9917355371900827,
            1.0,
            0.9669421487603306,
            0.9917355371900827,
            1.0
        ],
        "reward_history": [
            -310,
            -1210,
            -1210,
            -430,
            -1210,
            -82,
            -711,
            -630,
            -866,
            -316,
            -190,
            -86,
            -68,
            -127,
            -48,
            -227,
            -1210,
            -170,
            -20,
            -262,
            -124,
            -12
        ],
        "steps_history": [
            411,
            1210,
            1210,
            531,
            1210,
            183,
            812,
            731,
            967,
            417,
            291,
            187,
            169,
            228,
            149,
            328,
            1210,
            271,
            121,
            363,
            225,
            113
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "216/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4281458854675293,
        "final_policy_stability": 0.9752066115702479,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.7851239669421488,
            0.8677685950413223,
            0.8181818181818182,
            0.859504132231405,
            0.8925619834710744,
            0.8842975206611571,
            0.9256198347107438,
            0.9090909090909091,
            0.7933884297520661,
            0.8181818181818182,
            0.9008264462809917,
            0.9421487603305785,
            0.8677685950413223,
            0.9752066115702479,
            0.9752066115702479,
            0.9173553719008265,
            0.9752066115702479
        ],
        "reward_history": [
            -545,
            -1210,
            -1210,
            -822,
            -612,
            -263,
            -229,
            -93,
            -330,
            -1210,
            -847,
            -300,
            -217,
            -1028,
            -237,
            -156,
            -219,
            -225
        ],
        "steps_history": [
            646,
            1210,
            1210,
            923,
            713,
            364,
            330,
            194,
            431,
            1210,
            948,
            401,
            318,
            1129,
            338,
            257,
            320,
            326
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "217/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.912988901138306,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.859504132231405,
            0.8842975206611571,
            0.8760330578512396,
            0.859504132231405,
            0.859504132231405,
            0.8760330578512396,
            0.9421487603305785,
            0.9504132231404959,
            0.9834710743801653,
            0.9917355371900827,
            0.9504132231404959,
            0.9504132231404959,
            0.9669421487603306,
            0.9586776859504132,
            0.9504132231404959,
            0.9008264462809917,
            0.9834710743801653,
            0.9752066115702479,
            0.8512396694214877,
            0.9504132231404959,
            0.9917355371900827,
            0.9917355371900827,
            0.8264462809917356,
            1.0,
            0.9917355371900827,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -804,
            -649,
            -1210,
            -1210,
            -872,
            -1210,
            -70,
            -297,
            -78,
            14,
            -253,
            -272,
            -92,
            -156,
            -57,
            -337,
            -34,
            -3,
            -985,
            -103,
            33,
            -57,
            -483,
            -51,
            -50,
            -58,
            -534
        ],
        "steps_history": [
            1210,
            905,
            750,
            1210,
            1210,
            973,
            1210,
            171,
            398,
            179,
            87,
            354,
            373,
            193,
            257,
            158,
            438,
            135,
            104,
            1086,
            204,
            68,
            158,
            584,
            152,
            151,
            159,
            635
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "218/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4715893268585205,
        "final_policy_stability": 0.9090909090909091,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.859504132231405,
            0.8677685950413223,
            0.9008264462809917,
            0.8099173553719008,
            0.8925619834710744,
            0.9256198347107438,
            0.8512396694214877,
            0.9586776859504132,
            0.9090909090909091,
            0.9338842975206612,
            0.8842975206611571,
            0.8677685950413223,
            0.9586776859504132,
            0.8842975206611571,
            1.0,
            0.9834710743801653,
            0.9834710743801653,
            0.9090909090909091
        ],
        "reward_history": [
            -1210,
            -360,
            -188,
            -318,
            -1210,
            -220,
            -81,
            -1210,
            -36,
            -1210,
            -676,
            -465,
            -1210,
            -123,
            -430,
            -87,
            -160,
            -306,
            -510
        ],
        "steps_history": [
            1210,
            461,
            289,
            419,
            1210,
            321,
            182,
            1210,
            137,
            1210,
            777,
            566,
            1210,
            224,
            531,
            188,
            261,
            407,
            611
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "219/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4678430557250977,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.7603305785123967,
            0.8099173553719008,
            0.8512396694214877,
            0.9090909090909091,
            0.8925619834710744,
            0.9338842975206612,
            0.8347107438016529,
            0.8842975206611571,
            0.9917355371900827,
            0.9834710743801653,
            0.9752066115702479,
            0.9256198347107438,
            0.9338842975206612,
            0.9256198347107438,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -331,
            -1210,
            -884,
            -1210,
            -748,
            -1210,
            -402,
            -694,
            -1210,
            -100,
            -32,
            -130,
            -542,
            -423,
            -261,
            -8,
            43,
            -226,
            -64
        ],
        "steps_history": [
            432,
            1210,
            985,
            1210,
            849,
            1210,
            503,
            795,
            1210,
            201,
            133,
            231,
            643,
            524,
            362,
            109,
            58,
            327,
            165
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "220/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.125311851501465,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.743801652892562,
            0.7933884297520661,
            0.8016528925619835,
            0.8016528925619835,
            0.9173553719008265,
            0.9256198347107438,
            0.8760330578512396,
            0.9338842975206612,
            0.9752066115702479,
            0.9752066115702479,
            0.9586776859504132,
            0.9173553719008265,
            0.9669421487603306,
            0.8429752066115702,
            0.9834710743801653,
            0.9834710743801653,
            0.9586776859504132,
            0.9669421487603306,
            0.9917355371900827,
            0.9834710743801653,
            1.0,
            0.9834710743801653,
            0.9752066115702479,
            1.0
        ],
        "reward_history": [
            -462,
            -942,
            -1210,
            -1210,
            -1210,
            -114,
            -181,
            -434,
            -313,
            -229,
            -234,
            -149,
            -406,
            -53,
            -1210,
            -234,
            -116,
            -85,
            -55,
            -58,
            -86,
            -5,
            -159,
            -84,
            -52
        ],
        "steps_history": [
            563,
            1043,
            1210,
            1210,
            1210,
            215,
            282,
            535,
            414,
            330,
            335,
            250,
            507,
            154,
            1210,
            335,
            217,
            186,
            156,
            159,
            187,
            106,
            260,
            185,
            153
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "221/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.14760160446167,
        "final_policy_stability": 0.9421487603305785,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.8181818181818182,
            0.8760330578512396,
            0.8099173553719008,
            0.9008264462809917,
            0.9256198347107438,
            0.9256198347107438,
            0.9338842975206612,
            0.8677685950413223,
            0.9338842975206612,
            0.9008264462809917,
            0.8677685950413223,
            0.9752066115702479,
            0.8099173553719008,
            0.9504132231404959,
            0.9834710743801653,
            0.9669421487603306,
            0.9338842975206612,
            0.9504132231404959,
            0.9338842975206612,
            0.9834710743801653,
            0.9834710743801653,
            0.9421487603305785
        ],
        "reward_history": [
            -612,
            -558,
            -1210,
            -841,
            -148,
            -143,
            -207,
            -234,
            -1210,
            -222,
            -364,
            -394,
            -124,
            -869,
            -150,
            20,
            -173,
            -198,
            -68,
            -447,
            -20,
            -98,
            -160
        ],
        "steps_history": [
            713,
            659,
            1210,
            942,
            249,
            244,
            308,
            335,
            1210,
            323,
            465,
            495,
            225,
            970,
            251,
            81,
            274,
            299,
            169,
            548,
            121,
            199,
            261
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "222/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.977001428604126,
        "final_policy_stability": 0.9834710743801653,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8016528925619835,
            0.8760330578512396,
            0.8760330578512396,
            0.8264462809917356,
            0.8925619834710744,
            0.8429752066115702,
            0.9586776859504132,
            0.9090909090909091,
            0.9504132231404959,
            0.9256198347107438,
            0.9504132231404959,
            0.9504132231404959,
            0.9256198347107438,
            0.8760330578512396,
            0.9669421487603306,
            0.8264462809917356,
            0.9834710743801653,
            0.9338842975206612,
            0.9669421487603306,
            0.9917355371900827,
            0.9669421487603306,
            0.9917355371900827,
            1.0,
            1.0,
            0.9834710743801653,
            0.9834710743801653
        ],
        "reward_history": [
            -609,
            -1210,
            -164,
            -327,
            -1210,
            -181,
            -1210,
            -68,
            -158,
            -1,
            -209,
            -122,
            -46,
            -242,
            -1210,
            41,
            -473,
            16,
            -381,
            -158,
            20,
            -216,
            -100,
            -36,
            -5,
            -68,
            -295
        ],
        "steps_history": [
            710,
            1210,
            265,
            428,
            1210,
            282,
            1210,
            169,
            259,
            102,
            310,
            223,
            147,
            343,
            1210,
            60,
            574,
            85,
            482,
            259,
            81,
            317,
            201,
            137,
            106,
            169,
            396
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "223/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.304509878158569,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8347107438016529,
            0.768595041322314,
            0.768595041322314,
            0.9504132231404959,
            0.8429752066115702,
            0.9586776859504132,
            0.9338842975206612,
            0.9586776859504132,
            0.8677685950413223,
            0.9586776859504132,
            0.9421487603305785,
            0.8760330578512396,
            0.9504132231404959,
            0.9008264462809917,
            0.9752066115702479,
            0.9256198347107438,
            0.9090909090909091,
            0.9752066115702479,
            0.9008264462809917,
            0.9917355371900827,
            0.9752066115702479,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -708,
            -1210,
            -1046,
            -507,
            -47,
            -622,
            -79,
            -303,
            -112,
            -1210,
            -196,
            -216,
            -605,
            3,
            -1210,
            -107,
            -185,
            -963,
            -75,
            -378,
            -216,
            -297,
            -64,
            -9,
            -21
        ],
        "steps_history": [
            809,
            1210,
            1147,
            608,
            148,
            723,
            180,
            404,
            213,
            1210,
            297,
            317,
            706,
            98,
            1210,
            208,
            286,
            1064,
            176,
            479,
            317,
            398,
            165,
            110,
            122
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "224/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.132151126861572,
        "final_policy_stability": 0.9752066115702479,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.7933884297520661,
            0.8512396694214877,
            0.9008264462809917,
            0.9256198347107438,
            0.7603305785123967,
            0.8099173553719008,
            0.8347107438016529,
            0.8347107438016529,
            0.8677685950413223,
            0.9504132231404959,
            0.9917355371900827,
            0.8760330578512396,
            0.9090909090909091,
            0.9586776859504132,
            0.9338842975206612,
            0.9256198347107438,
            0.9917355371900827,
            1.0,
            0.9917355371900827,
            1.0,
            1.0,
            1.0,
            0.9752066115702479
        ],
        "reward_history": [
            -92,
            -1210,
            -534,
            -79,
            -110,
            -1210,
            -1210,
            -1210,
            -1039,
            -1030,
            -241,
            -32,
            -652,
            -145,
            -40,
            -231,
            -222,
            15,
            43,
            -74,
            -47,
            5,
            23,
            -152
        ],
        "steps_history": [
            193,
            1210,
            635,
            180,
            211,
            1210,
            1210,
            1210,
            1140,
            1131,
            342,
            133,
            753,
            246,
            141,
            332,
            323,
            86,
            58,
            175,
            148,
            96,
            78,
            253
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "225/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.449209451675415,
        "final_policy_stability": 0.9586776859504132,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8842975206611571,
            0.8677685950413223,
            0.9173553719008265,
            0.8512396694214877,
            0.9586776859504132,
            0.9256198347107438,
            0.8760330578512396,
            0.9586776859504132,
            0.9421487603305785,
            0.8842975206611571,
            0.9752066115702479,
            0.9421487603305785,
            0.859504132231405,
            0.9834710743801653,
            0.9504132231404959,
            1.0,
            0.8677685950413223,
            0.9752066115702479,
            0.9586776859504132
        ],
        "reward_history": [
            -1210,
            -280,
            -713,
            -224,
            -1210,
            -89,
            -525,
            -1210,
            -119,
            -820,
            -424,
            -120,
            -321,
            -1210,
            -122,
            -195,
            -244,
            -1210,
            -303,
            -543
        ],
        "steps_history": [
            1210,
            381,
            814,
            325,
            1210,
            190,
            626,
            1210,
            220,
            921,
            525,
            221,
            422,
            1210,
            223,
            296,
            345,
            1210,
            404,
            644
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "226/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.615736484527588,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8264462809917356,
            0.8429752066115702,
            0.8016528925619835,
            0.9008264462809917,
            0.9090909090909091,
            0.8512396694214877,
            0.9504132231404959,
            0.9504132231404959,
            0.859504132231405,
            0.8760330578512396,
            0.9173553719008265,
            0.9586776859504132,
            0.9834710743801653,
            0.9752066115702479,
            0.9504132231404959,
            0.9917355371900827,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -901,
            -1210,
            -736,
            -244,
            -431,
            -1210,
            -310,
            -398,
            -1210,
            -1210,
            -1210,
            -388,
            -434,
            -201,
            -425,
            -147,
            -246,
            -63
        ],
        "steps_history": [
            1210,
            1002,
            1210,
            837,
            345,
            532,
            1210,
            411,
            499,
            1210,
            1210,
            1210,
            489,
            535,
            302,
            526,
            248,
            347,
            164
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "227/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.516057014465332,
        "final_policy_stability": 0.9752066115702479,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.8099173553719008,
            0.8760330578512396,
            0.9090909090909091,
            0.8429752066115702,
            0.9421487603305785,
            0.8677685950413223,
            0.9173553719008265,
            0.8677685950413223,
            0.8181818181818182,
            0.9008264462809917,
            0.9338842975206612,
            0.9090909090909091,
            0.9752066115702479,
            0.9421487603305785,
            0.9834710743801653,
            0.9421487603305785,
            0.8925619834710744,
            0.9752066115702479
        ],
        "reward_history": [
            -231,
            -1210,
            -818,
            -1210,
            -345,
            -947,
            -178,
            -390,
            -446,
            -1210,
            -946,
            -1210,
            -267,
            -428,
            -203,
            -324,
            -63,
            -562,
            -903,
            -566
        ],
        "steps_history": [
            332,
            1210,
            919,
            1210,
            446,
            1048,
            279,
            491,
            547,
            1210,
            1047,
            1210,
            368,
            529,
            304,
            425,
            164,
            663,
            1004,
            667
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "228/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.478727340698242,
        "final_policy_stability": 0.9834710743801653,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8264462809917356,
            0.8347107438016529,
            0.8760330578512396,
            0.8677685950413223,
            0.8842975206611571,
            0.9090909090909091,
            0.859504132231405,
            0.8347107438016529,
            0.9586776859504132,
            0.9256198347107438,
            0.8677685950413223,
            0.8925619834710744,
            0.9752066115702479,
            0.9917355371900827,
            1.0,
            0.9917355371900827,
            1.0,
            0.9834710743801653
        ],
        "reward_history": [
            -1210,
            -1210,
            -816,
            -1210,
            -753,
            -1210,
            -247,
            -1210,
            -1210,
            -158,
            -330,
            -746,
            -1210,
            -296,
            -112,
            -124,
            -214,
            -13,
            -264
        ],
        "steps_history": [
            1210,
            1210,
            917,
            1210,
            854,
            1210,
            348,
            1210,
            1210,
            259,
            431,
            847,
            1210,
            397,
            213,
            225,
            315,
            114,
            365
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "229/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.3735218048095703,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8347107438016529,
            0.7603305785123967,
            0.8925619834710744,
            0.8181818181818182,
            0.9090909090909091,
            0.859504132231405,
            0.9090909090909091,
            0.9752066115702479,
            0.9338842975206612,
            0.859504132231405,
            0.9421487603305785,
            0.8925619834710744,
            0.9834710743801653,
            0.8512396694214877,
            0.9917355371900827,
            0.9752066115702479,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -155,
            -1210,
            -1210,
            -285,
            -865,
            -237,
            -1077,
            -1210,
            -49,
            -145,
            -1210,
            -201,
            -644,
            12,
            -1210,
            -342,
            -561,
            -230,
            -124,
            -314
        ],
        "steps_history": [
            256,
            1210,
            1210,
            386,
            966,
            338,
            1178,
            1210,
            150,
            246,
            1210,
            302,
            745,
            89,
            1210,
            443,
            662,
            331,
            225,
            415
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "230/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.355318069458008,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 21,
        "policy_stability_history": [
            0.0,
            0.7933884297520661,
            0.8264462809917356,
            0.9008264462809917,
            0.8347107438016529,
            0.9421487603305785,
            0.859504132231405,
            0.8760330578512396,
            0.9173553719008265,
            0.9090909090909091,
            0.9504132231404959,
            0.9834710743801653,
            0.9421487603305785,
            0.9834710743801653,
            0.9834710743801653,
            0.9752066115702479,
            0.8347107438016529,
            0.9917355371900827,
            1.0,
            0.9586776859504132,
            0.9917355371900827,
            1.0
        ],
        "reward_history": [
            -310,
            -1210,
            -1210,
            -430,
            -1210,
            -82,
            -711,
            -630,
            -866,
            -316,
            -190,
            -86,
            -68,
            -127,
            -48,
            -227,
            -1210,
            -170,
            -20,
            -262,
            -124,
            -12
        ],
        "steps_history": [
            411,
            1210,
            1210,
            531,
            1210,
            183,
            812,
            731,
            967,
            417,
            291,
            187,
            169,
            228,
            149,
            328,
            1210,
            271,
            121,
            363,
            225,
            113
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "231/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.499077558517456,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 20,
        "policy_stability_history": [
            0.0,
            0.7851239669421488,
            0.8677685950413223,
            0.8677685950413223,
            0.8181818181818182,
            0.9090909090909091,
            0.9256198347107438,
            0.8512396694214877,
            0.8347107438016529,
            0.8760330578512396,
            0.9586776859504132,
            0.9669421487603306,
            0.9504132231404959,
            0.9669421487603306,
            0.9917355371900827,
            0.9338842975206612,
            0.8925619834710744,
            1.0,
            0.9586776859504132,
            1.0,
            0.9917355371900827
        ],
        "reward_history": [
            -545,
            -1210,
            -1210,
            -1210,
            -1019,
            -142,
            -183,
            -618,
            -588,
            -550,
            -96,
            -65,
            -452,
            -33,
            -229,
            -219,
            -654,
            -84,
            -219,
            -225,
            -163
        ],
        "steps_history": [
            646,
            1210,
            1210,
            1210,
            1120,
            243,
            284,
            719,
            689,
            651,
            197,
            166,
            553,
            134,
            330,
            320,
            755,
            185,
            320,
            326,
            264
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "232/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.8603675365448,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.8512396694214877,
            0.8760330578512396,
            0.8760330578512396,
            0.8512396694214877,
            0.859504132231405,
            0.8842975206611571,
            0.9421487603305785,
            0.9504132231404959,
            0.9752066115702479,
            0.9917355371900827,
            0.9586776859504132,
            0.9504132231404959,
            0.9669421487603306,
            0.9586776859504132,
            0.9504132231404959,
            0.9008264462809917,
            0.9834710743801653,
            0.9752066115702479,
            0.8512396694214877,
            0.9504132231404959,
            0.9917355371900827,
            0.9917355371900827,
            0.8347107438016529,
            1.0,
            0.9917355371900827,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -804,
            -649,
            -1210,
            -1210,
            -872,
            -1210,
            -70,
            -297,
            -78,
            14,
            -253,
            -272,
            -92,
            -156,
            -57,
            -337,
            -34,
            -3,
            -985,
            -103,
            33,
            -57,
            -483,
            -51,
            -50,
            -58,
            -534
        ],
        "steps_history": [
            1210,
            905,
            750,
            1210,
            1210,
            973,
            1210,
            171,
            398,
            179,
            87,
            354,
            373,
            193,
            257,
            158,
            438,
            135,
            104,
            1086,
            204,
            68,
            158,
            584,
            152,
            151,
            159,
            635
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "233/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.5473856925964355,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.859504132231405,
            0.8677685950413223,
            0.9008264462809917,
            0.8099173553719008,
            0.8181818181818182,
            0.8181818181818182,
            0.8181818181818182,
            0.9256198347107438,
            0.9008264462809917,
            0.9008264462809917,
            0.9504132231404959,
            0.9338842975206612,
            0.8760330578512396,
            0.9917355371900827,
            0.9917355371900827,
            0.9090909090909091,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -360,
            -188,
            -318,
            -1210,
            -1210,
            -975,
            -638,
            -711,
            -465,
            -1210,
            -123,
            -184,
            -594,
            -306,
            -285,
            -455,
            -152,
            -79
        ],
        "steps_history": [
            1210,
            461,
            289,
            419,
            1210,
            1210,
            1076,
            739,
            812,
            566,
            1210,
            224,
            285,
            695,
            407,
            386,
            556,
            253,
            180
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "234/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.2093682289123535,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 21,
        "policy_stability_history": [
            0.0,
            0.7603305785123967,
            0.7933884297520661,
            0.8925619834710744,
            0.8264462809917356,
            0.9008264462809917,
            0.9504132231404959,
            0.859504132231405,
            0.9834710743801653,
            0.9669421487603306,
            0.8181818181818182,
            0.9421487603305785,
            0.9834710743801653,
            0.8016528925619835,
            0.9586776859504132,
            0.8512396694214877,
            0.8925619834710744,
            0.9834710743801653,
            0.9917355371900827,
            0.9917355371900827,
            1.0,
            1.0
        ],
        "reward_history": [
            -331,
            -1210,
            -1210,
            -535,
            -1097,
            -1210,
            -112,
            -1022,
            -56,
            -42,
            -972,
            -145,
            -17,
            -1210,
            -270,
            -758,
            -559,
            -139,
            -89,
            -230,
            -53,
            -144
        ],
        "steps_history": [
            432,
            1210,
            1210,
            636,
            1198,
            1210,
            213,
            1123,
            157,
            143,
            1073,
            246,
            118,
            1210,
            371,
            859,
            660,
            240,
            190,
            331,
            154,
            245
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "235/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.211976766586304,
        "final_policy_stability": 0.9752066115702479,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.743801652892562,
            0.7933884297520661,
            0.8016528925619835,
            0.8099173553719008,
            0.9173553719008265,
            0.9256198347107438,
            0.8677685950413223,
            0.9338842975206612,
            0.9752066115702479,
            0.9752066115702479,
            0.9586776859504132,
            0.9256198347107438,
            0.9504132231404959,
            0.8842975206611571,
            0.9669421487603306,
            0.9917355371900827,
            0.9421487603305785,
            0.9834710743801653,
            1.0,
            0.9834710743801653,
            0.9917355371900827,
            0.9752066115702479
        ],
        "reward_history": [
            -462,
            -942,
            -1210,
            -1210,
            -1210,
            -114,
            -181,
            -434,
            -313,
            -229,
            -234,
            -149,
            -406,
            -53,
            -1210,
            -234,
            3,
            -204,
            -105,
            -8,
            -86,
            -5,
            -159
        ],
        "steps_history": [
            563,
            1043,
            1210,
            1210,
            1210,
            215,
            282,
            535,
            414,
            330,
            335,
            250,
            507,
            154,
            1210,
            335,
            98,
            305,
            206,
            109,
            187,
            106,
            260
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "236/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.100113868713379,
        "final_policy_stability": 0.9421487603305785,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.8181818181818182,
            0.8760330578512396,
            0.8099173553719008,
            0.9008264462809917,
            0.9256198347107438,
            0.9256198347107438,
            0.9338842975206612,
            0.8677685950413223,
            0.9338842975206612,
            0.9008264462809917,
            0.8677685950413223,
            0.9752066115702479,
            0.8099173553719008,
            0.9504132231404959,
            0.9834710743801653,
            0.9669421487603306,
            0.9338842975206612,
            0.9504132231404959,
            0.9338842975206612,
            0.9834710743801653,
            0.9834710743801653,
            0.9421487603305785
        ],
        "reward_history": [
            -612,
            -558,
            -1210,
            -841,
            -148,
            -143,
            -207,
            -234,
            -1210,
            -222,
            -364,
            -394,
            -124,
            -869,
            -150,
            20,
            -173,
            -198,
            -68,
            -447,
            -20,
            -98,
            -160
        ],
        "steps_history": [
            713,
            659,
            1210,
            942,
            249,
            244,
            308,
            335,
            1210,
            323,
            465,
            495,
            225,
            970,
            251,
            81,
            274,
            299,
            169,
            548,
            121,
            199,
            261
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "237/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.501813888549805,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8016528925619835,
            0.8760330578512396,
            0.8760330578512396,
            0.8264462809917356,
            0.8925619834710744,
            0.8181818181818182,
            0.9586776859504132,
            0.9173553719008265,
            0.9173553719008265,
            0.9669421487603306,
            0.9090909090909091,
            0.8760330578512396,
            0.9669421487603306,
            0.9090909090909091,
            0.9586776859504132,
            0.9586776859504132,
            0.8925619834710744,
            0.9834710743801653,
            0.9752066115702479,
            0.9917355371900827,
            0.9917355371900827,
            1.0,
            0.9917355371900827
        ],
        "reward_history": [
            -609,
            -1210,
            -164,
            -327,
            -1210,
            -181,
            -1210,
            -68,
            -208,
            -261,
            -122,
            -431,
            -1210,
            -2,
            -404,
            -71,
            -126,
            -310,
            20,
            -109,
            -207,
            -36,
            -5,
            -68
        ],
        "steps_history": [
            710,
            1210,
            265,
            428,
            1210,
            282,
            1210,
            169,
            309,
            362,
            223,
            532,
            1210,
            103,
            505,
            172,
            227,
            411,
            81,
            210,
            308,
            137,
            106,
            169
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "238/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.2148966789245605,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8347107438016529,
            0.7603305785123967,
            0.7851239669421488,
            0.9669421487603306,
            0.9173553719008265,
            0.8925619834710744,
            0.9256198347107438,
            0.8264462809917356,
            0.9586776859504132,
            0.9834710743801653,
            0.9834710743801653,
            0.9586776859504132,
            0.8677685950413223,
            0.9421487603305785,
            0.9669421487603306,
            0.8842975206611571,
            0.8760330578512396,
            0.9917355371900827,
            0.9586776859504132,
            0.9173553719008265,
            0.8512396694214877,
            0.9834710743801653,
            0.9669421487603306,
            0.9421487603305785,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -708,
            -1210,
            -1046,
            -507,
            -47,
            -208,
            -250,
            -142,
            -972,
            -6,
            28,
            -73,
            -88,
            -407,
            -68,
            -130,
            -595,
            -1210,
            -132,
            -185,
            -560,
            -957,
            -49,
            -83,
            -280,
            -64,
            -9,
            -21,
            6
        ],
        "steps_history": [
            809,
            1210,
            1147,
            608,
            148,
            309,
            351,
            243,
            1073,
            107,
            73,
            174,
            189,
            508,
            169,
            231,
            696,
            1210,
            233,
            286,
            661,
            1058,
            150,
            184,
            381,
            165,
            110,
            122,
            95
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "239/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.466078519821167,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.7603305785123967,
            0.8842975206611571,
            0.9421487603305785,
            0.8099173553719008,
            0.7851239669421488,
            0.9586776859504132,
            0.9173553719008265,
            0.8677685950413223,
            0.9338842975206612,
            0.8347107438016529,
            0.9256198347107438,
            0.9752066115702479,
            0.9586776859504132,
            0.9256198347107438,
            0.9090909090909091,
            0.9421487603305785,
            0.9917355371900827,
            0.8099173553719008,
            0.9586776859504132,
            0.9834710743801653,
            0.9586776859504132,
            0.9834710743801653,
            1.0,
            1.0,
            0.9917355371900827
        ],
        "reward_history": [
            -92,
            -1210,
            -284,
            -330,
            -327,
            -1210,
            -85,
            -95,
            -361,
            -79,
            -1210,
            -373,
            -196,
            1,
            -136,
            -364,
            -23,
            -3,
            -846,
            -195,
            -123,
            -98,
            -145,
            -40,
            -67,
            -165
        ],
        "steps_history": [
            193,
            1210,
            385,
            431,
            428,
            1210,
            186,
            196,
            462,
            180,
            1210,
            474,
            297,
            100,
            237,
            465,
            124,
            104,
            947,
            296,
            224,
            199,
            246,
            141,
            168,
            266
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "240/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.3_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.1156835556030273,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8264462809917356,
            0.9504132231404959,
            0.8760330578512396,
            0.9421487603305785,
            0.9669421487603306,
            0.8512396694214877,
            0.9421487603305785,
            0.9008264462809917,
            0.9421487603305785,
            0.9669421487603306,
            0.8677685950413223,
            1.0,
            0.9586776859504132,
            0.9917355371900827,
            1.0
        ],
        "reward_history": [
            -1210,
            -1210,
            -209,
            -1210,
            -89,
            -491,
            -1210,
            -153,
            -933,
            -311,
            -120,
            -1210,
            -66,
            -483,
            -89,
            -244
        ],
        "steps_history": [
            1210,
            1210,
            310,
            1210,
            190,
            592,
            1210,
            254,
            1034,
            412,
            221,
            1210,
            167,
            584,
            190,
            345
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "241/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.3764808177948,
        "final_policy_stability": 0.9256198347107438,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.7933884297520661,
            0.7603305785123967,
            0.8760330578512396,
            0.9752066115702479,
            0.9586776859504132,
            0.859504132231405,
            0.9173553719008265,
            0.8429752066115702,
            0.9008264462809917,
            0.9173553719008265,
            0.9917355371900827,
            0.9752066115702479,
            0.9421487603305785,
            0.9917355371900827,
            1.0,
            1.0,
            0.9256198347107438
        ],
        "reward_history": [
            -1210,
            -901,
            -1038,
            -873,
            -178,
            -321,
            -1210,
            -277,
            -1210,
            -1210,
            -424,
            -168,
            -213,
            -671,
            -360,
            -434,
            -201,
            -425
        ],
        "steps_history": [
            1210,
            1002,
            1139,
            974,
            279,
            422,
            1210,
            378,
            1210,
            1210,
            525,
            269,
            314,
            772,
            461,
            535,
            302,
            526
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "242/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.469574451446533,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 16,
        "policy_stability_history": [
            0.0,
            0.8181818181818182,
            0.8264462809917356,
            0.8925619834710744,
            0.8512396694214877,
            0.8925619834710744,
            0.9421487603305785,
            0.859504132231405,
            0.9421487603305785,
            0.8760330578512396,
            0.9173553719008265,
            0.8925619834710744,
            0.9669421487603306,
            0.9256198347107438,
            0.9586776859504132,
            1.0,
            1.0
        ],
        "reward_history": [
            -231,
            -1210,
            -1210,
            -296,
            -867,
            -1210,
            -217,
            -736,
            -418,
            -1210,
            -427,
            -1210,
            -186,
            -459,
            -253,
            -65,
            -158
        ],
        "steps_history": [
            332,
            1210,
            1210,
            397,
            968,
            1210,
            318,
            837,
            519,
            1210,
            528,
            1210,
            287,
            560,
            354,
            166,
            259
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "243/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7083804607391357,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8264462809917356,
            0.8677685950413223,
            0.8512396694214877,
            0.8429752066115702,
            0.9421487603305785,
            0.9008264462809917,
            0.7933884297520661,
            0.8925619834710744,
            0.9752066115702479,
            0.9090909090909091,
            0.9586776859504132,
            0.9752066115702479,
            0.9917355371900827,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -1210,
            -816,
            -1210,
            -1210,
            -148,
            -264,
            -845,
            -490,
            -41,
            -458,
            -154,
            -213,
            -112,
            -146,
            -330
        ],
        "steps_history": [
            1210,
            1210,
            917,
            1210,
            1210,
            249,
            365,
            946,
            591,
            142,
            559,
            255,
            314,
            213,
            247,
            431
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "244/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.690300464630127,
        "final_policy_stability": 0.9008264462809917,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.7520661157024794,
            0.9256198347107438,
            0.8016528925619835,
            0.8181818181818182,
            0.8512396694214877,
            0.9008264462809917,
            0.859504132231405,
            0.8347107438016529,
            0.8842975206611571,
            0.9834710743801653,
            1.0,
            1.0,
            0.9008264462809917
        ],
        "reward_history": [
            -155,
            -1210,
            -338,
            -1210,
            -812,
            -711,
            -1210,
            -999,
            -740,
            -570,
            -340,
            -203,
            12,
            -1210
        ],
        "steps_history": [
            256,
            1210,
            439,
            1210,
            913,
            812,
            1210,
            1100,
            841,
            671,
            441,
            304,
            89,
            1210
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "245/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.511439561843872,
        "final_policy_stability": 0.9834710743801653,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8181818181818182,
            0.7933884297520661,
            0.8760330578512396,
            0.9504132231404959,
            0.8347107438016529,
            0.9421487603305785,
            0.9008264462809917,
            0.9669421487603306,
            0.8429752066115702,
            0.9173553719008265,
            0.9834710743801653,
            0.9669421487603306,
            0.9752066115702479,
            0.9834710743801653,
            0.9917355371900827,
            0.9669421487603306,
            0.9752066115702479,
            0.9834710743801653
        ],
        "reward_history": [
            -310,
            -1210,
            -1029,
            -1210,
            -21,
            -1210,
            -173,
            -367,
            -92,
            -1210,
            -364,
            -156,
            -68,
            -127,
            -48,
            -227,
            -280,
            -225,
            -242
        ],
        "steps_history": [
            411,
            1210,
            1130,
            1210,
            122,
            1210,
            274,
            468,
            193,
            1210,
            465,
            257,
            169,
            228,
            149,
            328,
            381,
            326,
            343
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "246/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.818148136138916,
        "final_policy_stability": 0.9752066115702479,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.7768595041322314,
            0.768595041322314,
            0.8016528925619835,
            0.9008264462809917,
            0.9256198347107438,
            0.9090909090909091,
            0.9090909090909091,
            0.8842975206611571,
            0.9421487603305785,
            0.9669421487603306,
            0.9256198347107438,
            0.8842975206611571,
            0.9586776859504132,
            0.9834710743801653,
            0.9752066115702479
        ],
        "reward_history": [
            -1210,
            -829,
            -1210,
            -1210,
            -41,
            -108,
            -366,
            -142,
            -1210,
            -48,
            -250,
            -261,
            -1210,
            -358,
            -219,
            -423
        ],
        "steps_history": [
            1210,
            930,
            1210,
            1210,
            142,
            209,
            467,
            243,
            1210,
            149,
            351,
            362,
            1210,
            459,
            320,
            524
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "247/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4759817123413086,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.8347107438016529,
            0.8264462809917356,
            0.8347107438016529,
            0.8760330578512396,
            0.8925619834710744,
            0.9338842975206612,
            0.859504132231405,
            0.8512396694214877,
            0.8264462809917356,
            0.859504132231405,
            0.9917355371900827,
            0.9834710743801653,
            0.9834710743801653,
            1.0,
            0.9917355371900827,
            1.0,
            1.0
        ],
        "reward_history": [
            -540,
            -652,
            -1210,
            -628,
            -638,
            -1210,
            -84,
            -917,
            -1210,
            -861,
            -722,
            -57,
            -137,
            -99,
            -138,
            -460,
            18,
            -170
        ],
        "steps_history": [
            641,
            753,
            1210,
            729,
            739,
            1210,
            185,
            1018,
            1210,
            962,
            823,
            158,
            238,
            200,
            239,
            561,
            83,
            271
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "248/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.49813175201416,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8429752066115702,
            0.8760330578512396,
            0.859504132231405,
            0.8181818181818182,
            0.8016528925619835,
            0.9586776859504132,
            0.9504132231404959,
            0.8264462809917356,
            0.8925619834710744,
            0.9752066115702479,
            0.8677685950413223,
            0.9669421487603306,
            0.8512396694214877,
            1.0,
            0.9834710743801653,
            0.9917355371900827,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -360,
            -188,
            -318,
            -680,
            -831,
            3,
            -275,
            -1210,
            -245,
            -17,
            -695,
            -289,
            -1210,
            22,
            -112,
            26,
            -32,
            -8,
            -80
        ],
        "steps_history": [
            1210,
            461,
            289,
            419,
            781,
            932,
            98,
            376,
            1210,
            346,
            118,
            796,
            390,
            1210,
            79,
            213,
            75,
            133,
            109,
            181
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "249/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.3572134971618652,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8264462809917356,
            0.7603305785123967,
            0.7933884297520661,
            0.9338842975206612,
            0.9090909090909091,
            0.9421487603305785,
            0.9256198347107438,
            0.9008264462809917,
            0.8264462809917356,
            0.8842975206611571,
            0.8347107438016529,
            0.9834710743801653,
            0.9752066115702479,
            0.9338842975206612,
            0.8925619834710744,
            0.9752066115702479,
            1.0,
            1.0
        ],
        "reward_history": [
            -331,
            -730,
            -807,
            -1210,
            -211,
            -218,
            -168,
            -304,
            -1210,
            -1210,
            -371,
            -1059,
            -130,
            -87,
            -354,
            -423,
            -261,
            -8,
            43
        ],
        "steps_history": [
            432,
            831,
            908,
            1210,
            312,
            319,
            269,
            405,
            1210,
            1210,
            472,
            1160,
            231,
            188,
            455,
            524,
            362,
            109,
            58
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "250/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.121541500091553,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.8347107438016529,
            0.9090909090909091,
            0.9338842975206612,
            0.8264462809917356,
            0.9421487603305785,
            0.9256198347107438,
            0.8429752066115702,
            0.8512396694214877,
            0.9917355371900827,
            0.9504132231404959,
            0.9834710743801653,
            0.9669421487603306,
            0.9586776859504132,
            0.9669421487603306,
            0.9752066115702479,
            0.9834710743801653,
            0.9917355371900827,
            0.8760330578512396,
            0.9917355371900827,
            0.9256198347107438,
            1.0,
            1.0
        ],
        "reward_history": [
            -462,
            -1210,
            -639,
            -251,
            -258,
            -1210,
            -172,
            -282,
            -542,
            -1210,
            -303,
            -149,
            -61,
            -93,
            -204,
            -157,
            -37,
            -144,
            -93,
            -707,
            -150,
            -268,
            -86,
            -5
        ],
        "steps_history": [
            563,
            1210,
            740,
            352,
            359,
            1210,
            273,
            383,
            643,
            1210,
            404,
            250,
            162,
            194,
            305,
            258,
            138,
            245,
            194,
            808,
            251,
            369,
            187,
            106
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "251/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.456390142440796,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.8512396694214877,
            0.9256198347107438,
            0.9090909090909091,
            0.8925619834710744,
            0.9173553719008265,
            0.9173553719008265,
            0.7933884297520661,
            1.0,
            0.8429752066115702,
            0.9834710743801653,
            0.9834710743801653,
            0.9669421487603306,
            0.9752066115702479,
            0.8677685950413223,
            0.9917355371900827,
            1.0,
            1.0
        ],
        "reward_history": [
            -612,
            -1210,
            -2,
            -90,
            -447,
            -257,
            -169,
            -1210,
            -28,
            -767,
            -60,
            -110,
            -85,
            -196,
            -1210,
            -136,
            -16,
            -112
        ],
        "steps_history": [
            713,
            1210,
            103,
            191,
            548,
            358,
            270,
            1210,
            129,
            868,
            161,
            211,
            186,
            297,
            1210,
            237,
            117,
            213
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "252/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4596588611602783,
        "final_policy_stability": 0.9090909090909091,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.9669421487603306,
            0.9090909090909091,
            0.8264462809917356,
            0.8925619834710744,
            0.8677685950413223,
            0.9173553719008265,
            0.9173553719008265,
            0.9586776859504132,
            0.8842975206611571,
            0.9586776859504132,
            0.9173553719008265,
            0.9917355371900827,
            0.9752066115702479,
            0.9338842975206612,
            0.9917355371900827,
            1.0,
            0.9917355371900827,
            0.9090909090909091
        ],
        "reward_history": [
            -1210,
            -685,
            -10,
            -195,
            -1210,
            -391,
            -1210,
            -377,
            -261,
            -122,
            -1210,
            3,
            -122,
            -57,
            -19,
            -312,
            -11,
            16,
            -2,
            -537
        ],
        "steps_history": [
            1210,
            786,
            111,
            296,
            1210,
            492,
            1210,
            478,
            362,
            223,
            1210,
            98,
            223,
            158,
            120,
            413,
            112,
            85,
            103,
            638
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "253/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.503354787826538,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 16,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.9256198347107438,
            0.8760330578512396,
            0.8925619834710744,
            0.8181818181818182,
            0.9586776859504132,
            0.9421487603305785,
            0.9586776859504132,
            0.9421487603305785,
            0.9669421487603306,
            0.8512396694214877,
            0.8512396694214877,
            0.9834710743801653,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -1106,
            -1210,
            -106,
            -144,
            -196,
            -507,
            21,
            -69,
            -106,
            -250,
            -142,
            -1210,
            -584,
            -128,
            -68,
            -47,
            -25
        ],
        "steps_history": [
            1207,
            1210,
            207,
            245,
            297,
            608,
            80,
            170,
            207,
            351,
            243,
            1210,
            685,
            229,
            169,
            148,
            126
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "254/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7438838481903076,
        "final_policy_stability": 0.8760330578512396,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.71900826446281,
            0.8264462809917356,
            0.9090909090909091,
            0.7520661157024794,
            0.9504132231404959,
            0.9008264462809917,
            0.9173553719008265,
            0.9834710743801653,
            0.8760330578512396,
            0.8429752066115702,
            0.8429752066115702,
            1.0,
            1.0,
            1.0,
            0.8760330578512396
        ],
        "reward_history": [
            -92,
            -1210,
            -284,
            -238,
            -1210,
            -282,
            -222,
            -169,
            -244,
            -1210,
            -644,
            -485,
            10,
            37,
            -209,
            -662
        ],
        "steps_history": [
            193,
            1210,
            385,
            339,
            1210,
            383,
            323,
            270,
            345,
            1210,
            745,
            586,
            91,
            64,
            310,
            763
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "255/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7209243774414062,
        "final_policy_stability": 0.9834710743801653,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.7768595041322314,
            0.9338842975206612,
            0.8347107438016529,
            0.9173553719008265,
            0.9090909090909091,
            0.8677685950413223,
            0.9256198347107438,
            0.9504132231404959,
            0.9173553719008265,
            0.9917355371900827,
            0.8925619834710744,
            1.0,
            0.9834710743801653
        ],
        "reward_history": [
            -1210,
            -1210,
            -209,
            -1210,
            -253,
            -327,
            -930,
            -332,
            -820,
            -424,
            -120,
            -1210,
            -66,
            -377
        ],
        "steps_history": [
            1210,
            1210,
            310,
            1210,
            354,
            428,
            1031,
            433,
            921,
            525,
            221,
            1210,
            167,
            478
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "256/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.486036777496338,
        "final_policy_stability": 0.9256198347107438,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.7933884297520661,
            0.7603305785123967,
            0.8760330578512396,
            0.9752066115702479,
            0.9586776859504132,
            0.859504132231405,
            0.9173553719008265,
            0.8429752066115702,
            0.9008264462809917,
            0.9173553719008265,
            0.9917355371900827,
            0.9752066115702479,
            0.9421487603305785,
            0.9917355371900827,
            0.9917355371900827,
            1.0,
            0.9256198347107438
        ],
        "reward_history": [
            -1210,
            -901,
            -1038,
            -873,
            -178,
            -321,
            -1210,
            -277,
            -1210,
            -1210,
            -424,
            -168,
            -213,
            -671,
            -360,
            -434,
            -201,
            -425
        ],
        "steps_history": [
            1210,
            1002,
            1139,
            974,
            279,
            422,
            1210,
            378,
            1210,
            1210,
            525,
            269,
            314,
            772,
            461,
            535,
            302,
            526
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "257/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4841902256011963,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 16,
        "policy_stability_history": [
            0.0,
            0.8181818181818182,
            0.8264462809917356,
            0.8925619834710744,
            0.8512396694214877,
            0.8925619834710744,
            0.9421487603305785,
            0.859504132231405,
            0.9338842975206612,
            0.8842975206611571,
            0.9173553719008265,
            0.8925619834710744,
            0.9669421487603306,
            0.9256198347107438,
            0.9586776859504132,
            1.0,
            1.0
        ],
        "reward_history": [
            -231,
            -1210,
            -1210,
            -296,
            -867,
            -1210,
            -217,
            -736,
            -418,
            -1210,
            -427,
            -1210,
            -186,
            -459,
            -253,
            -65,
            -158
        ],
        "steps_history": [
            332,
            1210,
            1210,
            397,
            968,
            1210,
            318,
            837,
            519,
            1210,
            528,
            1210,
            287,
            560,
            354,
            166,
            259
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "258/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7502200603485107,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8264462809917356,
            0.8677685950413223,
            0.8512396694214877,
            0.8429752066115702,
            0.9421487603305785,
            0.9008264462809917,
            0.7933884297520661,
            0.8925619834710744,
            0.9752066115702479,
            0.9090909090909091,
            0.9586776859504132,
            0.9752066115702479,
            0.9917355371900827,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -1210,
            -816,
            -1210,
            -1210,
            -148,
            -264,
            -845,
            -490,
            -41,
            -458,
            -154,
            -213,
            -112,
            -146,
            -330
        ],
        "steps_history": [
            1210,
            1210,
            917,
            1210,
            1210,
            249,
            365,
            946,
            591,
            142,
            559,
            255,
            314,
            213,
            247,
            431
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "259/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7486677169799805,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.7520661157024794,
            0.9173553719008265,
            0.7933884297520661,
            0.8181818181818182,
            0.8512396694214877,
            0.9008264462809917,
            0.9256198347107438,
            0.9752066115702479,
            0.9669421487603306,
            0.9421487603305785,
            0.9008264462809917,
            0.9834710743801653,
            1.0
        ],
        "reward_history": [
            -155,
            -1210,
            -338,
            -1210,
            -812,
            -711,
            -1210,
            -615,
            -37,
            -145,
            -285,
            -1025,
            -340,
            -203
        ],
        "steps_history": [
            256,
            1210,
            439,
            1210,
            913,
            812,
            1210,
            716,
            138,
            246,
            386,
            1126,
            441,
            304
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "260/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4275944232940674,
        "final_policy_stability": 0.9834710743801653,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8181818181818182,
            0.7933884297520661,
            0.8760330578512396,
            0.9504132231404959,
            0.8347107438016529,
            0.9421487603305785,
            0.9008264462809917,
            0.9669421487603306,
            0.8429752066115702,
            0.9173553719008265,
            0.9834710743801653,
            0.9669421487603306,
            0.9752066115702479,
            0.9834710743801653,
            0.9917355371900827,
            0.9669421487603306,
            0.9752066115702479,
            0.9834710743801653
        ],
        "reward_history": [
            -310,
            -1210,
            -1029,
            -1210,
            -21,
            -1210,
            -173,
            -367,
            -92,
            -1210,
            -364,
            -156,
            -68,
            -127,
            -48,
            -227,
            -280,
            -225,
            -242
        ],
        "steps_history": [
            411,
            1210,
            1130,
            1210,
            122,
            1210,
            274,
            468,
            193,
            1210,
            465,
            257,
            169,
            228,
            149,
            328,
            381,
            326,
            343
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "261/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.6231398582458496,
        "final_policy_stability": 0.9834710743801653,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.7851239669421488,
            0.7768595041322314,
            0.859504132231405,
            0.8842975206611571,
            0.9008264462809917,
            0.8512396694214877,
            0.9752066115702479,
            0.8016528925619835,
            0.9338842975206612,
            0.8842975206611571,
            0.8512396694214877,
            0.9090909090909091,
            0.9586776859504132,
            0.8760330578512396,
            1.0,
            1.0,
            0.9669421487603306,
            0.9834710743801653
        ],
        "reward_history": [
            -1210,
            -829,
            -1210,
            -438,
            -337,
            -274,
            -762,
            27,
            -902,
            -58,
            -429,
            -1210,
            -329,
            -290,
            -1210,
            -72,
            -96,
            -225,
            -163
        ],
        "steps_history": [
            1210,
            930,
            1210,
            539,
            438,
            375,
            863,
            74,
            1003,
            159,
            530,
            1210,
            430,
            391,
            1210,
            173,
            197,
            326,
            264
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "262/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8017618656158447,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8347107438016529,
            0.8264462809917356,
            0.8347107438016529,
            0.8760330578512396,
            0.8925619834710744,
            0.9338842975206612,
            0.8677685950413223,
            0.8264462809917356,
            0.9256198347107438,
            0.9834710743801653,
            0.9834710743801653,
            0.9173553719008265,
            0.859504132231405,
            1.0,
            0.9917355371900827
        ],
        "reward_history": [
            -540,
            -652,
            -1210,
            -628,
            -638,
            -1210,
            -84,
            -917,
            -1210,
            -221,
            -3,
            -66,
            -456,
            -1011,
            -18,
            -138
        ],
        "steps_history": [
            641,
            753,
            1210,
            729,
            739,
            1210,
            185,
            1018,
            1210,
            322,
            104,
            167,
            557,
            1112,
            119,
            239
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "263/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4323413372039795,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8429752066115702,
            0.8760330578512396,
            0.859504132231405,
            0.8181818181818182,
            0.8016528925619835,
            0.9586776859504132,
            0.9504132231404959,
            0.8264462809917356,
            0.8925619834710744,
            0.9752066115702479,
            0.8677685950413223,
            0.9669421487603306,
            0.8512396694214877,
            1.0,
            0.9834710743801653,
            0.9917355371900827,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -1210,
            -360,
            -188,
            -318,
            -680,
            -831,
            3,
            -275,
            -1210,
            -245,
            -17,
            -695,
            -289,
            -1210,
            22,
            -112,
            26,
            -32,
            -8,
            -80
        ],
        "steps_history": [
            1210,
            461,
            289,
            419,
            781,
            932,
            98,
            376,
            1210,
            346,
            118,
            796,
            390,
            1210,
            79,
            213,
            75,
            133,
            109,
            181
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "264/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.6982100009918213,
        "final_policy_stability": 0.9586776859504132,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8264462809917356,
            0.7603305785123967,
            0.7933884297520661,
            0.9338842975206612,
            0.9090909090909091,
            0.9421487603305785,
            0.9256198347107438,
            0.9008264462809917,
            0.8264462809917356,
            0.8925619834710744,
            0.8429752066115702,
            0.9834710743801653,
            0.9752066115702479,
            0.9338842975206612,
            0.9669421487603306,
            0.9917355371900827,
            1.0,
            0.9586776859504132
        ],
        "reward_history": [
            -331,
            -730,
            -807,
            -1210,
            -211,
            -218,
            -168,
            -304,
            -1210,
            -1210,
            -371,
            -1059,
            -130,
            -87,
            -354,
            -124,
            -94,
            -3,
            -261
        ],
        "steps_history": [
            432,
            831,
            908,
            1210,
            312,
            319,
            269,
            405,
            1210,
            1210,
            472,
            1160,
            231,
            188,
            455,
            225,
            195,
            104,
            362
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "265/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.211558103561401,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.8347107438016529,
            0.9173553719008265,
            0.9338842975206612,
            0.8264462809917356,
            0.9421487603305785,
            0.9256198347107438,
            0.8429752066115702,
            0.8512396694214877,
            0.9917355371900827,
            0.9504132231404959,
            0.9834710743801653,
            0.9669421487603306,
            0.9586776859504132,
            0.9669421487603306,
            0.9752066115702479,
            0.9834710743801653,
            0.9917355371900827,
            0.8760330578512396,
            0.9917355371900827,
            0.9173553719008265,
            1.0,
            1.0
        ],
        "reward_history": [
            -462,
            -1210,
            -639,
            -251,
            -258,
            -1210,
            -172,
            -282,
            -542,
            -1210,
            -303,
            -149,
            -61,
            -93,
            -204,
            -157,
            -37,
            -144,
            -93,
            -707,
            -150,
            -268,
            -86,
            -5
        ],
        "steps_history": [
            563,
            1210,
            740,
            352,
            359,
            1210,
            273,
            383,
            643,
            1210,
            404,
            250,
            162,
            194,
            305,
            258,
            138,
            245,
            194,
            808,
            251,
            369,
            187,
            106
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "266/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.620206117630005,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.8512396694214877,
            0.9256198347107438,
            0.9090909090909091,
            0.8925619834710744,
            0.9173553719008265,
            0.9173553719008265,
            0.7933884297520661,
            1.0,
            0.8512396694214877,
            0.9834710743801653,
            0.9834710743801653,
            0.9669421487603306,
            0.9752066115702479,
            0.8842975206611571,
            0.9256198347107438,
            1.0,
            1.0
        ],
        "reward_history": [
            -612,
            -1210,
            -2,
            -90,
            -447,
            -257,
            -169,
            -1210,
            -28,
            -767,
            -60,
            -110,
            -85,
            -196,
            -822,
            -540,
            -112,
            -15
        ],
        "steps_history": [
            713,
            1210,
            103,
            191,
            548,
            358,
            270,
            1210,
            129,
            868,
            161,
            211,
            186,
            297,
            923,
            641,
            213,
            116
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "267/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.108618259429932,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.7520661157024794,
            0.8842975206611571,
            0.9090909090909091,
            0.8842975206611571,
            0.8760330578512396,
            0.9421487603305785,
            0.9256198347107438,
            0.9669421487603306,
            0.859504132231405,
            0.9752066115702479,
            0.9090909090909091,
            0.8099173553719008,
            0.9669421487603306,
            0.9834710743801653,
            0.9421487603305785,
            0.9504132231404959,
            0.9669421487603306,
            0.9917355371900827,
            1.0,
            0.9752066115702479,
            0.9752066115702479,
            1.0,
            0.9504132231404959,
            1.0
        ],
        "reward_history": [
            -1210,
            -1210,
            -603,
            -359,
            -116,
            -340,
            -1210,
            -240,
            -261,
            -122,
            -1210,
            31,
            -42,
            -843,
            -54,
            -84,
            -352,
            -190,
            -207,
            -36,
            -5,
            -68,
            -295,
            18,
            -478,
            -89
        ],
        "steps_history": [
            1210,
            1210,
            704,
            460,
            217,
            441,
            1210,
            341,
            362,
            223,
            1210,
            70,
            143,
            944,
            155,
            185,
            453,
            291,
            308,
            137,
            106,
            169,
            396,
            83,
            579,
            190
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "268/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.131877422332764,
        "final_policy_stability": 0.9917355371900827,
        "episodes_to_convergence": 21,
        "policy_stability_history": [
            0.0,
            0.8099173553719008,
            0.9256198347107438,
            0.8760330578512396,
            0.8925619834710744,
            0.8181818181818182,
            0.9586776859504132,
            0.9338842975206612,
            0.9504132231404959,
            0.859504132231405,
            0.9586776859504132,
            0.8429752066115702,
            0.9834710743801653,
            0.9834710743801653,
            0.8925619834710744,
            1.0,
            0.8842975206611571,
            0.9586776859504132,
            0.9421487603305785,
            0.8842975206611571,
            0.9917355371900827,
            0.9917355371900827
        ],
        "reward_history": [
            -1106,
            -1210,
            -106,
            -144,
            -196,
            -507,
            -47,
            -290,
            -168,
            -1210,
            -74,
            -881,
            -46,
            -69,
            -605,
            3,
            -1210,
            -107,
            -185,
            -988,
            -50,
            -161
        ],
        "steps_history": [
            1207,
            1210,
            207,
            245,
            297,
            608,
            148,
            391,
            269,
            1210,
            175,
            982,
            147,
            170,
            706,
            98,
            1210,
            208,
            286,
            1089,
            151,
            262
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "269/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 5,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 20.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.840940237045288,
        "final_policy_stability": 0.8760330578512396,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.71900826446281,
            0.8264462809917356,
            0.9090909090909091,
            0.7520661157024794,
            0.9504132231404959,
            0.9008264462809917,
            0.9173553719008265,
            0.9834710743801653,
            0.8760330578512396,
            0.8429752066115702,
            0.8429752066115702,
            1.0,
            1.0,
            1.0,
            0.8760330578512396
        ],
        "reward_history": [
            -92,
            -1210,
            -284,
            -238,
            -1210,
            -282,
            -222,
            -169,
            -244,
            -1210,
            -644,
            -485,
            10,
            37,
            -209,
            -662
        ],
        "steps_history": [
            193,
            1210,
            385,
            339,
            1210,
            383,
            323,
            270,
            345,
            1210,
            745,
            586,
            91,
            64,
            310,
            763
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "270/810",
        "save_path": "experiments/20250131_160708/training_plots/size_5/lr0.4_df0.99_eps0.1_trial4"
    }
]