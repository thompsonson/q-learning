[
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.039107084274292,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 45,
        "policy_stability_history": [
            0.0,
            0.8235294117647058,
            0.8200692041522492,
            0.8304498269896193,
            0.903114186851211,
            0.8235294117647058,
            0.8754325259515571,
            0.903114186851211,
            0.9100346020761245,
            0.889273356401384,
            0.8581314878892734,
            0.8996539792387543,
            0.8858131487889274,
            0.903114186851211,
            0.9411764705882353,
            0.9134948096885813,
            0.8961937716262975,
            0.8788927335640139,
            0.9377162629757786,
            0.9100346020761245,
            0.9826989619377162,
            0.9446366782006921,
            0.9273356401384083,
            0.9515570934256056,
            0.9446366782006921,
            0.9688581314878892,
            0.972318339100346,
            0.9515570934256056,
            0.9653979238754326,
            0.9930795847750865,
            0.9515570934256056,
            0.9757785467128027,
            0.9792387543252595,
            0.9480968858131488,
            0.9930795847750865,
            0.9480968858131488,
            0.9930795847750865,
            0.9619377162629758,
            0.9965397923875432,
            0.9930795847750865,
            0.9688581314878892,
            0.9965397923875432,
            0.9584775086505191,
            0.9826989619377162,
            0.9792387543252595,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2250,
            -2890,
            -2890,
            -2890,
            -1271,
            -2890,
            -2390,
            -2671,
            -2068,
            -1448,
            -2890,
            -1027,
            -1658,
            -2130,
            -2155,
            -1635,
            -2618,
            -350,
            -1407,
            -2068,
            -764,
            -1071,
            -421,
            -1092,
            -1952,
            -793,
            -1263,
            -1152,
            -716,
            -590,
            -1586,
            -1266,
            -1412,
            -1156,
            -1283,
            -810,
            -730,
            -1156,
            -1054,
            -2656,
            -923,
            -1411,
            -489
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2351,
            2890,
            2890,
            2890,
            1372,
            2890,
            2491,
            2772,
            2169,
            1549,
            2890,
            1128,
            1759,
            2231,
            2256,
            1736,
            2719,
            451,
            1508,
            2169,
            865,
            1172,
            522,
            1193,
            2053,
            894,
            1364,
            1253,
            817,
            691,
            1687,
            1367,
            1513,
            1257,
            1384,
            911,
            831,
            1257,
            1155,
            2757,
            1024,
            1512,
            590
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "451/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.834880352020264,
        "final_policy_stability": 0.9653979238754326,
        "episodes_to_convergence": 49,
        "policy_stability_history": [
            0.0,
            0.8235294117647058,
            0.7577854671280276,
            0.9342560553633218,
            0.8685121107266436,
            0.8269896193771626,
            0.903114186851211,
            0.8512110726643599,
            0.8650519031141869,
            0.8685121107266436,
            0.9238754325259516,
            0.8442906574394463,
            0.8546712802768166,
            0.8823529411764706,
            0.7993079584775087,
            0.9134948096885813,
            0.8927335640138409,
            0.9238754325259516,
            0.9204152249134948,
            0.8858131487889274,
            0.8754325259515571,
            0.9307958477508651,
            0.9100346020761245,
            0.9619377162629758,
            0.9757785467128027,
            0.9619377162629758,
            0.972318339100346,
            0.9100346020761245,
            0.9653979238754326,
            0.8961937716262975,
            0.9653979238754326,
            0.903114186851211,
            0.9826989619377162,
            0.9550173010380623,
            0.9480968858131488,
            0.9826989619377162,
            0.9515570934256056,
            0.9377162629757786,
            0.9204152249134948,
            0.972318339100346,
            0.972318339100346,
            0.9584775086505191,
            0.9619377162629758,
            0.986159169550173,
            0.986159169550173,
            0.9826989619377162,
            0.9896193771626297,
            0.9930795847750865,
            0.9826989619377162,
            0.9653979238754326
        ],
        "reward_history": [
            -1046,
            -2890,
            -2890,
            -448,
            -2890,
            -2890,
            -1145,
            -2507,
            -1566,
            -2890,
            -1242,
            -2469,
            -2890,
            -2890,
            -2890,
            -2094,
            -1855,
            -2890,
            -1771,
            -2083,
            -2890,
            -1051,
            -2085,
            -570,
            -554,
            -777,
            -1024,
            -2890,
            -574,
            -2890,
            -787,
            -2890,
            -281,
            -1937,
            -1461,
            -1092,
            -1329,
            -936,
            -2519,
            -644,
            -771,
            -1310,
            -752,
            -367,
            -1174,
            -1226,
            -1381,
            -1920,
            -536,
            -2541
        ],
        "steps_history": [
            1147,
            2890,
            2890,
            549,
            2890,
            2890,
            1246,
            2608,
            1667,
            2890,
            1343,
            2570,
            2890,
            2890,
            2890,
            2195,
            1956,
            2890,
            1872,
            2184,
            2890,
            1152,
            2186,
            671,
            655,
            878,
            1125,
            2890,
            675,
            2890,
            888,
            2890,
            382,
            2038,
            1562,
            1193,
            1430,
            1037,
            2620,
            745,
            872,
            1411,
            853,
            468,
            1275,
            1327,
            1482,
            2021,
            637,
            2642
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "452/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.843875408172607,
        "final_policy_stability": 0.9446366782006921,
        "episodes_to_convergence": 45,
        "policy_stability_history": [
            0.0,
            0.8823529411764706,
            0.8304498269896193,
            0.9238754325259516,
            0.8304498269896193,
            0.8788927335640139,
            0.8719723183391004,
            0.8408304498269896,
            0.8200692041522492,
            0.8546712802768166,
            0.8823529411764706,
            0.8685121107266436,
            0.916955017301038,
            0.8442906574394463,
            0.8719723183391004,
            0.8996539792387543,
            0.9134948096885813,
            0.8512110726643599,
            0.9619377162629758,
            0.9515570934256056,
            0.8961937716262975,
            0.9653979238754326,
            0.9446366782006921,
            0.9238754325259516,
            0.9100346020761245,
            0.9896193771626297,
            0.9480968858131488,
            0.9757785467128027,
            0.9550173010380623,
            0.9307958477508651,
            0.9273356401384083,
            0.9480968858131488,
            0.9826989619377162,
            0.9653979238754326,
            0.9896193771626297,
            0.972318339100346,
            0.9550173010380623,
            0.9965397923875432,
            0.9307958477508651,
            0.986159169550173,
            0.9826989619377162,
            0.9584775086505191,
            1.0,
            0.9930795847750865,
            0.9792387543252595,
            0.9446366782006921
        ],
        "reward_history": [
            -2890,
            -2890,
            -2075,
            -1433,
            -2890,
            -1451,
            -2157,
            -2890,
            -2890,
            -1456,
            -2890,
            -2890,
            -1884,
            -2712,
            -1397,
            -897,
            -1138,
            -2890,
            -1138,
            -1084,
            -2351,
            -556,
            -978,
            -1254,
            -1641,
            -223,
            -1025,
            -728,
            -884,
            -1887,
            -1222,
            -1538,
            -746,
            -865,
            -630,
            -815,
            -899,
            -304,
            -2890,
            -1806,
            -419,
            -2078,
            -629,
            -1014,
            -918,
            -2890
        ],
        "steps_history": [
            2890,
            2890,
            2176,
            1534,
            2890,
            1552,
            2258,
            2890,
            2890,
            1557,
            2890,
            2890,
            1985,
            2813,
            1498,
            998,
            1239,
            2890,
            1239,
            1185,
            2452,
            657,
            1079,
            1355,
            1742,
            324,
            1126,
            829,
            985,
            1988,
            1323,
            1639,
            847,
            966,
            731,
            916,
            1000,
            405,
            2890,
            1907,
            520,
            2179,
            730,
            1115,
            1019,
            2890
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "453/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.9081339836120605,
        "final_policy_stability": 0.986159169550173,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.7716262975778547,
            0.889273356401384,
            0.8304498269896193,
            0.8754325259515571,
            0.8477508650519031,
            0.9238754325259516,
            0.8927335640138409,
            0.8685121107266436,
            0.8961937716262975,
            0.8927335640138409,
            0.8927335640138409,
            0.9100346020761245,
            0.8339100346020761,
            0.8788927335640139,
            0.9065743944636678,
            0.8581314878892734,
            0.8685121107266436,
            0.9342560553633218,
            0.916955017301038,
            0.9065743944636678,
            0.903114186851211,
            0.9307958477508651,
            0.9480968858131488,
            0.9757785467128027,
            0.8927335640138409,
            0.9411764705882353,
            0.9342560553633218,
            0.9480968858131488,
            0.9688581314878892,
            0.9930795847750865,
            0.9411764705882353,
            0.9411764705882353,
            0.9619377162629758,
            0.986159169550173,
            0.986159169550173,
            0.9584775086505191,
            0.986159169550173
        ],
        "reward_history": [
            -2890,
            -1798,
            -2890,
            -2890,
            -2890,
            -2605,
            -2890,
            -2890,
            -2557,
            -2285,
            -1892,
            -2890,
            -1111,
            -2890,
            -1580,
            -714,
            -2890,
            -1742,
            -1250,
            -2890,
            -1867,
            -1273,
            -2890,
            -933,
            -513,
            -1786,
            -1059,
            -1237,
            -990,
            -857,
            -993,
            -2564,
            -1345,
            -1123,
            -806,
            -289,
            -1271,
            -762
        ],
        "steps_history": [
            2890,
            1899,
            2890,
            2890,
            2890,
            2706,
            2890,
            2890,
            2658,
            2386,
            1993,
            2890,
            1212,
            2890,
            1681,
            815,
            2890,
            1843,
            1351,
            2890,
            1968,
            1374,
            2890,
            1034,
            614,
            1887,
            1160,
            1338,
            1091,
            958,
            1094,
            2665,
            1446,
            1224,
            907,
            390,
            1372,
            863
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "454/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.640629053115845,
        "final_policy_stability": 0.9930795847750865,
        "episodes_to_convergence": 46,
        "policy_stability_history": [
            0.0,
            0.8719723183391004,
            0.9100346020761245,
            0.8996539792387543,
            0.8685121107266436,
            0.8339100346020761,
            0.7889273356401384,
            0.9065743944636678,
            0.8996539792387543,
            0.8961937716262975,
            0.9307958477508651,
            0.889273356401384,
            0.8754325259515571,
            0.9100346020761245,
            0.9411764705882353,
            0.8858131487889274,
            0.8961937716262975,
            0.8615916955017301,
            0.916955017301038,
            0.972318339100346,
            0.9342560553633218,
            0.9584775086505191,
            0.9584775086505191,
            0.9238754325259516,
            0.9238754325259516,
            0.9757785467128027,
            0.9065743944636678,
            0.9619377162629758,
            0.9480968858131488,
            0.9826989619377162,
            0.9515570934256056,
            0.9930795847750865,
            0.972318339100346,
            0.9584775086505191,
            0.9826989619377162,
            0.9688581314878892,
            0.9792387543252595,
            0.9757785467128027,
            0.986159169550173,
            0.9930795847750865,
            0.9792387543252595,
            0.9826989619377162,
            0.986159169550173,
            0.9757785467128027,
            0.9930795847750865,
            1.0,
            0.9930795847750865
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -1041,
            -2890,
            -2629,
            -2610,
            -1445,
            -2107,
            -2376,
            -692,
            -2890,
            -2890,
            -1678,
            -960,
            -1474,
            -2890,
            -2377,
            -2890,
            -556,
            -1029,
            -683,
            -838,
            -1555,
            -2621,
            -594,
            -2154,
            -1389,
            -2738,
            -603,
            -1150,
            -713,
            -2476,
            -2385,
            -1091,
            -867,
            -817,
            -673,
            -1879,
            -322,
            -1574,
            -1422,
            -675,
            -614,
            -714,
            -713,
            -1586
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            1142,
            2890,
            2730,
            2711,
            1546,
            2208,
            2477,
            793,
            2890,
            2890,
            1779,
            1061,
            1575,
            2890,
            2478,
            2890,
            657,
            1130,
            784,
            939,
            1656,
            2722,
            695,
            2255,
            1490,
            2839,
            704,
            1251,
            814,
            2577,
            2486,
            1192,
            968,
            918,
            774,
            1980,
            423,
            1675,
            1523,
            776,
            715,
            815,
            814,
            1687
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "455/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.979213237762451,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 54,
        "policy_stability_history": [
            0.0,
            0.7820069204152249,
            0.8788927335640139,
            0.9100346020761245,
            0.8650519031141869,
            0.889273356401384,
            0.903114186851211,
            0.8304498269896193,
            0.9273356401384083,
            0.8927335640138409,
            0.9134948096885813,
            0.9134948096885813,
            0.8477508650519031,
            0.8927335640138409,
            0.8581314878892734,
            0.9134948096885813,
            0.8823529411764706,
            0.9377162629757786,
            0.9757785467128027,
            0.8927335640138409,
            0.8996539792387543,
            0.9688581314878892,
            0.9515570934256056,
            0.9411764705882353,
            0.9653979238754326,
            0.9550173010380623,
            0.9515570934256056,
            0.8650519031141869,
            0.916955017301038,
            0.9653979238754326,
            0.9342560553633218,
            0.9688581314878892,
            0.9757785467128027,
            0.986159169550173,
            0.9930795847750865,
            0.986159169550173,
            0.9377162629757786,
            0.9653979238754326,
            0.986159169550173,
            0.9896193771626297,
            0.9307958477508651,
            0.9342560553633218,
            0.9896193771626297,
            0.986159169550173,
            0.9896193771626297,
            0.9930795847750865,
            0.9550173010380623,
            0.972318339100346,
            1.0,
            0.9965397923875432,
            0.972318339100346,
            1.0,
            0.9930795847750865,
            0.9896193771626297,
            1.0
        ],
        "reward_history": [
            -1527,
            -2890,
            -2890,
            -2890,
            -2890,
            -1591,
            -1137,
            -2890,
            -842,
            -2890,
            -533,
            -1197,
            -2048,
            -1084,
            -2890,
            -1746,
            -1646,
            -1430,
            -511,
            -1301,
            -1347,
            -332,
            -1248,
            -792,
            -353,
            -1075,
            -1226,
            -2890,
            -1729,
            -705,
            -1452,
            -564,
            -454,
            -213,
            -370,
            -531,
            -2247,
            -534,
            -372,
            -382,
            -2012,
            -1254,
            -710,
            -597,
            -430,
            -893,
            -1761,
            -1036,
            -497,
            -477,
            -1632,
            -592,
            -808,
            -821,
            -470
        ],
        "steps_history": [
            1628,
            2890,
            2890,
            2890,
            2890,
            1692,
            1238,
            2890,
            943,
            2890,
            634,
            1298,
            2149,
            1185,
            2890,
            1847,
            1747,
            1531,
            612,
            1402,
            1448,
            433,
            1349,
            893,
            454,
            1176,
            1327,
            2890,
            1830,
            806,
            1553,
            665,
            555,
            314,
            471,
            632,
            2348,
            635,
            473,
            483,
            2113,
            1355,
            811,
            698,
            531,
            994,
            1862,
            1137,
            598,
            578,
            1733,
            693,
            909,
            922,
            571
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "456/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.147899150848389,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 54,
        "policy_stability_history": [
            0.0,
            0.8062283737024222,
            0.7612456747404844,
            0.8339100346020761,
            0.8823529411764706,
            0.9065743944636678,
            0.8477508650519031,
            0.889273356401384,
            0.8304498269896193,
            0.8096885813148789,
            0.8546712802768166,
            0.8685121107266436,
            0.9619377162629758,
            0.9238754325259516,
            0.8650519031141869,
            0.8719723183391004,
            0.9134948096885813,
            0.9273356401384083,
            0.9134948096885813,
            0.9619377162629758,
            0.972318339100346,
            0.9100346020761245,
            0.9446366782006921,
            0.9411764705882353,
            0.9273356401384083,
            0.9204152249134948,
            0.9411764705882353,
            0.9446366782006921,
            0.9515570934256056,
            0.9653979238754326,
            0.9411764705882353,
            0.9273356401384083,
            0.972318339100346,
            0.986159169550173,
            0.9584775086505191,
            0.9826989619377162,
            0.9584775086505191,
            0.972318339100346,
            0.9826989619377162,
            0.9826989619377162,
            0.9307958477508651,
            0.9653979238754326,
            0.9480968858131488,
            0.9411764705882353,
            0.9619377162629758,
            0.986159169550173,
            0.972318339100346,
            0.9965397923875432,
            0.9930795847750865,
            0.9688581314878892,
            0.9965397923875432,
            0.9550173010380623,
            1.0,
            0.9965397923875432,
            0.9965397923875432
        ],
        "reward_history": [
            -1867,
            -2890,
            -1786,
            -2890,
            -1585,
            -875,
            -2216,
            -2890,
            -1966,
            -2890,
            -2890,
            -1954,
            -537,
            -618,
            -2890,
            -1545,
            -1459,
            -1075,
            -2890,
            -668,
            -274,
            -1356,
            -420,
            -964,
            -720,
            -1291,
            -1749,
            -1125,
            -1006,
            -589,
            -1624,
            -1307,
            -498,
            -699,
            -554,
            -916,
            -492,
            -469,
            -437,
            -403,
            -1295,
            -607,
            -929,
            -1846,
            -1090,
            -920,
            -913,
            -244,
            -560,
            -712,
            -258,
            -1254,
            -284,
            -495,
            -188
        ],
        "steps_history": [
            1968,
            2890,
            1887,
            2890,
            1686,
            976,
            2317,
            2890,
            2067,
            2890,
            2890,
            2055,
            638,
            719,
            2890,
            1646,
            1560,
            1176,
            2890,
            769,
            375,
            1457,
            521,
            1065,
            821,
            1392,
            1850,
            1226,
            1107,
            690,
            1725,
            1408,
            599,
            800,
            655,
            1017,
            593,
            570,
            538,
            504,
            1396,
            708,
            1030,
            1947,
            1191,
            1021,
            1014,
            345,
            661,
            813,
            359,
            1355,
            385,
            596,
            289
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "457/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.922261476516724,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 51,
        "policy_stability_history": [
            0.0,
            0.889273356401384,
            0.8096885813148789,
            0.8650519031141869,
            0.889273356401384,
            0.8408304498269896,
            0.8581314878892734,
            0.7820069204152249,
            0.8685121107266436,
            0.8166089965397924,
            0.8477508650519031,
            0.8373702422145328,
            0.8304498269896193,
            0.8166089965397924,
            0.8719723183391004,
            0.8719723183391004,
            0.8685121107266436,
            0.8858131487889274,
            0.8788927335640139,
            0.9204152249134948,
            0.9515570934256056,
            0.8858131487889274,
            0.8650519031141869,
            0.8996539792387543,
            0.9273356401384083,
            0.9377162629757786,
            0.9688581314878892,
            0.972318339100346,
            0.9688581314878892,
            0.9584775086505191,
            0.916955017301038,
            0.9584775086505191,
            0.9757785467128027,
            0.9411764705882353,
            0.9273356401384083,
            0.972318339100346,
            0.9757785467128027,
            0.9515570934256056,
            0.9930795847750865,
            0.9757785467128027,
            0.9792387543252595,
            0.9515570934256056,
            0.9930795847750865,
            0.9619377162629758,
            0.9757785467128027,
            0.9411764705882353,
            0.9826989619377162,
            0.9965397923875432,
            0.986159169550173,
            0.986159169550173,
            0.9653979238754326,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -644,
            -2053,
            -1882,
            -2890,
            -1547,
            -2394,
            -1708,
            -2890,
            -2890,
            -2890,
            -2569,
            -976,
            -2636,
            -1688,
            -1453,
            -1586,
            -796,
            -1515,
            -2258,
            -1508,
            -1621,
            -947,
            -680,
            -609,
            -249,
            -394,
            -1925,
            -1407,
            -354,
            -1551,
            -986,
            -638,
            -771,
            -1437,
            -863,
            -392,
            -667,
            -2074,
            -174,
            -1881,
            -1014,
            -1769,
            -752,
            -679,
            -637,
            -1593,
            -1931,
            -604
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            745,
            2154,
            1983,
            2890,
            1648,
            2495,
            1809,
            2890,
            2890,
            2890,
            2670,
            1077,
            2737,
            1789,
            1554,
            1687,
            897,
            1616,
            2359,
            1609,
            1722,
            1048,
            781,
            710,
            350,
            495,
            2026,
            1508,
            455,
            1652,
            1087,
            739,
            872,
            1538,
            964,
            493,
            768,
            2175,
            275,
            1982,
            1115,
            1870,
            853,
            780,
            738,
            1694,
            2032,
            705
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "458/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.457083225250244,
        "final_policy_stability": 0.9930795847750865,
        "episodes_to_convergence": 49,
        "policy_stability_history": [
            0.0,
            0.7750865051903114,
            0.9342560553633218,
            0.8062283737024222,
            0.8477508650519031,
            0.8512110726643599,
            0.8269896193771626,
            0.8615916955017301,
            0.8408304498269896,
            0.8754325259515571,
            0.8858131487889274,
            0.8685121107266436,
            0.8512110726643599,
            0.9065743944636678,
            0.8719723183391004,
            0.916955017301038,
            0.903114186851211,
            0.9134948096885813,
            0.9238754325259516,
            0.8927335640138409,
            0.9342560553633218,
            0.8581314878892734,
            0.9273356401384083,
            0.9480968858131488,
            0.9515570934256056,
            0.9688581314878892,
            0.9204152249134948,
            0.986159169550173,
            0.9619377162629758,
            0.972318339100346,
            0.9826989619377162,
            0.9653979238754326,
            0.9550173010380623,
            0.9307958477508651,
            0.9826989619377162,
            0.9792387543252595,
            0.972318339100346,
            0.9688581314878892,
            0.9653979238754326,
            0.9896193771626297,
            0.9826989619377162,
            0.9965397923875432,
            0.9896193771626297,
            0.9446366782006921,
            0.972318339100346,
            0.9446366782006921,
            0.9792387543252595,
            0.9930795847750865,
            0.986159169550173,
            0.9930795847750865
        ],
        "reward_history": [
            -2890,
            -2362,
            -2890,
            -2890,
            -1660,
            -2890,
            -2156,
            -1309,
            -2890,
            -2890,
            -1546,
            -2147,
            -2604,
            -1667,
            -1730,
            -860,
            -1548,
            -1939,
            -1411,
            -2240,
            -1339,
            -2622,
            -1275,
            -884,
            -417,
            -965,
            -2478,
            -408,
            -596,
            -373,
            -296,
            -293,
            -625,
            -1011,
            -708,
            -283,
            -1307,
            -554,
            -1081,
            -440,
            -715,
            -514,
            -733,
            -1079,
            -985,
            -1338,
            -1029,
            -1004,
            -869,
            -522
        ],
        "steps_history": [
            2890,
            2463,
            2890,
            2890,
            1761,
            2890,
            2257,
            1410,
            2890,
            2890,
            1647,
            2248,
            2705,
            1768,
            1831,
            961,
            1649,
            2040,
            1512,
            2341,
            1440,
            2723,
            1376,
            985,
            518,
            1066,
            2579,
            509,
            697,
            474,
            397,
            394,
            726,
            1112,
            809,
            384,
            1408,
            655,
            1182,
            541,
            816,
            615,
            834,
            1180,
            1086,
            1439,
            1130,
            1105,
            970,
            623
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "459/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.755007982254028,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 50,
        "policy_stability_history": [
            0.0,
            0.7647058823529411,
            0.9100346020761245,
            0.8235294117647058,
            0.8512110726643599,
            0.916955017301038,
            0.8200692041522492,
            0.889273356401384,
            0.8027681660899654,
            0.8961937716262975,
            0.8477508650519031,
            0.9307958477508651,
            0.8235294117647058,
            0.9100346020761245,
            0.8719723183391004,
            0.8615916955017301,
            0.9377162629757786,
            0.8512110726643599,
            0.9515570934256056,
            0.8235294117647058,
            0.9342560553633218,
            0.8961937716262975,
            0.9065743944636678,
            0.9377162629757786,
            0.9307958477508651,
            0.9688581314878892,
            0.8823529411764706,
            0.9688581314878892,
            0.9377162629757786,
            0.9238754325259516,
            0.9100346020761245,
            0.9688581314878892,
            0.9653979238754326,
            0.9134948096885813,
            0.9619377162629758,
            0.972318339100346,
            0.9653979238754326,
            0.9930795847750865,
            0.9965397923875432,
            0.9515570934256056,
            0.986159169550173,
            0.9965397923875432,
            0.9757785467128027,
            0.9826989619377162,
            0.9619377162629758,
            0.9930795847750865,
            0.9515570934256056,
            0.9965397923875432,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -2890,
            -922,
            -2890,
            -995,
            -2679,
            -2890,
            -2329,
            -643,
            -2049,
            -2890,
            -2039,
            -1783,
            -678,
            -2333,
            -652,
            -2890,
            -661,
            -837,
            -2890,
            -1171,
            -1621,
            -245,
            -2308,
            -371,
            -937,
            -1646,
            -1571,
            -532,
            -613,
            -2115,
            -1412,
            -407,
            -738,
            -1675,
            -435,
            -1846,
            -628,
            -830,
            -737,
            -593,
            -1532,
            -686,
            -1843,
            -1889,
            -837,
            -828,
            -911
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            2890,
            1023,
            2890,
            1096,
            2780,
            2890,
            2430,
            744,
            2150,
            2890,
            2140,
            1884,
            779,
            2434,
            753,
            2890,
            762,
            938,
            2890,
            1272,
            1722,
            346,
            2409,
            472,
            1038,
            1747,
            1672,
            633,
            714,
            2216,
            1513,
            508,
            839,
            1776,
            536,
            1947,
            729,
            931,
            838,
            694,
            1633,
            787,
            1944,
            1990,
            938,
            929,
            1012
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "460/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.69207239151001,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 58,
        "policy_stability_history": [
            0.0,
            0.8823529411764706,
            0.8200692041522492,
            0.7889273356401384,
            0.8477508650519031,
            0.8650519031141869,
            0.8650519031141869,
            0.903114186851211,
            0.8200692041522492,
            0.8408304498269896,
            0.8650519031141869,
            0.8235294117647058,
            0.8477508650519031,
            0.9238754325259516,
            0.8062283737024222,
            0.889273356401384,
            0.9307958477508651,
            0.9307958477508651,
            0.8615916955017301,
            0.8858131487889274,
            0.9204152249134948,
            0.916955017301038,
            0.9446366782006921,
            0.9480968858131488,
            0.9134948096885813,
            0.9446366782006921,
            0.8961937716262975,
            0.9550173010380623,
            0.9584775086505191,
            0.9100346020761245,
            0.8996539792387543,
            0.9411764705882353,
            0.9480968858131488,
            0.9584775086505191,
            0.9342560553633218,
            0.9653979238754326,
            0.9826989619377162,
            0.9757785467128027,
            0.972318339100346,
            0.9515570934256056,
            0.986159169550173,
            0.9446366782006921,
            0.9550173010380623,
            0.9688581314878892,
            0.9792387543252595,
            0.9550173010380623,
            0.9930795847750865,
            0.9515570934256056,
            0.9653979238754326,
            0.9965397923875432,
            0.9965397923875432,
            1.0,
            0.9965397923875432,
            0.9896193771626297,
            0.9965397923875432,
            0.9930795847750865,
            0.9584775086505191,
            1.0,
            1.0
        ],
        "reward_history": [
            -2108,
            -2890,
            -1854,
            -2392,
            -2890,
            -2655,
            -1693,
            -1108,
            -2890,
            -2464,
            -1209,
            -2890,
            -1604,
            -717,
            -2890,
            -1787,
            -802,
            -500,
            -2306,
            -1456,
            -991,
            -2395,
            -697,
            -872,
            -624,
            -600,
            -1158,
            -685,
            -589,
            -1522,
            -893,
            -946,
            -592,
            -566,
            -991,
            -470,
            -153,
            -526,
            -579,
            -1352,
            -236,
            -816,
            -1243,
            -471,
            -280,
            -1356,
            -315,
            -600,
            -840,
            -509,
            -404,
            -397,
            -330,
            -562,
            -270,
            -271,
            -1596,
            -414,
            -619
        ],
        "steps_history": [
            2209,
            2890,
            1955,
            2493,
            2890,
            2756,
            1794,
            1209,
            2890,
            2565,
            1310,
            2890,
            1705,
            818,
            2890,
            1888,
            903,
            601,
            2407,
            1557,
            1092,
            2496,
            798,
            973,
            725,
            701,
            1259,
            786,
            690,
            1623,
            994,
            1047,
            693,
            667,
            1092,
            571,
            254,
            627,
            680,
            1453,
            337,
            917,
            1344,
            572,
            381,
            1457,
            416,
            701,
            941,
            610,
            505,
            498,
            431,
            663,
            371,
            372,
            1697,
            515,
            720
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "461/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.84218716621399,
        "final_policy_stability": 0.9930795847750865,
        "episodes_to_convergence": 53,
        "policy_stability_history": [
            0.0,
            0.8131487889273357,
            0.9065743944636678,
            0.8788927335640139,
            0.8788927335640139,
            0.903114186851211,
            0.7993079584775087,
            0.8823529411764706,
            0.7889273356401384,
            0.8546712802768166,
            0.8961937716262975,
            0.8996539792387543,
            0.8581314878892734,
            0.8858131487889274,
            0.8719723183391004,
            0.8581314878892734,
            0.9134948096885813,
            0.916955017301038,
            0.9307958477508651,
            0.8996539792387543,
            0.8996539792387543,
            0.8961937716262975,
            0.9377162629757786,
            0.9584775086505191,
            0.9688581314878892,
            0.9584775086505191,
            0.9584775086505191,
            0.9653979238754326,
            0.9273356401384083,
            0.9653979238754326,
            0.8615916955017301,
            0.972318339100346,
            0.9480968858131488,
            0.9688581314878892,
            0.9342560553633218,
            0.9826989619377162,
            0.9550173010380623,
            0.972318339100346,
            0.9584775086505191,
            0.9757785467128027,
            0.9411764705882353,
            0.9826989619377162,
            0.9757785467128027,
            0.9584775086505191,
            0.9411764705882353,
            0.9342560553633218,
            0.9273356401384083,
            0.9965397923875432,
            0.972318339100346,
            0.9515570934256056,
            0.9965397923875432,
            0.9619377162629758,
            0.9965397923875432,
            0.9930795847750865
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -1406,
            -2890,
            -2890,
            -2890,
            -1105,
            -2890,
            -1456,
            -782,
            -958,
            -2033,
            -993,
            -2890,
            -2081,
            -1834,
            -1201,
            -726,
            -1080,
            -1042,
            -1453,
            -788,
            -349,
            -623,
            -413,
            -609,
            -569,
            -702,
            -348,
            -2515,
            -587,
            -898,
            -169,
            -966,
            -416,
            -501,
            -606,
            -815,
            -373,
            -1174,
            -374,
            -601,
            -777,
            -762,
            -781,
            -2500,
            -248,
            -401,
            -1162,
            -477,
            -612,
            -453,
            -437
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            1507,
            2890,
            2890,
            2890,
            1206,
            2890,
            1557,
            883,
            1059,
            2134,
            1094,
            2890,
            2182,
            1935,
            1302,
            827,
            1181,
            1143,
            1554,
            889,
            450,
            724,
            514,
            710,
            670,
            803,
            449,
            2616,
            688,
            999,
            270,
            1067,
            517,
            602,
            707,
            916,
            474,
            1275,
            475,
            702,
            878,
            863,
            882,
            2601,
            349,
            502,
            1263,
            578,
            713,
            554,
            538
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "462/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.993358135223389,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 55,
        "policy_stability_history": [
            0.0,
            0.7750865051903114,
            0.8096885813148789,
            0.7958477508650519,
            0.8131487889273357,
            0.889273356401384,
            0.8719723183391004,
            0.8269896193771626,
            0.8788927335640139,
            0.8235294117647058,
            0.8062283737024222,
            0.7923875432525952,
            0.8754325259515571,
            0.8685121107266436,
            0.8235294117647058,
            0.889273356401384,
            0.8754325259515571,
            0.9065743944636678,
            0.8927335640138409,
            0.9273356401384083,
            0.9515570934256056,
            0.972318339100346,
            0.9584775086505191,
            0.8927335640138409,
            0.9515570934256056,
            0.9273356401384083,
            0.9377162629757786,
            0.8858131487889274,
            0.972318339100346,
            0.9134948096885813,
            0.9584775086505191,
            0.9826989619377162,
            0.9688581314878892,
            0.9342560553633218,
            0.9792387543252595,
            0.9480968858131488,
            0.9826989619377162,
            0.9446366782006921,
            0.9100346020761245,
            0.9619377162629758,
            0.9826989619377162,
            0.9757785467128027,
            0.986159169550173,
            0.9688581314878892,
            0.9896193771626297,
            0.9792387543252595,
            0.9584775086505191,
            0.9826989619377162,
            0.972318339100346,
            0.9584775086505191,
            0.9653979238754326,
            0.9757785467128027,
            0.9688581314878892,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -2890,
            -761,
            -1065,
            -2890,
            -816,
            -2555,
            -2890,
            -2246,
            -1036,
            -1372,
            -2890,
            -2202,
            -2890,
            -1545,
            -1365,
            -1345,
            -377,
            -154,
            -546,
            -1298,
            -257,
            -763,
            -688,
            -1724,
            -133,
            -1306,
            -838,
            -438,
            -417,
            -1772,
            -483,
            -691,
            -444,
            -865,
            -922,
            -740,
            -227,
            -623,
            -377,
            -705,
            -367,
            -352,
            -1267,
            -793,
            -886,
            -979,
            -1199,
            -955,
            -965,
            -394,
            -388,
            -273
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            2890,
            862,
            1166,
            2890,
            917,
            2656,
            2890,
            2347,
            1137,
            1473,
            2890,
            2303,
            2890,
            1646,
            1466,
            1446,
            478,
            255,
            647,
            1399,
            358,
            864,
            789,
            1825,
            234,
            1407,
            939,
            539,
            518,
            1873,
            584,
            792,
            545,
            966,
            1023,
            841,
            328,
            724,
            478,
            806,
            468,
            453,
            1368,
            894,
            987,
            1080,
            1300,
            1056,
            1066,
            495,
            489,
            374
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "463/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.521266460418701,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 59,
        "policy_stability_history": [
            0.0,
            0.8754325259515571,
            0.7820069204152249,
            0.8062283737024222,
            0.8373702422145328,
            0.8027681660899654,
            0.9100346020761245,
            0.8269896193771626,
            0.8788927335640139,
            0.8546712802768166,
            0.8408304498269896,
            0.8788927335640139,
            0.8615916955017301,
            0.9238754325259516,
            0.8235294117647058,
            0.903114186851211,
            0.916955017301038,
            0.889273356401384,
            0.8754325259515571,
            0.9480968858131488,
            0.8477508650519031,
            0.8927335640138409,
            0.9238754325259516,
            0.9446366782006921,
            0.9653979238754326,
            0.9480968858131488,
            0.9342560553633218,
            0.9653979238754326,
            0.9065743944636678,
            0.9100346020761245,
            0.9100346020761245,
            0.972318339100346,
            0.9653979238754326,
            0.9688581314878892,
            0.9792387543252595,
            0.9792387543252595,
            0.9792387543252595,
            0.9826989619377162,
            0.9688581314878892,
            0.986159169550173,
            0.986159169550173,
            0.9792387543252595,
            0.9757785467128027,
            0.9550173010380623,
            0.9619377162629758,
            0.9930795847750865,
            0.8961937716262975,
            0.9584775086505191,
            0.9792387543252595,
            0.9965397923875432,
            0.9584775086505191,
            0.9965397923875432,
            0.9896193771626297,
            0.9965397923875432,
            0.9896193771626297,
            0.9653979238754326,
            0.9930795847750865,
            0.9826989619377162,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -1061,
            -2890,
            -2890,
            -2405,
            -2890,
            -913,
            -2325,
            -708,
            -1800,
            -1534,
            -1777,
            -2890,
            -529,
            -2890,
            -2355,
            -930,
            -1360,
            -1774,
            -541,
            -2667,
            -1644,
            -837,
            -1131,
            -454,
            -529,
            -1114,
            -197,
            -1565,
            -1347,
            -1557,
            -585,
            -343,
            -508,
            -216,
            -329,
            -463,
            -176,
            -539,
            -455,
            -227,
            -354,
            -447,
            -734,
            -357,
            -243,
            -2179,
            -654,
            -470,
            -535,
            -1018,
            -670,
            -484,
            -235,
            -711,
            -774,
            -913,
            -299,
            -524,
            -362
        ],
        "steps_history": [
            2890,
            1162,
            2890,
            2890,
            2506,
            2890,
            1014,
            2426,
            809,
            1901,
            1635,
            1878,
            2890,
            630,
            2890,
            2456,
            1031,
            1461,
            1875,
            642,
            2768,
            1745,
            938,
            1232,
            555,
            630,
            1215,
            298,
            1666,
            1448,
            1658,
            686,
            444,
            609,
            317,
            430,
            564,
            277,
            640,
            556,
            328,
            455,
            548,
            835,
            458,
            344,
            2280,
            755,
            571,
            636,
            1119,
            771,
            585,
            336,
            812,
            875,
            1014,
            400,
            625,
            463
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "464/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.51194167137146,
        "final_policy_stability": 0.9930795847750865,
        "episodes_to_convergence": 48,
        "policy_stability_history": [
            0.0,
            0.8166089965397924,
            0.8823529411764706,
            0.8477508650519031,
            0.8235294117647058,
            0.8373702422145328,
            0.8858131487889274,
            0.8823529411764706,
            0.7854671280276817,
            0.8408304498269896,
            0.8685121107266436,
            0.8442906574394463,
            0.8754325259515571,
            0.8581314878892734,
            0.9065743944636678,
            0.9100346020761245,
            0.8927335640138409,
            0.9134948096885813,
            0.8581314878892734,
            0.8927335640138409,
            0.8754325259515571,
            0.8788927335640139,
            0.9446366782006921,
            0.9411764705882353,
            0.8961937716262975,
            0.9688581314878892,
            0.9204152249134948,
            0.9653979238754326,
            0.9411764705882353,
            0.9134948096885813,
            0.9307958477508651,
            0.9584775086505191,
            0.9515570934256056,
            0.972318339100346,
            0.9307958477508651,
            0.9826989619377162,
            0.9826989619377162,
            0.9688581314878892,
            0.9411764705882353,
            0.9619377162629758,
            0.9930795847750865,
            0.9826989619377162,
            0.9446366782006921,
            0.972318339100346,
            0.9653979238754326,
            0.9965397923875432,
            0.9896193771626297,
            0.972318339100346,
            0.9930795847750865
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -1241,
            -2890,
            -2890,
            -1577,
            -1054,
            -2890,
            -1808,
            -913,
            -1785,
            -1472,
            -1352,
            -792,
            -859,
            -1188,
            -1027,
            -2632,
            -736,
            -2259,
            -2365,
            -571,
            -759,
            -2469,
            -331,
            -1006,
            -212,
            -435,
            -891,
            -697,
            -651,
            -846,
            -461,
            -2043,
            -613,
            -352,
            -677,
            -850,
            -885,
            -412,
            -420,
            -1555,
            -744,
            -572,
            -362,
            -258,
            -704,
            -604
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            1342,
            2890,
            2890,
            1678,
            1155,
            2890,
            1909,
            1014,
            1886,
            1573,
            1453,
            893,
            960,
            1289,
            1128,
            2733,
            837,
            2360,
            2466,
            672,
            860,
            2570,
            432,
            1107,
            313,
            536,
            992,
            798,
            752,
            947,
            562,
            2144,
            714,
            453,
            778,
            951,
            986,
            513,
            521,
            1656,
            845,
            673,
            463,
            359,
            805,
            705
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "465/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.047709226608276,
        "final_policy_stability": 0.986159169550173,
        "episodes_to_convergence": 53,
        "policy_stability_history": [
            0.0,
            0.8235294117647058,
            0.8166089965397924,
            0.8304498269896193,
            0.903114186851211,
            0.8235294117647058,
            0.8685121107266436,
            0.8788927335640139,
            0.8200692041522492,
            0.8858131487889274,
            0.8062283737024222,
            0.916955017301038,
            0.8512110726643599,
            0.8442906574394463,
            0.8373702422145328,
            0.8823529411764706,
            0.9065743944636678,
            0.8754325259515571,
            0.9204152249134948,
            0.8996539792387543,
            0.9238754325259516,
            0.9619377162629758,
            0.9273356401384083,
            0.9307958477508651,
            0.9065743944636678,
            0.8719723183391004,
            0.9619377162629758,
            0.9411764705882353,
            0.9619377162629758,
            0.9757785467128027,
            0.9550173010380623,
            0.9653979238754326,
            0.9757785467128027,
            0.9273356401384083,
            0.9826989619377162,
            0.9584775086505191,
            0.9377162629757786,
            0.9792387543252595,
            0.9792387543252595,
            0.9965397923875432,
            0.9826989619377162,
            0.9965397923875432,
            0.9965397923875432,
            0.9757785467128027,
            0.9930795847750865,
            1.0,
            1.0,
            1.0,
            0.9826989619377162,
            0.9965397923875432,
            0.9965397923875432,
            0.9757785467128027,
            0.986159169550173,
            0.986159169550173
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2250,
            -2890,
            -2890,
            -2890,
            -2160,
            -2608,
            -2890,
            -2346,
            -1286,
            -2678,
            -2890,
            -2250,
            -1436,
            -2890,
            -2144,
            -1402,
            -2101,
            -1110,
            -687,
            -2458,
            -1092,
            -1895,
            -2890,
            -577,
            -1407,
            -1365,
            -748,
            -2143,
            -1164,
            -615,
            -2309,
            -842,
            -1368,
            -2656,
            -723,
            -1611,
            -489,
            -864,
            -999,
            -569,
            -2890,
            -511,
            -929,
            -667,
            -638,
            -1289,
            -756,
            -657,
            -1768,
            -2182,
            -1061
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2351,
            2890,
            2890,
            2890,
            2261,
            2709,
            2890,
            2447,
            1387,
            2779,
            2890,
            2351,
            1537,
            2890,
            2245,
            1503,
            2202,
            1211,
            788,
            2559,
            1193,
            1996,
            2890,
            678,
            1508,
            1466,
            849,
            2244,
            1265,
            716,
            2410,
            943,
            1469,
            2757,
            824,
            1712,
            590,
            965,
            1100,
            670,
            2890,
            612,
            1030,
            768,
            739,
            1390,
            857,
            758,
            1869,
            2283,
            1162
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "466/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.687158823013306,
        "final_policy_stability": 0.986159169550173,
        "episodes_to_convergence": 46,
        "policy_stability_history": [
            0.0,
            0.8235294117647058,
            0.7577854671280276,
            0.9342560553633218,
            0.8685121107266436,
            0.8373702422145328,
            0.8512110726643599,
            0.8615916955017301,
            0.8546712802768166,
            0.8615916955017301,
            0.9065743944636678,
            0.9307958477508651,
            0.8719723183391004,
            0.8788927335640139,
            0.916955017301038,
            0.903114186851211,
            0.8754325259515571,
            0.8788927335640139,
            0.8823529411764706,
            0.9065743944636678,
            0.9550173010380623,
            0.889273356401384,
            0.9411764705882353,
            0.8719723183391004,
            0.9377162629757786,
            0.9134948096885813,
            0.9619377162629758,
            0.9273356401384083,
            0.9377162629757786,
            0.8927335640138409,
            0.972318339100346,
            0.9896193771626297,
            0.9273356401384083,
            0.9480968858131488,
            0.9550173010380623,
            0.9377162629757786,
            0.9688581314878892,
            0.9930795847750865,
            0.9965397923875432,
            0.986159169550173,
            0.9584775086505191,
            0.972318339100346,
            0.9965397923875432,
            0.9792387543252595,
            0.9619377162629758,
            0.9896193771626297,
            0.986159169550173
        ],
        "reward_history": [
            -1046,
            -2890,
            -2890,
            -448,
            -2890,
            -2890,
            -2890,
            -2731,
            -1937,
            -1893,
            -2890,
            -976,
            -2018,
            -2890,
            -1391,
            -2890,
            -2745,
            -2890,
            -2890,
            -2203,
            -494,
            -2493,
            -1399,
            -2890,
            -975,
            -1589,
            -725,
            -1577,
            -1008,
            -2890,
            -964,
            -207,
            -1937,
            -1461,
            -2522,
            -1796,
            -1255,
            -303,
            -644,
            -771,
            -1344,
            -718,
            -367,
            -1174,
            -2708,
            -1920,
            -536
        ],
        "steps_history": [
            1147,
            2890,
            2890,
            549,
            2890,
            2890,
            2890,
            2832,
            2038,
            1994,
            2890,
            1077,
            2119,
            2890,
            1492,
            2890,
            2846,
            2890,
            2890,
            2304,
            595,
            2594,
            1500,
            2890,
            1076,
            1690,
            826,
            1678,
            1109,
            2890,
            1065,
            308,
            2038,
            1562,
            2623,
            1897,
            1356,
            404,
            745,
            872,
            1445,
            819,
            468,
            1275,
            2809,
            2021,
            637
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "467/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.529473781585693,
        "final_policy_stability": 0.9792387543252595,
        "episodes_to_convergence": 46,
        "policy_stability_history": [
            0.0,
            0.8823529411764706,
            0.8235294117647058,
            0.9204152249134948,
            0.8754325259515571,
            0.8719723183391004,
            0.9100346020761245,
            0.8512110726643599,
            0.8512110726643599,
            0.8788927335640139,
            0.8442906574394463,
            0.8719723183391004,
            0.8685121107266436,
            0.8650519031141869,
            0.9307958477508651,
            0.8650519031141869,
            0.8961937716262975,
            0.9273356401384083,
            0.9480968858131488,
            0.9550173010380623,
            0.916955017301038,
            0.9307958477508651,
            0.9342560553633218,
            0.9792387543252595,
            0.986159169550173,
            0.903114186851211,
            0.9377162629757786,
            0.9896193771626297,
            0.986159169550173,
            0.9757785467128027,
            0.9515570934256056,
            0.9792387543252595,
            0.9411764705882353,
            0.9619377162629758,
            0.9792387543252595,
            0.9896193771626297,
            0.9792387543252595,
            0.9377162629757786,
            1.0,
            0.9550173010380623,
            0.986159169550173,
            0.9965397923875432,
            0.986159169550173,
            0.9792387543252595,
            0.9826989619377162,
            0.9930795847750865,
            0.9792387543252595
        ],
        "reward_history": [
            -2890,
            -2890,
            -2075,
            -1433,
            -2890,
            -2029,
            -2644,
            -2890,
            -2890,
            -1870,
            -2890,
            -1431,
            -1763,
            -2890,
            -1664,
            -2890,
            -2502,
            -1291,
            -1031,
            -774,
            -972,
            -1512,
            -2321,
            -391,
            -595,
            -2214,
            -2140,
            -458,
            -367,
            -818,
            -915,
            -746,
            -1971,
            -809,
            -695,
            -525,
            -1119,
            -2890,
            -465,
            -1450,
            -782,
            -629,
            -1014,
            -836,
            -1289,
            -1632,
            -592
        ],
        "steps_history": [
            2890,
            2890,
            2176,
            1534,
            2890,
            2130,
            2745,
            2890,
            2890,
            1971,
            2890,
            1532,
            1864,
            2890,
            1765,
            2890,
            2603,
            1392,
            1132,
            875,
            1073,
            1613,
            2422,
            492,
            696,
            2315,
            2241,
            559,
            468,
            919,
            1016,
            847,
            2072,
            910,
            796,
            626,
            1220,
            2890,
            566,
            1551,
            883,
            730,
            1115,
            937,
            1390,
            1733,
            693
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "468/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.75620436668396,
        "final_policy_stability": 0.9792387543252595,
        "episodes_to_convergence": 47,
        "policy_stability_history": [
            0.0,
            0.7716262975778547,
            0.889273356401384,
            0.8304498269896193,
            0.8754325259515571,
            0.8477508650519031,
            0.9238754325259516,
            0.8927335640138409,
            0.8685121107266436,
            0.8961937716262975,
            0.8961937716262975,
            0.8927335640138409,
            0.9204152249134948,
            0.8719723183391004,
            0.8581314878892734,
            0.9342560553633218,
            0.8788927335640139,
            0.903114186851211,
            0.8961937716262975,
            0.9688581314878892,
            0.8996539792387543,
            0.9480968858131488,
            0.889273356401384,
            0.9342560553633218,
            0.9100346020761245,
            0.9515570934256056,
            0.9238754325259516,
            0.8996539792387543,
            0.9619377162629758,
            0.8823529411764706,
            0.9515570934256056,
            0.972318339100346,
            0.9515570934256056,
            0.9273356401384083,
            0.9204152249134948,
            0.9480968858131488,
            0.986159169550173,
            0.9688581314878892,
            0.986159169550173,
            0.9930795847750865,
            0.9826989619377162,
            0.9480968858131488,
            0.9584775086505191,
            0.9930795847750865,
            0.9965397923875432,
            0.9930795847750865,
            0.9826989619377162,
            0.9792387543252595
        ],
        "reward_history": [
            -2890,
            -1798,
            -2890,
            -2890,
            -2890,
            -2605,
            -2890,
            -2890,
            -2557,
            -2285,
            -1892,
            -2890,
            -913,
            -2890,
            -2737,
            -997,
            -2154,
            -1641,
            -1502,
            -368,
            -1516,
            -587,
            -1384,
            -938,
            -1936,
            -575,
            -1329,
            -1987,
            -752,
            -2377,
            -787,
            -1177,
            -813,
            -2390,
            -1119,
            -998,
            -635,
            -585,
            -743,
            -427,
            -866,
            -2890,
            -2083,
            -1650,
            -622,
            -557,
            -1262,
            -2286
        ],
        "steps_history": [
            2890,
            1899,
            2890,
            2890,
            2890,
            2706,
            2890,
            2890,
            2658,
            2386,
            1993,
            2890,
            1014,
            2890,
            2838,
            1098,
            2255,
            1742,
            1603,
            469,
            1617,
            688,
            1485,
            1039,
            2037,
            676,
            1430,
            2088,
            853,
            2478,
            888,
            1278,
            914,
            2491,
            1220,
            1099,
            736,
            686,
            844,
            528,
            967,
            2890,
            2184,
            1751,
            723,
            658,
            1363,
            2387
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "469/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.746249437332153,
        "final_policy_stability": 0.9792387543252595,
        "episodes_to_convergence": 47,
        "policy_stability_history": [
            0.0,
            0.8650519031141869,
            0.8754325259515571,
            0.8927335640138409,
            0.8996539792387543,
            0.8442906574394463,
            0.8823529411764706,
            0.9307958477508651,
            0.9411764705882353,
            0.889273356401384,
            0.8200692041522492,
            0.8615916955017301,
            0.9065743944636678,
            0.8546712802768166,
            0.889273356401384,
            0.8650519031141869,
            0.8339100346020761,
            0.9065743944636678,
            0.8754325259515571,
            0.8546712802768166,
            0.916955017301038,
            0.8788927335640139,
            0.9619377162629758,
            0.9100346020761245,
            0.9653979238754326,
            0.9134948096885813,
            0.9446366782006921,
            0.8685121107266436,
            0.9550173010380623,
            0.9653979238754326,
            0.8996539792387543,
            0.9377162629757786,
            0.9619377162629758,
            0.972318339100346,
            0.9446366782006921,
            0.9446366782006921,
            0.9757785467128027,
            0.972318339100346,
            0.9688581314878892,
            0.972318339100346,
            0.9446366782006921,
            0.9757785467128027,
            0.986159169550173,
            0.986159169550173,
            0.9688581314878892,
            0.9826989619377162,
            1.0,
            0.9792387543252595
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -785,
            -2890,
            -2890,
            -1448,
            -845,
            -439,
            -1080,
            -2890,
            -2357,
            -2106,
            -2890,
            -1803,
            -1735,
            -2890,
            -1091,
            -2377,
            -2481,
            -2690,
            -1716,
            -1198,
            -2574,
            -1259,
            -1350,
            -1294,
            -2890,
            -440,
            -1224,
            -2092,
            -2890,
            -1012,
            -858,
            -1100,
            -817,
            -673,
            -1034,
            -744,
            -322,
            -1574,
            -1422,
            -675,
            -477,
            -1665,
            -1586,
            -771,
            -621
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            886,
            2890,
            2890,
            1549,
            946,
            540,
            1181,
            2890,
            2458,
            2207,
            2890,
            1904,
            1836,
            2890,
            1192,
            2478,
            2582,
            2791,
            1817,
            1299,
            2675,
            1360,
            1451,
            1395,
            2890,
            541,
            1325,
            2193,
            2890,
            1113,
            959,
            1201,
            918,
            774,
            1135,
            845,
            423,
            1675,
            1523,
            776,
            578,
            1766,
            1687,
            872,
            722
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "470/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.328595161437988,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 53,
        "policy_stability_history": [
            0.0,
            0.7820069204152249,
            0.8027681660899654,
            0.8408304498269896,
            0.8373702422145328,
            0.8235294117647058,
            0.8685121107266436,
            0.8719723183391004,
            0.8512110726643599,
            0.8166089965397924,
            0.903114186851211,
            0.8269896193771626,
            0.8615916955017301,
            0.916955017301038,
            0.9342560553633218,
            0.903114186851211,
            0.8685121107266436,
            0.8442906574394463,
            0.9515570934256056,
            0.8961937716262975,
            0.9757785467128027,
            0.903114186851211,
            0.8858131487889274,
            0.8685121107266436,
            0.903114186851211,
            0.8961937716262975,
            0.9619377162629758,
            0.9134948096885813,
            0.8996539792387543,
            0.916955017301038,
            0.9204152249134948,
            0.9757785467128027,
            0.9688581314878892,
            0.9446366782006921,
            0.9584775086505191,
            0.9792387543252595,
            0.9757785467128027,
            0.9619377162629758,
            0.9653979238754326,
            0.9930795847750865,
            0.9688581314878892,
            0.9204152249134948,
            0.9965397923875432,
            0.9826989619377162,
            0.986159169550173,
            0.9826989619377162,
            0.9653979238754326,
            0.9896193771626297,
            1.0,
            0.9515570934256056,
            0.9930795847750865,
            1.0,
            0.9757785467128027,
            1.0
        ],
        "reward_history": [
            -1527,
            -2890,
            -2890,
            -2890,
            -2302,
            -2890,
            -2890,
            -1652,
            -1832,
            -2890,
            -1497,
            -2890,
            -1949,
            -775,
            -992,
            -1675,
            -2488,
            -1400,
            -548,
            -2079,
            -405,
            -1629,
            -1779,
            -2890,
            -1376,
            -1452,
            -671,
            -2008,
            -2003,
            -897,
            -1371,
            -818,
            -313,
            -1072,
            -793,
            -597,
            -1424,
            -1793,
            -923,
            -578,
            -520,
            -1589,
            -592,
            -808,
            -821,
            -470,
            -629,
            -400,
            -738,
            -982,
            -936,
            -269,
            -1431,
            -659
        ],
        "steps_history": [
            1628,
            2890,
            2890,
            2890,
            2403,
            2890,
            2890,
            1753,
            1933,
            2890,
            1598,
            2890,
            2050,
            876,
            1093,
            1776,
            2589,
            1501,
            649,
            2180,
            506,
            1730,
            1880,
            2890,
            1477,
            1553,
            772,
            2109,
            2104,
            998,
            1472,
            919,
            414,
            1173,
            894,
            698,
            1525,
            1894,
            1024,
            679,
            621,
            1690,
            693,
            909,
            922,
            571,
            730,
            501,
            839,
            1083,
            1037,
            370,
            1532,
            760
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "471/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.138986110687256,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 55,
        "policy_stability_history": [
            0.0,
            0.8062283737024222,
            0.7612456747404844,
            0.8339100346020761,
            0.8719723183391004,
            0.7958477508650519,
            0.8685121107266436,
            0.8823529411764706,
            0.8096885813148789,
            0.8581314878892734,
            0.8581314878892734,
            0.9134948096885813,
            0.9342560553633218,
            0.8961937716262975,
            0.9134948096885813,
            0.8615916955017301,
            0.9480968858131488,
            0.9446366782006921,
            0.9619377162629758,
            0.9515570934256056,
            0.9792387543252595,
            0.9792387543252595,
            0.9653979238754326,
            0.9446366782006921,
            0.9446366782006921,
            0.8996539792387543,
            0.986159169550173,
            0.9480968858131488,
            0.9515570934256056,
            0.9377162629757786,
            0.8823529411764706,
            0.9896193771626297,
            0.9411764705882353,
            0.9515570934256056,
            0.972318339100346,
            0.986159169550173,
            0.9377162629757786,
            0.9446366782006921,
            0.9826989619377162,
            0.9757785467128027,
            0.9619377162629758,
            0.986159169550173,
            0.972318339100346,
            0.9965397923875432,
            0.9792387543252595,
            0.9965397923875432,
            0.9411764705882353,
            0.9965397923875432,
            1.0,
            1.0,
            0.9965397923875432,
            0.9757785467128027,
            0.9792387543252595,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -1867,
            -2890,
            -1786,
            -2890,
            -2561,
            -2890,
            -2890,
            -1393,
            -2637,
            -1983,
            -2210,
            -1346,
            -628,
            -1386,
            -2890,
            -2677,
            -1019,
            -1058,
            -390,
            -933,
            -129,
            -229,
            -1254,
            -1132,
            -666,
            -1835,
            -186,
            -1097,
            -653,
            -984,
            -2363,
            -569,
            -2243,
            -1809,
            -492,
            -469,
            -2043,
            -997,
            -421,
            -2258,
            -1090,
            -920,
            -913,
            -244,
            -800,
            -472,
            -1613,
            -504,
            -275,
            -188,
            -520,
            -1349,
            -1283,
            -469,
            -279,
            -392
        ],
        "steps_history": [
            1968,
            2890,
            1887,
            2890,
            2662,
            2890,
            2890,
            1494,
            2738,
            2084,
            2311,
            1447,
            729,
            1487,
            2890,
            2778,
            1120,
            1159,
            491,
            1034,
            230,
            330,
            1355,
            1233,
            767,
            1936,
            287,
            1198,
            754,
            1085,
            2464,
            670,
            2344,
            1910,
            593,
            570,
            2144,
            1098,
            522,
            2359,
            1191,
            1021,
            1014,
            345,
            901,
            573,
            1714,
            605,
            376,
            289,
            621,
            1450,
            1384,
            570,
            380,
            493
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "472/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.011560916900635,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 51,
        "policy_stability_history": [
            0.0,
            0.8858131487889274,
            0.7716262975778547,
            0.8304498269896193,
            0.8477508650519031,
            0.8546712802768166,
            0.8304498269896193,
            0.9065743944636678,
            0.8546712802768166,
            0.8546712802768166,
            0.8581314878892734,
            0.8996539792387543,
            0.8512110726643599,
            0.9411764705882353,
            0.8546712802768166,
            0.972318339100346,
            0.8304498269896193,
            0.889273356401384,
            0.9238754325259516,
            0.9515570934256056,
            0.8615916955017301,
            0.9342560553633218,
            0.9480968858131488,
            0.9619377162629758,
            0.889273356401384,
            0.9619377162629758,
            0.9584775086505191,
            0.9273356401384083,
            0.9342560553633218,
            0.9446366782006921,
            0.9307958477508651,
            0.9550173010380623,
            0.9584775086505191,
            0.9273356401384083,
            0.9826989619377162,
            0.986159169550173,
            0.9307958477508651,
            0.972318339100346,
            0.9584775086505191,
            0.986159169550173,
            0.9792387543252595,
            0.986159169550173,
            0.9653979238754326,
            0.986159169550173,
            0.9584775086505191,
            0.9965397923875432,
            0.972318339100346,
            0.9896193771626297,
            0.9204152249134948,
            0.9896193771626297,
            0.9930795847750865,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2110,
            -2890,
            -2890,
            -2617,
            -575,
            -2890,
            -2890,
            -2707,
            -1018,
            -1335,
            -554,
            -2558,
            -208,
            -2890,
            -2890,
            -1377,
            -973,
            -2890,
            -1138,
            -496,
            -598,
            -1790,
            -482,
            -401,
            -1111,
            -1206,
            -802,
            -1196,
            -681,
            -908,
            -2424,
            -619,
            -340,
            -845,
            -947,
            -1150,
            -603,
            -1132,
            -863,
            -1339,
            -784,
            -1010,
            -174,
            -608,
            -1172,
            -2594,
            -314,
            -627,
            -679
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2211,
            2890,
            2890,
            2718,
            676,
            2890,
            2890,
            2808,
            1119,
            1436,
            655,
            2659,
            309,
            2890,
            2890,
            1478,
            1074,
            2890,
            1239,
            597,
            699,
            1891,
            583,
            502,
            1212,
            1307,
            903,
            1297,
            782,
            1009,
            2525,
            720,
            441,
            946,
            1048,
            1251,
            704,
            1233,
            964,
            1440,
            885,
            1111,
            275,
            709,
            1273,
            2695,
            415,
            728,
            780
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "473/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.407355070114136,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 46,
        "policy_stability_history": [
            0.0,
            0.7750865051903114,
            0.9411764705882353,
            0.7923875432525952,
            0.8615916955017301,
            0.8096885813148789,
            0.8166089965397924,
            0.8408304498269896,
            0.8961937716262975,
            0.9377162629757786,
            0.8961937716262975,
            0.8961937716262975,
            0.8927335640138409,
            0.8961937716262975,
            0.8304498269896193,
            0.8858131487889274,
            0.8961937716262975,
            0.9204152249134948,
            0.9342560553633218,
            0.9204152249134948,
            0.9238754325259516,
            0.8996539792387543,
            0.9273356401384083,
            0.9411764705882353,
            0.9342560553633218,
            0.9826989619377162,
            0.9757785467128027,
            0.9792387543252595,
            0.9411764705882353,
            0.9757785467128027,
            0.9619377162629758,
            0.9688581314878892,
            0.9965397923875432,
            0.9792387543252595,
            0.9688581314878892,
            0.9792387543252595,
            0.972318339100346,
            0.9896193771626297,
            0.9653979238754326,
            0.9653979238754326,
            0.9965397923875432,
            0.9896193771626297,
            0.9965397923875432,
            0.9619377162629758,
            0.9757785467128027,
            0.9896193771626297,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -2362,
            -2890,
            -2890,
            -807,
            -2268,
            -2237,
            -2305,
            -1123,
            -349,
            -2890,
            -2113,
            -991,
            -949,
            -2890,
            -2602,
            -1334,
            -1482,
            -1122,
            -1096,
            -1979,
            -2407,
            -1672,
            -819,
            -1329,
            -487,
            -376,
            -490,
            -1402,
            -536,
            -542,
            -978,
            -222,
            -452,
            -818,
            -596,
            -649,
            -414,
            -750,
            -1149,
            -445,
            -732,
            -858,
            -554,
            -867,
            -654,
            -715
        ],
        "steps_history": [
            2890,
            2463,
            2890,
            2890,
            908,
            2369,
            2338,
            2406,
            1224,
            450,
            2890,
            2214,
            1092,
            1050,
            2890,
            2703,
            1435,
            1583,
            1223,
            1197,
            2080,
            2508,
            1773,
            920,
            1430,
            588,
            477,
            591,
            1503,
            637,
            643,
            1079,
            323,
            553,
            919,
            697,
            750,
            515,
            851,
            1250,
            546,
            833,
            959,
            655,
            968,
            755,
            816
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "474/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.889581203460693,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 55,
        "policy_stability_history": [
            0.0,
            0.7612456747404844,
            0.9100346020761245,
            0.8200692041522492,
            0.8719723183391004,
            0.8823529411764706,
            0.8512110726643599,
            0.8996539792387543,
            0.8408304498269896,
            0.889273356401384,
            0.9065743944636678,
            0.8685121107266436,
            0.8408304498269896,
            0.9480968858131488,
            0.8546712802768166,
            0.8823529411764706,
            0.8408304498269896,
            0.8754325259515571,
            0.889273356401384,
            0.8996539792387543,
            0.916955017301038,
            0.9204152249134948,
            0.9446366782006921,
            0.9100346020761245,
            0.9238754325259516,
            0.889273356401384,
            0.9134948096885813,
            0.8927335640138409,
            0.9619377162629758,
            0.9204152249134948,
            0.9411764705882353,
            0.9377162629757786,
            0.9307958477508651,
            0.986159169550173,
            0.9792387543252595,
            0.9896193771626297,
            0.986159169550173,
            0.9411764705882353,
            0.9930795847750865,
            0.9826989619377162,
            0.9515570934256056,
            0.9411764705882353,
            0.9896193771626297,
            0.986159169550173,
            0.9930795847750865,
            0.9930795847750865,
            0.9896193771626297,
            0.9930795847750865,
            0.9826989619377162,
            0.9826989619377162,
            0.986159169550173,
            0.9619377162629758,
            0.9965397923875432,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -2890,
            -1026,
            -1449,
            -1038,
            -2890,
            -2890,
            -1048,
            -2358,
            -1469,
            -702,
            -1739,
            -1478,
            -2890,
            -1480,
            -2890,
            -948,
            -1426,
            -1369,
            -772,
            -1460,
            -1367,
            -1629,
            -2106,
            -2890,
            -640,
            -2440,
            -1873,
            -995,
            -1225,
            -184,
            -672,
            -148,
            -266,
            -879,
            -318,
            -397,
            -1729,
            -1411,
            -628,
            -830,
            -737,
            -417,
            -660,
            -338,
            -508,
            -686,
            -766,
            -976,
            -1889,
            -837,
            -828,
            -911
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            2890,
            1127,
            1550,
            1139,
            2890,
            2890,
            1149,
            2459,
            1570,
            803,
            1840,
            1579,
            2890,
            1581,
            2890,
            1049,
            1527,
            1470,
            873,
            1561,
            1468,
            1730,
            2207,
            2890,
            741,
            2541,
            1974,
            1096,
            1326,
            285,
            773,
            249,
            367,
            980,
            419,
            498,
            1830,
            1512,
            729,
            931,
            838,
            518,
            761,
            439,
            609,
            787,
            867,
            1077,
            1990,
            938,
            929,
            1012
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "475/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.544859647750854,
        "final_policy_stability": 0.9584775086505191,
        "episodes_to_convergence": 54,
        "policy_stability_history": [
            0.0,
            0.7716262975778547,
            0.8408304498269896,
            0.8408304498269896,
            0.889273356401384,
            0.8304498269896193,
            0.8442906574394463,
            0.8719723183391004,
            0.8235294117647058,
            0.8477508650519031,
            0.8615916955017301,
            0.8581314878892734,
            0.8927335640138409,
            0.8581314878892734,
            0.9480968858131488,
            0.9100346020761245,
            0.8615916955017301,
            0.8858131487889274,
            0.9307958477508651,
            0.9238754325259516,
            0.9307958477508651,
            0.9342560553633218,
            0.9446366782006921,
            0.9411764705882353,
            0.986159169550173,
            0.9550173010380623,
            0.972318339100346,
            0.9411764705882353,
            0.9134948096885813,
            0.9792387543252595,
            0.9792387543252595,
            0.9377162629757786,
            0.9204152249134948,
            0.9480968858131488,
            0.9792387543252595,
            0.9515570934256056,
            0.9792387543252595,
            0.9377162629757786,
            1.0,
            0.972318339100346,
            0.9446366782006921,
            0.9792387543252595,
            0.9930795847750865,
            0.986159169550173,
            0.9965397923875432,
            0.9550173010380623,
            0.9792387543252595,
            0.9965397923875432,
            0.9896193771626297,
            0.9965397923875432,
            0.972318339100346,
            1.0,
            1.0,
            1.0,
            0.9584775086505191
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -1124,
            -1880,
            -1853,
            -1905,
            -2345,
            -2890,
            -2890,
            -2890,
            -1283,
            -2486,
            -512,
            -922,
            -2551,
            -2890,
            -1740,
            -864,
            -1647,
            -563,
            -984,
            -566,
            -271,
            -868,
            -213,
            -940,
            -1899,
            -376,
            -353,
            -1279,
            -1409,
            -1129,
            -579,
            -1111,
            -477,
            -872,
            -292,
            -794,
            -852,
            -439,
            -292,
            -469,
            -269,
            -783,
            -657,
            -137,
            -437,
            -376,
            -607,
            -474,
            -441,
            -271,
            -2111
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            1225,
            1981,
            1954,
            2006,
            2446,
            2890,
            2890,
            2890,
            1384,
            2587,
            613,
            1023,
            2652,
            2890,
            1841,
            965,
            1748,
            664,
            1085,
            667,
            372,
            969,
            314,
            1041,
            2000,
            477,
            454,
            1380,
            1510,
            1230,
            680,
            1212,
            578,
            973,
            393,
            895,
            953,
            540,
            393,
            570,
            370,
            884,
            758,
            238,
            538,
            477,
            708,
            575,
            542,
            372,
            2212
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "476/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.441203355789185,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 61,
        "policy_stability_history": [
            0.0,
            0.8166089965397924,
            0.9134948096885813,
            0.916955017301038,
            0.8512110726643599,
            0.8719723183391004,
            0.8477508650519031,
            0.8927335640138409,
            0.8512110726643599,
            0.8200692041522492,
            0.9584775086505191,
            0.8408304498269896,
            0.8858131487889274,
            0.903114186851211,
            0.9134948096885813,
            0.8858131487889274,
            0.9550173010380623,
            0.8650519031141869,
            0.9100346020761245,
            0.9411764705882353,
            0.9515570934256056,
            0.9515570934256056,
            0.8442906574394463,
            0.9307958477508651,
            0.9619377162629758,
            0.9342560553633218,
            0.9204152249134948,
            0.9307958477508651,
            0.9619377162629758,
            0.9619377162629758,
            0.9965397923875432,
            0.9896193771626297,
            0.9307958477508651,
            0.9377162629757786,
            0.9792387543252595,
            0.9653979238754326,
            0.9550173010380623,
            0.9446366782006921,
            0.9757785467128027,
            0.9619377162629758,
            0.9757785467128027,
            0.9792387543252595,
            0.9688581314878892,
            0.9930795847750865,
            0.9930795847750865,
            0.986159169550173,
            0.9550173010380623,
            0.9688581314878892,
            0.9930795847750865,
            0.9619377162629758,
            0.9688581314878892,
            0.9619377162629758,
            0.9965397923875432,
            0.9965397923875432,
            0.9930795847750865,
            1.0,
            1.0,
            0.9965397923875432,
            0.9896193771626297,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -2890,
            -951,
            -1378,
            -1190,
            -2890,
            -2890,
            -549,
            -2585,
            -1698,
            -1018,
            -2890,
            -2890,
            -379,
            -2890,
            -907,
            -217,
            -443,
            -357,
            -2890,
            -1196,
            -569,
            -1071,
            -1200,
            -756,
            -949,
            -1074,
            -169,
            -397,
            -1587,
            -606,
            -815,
            -414,
            -731,
            -776,
            -601,
            -868,
            -178,
            -392,
            -426,
            -254,
            -244,
            -333,
            -1721,
            -694,
            -151,
            -1770,
            -922,
            -1115,
            -457,
            -253,
            -303,
            -289,
            -324,
            -1123,
            -324,
            -625,
            -361,
            -415
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            2890,
            1052,
            1479,
            1291,
            2890,
            2890,
            650,
            2686,
            1799,
            1119,
            2890,
            2890,
            480,
            2890,
            1008,
            318,
            544,
            458,
            2890,
            1297,
            670,
            1172,
            1301,
            857,
            1050,
            1175,
            270,
            498,
            1688,
            707,
            916,
            515,
            832,
            877,
            702,
            969,
            279,
            493,
            527,
            355,
            345,
            434,
            1822,
            795,
            252,
            1871,
            1023,
            1216,
            558,
            354,
            404,
            390,
            425,
            1224,
            425,
            726,
            462,
            516
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "477/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.075134515762329,
        "final_policy_stability": 0.9930795847750865,
        "episodes_to_convergence": 52,
        "policy_stability_history": [
            0.0,
            0.7785467128027682,
            0.8027681660899654,
            0.8477508650519031,
            0.8200692041522492,
            0.8062283737024222,
            0.7958477508650519,
            0.8512110726643599,
            0.9342560553633218,
            0.8339100346020761,
            0.8719723183391004,
            0.9273356401384083,
            0.8269896193771626,
            0.8788927335640139,
            0.9204152249134948,
            0.9100346020761245,
            0.8581314878892734,
            0.8788927335640139,
            0.9550173010380623,
            0.8339100346020761,
            0.9480968858131488,
            0.8961937716262975,
            0.9100346020761245,
            0.9307958477508651,
            0.916955017301038,
            0.9273356401384083,
            0.9100346020761245,
            0.889273356401384,
            0.9377162629757786,
            0.9273356401384083,
            0.9757785467128027,
            0.9273356401384083,
            0.9480968858131488,
            0.9446366782006921,
            0.9653979238754326,
            0.8996539792387543,
            0.9688581314878892,
            0.9757785467128027,
            0.9757785467128027,
            0.9065743944636678,
            0.9688581314878892,
            0.9515570934256056,
            0.9688581314878892,
            0.986159169550173,
            0.9792387543252595,
            0.9619377162629758,
            0.972318339100346,
            0.9930795847750865,
            0.9965397923875432,
            0.9584775086505191,
            0.9965397923875432,
            0.9792387543252595,
            0.9930795847750865
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -1579,
            -1725,
            -2890,
            -2890,
            -1763,
            -2890,
            -2553,
            -1806,
            -718,
            -2152,
            -1357,
            -711,
            -1189,
            -2079,
            -1758,
            -665,
            -2890,
            -349,
            -1290,
            -521,
            -773,
            -1523,
            -1226,
            -1381,
            -2326,
            -728,
            -680,
            -570,
            -1647,
            -456,
            -448,
            -465,
            -1867,
            -740,
            -227,
            -623,
            -1183,
            -470,
            -902,
            -614,
            -793,
            -886,
            -1414,
            -1066,
            -197,
            -355,
            -965,
            -394,
            -726,
            -704
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            1680,
            1826,
            2890,
            2890,
            1864,
            2890,
            2654,
            1907,
            819,
            2253,
            1458,
            812,
            1290,
            2180,
            1859,
            766,
            2890,
            450,
            1391,
            622,
            874,
            1624,
            1327,
            1482,
            2427,
            829,
            781,
            671,
            1748,
            557,
            549,
            566,
            1968,
            841,
            328,
            724,
            1284,
            571,
            1003,
            715,
            894,
            987,
            1515,
            1167,
            298,
            456,
            1066,
            495,
            827,
            805
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "478/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.557207345962524,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 49,
        "policy_stability_history": [
            0.0,
            0.8823529411764706,
            0.7958477508650519,
            0.8858131487889274,
            0.7923875432525952,
            0.8339100346020761,
            0.8304498269896193,
            0.8685121107266436,
            0.8339100346020761,
            0.8754325259515571,
            0.8719723183391004,
            0.8788927335640139,
            0.9307958477508651,
            0.8512110726643599,
            0.9134948096885813,
            0.889273356401384,
            0.9377162629757786,
            0.9584775086505191,
            0.8719723183391004,
            0.8961937716262975,
            0.8546712802768166,
            0.8858131487889274,
            0.9100346020761245,
            0.9688581314878892,
            0.9515570934256056,
            0.9688581314878892,
            0.9273356401384083,
            0.8858131487889274,
            0.9273356401384083,
            0.9515570934256056,
            0.9134948096885813,
            0.9134948096885813,
            0.9480968858131488,
            0.9619377162629758,
            0.9584775086505191,
            0.9446366782006921,
            0.9792387543252595,
            0.986159169550173,
            0.9826989619377162,
            0.9688581314878892,
            0.9584775086505191,
            0.9584775086505191,
            0.9965397923875432,
            0.9792387543252595,
            0.9757785467128027,
            0.9965397923875432,
            0.9896193771626297,
            0.9965397923875432,
            0.9653979238754326,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -1061,
            -2890,
            -1008,
            -2890,
            -2890,
            -2170,
            -2541,
            -2115,
            -1172,
            -1593,
            -1983,
            -647,
            -2748,
            -1154,
            -1176,
            -1053,
            -362,
            -1937,
            -2081,
            -2182,
            -1217,
            -710,
            -219,
            -809,
            -497,
            -1131,
            -1084,
            -1347,
            -841,
            -2136,
            -1875,
            -1417,
            -479,
            -726,
            -623,
            -455,
            -227,
            -479,
            -322,
            -734,
            -599,
            -108,
            -659,
            -540,
            -131,
            -439,
            -253,
            -1069,
            -337
        ],
        "steps_history": [
            2890,
            1162,
            2890,
            1109,
            2890,
            2890,
            2271,
            2642,
            2216,
            1273,
            1694,
            2084,
            748,
            2849,
            1255,
            1277,
            1154,
            463,
            2038,
            2182,
            2283,
            1318,
            811,
            320,
            910,
            598,
            1232,
            1185,
            1448,
            942,
            2237,
            1976,
            1518,
            580,
            827,
            724,
            556,
            328,
            580,
            423,
            835,
            700,
            209,
            760,
            641,
            232,
            540,
            354,
            1170,
            438
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "479/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.933948040008545,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 51,
        "policy_stability_history": [
            0.0,
            0.8200692041522492,
            0.8823529411764706,
            0.8477508650519031,
            0.8235294117647058,
            0.8096885813148789,
            0.8477508650519031,
            0.8373702422145328,
            0.8304498269896193,
            0.8788927335640139,
            0.9342560553633218,
            0.8096885813148789,
            0.8754325259515571,
            0.9377162629757786,
            0.8961937716262975,
            0.8858131487889274,
            0.8719723183391004,
            0.8477508650519031,
            0.9446366782006921,
            0.8685121107266436,
            0.889273356401384,
            0.986159169550173,
            0.8754325259515571,
            0.9515570934256056,
            0.9377162629757786,
            0.9619377162629758,
            0.9100346020761245,
            0.9688581314878892,
            0.9100346020761245,
            0.9550173010380623,
            0.9757785467128027,
            0.9273356401384083,
            0.9550173010380623,
            0.9792387543252595,
            0.9411764705882353,
            0.9757785467128027,
            0.9757785467128027,
            0.9584775086505191,
            0.9826989619377162,
            0.9826989619377162,
            0.986159169550173,
            0.9792387543252595,
            0.9584775086505191,
            0.9930795847750865,
            0.9965397923875432,
            0.9965397923875432,
            0.986159169550173,
            0.9792387543252595,
            1.0,
            0.9965397923875432,
            0.9930795847750865,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -1241,
            -2374,
            -2890,
            -1072,
            -1974,
            -2156,
            -1348,
            -389,
            -2890,
            -1797,
            -450,
            -1560,
            -1410,
            -1312,
            -2029,
            -784,
            -1784,
            -2890,
            -136,
            -1815,
            -759,
            -834,
            -621,
            -933,
            -378,
            -1873,
            -1020,
            -538,
            -921,
            -632,
            -434,
            -1971,
            -500,
            -352,
            -1467,
            -588,
            -481,
            -288,
            -465,
            -2158,
            -374,
            -294,
            -362,
            -258,
            -704,
            -604,
            -563,
            -398,
            -420
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            1342,
            2475,
            2890,
            1173,
            2075,
            2257,
            1449,
            490,
            2890,
            1898,
            551,
            1661,
            1511,
            1413,
            2130,
            885,
            1885,
            2890,
            237,
            1916,
            860,
            935,
            722,
            1034,
            479,
            1974,
            1121,
            639,
            1022,
            733,
            535,
            2072,
            601,
            453,
            1568,
            689,
            582,
            389,
            566,
            2259,
            475,
            395,
            463,
            359,
            805,
            705,
            664,
            499,
            521
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "480/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.2_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.221553564071655,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.7854671280276817,
            0.8096885813148789,
            0.8927335640138409,
            0.8269896193771626,
            0.8685121107266436,
            0.8304498269896193,
            0.8719723183391004,
            0.8650519031141869,
            0.8339100346020761,
            0.8719723183391004,
            0.8685121107266436,
            0.8477508650519031,
            0.8996539792387543,
            0.8788927335640139,
            0.8754325259515571,
            0.8788927335640139,
            0.9480968858131488,
            0.9515570934256056,
            0.9896193771626297,
            0.9826989619377162,
            0.9930795847750865,
            0.9896193771626297,
            0.9653979238754326,
            0.9550173010380623,
            0.9757785467128027,
            0.9792387543252595,
            0.9584775086505191,
            0.9896193771626297,
            0.9965397923875432,
            0.9965397923875432,
            1.0,
            0.9653979238754326,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2593,
            -1031,
            -2890,
            -2377,
            -1916,
            -2890,
            -2086,
            -2890,
            -2483,
            -1154,
            -2241,
            -2535,
            -2890,
            -2775,
            -2385,
            -1478,
            -1227,
            -658,
            -859,
            -826,
            -423,
            -2268,
            -1207,
            -755,
            -1602,
            -1092,
            -1490,
            -361,
            -793,
            -1263,
            -1152,
            -716
        ],
        "steps_history": [
            2890,
            2890,
            2694,
            1132,
            2890,
            2478,
            2017,
            2890,
            2187,
            2890,
            2584,
            1255,
            2342,
            2636,
            2890,
            2876,
            2486,
            1579,
            1328,
            759,
            960,
            927,
            524,
            2369,
            1308,
            856,
            1703,
            1193,
            1591,
            462,
            894,
            1364,
            1253,
            817
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "481/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.766749858856201,
        "final_policy_stability": 0.9930795847750865,
        "episodes_to_convergence": 41,
        "policy_stability_history": [
            0.0,
            0.8304498269896193,
            0.7993079584775087,
            0.8477508650519031,
            0.8235294117647058,
            0.8166089965397924,
            0.8927335640138409,
            0.8512110726643599,
            0.8269896193771626,
            0.9134948096885813,
            0.9065743944636678,
            0.8615916955017301,
            0.8650519031141869,
            0.9515570934256056,
            0.889273356401384,
            0.9584775086505191,
            0.8719723183391004,
            0.972318339100346,
            0.9826989619377162,
            0.9688581314878892,
            0.9377162629757786,
            0.9515570934256056,
            0.9619377162629758,
            0.9792387543252595,
            0.9792387543252595,
            0.986159169550173,
            0.9065743944636678,
            0.9653979238754326,
            0.9480968858131488,
            0.9550173010380623,
            0.9792387543252595,
            0.9965397923875432,
            0.9930795847750865,
            0.9930795847750865,
            0.9480968858131488,
            0.9930795847750865,
            0.9965397923875432,
            0.9688581314878892,
            0.986159169550173,
            0.986159169550173,
            0.9757785467128027,
            0.9930795847750865
        ],
        "reward_history": [
            -1046,
            -2890,
            -2890,
            -2454,
            -2890,
            -2890,
            -1601,
            -1913,
            -2890,
            -1013,
            -2890,
            -2070,
            -2176,
            -2890,
            -2591,
            -442,
            -2890,
            -708,
            -345,
            -859,
            -1641,
            -1209,
            -644,
            -812,
            -479,
            -303,
            -2574,
            -945,
            -1835,
            -1902,
            -718,
            -596,
            -720,
            -1127,
            -1164,
            -803,
            -547,
            -2046,
            -962,
            -922,
            -2024,
            -1374
        ],
        "steps_history": [
            1147,
            2890,
            2890,
            2555,
            2890,
            2890,
            1702,
            2014,
            2890,
            1114,
            2890,
            2171,
            2277,
            2890,
            2692,
            543,
            2890,
            809,
            446,
            960,
            1742,
            1310,
            745,
            913,
            580,
            404,
            2675,
            1046,
            1936,
            2003,
            819,
            697,
            821,
            1228,
            1265,
            904,
            648,
            2147,
            1063,
            1023,
            2125,
            1475
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "482/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.261680841445923,
        "final_policy_stability": 0.9757785467128027,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8131487889273357,
            0.889273356401384,
            0.8304498269896193,
            0.7716262975778547,
            0.8546712802768166,
            0.8269896193771626,
            0.9100346020761245,
            0.8442906574394463,
            0.8961937716262975,
            0.8339100346020761,
            0.8166089965397924,
            0.8339100346020761,
            0.9307958477508651,
            0.8754325259515571,
            0.9619377162629758,
            0.8823529411764706,
            0.9446366782006921,
            0.9411764705882353,
            0.972318339100346,
            0.9480968858131488,
            0.9480968858131488,
            0.9446366782006921,
            0.9446366782006921,
            0.986159169550173,
            0.9377162629757786,
            0.9653979238754326,
            0.9411764705882353,
            0.986159169550173,
            1.0,
            0.9930795847750865,
            0.9965397923875432,
            1.0,
            0.9757785467128027,
            0.9757785467128027
        ],
        "reward_history": [
            -2890,
            -2656,
            -2890,
            -1454,
            -2890,
            -1264,
            -2890,
            -883,
            -2890,
            -2315,
            -2656,
            -2890,
            -2511,
            -620,
            -2890,
            -449,
            -2890,
            -1201,
            -1558,
            -675,
            -1235,
            -828,
            -2890,
            -2629,
            -712,
            -1707,
            -522,
            -1887,
            -458,
            -367,
            -818,
            -915,
            -746,
            -865,
            -1546
        ],
        "steps_history": [
            2890,
            2757,
            2890,
            1555,
            2890,
            1365,
            2890,
            984,
            2890,
            2416,
            2757,
            2890,
            2612,
            721,
            2890,
            550,
            2890,
            1302,
            1659,
            776,
            1336,
            929,
            2890,
            2730,
            813,
            1808,
            623,
            1988,
            559,
            468,
            919,
            1016,
            847,
            966,
            1647
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "483/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.537134408950806,
        "final_policy_stability": 0.9792387543252595,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.7958477508650519,
            0.8650519031141869,
            0.8027681660899654,
            0.8512110726643599,
            0.8823529411764706,
            0.8581314878892734,
            0.8304498269896193,
            0.8788927335640139,
            0.8408304498269896,
            0.8062283737024222,
            0.9100346020761245,
            0.9134948096885813,
            0.8442906574394463,
            0.8512110726643599,
            0.8996539792387543,
            0.8996539792387543,
            0.9480968858131488,
            0.9134948096885813,
            0.9446366782006921,
            0.9619377162629758,
            0.9757785467128027,
            0.9550173010380623,
            0.9688581314878892,
            0.9826989619377162,
            0.9896193771626297,
            0.9930795847750865,
            0.9792387543252595
        ],
        "reward_history": [
            -2890,
            -2280,
            -2890,
            -2546,
            -2890,
            -2890,
            -1780,
            -2368,
            -2890,
            -2890,
            -2890,
            -1996,
            -2890,
            -2121,
            -1601,
            -1092,
            -2890,
            -1006,
            -1602,
            -1885,
            -1559,
            -1386,
            -1471,
            -1105,
            -670,
            -393,
            -634,
            -1417
        ],
        "steps_history": [
            2890,
            2381,
            2890,
            2647,
            2890,
            2890,
            1881,
            2469,
            2890,
            2890,
            2890,
            2097,
            2890,
            2222,
            1702,
            1193,
            2890,
            1107,
            1703,
            1986,
            1660,
            1487,
            1572,
            1206,
            771,
            494,
            735,
            1518
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "484/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.716023206710815,
        "final_policy_stability": 0.986159169550173,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.8581314878892734,
            0.9204152249134948,
            0.8131487889273357,
            0.7958477508650519,
            0.8339100346020761,
            0.8581314878892734,
            0.8304498269896193,
            0.8823529411764706,
            0.9134948096885813,
            0.8477508650519031,
            0.9515570934256056,
            0.8823529411764706,
            0.8788927335640139,
            0.9238754325259516,
            0.8996539792387543,
            0.9342560553633218,
            0.9065743944636678,
            0.9653979238754326,
            0.9757785467128027,
            0.9411764705882353,
            0.9619377162629758,
            0.9377162629757786,
            0.9550173010380623,
            0.9134948096885813,
            0.9584775086505191,
            0.9480968858131488,
            0.9792387543252595,
            0.9515570934256056,
            0.9896193771626297,
            0.9792387543252595,
            0.9584775086505191,
            0.9965397923875432,
            0.9757785467128027,
            1.0,
            0.986159169550173
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -1439,
            -2471,
            -2890,
            -1959,
            -2890,
            -2890,
            -895,
            -2687,
            -628,
            -1489,
            -2584,
            -1299,
            -1561,
            -1769,
            -2890,
            -821,
            -489,
            -532,
            -819,
            -2281,
            -521,
            -2779,
            -887,
            -2043,
            -517,
            -1536,
            -588,
            -700,
            -2738,
            -603,
            -1150,
            -713,
            -644
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            1540,
            2572,
            2890,
            2060,
            2890,
            2890,
            996,
            2788,
            729,
            1590,
            2685,
            1400,
            1662,
            1870,
            2890,
            922,
            590,
            633,
            920,
            2382,
            622,
            2880,
            988,
            2144,
            618,
            1637,
            689,
            801,
            2839,
            704,
            1251,
            814,
            745
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "485/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.056955814361572,
        "final_policy_stability": 0.9896193771626297,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8200692041522492,
            0.8650519031141869,
            0.8961937716262975,
            0.9342560553633218,
            0.8961937716262975,
            0.8477508650519031,
            0.8304498269896193,
            0.8235294117647058,
            0.8823529411764706,
            0.8200692041522492,
            0.9238754325259516,
            0.8373702422145328,
            0.8235294117647058,
            0.889273356401384,
            0.9446366782006921,
            0.9411764705882353,
            0.8477508650519031,
            0.8512110726643599,
            0.9584775086505191,
            0.889273356401384,
            0.9342560553633218,
            0.9688581314878892,
            0.9896193771626297,
            0.9377162629757786,
            0.9965397923875432,
            0.9826989619377162,
            0.8996539792387543,
            0.9688581314878892,
            0.9930795847750865,
            0.972318339100346,
            0.9342560553633218,
            0.986159169550173,
            0.986159169550173,
            0.986159169550173,
            0.9896193771626297,
            0.9896193771626297,
            0.9930795847750865,
            0.9965397923875432,
            0.9896193771626297,
            0.9896193771626297
        ],
        "reward_history": [
            -2890,
            -2437,
            -2631,
            -2890,
            -2890,
            -839,
            -1734,
            -1675,
            -2703,
            -1502,
            -2890,
            -910,
            -2890,
            -2890,
            -1639,
            -476,
            -1125,
            -1913,
            -2890,
            -605,
            -2890,
            -948,
            -603,
            -345,
            -971,
            -398,
            -811,
            -1337,
            -1257,
            -559,
            -654,
            -1848,
            -1715,
            -534,
            -372,
            -382,
            -153,
            -854,
            -488,
            -544,
            -924
        ],
        "steps_history": [
            2890,
            2538,
            2732,
            2890,
            2890,
            940,
            1835,
            1776,
            2804,
            1603,
            2890,
            1011,
            2890,
            2890,
            1740,
            577,
            1226,
            2014,
            2890,
            706,
            2890,
            1049,
            704,
            446,
            1072,
            499,
            912,
            1438,
            1358,
            660,
            755,
            1949,
            1816,
            635,
            473,
            483,
            254,
            955,
            589,
            645,
            1025
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "486/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.656152725219727,
        "final_policy_stability": 0.986159169550173,
        "episodes_to_convergence": 43,
        "policy_stability_history": [
            0.0,
            0.7716262975778547,
            0.8339100346020761,
            0.8096885813148789,
            0.8858131487889274,
            0.7889273356401384,
            0.8442906574394463,
            0.8685121107266436,
            0.889273356401384,
            0.9065743944636678,
            0.8650519031141869,
            0.903114186851211,
            0.8788927335640139,
            0.9204152249134948,
            0.889273356401384,
            0.903114186851211,
            0.9757785467128027,
            0.9204152249134948,
            0.9204152249134948,
            0.9342560553633218,
            0.9411764705882353,
            0.8858131487889274,
            0.9515570934256056,
            0.9688581314878892,
            0.9619377162629758,
            0.9480968858131488,
            0.986159169550173,
            0.9757785467128027,
            0.986159169550173,
            0.9515570934256056,
            0.9792387543252595,
            0.972318339100346,
            0.9688581314878892,
            0.9653979238754326,
            0.9688581314878892,
            0.9688581314878892,
            0.9550173010380623,
            0.9688581314878892,
            0.9965397923875432,
            0.9965397923875432,
            0.9965397923875432,
            0.9930795847750865,
            0.9965397923875432,
            0.986159169550173
        ],
        "reward_history": [
            -2890,
            -2890,
            -1722,
            -2122,
            -2890,
            -2466,
            -2890,
            -1194,
            -1711,
            -751,
            -2160,
            -1303,
            -2425,
            -2890,
            -1985,
            -2174,
            -308,
            -1071,
            -596,
            -731,
            -909,
            -2380,
            -883,
            -494,
            -617,
            -1117,
            -531,
            -350,
            -547,
            -1363,
            -692,
            -841,
            -757,
            -505,
            -642,
            -1564,
            -792,
            -916,
            -492,
            -469,
            -437,
            -403,
            -668,
            -598
        ],
        "steps_history": [
            2890,
            2890,
            1823,
            2223,
            2890,
            2567,
            2890,
            1295,
            1812,
            852,
            2261,
            1404,
            2526,
            2890,
            2086,
            2275,
            409,
            1172,
            697,
            832,
            1010,
            2481,
            984,
            595,
            718,
            1218,
            632,
            451,
            648,
            1464,
            793,
            942,
            858,
            606,
            743,
            1665,
            893,
            1017,
            593,
            570,
            538,
            504,
            769,
            699
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "487/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.141201496124268,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.7820069204152249,
            0.9204152249134948,
            0.8166089965397924,
            0.8235294117647058,
            0.8719723183391004,
            0.8373702422145328,
            0.8512110726643599,
            0.8685121107266436,
            0.8235294117647058,
            0.8719723183391004,
            0.8581314878892734,
            0.9273356401384083,
            0.8996539792387543,
            0.8823529411764706,
            0.9134948096885813,
            0.889273356401384,
            0.9446366782006921,
            0.9550173010380623,
            0.8961937716262975,
            0.9792387543252595,
            0.9273356401384083,
            0.9792387543252595,
            0.9273356401384083,
            0.9792387543252595,
            0.9757785467128027,
            0.9757785467128027,
            0.9792387543252595,
            0.9896193771626297,
            0.9930795847750865,
            0.9965397923875432,
            1.0,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -2485,
            -2890,
            -2890,
            -2675,
            -2890,
            -2266,
            -2890,
            -1094,
            -2890,
            -1416,
            -1773,
            -389,
            -1779,
            -1374,
            -980,
            -2200,
            -705,
            -751,
            -2890,
            -548,
            -1097,
            -812,
            -2115,
            -1073,
            -502,
            -356,
            -503,
            -341,
            -167,
            -414,
            -482,
            -501
        ],
        "steps_history": [
            2890,
            2586,
            2890,
            2890,
            2776,
            2890,
            2367,
            2890,
            1195,
            2890,
            1517,
            1874,
            490,
            1880,
            1475,
            1081,
            2301,
            806,
            852,
            2890,
            649,
            1198,
            913,
            2216,
            1174,
            603,
            457,
            604,
            442,
            268,
            515,
            583,
            602
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "488/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.020769357681274,
        "final_policy_stability": 0.9792387543252595,
        "episodes_to_convergence": 43,
        "policy_stability_history": [
            0.0,
            0.8408304498269896,
            0.8546712802768166,
            0.8788927335640139,
            0.8027681660899654,
            0.7854671280276817,
            0.8235294117647058,
            0.889273356401384,
            0.8339100346020761,
            0.8546712802768166,
            0.7854671280276817,
            0.8823529411764706,
            0.8442906574394463,
            0.9204152249134948,
            0.8650519031141869,
            0.9411764705882353,
            0.9134948096885813,
            0.8373702422145328,
            0.9377162629757786,
            0.9204152249134948,
            0.889273356401384,
            0.9584775086505191,
            0.9550173010380623,
            0.9619377162629758,
            0.9584775086505191,
            0.9515570934256056,
            0.9377162629757786,
            0.9342560553633218,
            0.9965397923875432,
            0.9896193771626297,
            0.9965397923875432,
            0.9273356401384083,
            0.9792387543252595,
            0.9896193771626297,
            0.986159169550173,
            0.986159169550173,
            1.0,
            0.986159169550173,
            0.9930795847750865,
            0.9826989619377162,
            0.9653979238754326,
            1.0,
            0.9965397923875432,
            0.9792387543252595
        ],
        "reward_history": [
            -2890,
            -1069,
            -2890,
            -901,
            -1666,
            -2890,
            -2890,
            -993,
            -2218,
            -1344,
            -2890,
            -2584,
            -2890,
            -1081,
            -2890,
            -756,
            -1519,
            -2890,
            -781,
            -1598,
            -1702,
            -373,
            -798,
            -595,
            -558,
            -912,
            -585,
            -1290,
            -317,
            -490,
            -197,
            -1374,
            -695,
            -407,
            -644,
            -262,
            -452,
            -309,
            -408,
            -596,
            -649,
            -414,
            -625,
            -590
        ],
        "steps_history": [
            2890,
            1170,
            2890,
            1002,
            1767,
            2890,
            2890,
            1094,
            2319,
            1445,
            2890,
            2685,
            2890,
            1182,
            2890,
            857,
            1620,
            2890,
            882,
            1699,
            1803,
            474,
            899,
            696,
            659,
            1013,
            686,
            1391,
            418,
            591,
            298,
            1475,
            796,
            508,
            745,
            363,
            553,
            410,
            509,
            697,
            750,
            515,
            726,
            691
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "489/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.1167426109313965,
        "final_policy_stability": 0.9653979238754326,
        "episodes_to_convergence": 39,
        "policy_stability_history": [
            0.0,
            0.8166089965397924,
            0.9100346020761245,
            0.7889273356401384,
            0.8823529411764706,
            0.8304498269896193,
            0.8131487889273357,
            0.8339100346020761,
            0.8615916955017301,
            0.9100346020761245,
            0.8754325259515571,
            0.8685121107266436,
            0.8408304498269896,
            0.8788927335640139,
            0.9134948096885813,
            0.8996539792387543,
            0.9204152249134948,
            0.9307958477508651,
            0.9134948096885813,
            0.9411764705882353,
            0.9480968858131488,
            0.9307958477508651,
            0.9238754325259516,
            0.9653979238754326,
            0.9792387543252595,
            0.972318339100346,
            0.8927335640138409,
            0.9792387543252595,
            0.9584775086505191,
            0.916955017301038,
            0.972318339100346,
            0.916955017301038,
            0.9896193771626297,
            0.986159169550173,
            0.986159169550173,
            0.9826989619377162,
            0.9930795847750865,
            0.986159169550173,
            0.9757785467128027,
            0.9653979238754326
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -2890,
            -2890,
            -2890,
            -2890,
            -1461,
            -897,
            -2135,
            -2097,
            -1696,
            -1921,
            -684,
            -1234,
            -1213,
            -1788,
            -1575,
            -754,
            -493,
            -1005,
            -865,
            -729,
            -342,
            -548,
            -1398,
            -458,
            -597,
            -2410,
            -410,
            -2308,
            -326,
            -982,
            -660,
            -829,
            -489,
            -1009,
            -721,
            -1097
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            2890,
            2890,
            2890,
            2890,
            1562,
            998,
            2236,
            2198,
            1797,
            2022,
            785,
            1335,
            1314,
            1889,
            1676,
            855,
            594,
            1106,
            966,
            830,
            443,
            649,
            1499,
            559,
            698,
            2511,
            511,
            2409,
            427,
            1083,
            761,
            930,
            590,
            1110,
            822,
            1198
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "490/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.822407245635986,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8754325259515571,
            0.8200692041522492,
            0.8650519031141869,
            0.7889273356401384,
            0.8096885813148789,
            0.8650519031141869,
            0.9100346020761245,
            0.8615916955017301,
            0.7889273356401384,
            0.8685121107266436,
            0.8961937716262975,
            0.8096885813148789,
            0.8477508650519031,
            0.9065743944636678,
            0.9411764705882353,
            0.9342560553633218,
            0.916955017301038,
            0.9273356401384083,
            0.916955017301038,
            0.9515570934256056,
            0.9342560553633218,
            0.9653979238754326,
            0.9653979238754326,
            0.8961937716262975,
            0.9619377162629758,
            0.9826989619377162,
            0.9896193771626297,
            0.9550173010380623,
            0.9653979238754326,
            0.9826989619377162,
            0.9792387543252595,
            0.986159169550173,
            0.986159169550173,
            0.9757785467128027,
            0.9792387543252595,
            0.9446366782006921,
            0.9757785467128027,
            0.9965397923875432,
            0.9826989619377162,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -1661,
            -561,
            -2890,
            -2890,
            -828,
            -639,
            -947,
            -2787,
            -1136,
            -752,
            -2890,
            -2352,
            -810,
            -565,
            -1005,
            -1328,
            -1085,
            -1156,
            -580,
            -793,
            -466,
            -618,
            -1627,
            -815,
            -298,
            -292,
            -1575,
            -791,
            -289,
            -682,
            -492,
            -804,
            -391,
            -296,
            -1191,
            -566,
            -171,
            -1116,
            -516
        ],
        "steps_history": [
            2890,
            2890,
            1762,
            662,
            2890,
            2890,
            929,
            740,
            1048,
            2888,
            1237,
            853,
            2890,
            2453,
            911,
            666,
            1106,
            1429,
            1186,
            1257,
            681,
            894,
            567,
            719,
            1728,
            916,
            399,
            393,
            1676,
            892,
            390,
            783,
            593,
            905,
            492,
            397,
            1292,
            667,
            272,
            1217,
            617
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "491/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.407862424850464,
        "final_policy_stability": 0.9930795847750865,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.8719723183391004,
            0.8200692041522492,
            0.8477508650519031,
            0.7647058823529411,
            0.8615916955017301,
            0.8961937716262975,
            0.8858131487889274,
            0.8615916955017301,
            0.8096885813148789,
            0.8650519031141869,
            0.7854671280276817,
            0.8823529411764706,
            0.9100346020761245,
            0.8823529411764706,
            0.8996539792387543,
            0.8685121107266436,
            0.8996539792387543,
            0.9480968858131488,
            0.9307958477508651,
            0.9411764705882353,
            0.903114186851211,
            0.9826989619377162,
            0.9204152249134948,
            0.9515570934256056,
            0.9550173010380623,
            0.9515570934256056,
            0.9515570934256056,
            0.9515570934256056,
            0.9515570934256056,
            0.9550173010380623,
            0.9792387543252595,
            0.9965397923875432,
            0.9411764705882353,
            0.9930795847750865,
            0.9930795847750865
        ],
        "reward_history": [
            -2890,
            -2890,
            -1502,
            -2890,
            -2890,
            -1125,
            -2890,
            -892,
            -2090,
            -2890,
            -1815,
            -2223,
            -1040,
            -835,
            -1125,
            -1291,
            -1444,
            -1675,
            -489,
            -1004,
            -400,
            -1496,
            -484,
            -2119,
            -642,
            -769,
            -844,
            -1250,
            -1596,
            -1020,
            -772,
            -593,
            -419,
            -1074,
            -169,
            -397
        ],
        "steps_history": [
            2890,
            2890,
            1603,
            2890,
            2890,
            1226,
            2890,
            993,
            2191,
            2890,
            1916,
            2324,
            1141,
            936,
            1226,
            1392,
            1545,
            1776,
            590,
            1105,
            501,
            1597,
            585,
            2220,
            743,
            870,
            945,
            1351,
            1697,
            1121,
            873,
            694,
            520,
            1175,
            270,
            498
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "492/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.848006963729858,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 39,
        "policy_stability_history": [
            0.0,
            0.8927335640138409,
            0.7785467128027682,
            0.8961937716262975,
            0.8581314878892734,
            0.8477508650519031,
            0.8131487889273357,
            0.8788927335640139,
            0.8131487889273357,
            0.9100346020761245,
            0.9204152249134948,
            0.8546712802768166,
            0.7958477508650519,
            0.9065743944636678,
            0.8788927335640139,
            0.9411764705882353,
            0.9342560553633218,
            0.8927335640138409,
            0.9550173010380623,
            0.9550173010380623,
            0.9238754325259516,
            0.9653979238754326,
            0.9411764705882353,
            0.9273356401384083,
            0.972318339100346,
            0.9584775086505191,
            0.9619377162629758,
            0.9480968858131488,
            0.9446366782006921,
            0.9584775086505191,
            0.9826989619377162,
            0.972318339100346,
            0.9896193771626297,
            0.9826989619377162,
            0.9896193771626297,
            0.9619377162629758,
            1.0,
            0.9584775086505191,
            1.0,
            1.0
        ],
        "reward_history": [
            -2302,
            -2890,
            -2229,
            -548,
            -1246,
            -2442,
            -2890,
            -1089,
            -2062,
            -722,
            -1040,
            -2052,
            -2698,
            -1113,
            -1472,
            -283,
            -702,
            -1464,
            -408,
            -331,
            -922,
            -795,
            -919,
            -1140,
            -761,
            -577,
            -672,
            -2122,
            -1365,
            -859,
            -385,
            -402,
            -582,
            -283,
            -365,
            -856,
            -390,
            -1022,
            -613,
            -517
        ],
        "steps_history": [
            2403,
            2890,
            2330,
            649,
            1347,
            2543,
            2890,
            1190,
            2163,
            823,
            1141,
            2153,
            2799,
            1214,
            1573,
            384,
            803,
            1565,
            509,
            432,
            1023,
            896,
            1020,
            1241,
            862,
            678,
            773,
            2223,
            1466,
            960,
            486,
            503,
            683,
            384,
            466,
            957,
            491,
            1123,
            714,
            618
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "493/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.696345090866089,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 44,
        "policy_stability_history": [
            0.0,
            0.8339100346020761,
            0.8373702422145328,
            0.8650519031141869,
            0.8512110726643599,
            0.8200692041522492,
            0.8096885813148789,
            0.8166089965397924,
            0.903114186851211,
            0.8927335640138409,
            0.8650519031141869,
            0.8961937716262975,
            0.9446366782006921,
            0.8512110726643599,
            0.8927335640138409,
            0.9134948096885813,
            0.9238754325259516,
            0.9377162629757786,
            0.9342560553633218,
            0.9688581314878892,
            0.9377162629757786,
            0.9377162629757786,
            0.8927335640138409,
            0.9411764705882353,
            0.9550173010380623,
            0.9411764705882353,
            0.9757785467128027,
            0.9307958477508651,
            0.9480968858131488,
            0.986159169550173,
            0.972318339100346,
            0.9446366782006921,
            0.9411764705882353,
            0.9515570934256056,
            0.9792387543252595,
            0.9584775086505191,
            0.986159169550173,
            0.972318339100346,
            0.986159169550173,
            0.9792387543252595,
            0.9930795847750865,
            0.9792387543252595,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -1272,
            -1452,
            -2890,
            -1373,
            -2742,
            -1971,
            -2890,
            -1027,
            -1054,
            -2425,
            -1688,
            -709,
            -2765,
            -1248,
            -1078,
            -1252,
            -462,
            -678,
            -300,
            -804,
            -285,
            -1919,
            -688,
            -968,
            -1335,
            -187,
            -657,
            -754,
            -219,
            -809,
            -1729,
            -1151,
            -837,
            -284,
            -657,
            -155,
            -674,
            -200,
            -395,
            -387,
            -1325,
            -229,
            -650,
            -276
        ],
        "steps_history": [
            2890,
            1373,
            1553,
            2890,
            1474,
            2843,
            2072,
            2890,
            1128,
            1155,
            2526,
            1789,
            810,
            2866,
            1349,
            1179,
            1353,
            563,
            779,
            401,
            905,
            386,
            2020,
            789,
            1069,
            1436,
            288,
            758,
            855,
            320,
            910,
            1830,
            1252,
            938,
            385,
            758,
            256,
            775,
            301,
            496,
            488,
            1426,
            330,
            751,
            377
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "494/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.657955169677734,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 44,
        "policy_stability_history": [
            0.0,
            0.7612456747404844,
            0.8235294117647058,
            0.8685121107266436,
            0.8339100346020761,
            0.7820069204152249,
            0.8961937716262975,
            0.8719723183391004,
            0.889273356401384,
            0.8823529411764706,
            0.9480968858131488,
            0.8650519031141869,
            0.8408304498269896,
            0.9134948096885813,
            0.9134948096885813,
            0.9480968858131488,
            0.8927335640138409,
            0.8615916955017301,
            0.9134948096885813,
            0.9342560553633218,
            0.9100346020761245,
            0.9377162629757786,
            0.9446366782006921,
            0.9100346020761245,
            0.972318339100346,
            0.972318339100346,
            0.8996539792387543,
            0.9238754325259516,
            0.9342560553633218,
            0.9515570934256056,
            0.9757785467128027,
            0.986159169550173,
            0.986159169550173,
            0.9896193771626297,
            0.9584775086505191,
            0.9480968858131488,
            0.9965397923875432,
            0.9965397923875432,
            0.9792387543252595,
            0.9930795847750865,
            0.9930795847750865,
            0.9653979238754326,
            0.986159169550173,
            1.0,
            0.9965397923875432
        ],
        "reward_history": [
            -804,
            -2890,
            -2014,
            -2890,
            -2890,
            -2890,
            -1050,
            -2056,
            -925,
            -2303,
            -961,
            -1852,
            -2490,
            -821,
            -950,
            -378,
            -1269,
            -2155,
            -1340,
            -926,
            -857,
            -1547,
            -270,
            -1349,
            -330,
            -343,
            -1228,
            -1346,
            -731,
            -557,
            -481,
            -397,
            -331,
            -558,
            -565,
            -1522,
            -445,
            -238,
            -990,
            -632,
            -434,
            -1811,
            -660,
            -352,
            -677
        ],
        "steps_history": [
            905,
            2890,
            2115,
            2890,
            2890,
            2890,
            1151,
            2157,
            1026,
            2404,
            1062,
            1953,
            2591,
            922,
            1051,
            479,
            1370,
            2256,
            1441,
            1027,
            958,
            1648,
            371,
            1450,
            431,
            444,
            1329,
            1447,
            832,
            658,
            582,
            498,
            432,
            659,
            666,
            1623,
            546,
            339,
            1091,
            733,
            535,
            1912,
            761,
            453,
            778
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "495/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.162297010421753,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.7820069204152249,
            0.8131487889273357,
            0.8927335640138409,
            0.8269896193771626,
            0.8685121107266436,
            0.8339100346020761,
            0.8927335640138409,
            0.8615916955017301,
            0.8685121107266436,
            0.8615916955017301,
            0.8996539792387543,
            0.8961937716262975,
            0.916955017301038,
            0.9377162629757786,
            0.9515570934256056,
            0.9480968858131488,
            0.8996539792387543,
            0.9065743944636678,
            0.8927335640138409,
            0.9446366782006921,
            0.8754325259515571,
            0.9342560553633218,
            0.9619377162629758,
            0.9584775086505191,
            0.9446366782006921,
            0.972318339100346,
            0.9826989619377162,
            0.972318339100346,
            0.9688581314878892,
            0.9792387543252595,
            0.9688581314878892,
            0.9826989619377162,
            0.9792387543252595,
            0.9965397923875432,
            0.972318339100346,
            0.9826989619377162,
            1.0,
            0.986159169550173,
            0.9965397923875432,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -2890,
            -2593,
            -1031,
            -2890,
            -2377,
            -1916,
            -2890,
            -2538,
            -2079,
            -2004,
            -1860,
            -2890,
            -1177,
            -1129,
            -592,
            -710,
            -2206,
            -562,
            -2356,
            -831,
            -2890,
            -2294,
            -691,
            -388,
            -1331,
            -988,
            -1029,
            -531,
            -1363,
            -1092,
            -1490,
            -490,
            -664,
            -1263,
            -1152,
            -716,
            -590,
            -1586,
            -1266,
            -336
        ],
        "steps_history": [
            2890,
            2890,
            2694,
            1132,
            2890,
            2478,
            2017,
            2890,
            2639,
            2180,
            2105,
            1961,
            2890,
            1278,
            1230,
            693,
            811,
            2307,
            663,
            2457,
            932,
            2890,
            2395,
            792,
            489,
            1432,
            1089,
            1130,
            632,
            1464,
            1193,
            1591,
            591,
            765,
            1364,
            1253,
            817,
            691,
            1687,
            1367,
            437
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "496/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.188852548599243,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.8304498269896193,
            0.7993079584775087,
            0.8477508650519031,
            0.8235294117647058,
            0.8200692041522492,
            0.889273356401384,
            0.8339100346020761,
            0.8477508650519031,
            0.9377162629757786,
            0.8685121107266436,
            0.8788927335640139,
            0.8235294117647058,
            0.9238754325259516,
            0.8477508650519031,
            0.9446366782006921,
            0.9480968858131488,
            0.9238754325259516,
            0.8996539792387543,
            0.9307958477508651,
            0.986159169550173,
            0.9377162629757786,
            0.9446366782006921,
            0.9757785467128027,
            0.9377162629757786,
            0.9480968858131488,
            0.9411764705882353,
            0.9792387543252595,
            0.9550173010380623,
            0.9757785467128027,
            0.9273356401384083,
            0.9619377162629758,
            0.9653979238754326,
            1.0,
            0.9826989619377162,
            1.0,
            1.0
        ],
        "reward_history": [
            -1046,
            -2890,
            -2890,
            -2454,
            -2890,
            -2890,
            -1601,
            -2707,
            -2487,
            -2890,
            -2890,
            -2483,
            -2640,
            -2336,
            -2890,
            -930,
            -1104,
            -1253,
            -2033,
            -984,
            -447,
            -1643,
            -1395,
            -997,
            -2313,
            -1989,
            -955,
            -1093,
            -1127,
            -865,
            -1750,
            -862,
            -2890,
            -279,
            -1937,
            -1461,
            -676
        ],
        "steps_history": [
            1147,
            2890,
            2890,
            2555,
            2890,
            2890,
            1702,
            2808,
            2588,
            2890,
            2890,
            2584,
            2741,
            2437,
            2890,
            1031,
            1205,
            1354,
            2134,
            1085,
            548,
            1744,
            1496,
            1098,
            2414,
            2090,
            1056,
            1194,
            1228,
            966,
            1851,
            963,
            2890,
            380,
            2038,
            1562,
            777
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "497/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.786424398422241,
        "final_policy_stability": 0.9930795847750865,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8131487889273357,
            0.889273356401384,
            0.8304498269896193,
            0.8062283737024222,
            0.8027681660899654,
            0.8823529411764706,
            0.8512110726643599,
            0.8477508650519031,
            0.8166089965397924,
            0.9238754325259516,
            0.8823529411764706,
            0.889273356401384,
            0.9065743944636678,
            0.8754325259515571,
            0.8788927335640139,
            0.9273356401384083,
            0.9411764705882353,
            0.9584775086505191,
            0.9688581314878892,
            0.9792387543252595,
            0.9896193771626297,
            0.986159169550173,
            0.9584775086505191,
            0.9792387543252595,
            0.9688581314878892,
            0.9307958477508651,
            0.9826989619377162,
            0.9965397923875432,
            0.9688581314878892,
            0.9930795847750865
        ],
        "reward_history": [
            -2890,
            -2656,
            -2890,
            -1454,
            -2315,
            -2890,
            -817,
            -2890,
            -2890,
            -2574,
            -859,
            -1612,
            -1339,
            -1978,
            -2890,
            -2647,
            -701,
            -2890,
            -1404,
            -933,
            -977,
            -327,
            -400,
            -1335,
            -966,
            -955,
            -1892,
            -643,
            -731,
            -1012,
            -522
        ],
        "steps_history": [
            2890,
            2757,
            2890,
            1555,
            2416,
            2890,
            918,
            2890,
            2890,
            2675,
            960,
            1713,
            1440,
            2079,
            2890,
            2748,
            802,
            2890,
            1505,
            1034,
            1078,
            428,
            501,
            1436,
            1067,
            1056,
            1993,
            744,
            832,
            1113,
            623
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "498/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.427323818206787,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.8788927335640139,
            0.8339100346020761,
            0.8823529411764706,
            0.8373702422145328,
            0.8339100346020761,
            0.8788927335640139,
            0.8373702422145328,
            0.8719723183391004,
            0.8961937716262975,
            0.9134948096885813,
            0.8512110726643599,
            0.889273356401384,
            0.903114186851211,
            0.8961937716262975,
            0.8408304498269896,
            0.889273356401384,
            0.9377162629757786,
            0.986159169550173,
            0.9273356401384083,
            0.972318339100346,
            0.9896193771626297,
            0.9619377162629758,
            0.9653979238754326,
            0.9515570934256056,
            0.9411764705882353,
            0.9446366782006921,
            0.9930795847750865,
            0.9896193771626297,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -793,
            -2580,
            -2095,
            -2890,
            -2890,
            -2890,
            -1391,
            -1115,
            -2615,
            -1297,
            -1671,
            -1863,
            -2607,
            -2349,
            -1456,
            -943,
            -2095,
            -1474,
            -106,
            -1103,
            -687,
            -1559,
            -1386,
            -1471,
            -700,
            -534,
            -440
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            894,
            2681,
            2196,
            2890,
            2890,
            2890,
            1492,
            1216,
            2716,
            1398,
            1772,
            1964,
            2708,
            2450,
            1557,
            1044,
            2196,
            1575,
            207,
            1204,
            788,
            1660,
            1487,
            1572,
            801,
            635,
            541
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "499/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.926619291305542,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.7958477508650519,
            0.9065743944636678,
            0.8131487889273357,
            0.8858131487889274,
            0.8131487889273357,
            0.8235294117647058,
            0.916955017301038,
            0.903114186851211,
            0.8269896193771626,
            0.9550173010380623,
            0.8512110726643599,
            0.8858131487889274,
            0.8927335640138409,
            0.8546712802768166,
            0.8650519031141869,
            0.8996539792387543,
            0.9134948096885813,
            0.8858131487889274,
            0.9134948096885813,
            0.9204152249134948,
            0.9342560553633218,
            0.9065743944636678,
            0.9480968858131488,
            0.986159169550173,
            0.986159169550173,
            0.9896193771626297,
            0.9896193771626297,
            0.9377162629757786,
            0.9896193771626297,
            0.9550173010380623,
            0.986159169550173,
            0.9653979238754326,
            0.9896193771626297,
            0.9411764705882353,
            0.9688581314878892,
            0.986159169550173,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -1074,
            -2890,
            -2236,
            -960,
            -2890,
            -2890,
            -263,
            -2890,
            -2077,
            -1810,
            -2890,
            -2890,
            -2890,
            -1169,
            -1668,
            -1022,
            -2408,
            -1198,
            -1814,
            -706,
            -594,
            -517,
            -458,
            -402,
            -1683,
            -1264,
            -1654,
            -603,
            -1150,
            -713,
            -2890,
            -2072,
            -858,
            -447
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            1175,
            2890,
            2337,
            1061,
            2890,
            2890,
            364,
            2890,
            2178,
            1911,
            2890,
            2890,
            2890,
            1270,
            1769,
            1123,
            2509,
            1299,
            1915,
            807,
            695,
            618,
            559,
            503,
            1784,
            1365,
            1755,
            704,
            1251,
            814,
            2890,
            2173,
            959,
            548
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "500/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.125605821609497,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.8235294117647058,
            0.8512110726643599,
            0.9238754325259516,
            0.8339100346020761,
            0.8269896193771626,
            0.8408304498269896,
            0.8927335640138409,
            0.8408304498269896,
            0.9480968858131488,
            0.8650519031141869,
            0.8858131487889274,
            0.8512110726643599,
            0.889273356401384,
            0.9342560553633218,
            0.8650519031141869,
            0.903114186851211,
            0.9065743944636678,
            0.9342560553633218,
            0.8615916955017301,
            0.8996539792387543,
            0.9688581314878892,
            0.9307958477508651,
            0.9204152249134948,
            0.9134948096885813,
            0.9792387543252595,
            0.9792387543252595,
            0.972318339100346,
            0.9930795847750865,
            0.9792387543252595,
            0.986159169550173,
            0.9273356401384083,
            0.9930795847750865,
            0.9965397923875432,
            0.9896193771626297,
            1.0
        ],
        "reward_history": [
            -2890,
            -2437,
            -2890,
            -2890,
            -2890,
            -2516,
            -2890,
            -1388,
            -2890,
            -701,
            -2019,
            -1840,
            -2890,
            -1465,
            -658,
            -2890,
            -876,
            -865,
            -1021,
            -2141,
            -2890,
            -289,
            -1181,
            -2382,
            -1337,
            -371,
            -779,
            -565,
            -206,
            -407,
            -449,
            -2890,
            -799,
            -372,
            -382,
            -153
        ],
        "steps_history": [
            2890,
            2538,
            2890,
            2890,
            2890,
            2617,
            2890,
            1489,
            2890,
            802,
            2120,
            1941,
            2890,
            1566,
            759,
            2890,
            977,
            966,
            1122,
            2242,
            2890,
            390,
            1282,
            2483,
            1438,
            472,
            880,
            666,
            307,
            508,
            550,
            2890,
            900,
            473,
            483,
            254
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "501/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.945330619812012,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 42,
        "policy_stability_history": [
            0.0,
            0.7750865051903114,
            0.8339100346020761,
            0.8166089965397924,
            0.8788927335640139,
            0.8304498269896193,
            0.8477508650519031,
            0.8442906574394463,
            0.8788927335640139,
            0.9307958477508651,
            0.903114186851211,
            0.9411764705882353,
            0.9134948096885813,
            0.8927335640138409,
            0.8650519031141869,
            0.8719723183391004,
            0.903114186851211,
            0.9342560553633218,
            0.9377162629757786,
            0.9204152249134948,
            0.9480968858131488,
            0.8754325259515571,
            0.9584775086505191,
            0.9411764705882353,
            0.9688581314878892,
            0.903114186851211,
            0.9377162629757786,
            0.9584775086505191,
            0.9619377162629758,
            0.9688581314878892,
            0.9550173010380623,
            0.9515570934256056,
            0.9896193771626297,
            0.9584775086505191,
            0.9653979238754326,
            0.9930795847750865,
            0.9584775086505191,
            0.986159169550173,
            0.9965397923875432,
            0.9965397923875432,
            0.972318339100346,
            0.9896193771626297,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -1722,
            -2122,
            -2890,
            -2890,
            -2890,
            -2243,
            -1368,
            -2890,
            -1565,
            -1078,
            -1175,
            -1292,
            -2140,
            -2541,
            -1458,
            -596,
            -651,
            -1309,
            -581,
            -2890,
            -858,
            -1044,
            -531,
            -1506,
            -1648,
            -841,
            -757,
            -1064,
            -1748,
            -792,
            -916,
            -492,
            -1007,
            -403,
            -1367,
            -535,
            -327,
            -501,
            -1846,
            -926,
            -792
        ],
        "steps_history": [
            2890,
            2890,
            1823,
            2223,
            2890,
            2890,
            2890,
            2344,
            1469,
            2890,
            1666,
            1179,
            1276,
            1393,
            2241,
            2642,
            1559,
            697,
            752,
            1410,
            682,
            2890,
            959,
            1145,
            632,
            1607,
            1749,
            942,
            858,
            1165,
            1849,
            893,
            1017,
            593,
            1108,
            504,
            1468,
            636,
            428,
            602,
            1947,
            1027,
            893
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "502/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.8213348388671875,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.7820069204152249,
            0.9100346020761245,
            0.7958477508650519,
            0.8131487889273357,
            0.8096885813148789,
            0.8442906574394463,
            0.8685121107266436,
            0.9238754325259516,
            0.7612456747404844,
            0.9584775086505191,
            0.8062283737024222,
            0.889273356401384,
            0.9342560553633218,
            0.9134948096885813,
            0.8858131487889274,
            0.9204152249134948,
            0.8685121107266436,
            0.9238754325259516,
            0.9757785467128027,
            0.9377162629757786,
            0.9411764705882353,
            0.972318339100346,
            0.9619377162629758,
            0.9377162629757786,
            0.9792387543252595,
            0.9480968858131488,
            0.9273356401384083,
            0.9930795847750865,
            0.9930795847750865,
            0.9896193771626297,
            0.9757785467128027,
            1.0,
            0.9688581314878892,
            0.9965397923875432,
            1.0,
            0.9826989619377162,
            0.9792387543252595,
            0.9930795847750865,
            0.9965397923875432,
            1.0
        ],
        "reward_history": [
            -2890,
            -2485,
            -2890,
            -2890,
            -2890,
            -2060,
            -2890,
            -1289,
            -1444,
            -2890,
            -350,
            -2890,
            -1681,
            -899,
            -779,
            -2101,
            -1807,
            -1500,
            -1803,
            -698,
            -1539,
            -1265,
            -289,
            -1093,
            -1479,
            -870,
            -959,
            -2311,
            -308,
            -387,
            -716,
            -838,
            -246,
            -1347,
            -458,
            -279,
            -515,
            -1654,
            -609,
            -968,
            -354
        ],
        "steps_history": [
            2890,
            2586,
            2890,
            2890,
            2890,
            2161,
            2890,
            1390,
            1545,
            2890,
            451,
            2890,
            1782,
            1000,
            880,
            2202,
            1908,
            1601,
            1904,
            799,
            1640,
            1366,
            390,
            1194,
            1580,
            971,
            1060,
            2412,
            409,
            488,
            817,
            939,
            347,
            1448,
            559,
            380,
            616,
            1755,
            710,
            1069,
            455
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "503/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.67598271369934,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 49,
        "policy_stability_history": [
            0.0,
            0.8408304498269896,
            0.8546712802768166,
            0.8788927335640139,
            0.7923875432525952,
            0.7958477508650519,
            0.8131487889273357,
            0.8373702422145328,
            0.8477508650519031,
            0.8339100346020761,
            0.903114186851211,
            0.8581314878892734,
            0.8996539792387543,
            0.8304498269896193,
            0.889273356401384,
            0.9377162629757786,
            0.903114186851211,
            0.9446366782006921,
            0.9619377162629758,
            0.9584775086505191,
            0.9653979238754326,
            0.9411764705882353,
            0.9550173010380623,
            0.9065743944636678,
            0.9065743944636678,
            0.9342560553633218,
            0.9930795847750865,
            0.9757785467128027,
            0.9792387543252595,
            0.9930795847750865,
            0.9826989619377162,
            0.9342560553633218,
            0.9653979238754326,
            0.9826989619377162,
            0.9965397923875432,
            0.9896193771626297,
            0.9930795847750865,
            0.986159169550173,
            0.9930795847750865,
            0.9792387543252595,
            0.9411764705882353,
            1.0,
            0.9826989619377162,
            0.9965397923875432,
            0.9826989619377162,
            0.9930795847750865,
            0.9930795847750865,
            0.9930795847750865,
            0.9515570934256056,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -1069,
            -2890,
            -901,
            -2890,
            -2890,
            -1876,
            -2890,
            -1771,
            -2890,
            -2890,
            -1413,
            -1413,
            -2890,
            -2890,
            -636,
            -1436,
            -731,
            -631,
            -510,
            -722,
            -1160,
            -719,
            -1420,
            -1432,
            -1898,
            -250,
            -487,
            -376,
            -490,
            -197,
            -1374,
            -695,
            -407,
            -644,
            -262,
            -452,
            -309,
            -408,
            -596,
            -1114,
            -675,
            -590,
            -169,
            -859,
            -283,
            -1307,
            -554,
            -1474,
            -863
        ],
        "steps_history": [
            2890,
            1170,
            2890,
            1002,
            2890,
            2890,
            1977,
            2890,
            1872,
            2890,
            2890,
            1514,
            1514,
            2890,
            2890,
            737,
            1537,
            832,
            732,
            611,
            823,
            1261,
            820,
            1521,
            1533,
            1999,
            351,
            588,
            477,
            591,
            298,
            1475,
            796,
            508,
            745,
            363,
            553,
            410,
            509,
            697,
            1215,
            776,
            691,
            270,
            960,
            384,
            1408,
            655,
            1575,
            964
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "504/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.023331642150879,
        "final_policy_stability": 0.986159169550173,
        "episodes_to_convergence": 38,
        "policy_stability_history": [
            0.0,
            0.9377162629757786,
            0.7854671280276817,
            0.8615916955017301,
            0.8408304498269896,
            0.8373702422145328,
            0.8650519031141869,
            0.9307958477508651,
            0.8442906574394463,
            0.8269896193771626,
            0.8373702422145328,
            0.9377162629757786,
            0.8512110726643599,
            0.8961937716262975,
            0.9100346020761245,
            0.9065743944636678,
            0.9134948096885813,
            0.9273356401384083,
            0.9653979238754326,
            0.9307958477508651,
            0.9480968858131488,
            0.9550173010380623,
            0.916955017301038,
            0.9515570934256056,
            0.9792387543252595,
            0.9238754325259516,
            0.9653979238754326,
            0.9688581314878892,
            0.9480968858131488,
            0.9377162629757786,
            0.9896193771626297,
            0.9273356401384083,
            0.9826989619377162,
            1.0,
            0.986159169550173,
            0.9515570934256056,
            0.9965397923875432,
            1.0,
            0.986159169550173
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -1972,
            -2332,
            -1263,
            -553,
            -1731,
            -2890,
            -2890,
            -1028,
            -2890,
            -1263,
            -1042,
            -1572,
            -1194,
            -2076,
            -449,
            -636,
            -1146,
            -1012,
            -2268,
            -604,
            -398,
            -1119,
            -467,
            -749,
            -1107,
            -1766,
            -700,
            -2890,
            -470,
            -326,
            -982,
            -2180,
            -432,
            -476,
            -1329
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            2073,
            2433,
            1364,
            654,
            1832,
            2890,
            2890,
            1129,
            2890,
            1364,
            1143,
            1673,
            1295,
            2177,
            550,
            737,
            1247,
            1113,
            2369,
            705,
            499,
            1220,
            568,
            850,
            1208,
            1867,
            801,
            2890,
            571,
            427,
            1083,
            2281,
            533,
            577,
            1430
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "505/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.835612058639526,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 38,
        "policy_stability_history": [
            0.0,
            0.8719723183391004,
            0.8235294117647058,
            0.8650519031141869,
            0.8027681660899654,
            0.8269896193771626,
            0.8442906574394463,
            0.8096885813148789,
            0.8339100346020761,
            0.8581314878892734,
            0.8961937716262975,
            0.8615916955017301,
            0.889273356401384,
            0.9134948096885813,
            0.9238754325259516,
            0.9584775086505191,
            0.9550173010380623,
            0.9342560553633218,
            0.9480968858131488,
            0.9584775086505191,
            0.972318339100346,
            0.986159169550173,
            0.9446366782006921,
            0.9480968858131488,
            0.9550173010380623,
            0.9792387543252595,
            0.9342560553633218,
            0.9826989619377162,
            0.9584775086505191,
            0.9896193771626297,
            0.9515570934256056,
            0.9826989619377162,
            1.0,
            0.9965397923875432,
            0.9930795847750865,
            1.0,
            1.0,
            0.9826989619377162,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -1661,
            -561,
            -2890,
            -2777,
            -1326,
            -2890,
            -2594,
            -2591,
            -861,
            -2060,
            -1177,
            -2618,
            -964,
            -358,
            -1156,
            -580,
            -502,
            -487,
            -744,
            -222,
            -681,
            -666,
            -815,
            -298,
            -1038,
            -991,
            -1802,
            -1397,
            -1689,
            -401,
            -230,
            -370,
            -356,
            -537,
            -339,
            -814,
            -346
        ],
        "steps_history": [
            2890,
            2890,
            1762,
            662,
            2890,
            2878,
            1427,
            2890,
            2695,
            2692,
            962,
            2161,
            1278,
            2719,
            1065,
            459,
            1257,
            681,
            603,
            588,
            845,
            323,
            782,
            767,
            916,
            399,
            1139,
            1092,
            1903,
            1498,
            1790,
            502,
            331,
            471,
            457,
            638,
            440,
            915,
            447
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "506/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.714547872543335,
        "final_policy_stability": 0.9930795847750865,
        "episodes_to_convergence": 39,
        "policy_stability_history": [
            0.0,
            0.8339100346020761,
            0.8650519031141869,
            0.8546712802768166,
            0.8200692041522492,
            0.8235294117647058,
            0.8615916955017301,
            0.8062283737024222,
            0.8581314878892734,
            0.8927335640138409,
            0.916955017301038,
            0.9204152249134948,
            0.9584775086505191,
            0.8961937716262975,
            0.9653979238754326,
            0.8788927335640139,
            0.9273356401384083,
            0.903114186851211,
            0.903114186851211,
            0.9653979238754326,
            0.9515570934256056,
            0.9238754325259516,
            0.9757785467128027,
            0.9584775086505191,
            0.9134948096885813,
            0.9065743944636678,
            0.9757785467128027,
            0.9826989619377162,
            0.986159169550173,
            0.9446366782006921,
            0.9792387543252595,
            0.9965397923875432,
            0.9653979238754326,
            0.9653979238754326,
            0.9550173010380623,
            0.9965397923875432,
            0.9896193771626297,
            0.9930795847750865,
            0.9965397923875432,
            0.9930795847750865
        ],
        "reward_history": [
            -2890,
            -2890,
            -806,
            -2890,
            -2890,
            -2890,
            -1891,
            -2890,
            -2314,
            -1215,
            -1142,
            -707,
            -459,
            -1349,
            -354,
            -1871,
            -1263,
            -656,
            -1753,
            -541,
            -433,
            -513,
            -353,
            -565,
            -1710,
            -1864,
            -658,
            -705,
            -644,
            -1302,
            -1183,
            -424,
            -1071,
            -1200,
            -1286,
            -419,
            -903,
            -496,
            -810,
            -1018
        ],
        "steps_history": [
            2890,
            2890,
            907,
            2890,
            2890,
            2890,
            1992,
            2890,
            2415,
            1316,
            1243,
            808,
            560,
            1450,
            455,
            1972,
            1364,
            757,
            1854,
            642,
            534,
            614,
            454,
            666,
            1811,
            1965,
            759,
            806,
            745,
            1403,
            1284,
            525,
            1172,
            1301,
            1387,
            520,
            1004,
            597,
            911,
            1119
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "507/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.853577375411987,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8927335640138409,
            0.7820069204152249,
            0.8961937716262975,
            0.8581314878892734,
            0.8477508650519031,
            0.8131487889273357,
            0.889273356401384,
            0.8166089965397924,
            0.9238754325259516,
            0.8581314878892734,
            0.7958477508650519,
            0.9446366782006921,
            0.9307958477508651,
            0.8996539792387543,
            0.903114186851211,
            0.9411764705882353,
            0.972318339100346,
            0.9619377162629758,
            0.9653979238754326,
            0.9307958477508651,
            0.9238754325259516,
            0.9377162629757786,
            0.9515570934256056,
            0.9826989619377162,
            0.9446366782006921,
            0.9826989619377162,
            0.9584775086505191,
            0.986159169550173,
            0.9930795847750865,
            1.0,
            0.972318339100346,
            0.9896193771626297,
            0.9688581314878892,
            0.9965397923875432,
            0.9757785467128027,
            0.9826989619377162,
            0.986159169550173,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2302,
            -2890,
            -2229,
            -548,
            -1246,
            -2442,
            -2890,
            -1089,
            -2062,
            -722,
            -1795,
            -2529,
            -502,
            -975,
            -2168,
            -2207,
            -749,
            -333,
            -406,
            -433,
            -2304,
            -1140,
            -852,
            -486,
            -307,
            -2487,
            -462,
            -802,
            -265,
            -299,
            -579,
            -402,
            -582,
            -1492,
            -130,
            -745,
            -414,
            -849,
            -517,
            -93,
            -494
        ],
        "steps_history": [
            2403,
            2890,
            2330,
            649,
            1347,
            2543,
            2890,
            1190,
            2163,
            823,
            1896,
            2630,
            603,
            1076,
            2269,
            2308,
            850,
            434,
            507,
            534,
            2405,
            1241,
            953,
            587,
            408,
            2588,
            563,
            903,
            366,
            400,
            680,
            503,
            683,
            1593,
            231,
            846,
            515,
            950,
            618,
            194,
            595
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "508/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.724528789520264,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 41,
        "policy_stability_history": [
            0.0,
            0.8339100346020761,
            0.8373702422145328,
            0.8754325259515571,
            0.7716262975778547,
            0.7993079584775087,
            0.7750865051903114,
            0.8823529411764706,
            0.8339100346020761,
            0.8615916955017301,
            0.8581314878892734,
            0.8996539792387543,
            0.9238754325259516,
            0.916955017301038,
            0.8788927335640139,
            0.9204152249134948,
            0.9204152249134948,
            0.9273356401384083,
            0.889273356401384,
            0.9619377162629758,
            0.9238754325259516,
            0.9065743944636678,
            0.9307958477508651,
            0.9307958477508651,
            0.9515570934256056,
            0.9446366782006921,
            0.9307958477508651,
            0.986159169550173,
            0.9515570934256056,
            0.9757785467128027,
            0.986159169550173,
            0.9896193771626297,
            0.9688581314878892,
            0.9965397923875432,
            0.9965397923875432,
            0.9688581314878892,
            0.9896193771626297,
            0.9619377162629758,
            0.9896193771626297,
            1.0,
            0.9965397923875432,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -1272,
            -1452,
            -2890,
            -2890,
            -2890,
            -2890,
            -1157,
            -2253,
            -1008,
            -2112,
            -1383,
            -668,
            -671,
            -823,
            -575,
            -1162,
            -637,
            -2119,
            -380,
            -1396,
            -1650,
            -541,
            -1138,
            -580,
            -940,
            -989,
            -154,
            -616,
            -628,
            -239,
            -497,
            -1131,
            -454,
            -426,
            -1007,
            -284,
            -657,
            -155,
            -328,
            -332,
            -359
        ],
        "steps_history": [
            2890,
            1373,
            1553,
            2890,
            2890,
            2890,
            2890,
            1258,
            2354,
            1109,
            2213,
            1484,
            769,
            772,
            924,
            676,
            1263,
            738,
            2220,
            481,
            1497,
            1751,
            642,
            1239,
            681,
            1041,
            1090,
            255,
            717,
            729,
            340,
            598,
            1232,
            555,
            527,
            1108,
            385,
            758,
            256,
            429,
            433,
            460
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "509/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.508198261260986,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 41,
        "policy_stability_history": [
            0.0,
            0.7577854671280276,
            0.7958477508650519,
            0.8615916955017301,
            0.8304498269896193,
            0.8650519031141869,
            0.8200692041522492,
            0.8615916955017301,
            0.7993079584775087,
            0.8062283737024222,
            0.8788927335640139,
            0.8546712802768166,
            0.8719723183391004,
            0.9342560553633218,
            0.8788927335640139,
            0.8996539792387543,
            0.903114186851211,
            0.9411764705882353,
            0.8546712802768166,
            0.9238754325259516,
            0.9446366782006921,
            0.9653979238754326,
            0.8719723183391004,
            0.9515570934256056,
            0.9480968858131488,
            0.9584775086505191,
            0.9584775086505191,
            0.9792387543252595,
            0.9896193771626297,
            0.9238754325259516,
            0.9411764705882353,
            0.9411764705882353,
            0.9757785467128027,
            0.986159169550173,
            0.9619377162629758,
            0.9826989619377162,
            0.9896193771626297,
            0.9896193771626297,
            0.986159169550173,
            0.9584775086505191,
            0.9896193771626297,
            1.0
        ],
        "reward_history": [
            -804,
            -2890,
            -2890,
            -2890,
            -2502,
            -1061,
            -2688,
            -1325,
            -2153,
            -2890,
            -1912,
            -2039,
            -1100,
            -380,
            -2569,
            -1137,
            -1635,
            -510,
            -2300,
            -1139,
            -604,
            -661,
            -1819,
            -770,
            -759,
            -707,
            -581,
            -182,
            -209,
            -986,
            -1056,
            -1522,
            -445,
            -486,
            -742,
            -632,
            -434,
            -382,
            -254,
            -1133,
            -500,
            -352
        ],
        "steps_history": [
            905,
            2890,
            2890,
            2890,
            2603,
            1162,
            2789,
            1426,
            2254,
            2890,
            2013,
            2140,
            1201,
            481,
            2670,
            1238,
            1736,
            611,
            2401,
            1240,
            705,
            762,
            1920,
            871,
            860,
            808,
            682,
            283,
            310,
            1087,
            1157,
            1623,
            546,
            587,
            843,
            733,
            535,
            483,
            355,
            1234,
            601,
            453
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "510/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.3_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.35276198387146,
        "final_policy_stability": 0.9896193771626297,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.8235294117647058,
            0.8512110726643599,
            0.8200692041522492,
            0.8581314878892734,
            0.8546712802768166,
            0.8927335640138409,
            0.8788927335640139,
            0.8754325259515571,
            0.889273356401384,
            0.9238754325259516,
            0.9377162629757786,
            0.9515570934256056,
            0.8754325259515571,
            0.9653979238754326,
            0.9584775086505191,
            0.9100346020761245,
            0.9480968858131488,
            0.9688581314878892,
            0.972318339100346,
            0.9515570934256056,
            0.9273356401384083,
            0.9446366782006921,
            0.9619377162629758,
            0.9930795847750865,
            0.986159169550173,
            0.9446366782006921,
            0.9896193771626297
        ],
        "reward_history": [
            -2890,
            -2890,
            -1049,
            -2890,
            -2549,
            -2890,
            -1193,
            -1949,
            -2346,
            -2062,
            -2890,
            -429,
            -707,
            -2890,
            -606,
            -917,
            -1139,
            -856,
            -459,
            -653,
            -1730,
            -1038,
            -1320,
            -1382,
            -845,
            -609,
            -1936,
            -672
        ],
        "steps_history": [
            2890,
            2890,
            1150,
            2890,
            2650,
            2890,
            1294,
            2050,
            2447,
            2163,
            2890,
            530,
            808,
            2890,
            707,
            1018,
            1240,
            957,
            560,
            754,
            1831,
            1139,
            1421,
            1483,
            946,
            710,
            2037,
            773
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "511/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.454365968704224,
        "final_policy_stability": 0.9826989619377162,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.7681660899653979,
            0.9065743944636678,
            0.7785467128027682,
            0.9342560553633218,
            0.8200692041522492,
            0.8858131487889274,
            0.7923875432525952,
            0.8408304498269896,
            0.8650519031141869,
            0.8339100346020761,
            0.903114186851211,
            0.8408304498269896,
            0.8927335640138409,
            0.9550173010380623,
            0.8858131487889274,
            0.9134948096885813,
            0.9792387543252595,
            0.9757785467128027,
            0.9515570934256056,
            0.9826989619377162,
            0.9584775086505191,
            0.986159169550173,
            0.9688581314878892,
            0.9584775086505191,
            0.9377162629757786,
            0.972318339100346,
            1.0,
            0.9965397923875432,
            0.9930795847750865,
            0.9930795847750865,
            0.9757785467128027,
            1.0,
            0.9965397923875432,
            0.9826989619377162
        ],
        "reward_history": [
            -2890,
            -2676,
            -2890,
            -2890,
            -467,
            -2890,
            -2283,
            -2309,
            -2890,
            -2890,
            -2536,
            -2131,
            -2890,
            -2563,
            -822,
            -2890,
            -2378,
            -548,
            -495,
            -976,
            -708,
            -1414,
            -303,
            -1265,
            -1208,
            -2890,
            -2081,
            -631,
            -596,
            -720,
            -1127,
            -2068,
            -547,
            -862,
            -2146
        ],
        "steps_history": [
            2890,
            2777,
            2890,
            2890,
            568,
            2890,
            2384,
            2410,
            2890,
            2890,
            2637,
            2232,
            2890,
            2664,
            923,
            2890,
            2479,
            649,
            596,
            1077,
            809,
            1515,
            404,
            1366,
            1309,
            2890,
            2182,
            732,
            697,
            821,
            1228,
            2169,
            648,
            963,
            2247
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "512/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.139665126800537,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8961937716262975,
            0.8269896193771626,
            0.8754325259515571,
            0.8096885813148789,
            0.7854671280276817,
            0.8788927335640139,
            0.9273356401384083,
            0.916955017301038,
            0.9273356401384083,
            0.8927335640138409,
            0.8615916955017301,
            0.9100346020761245,
            0.9238754325259516,
            0.8581314878892734,
            0.8996539792387543,
            0.9792387543252595,
            0.9480968858131488,
            0.9204152249134948,
            0.9826989619377162,
            0.9826989619377162,
            0.9930795847750865,
            0.986159169550173,
            0.972318339100346,
            1.0,
            0.9965397923875432,
            0.9619377162629758,
            0.9930795847750865,
            0.9826989619377162,
            1.0,
            1.0,
            0.9965397923875432
        ],
        "reward_history": [
            -1543,
            -2890,
            -2890,
            -1965,
            -2890,
            -2890,
            -969,
            -2890,
            -1226,
            -2890,
            -2890,
            -1512,
            -1377,
            -919,
            -2890,
            -2164,
            -243,
            -1302,
            -1893,
            -2353,
            -933,
            -977,
            -1148,
            -1015,
            -966,
            -955,
            -1598,
            -1413,
            -1368,
            -522,
            -763,
            -1719
        ],
        "steps_history": [
            1644,
            2890,
            2890,
            2066,
            2890,
            2890,
            1070,
            2890,
            1327,
            2890,
            2890,
            1613,
            1478,
            1020,
            2890,
            2265,
            344,
            1403,
            1994,
            2454,
            1034,
            1078,
            1249,
            1116,
            1067,
            1056,
            1699,
            1514,
            1469,
            623,
            864,
            1820
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "513/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.135313510894775,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.7958477508650519,
            0.8304498269896193,
            0.8546712802768166,
            0.8304498269896193,
            0.8408304498269896,
            0.8027681660899654,
            0.8650519031141869,
            0.8996539792387543,
            0.8927335640138409,
            0.8546712802768166,
            0.8961937716262975,
            0.8477508650519031,
            0.9480968858131488,
            0.9515570934256056,
            0.9377162629757786,
            0.8685121107266436,
            0.916955017301038,
            0.9238754325259516,
            0.9065743944636678,
            0.9446366782006921,
            0.9584775086505191,
            0.9826989619377162,
            0.9238754325259516,
            0.986159169550173,
            0.9377162629757786,
            0.9688581314878892,
            0.9792387543252595,
            1.0,
            1.0,
            0.9688581314878892,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -793,
            -2890,
            -1886,
            -2890,
            -1048,
            -749,
            -1236,
            -2890,
            -899,
            -2890,
            -643,
            -655,
            -815,
            -2890,
            -1792,
            -1009,
            -1579,
            -982,
            -1148,
            -806,
            -2516,
            -1806,
            -2098,
            -1336,
            -1609,
            -391,
            -807,
            -872,
            -534
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            894,
            2890,
            1987,
            2890,
            1149,
            850,
            1337,
            2890,
            1000,
            2890,
            744,
            756,
            916,
            2890,
            1893,
            1110,
            1680,
            1083,
            1249,
            907,
            2617,
            1907,
            2199,
            1437,
            1710,
            492,
            908,
            973,
            635
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "514/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.562016725540161,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.7612456747404844,
            0.8373702422145328,
            0.8927335640138409,
            0.8373702422145328,
            0.8304498269896193,
            0.8373702422145328,
            0.8339100346020761,
            0.8685121107266436,
            0.9550173010380623,
            0.8961937716262975,
            0.9377162629757786,
            0.9065743944636678,
            0.8754325259515571,
            0.9480968858131488,
            0.9100346020761245,
            0.972318339100346,
            0.8961937716262975,
            0.9342560553633218,
            0.9204152249134948,
            0.9550173010380623,
            0.986159169550173,
            0.9792387543252595,
            0.9826989619377162,
            0.986159169550173,
            0.9792387543252595,
            0.9653979238754326,
            0.972318339100346,
            0.9930795847750865,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -1235,
            -2890,
            -2280,
            -2890,
            -2584,
            -2784,
            -2890,
            -536,
            -1135,
            -932,
            -1562,
            -2463,
            -930,
            -1698,
            -381,
            -2890,
            -1611,
            -1412,
            -1021,
            -747,
            -819,
            -759,
            -407,
            -1024,
            -2890,
            -1165,
            -764,
            -706,
            -594
        ],
        "steps_history": [
            2890,
            2890,
            1336,
            2890,
            2381,
            2890,
            2685,
            2885,
            2890,
            637,
            1236,
            1033,
            1663,
            2564,
            1031,
            1799,
            482,
            2890,
            1712,
            1513,
            1122,
            848,
            920,
            860,
            508,
            1125,
            2890,
            1266,
            865,
            807,
            695
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "515/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.294484853744507,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8062283737024222,
            0.8373702422145328,
            0.8650519031141869,
            0.8339100346020761,
            0.9100346020761245,
            0.9204152249134948,
            0.8858131487889274,
            0.8408304498269896,
            0.8477508650519031,
            0.9238754325259516,
            0.8927335640138409,
            0.903114186851211,
            0.9134948096885813,
            0.9065743944636678,
            0.8685121107266436,
            0.9757785467128027,
            0.8615916955017301,
            0.9480968858131488,
            0.972318339100346,
            0.9584775086505191,
            0.9377162629757786,
            0.9550173010380623,
            0.9100346020761245,
            0.9411764705882353,
            0.9653979238754326,
            0.9550173010380623,
            0.9896193771626297,
            0.9826989619377162,
            0.9688581314878892,
            0.972318339100346,
            0.9792387543252595,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -2576,
            -1907,
            -2890,
            -1959,
            -2890,
            -539,
            -1703,
            -2097,
            -2890,
            -652,
            -1280,
            -1617,
            -1207,
            -1274,
            -2890,
            -366,
            -1651,
            -648,
            -681,
            -924,
            -681,
            -750,
            -2485,
            -1246,
            -1075,
            -1288,
            -634,
            -707,
            -542,
            -1739,
            -1337,
            -341,
            -815,
            -559
        ],
        "steps_history": [
            2890,
            2677,
            2008,
            2890,
            2060,
            2890,
            640,
            1804,
            2198,
            2890,
            753,
            1381,
            1718,
            1308,
            1375,
            2890,
            467,
            1752,
            749,
            782,
            1025,
            782,
            851,
            2586,
            1347,
            1176,
            1389,
            735,
            808,
            643,
            1840,
            1438,
            442,
            916,
            660
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "516/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.3982508182525635,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.7889273356401384,
            0.8304498269896193,
            0.8131487889273357,
            0.8131487889273357,
            0.8650519031141869,
            0.8754325259515571,
            0.889273356401384,
            0.8235294117647058,
            0.9342560553633218,
            0.9342560553633218,
            0.9134948096885813,
            0.9515570934256056,
            0.9446366782006921,
            0.9342560553633218,
            0.9238754325259516,
            0.8858131487889274,
            0.9653979238754326,
            0.9342560553633218,
            0.903114186851211,
            0.9757785467128027,
            0.986159169550173,
            0.9653979238754326,
            0.9653979238754326,
            0.9896193771626297,
            0.9550173010380623,
            1.0,
            0.986159169550173,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2517,
            -2047,
            -2890,
            -1662,
            -959,
            -2890,
            -780,
            -425,
            -1408,
            -741,
            -688,
            -1464,
            -758,
            -2890,
            -953,
            -1314,
            -2685,
            -645,
            -602,
            -989,
            -1536,
            -322,
            -1132,
            -666,
            -617,
            -277,
            -598
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2618,
            2148,
            2890,
            1763,
            1060,
            2890,
            881,
            526,
            1509,
            842,
            789,
            1565,
            859,
            2890,
            1054,
            1415,
            2786,
            746,
            703,
            1090,
            1637,
            423,
            1233,
            767,
            718,
            378,
            699
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "517/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.228549242019653,
        "final_policy_stability": 0.9826989619377162,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8546712802768166,
            0.9238754325259516,
            0.8269896193771626,
            0.9204152249134948,
            0.8200692041522492,
            0.9065743944636678,
            0.8235294117647058,
            0.8685121107266436,
            0.8581314878892734,
            0.8650519031141869,
            0.9550173010380623,
            0.8996539792387543,
            0.903114186851211,
            0.9377162629757786,
            0.972318339100346,
            0.9550173010380623,
            0.9826989619377162,
            0.9377162629757786,
            0.9446366782006921,
            0.9757785467128027,
            0.9446366782006921,
            0.9896193771626297,
            0.9446366782006921,
            0.9653979238754326,
            1.0,
            0.9584775086505191,
            1.0,
            0.9826989619377162
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -644,
            -1588,
            -1067,
            -2414,
            -2890,
            -2653,
            -1772,
            -431,
            -1954,
            -1220,
            -1455,
            -386,
            -919,
            -462,
            -1752,
            -1453,
            -780,
            -584,
            -664,
            -2259,
            -671,
            -380,
            -1097,
            -812,
            -654
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            745,
            1689,
            1168,
            2515,
            2890,
            2754,
            1873,
            532,
            2055,
            1321,
            1556,
            487,
            1020,
            563,
            1853,
            1554,
            881,
            685,
            765,
            2360,
            772,
            481,
            1198,
            913,
            755
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "518/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.324148178100586,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.889273356401384,
            0.7439446366782007,
            0.8062283737024222,
            0.889273356401384,
            0.8304498269896193,
            0.8788927335640139,
            0.8615916955017301,
            0.8858131487889274,
            0.8373702422145328,
            0.9411764705882353,
            0.9411764705882353,
            0.9273356401384083,
            0.8200692041522492,
            0.9377162629757786,
            0.903114186851211,
            0.903114186851211,
            0.8685121107266436,
            0.9653979238754326,
            0.9377162629757786,
            0.9204152249134948,
            0.9411764705882353,
            0.9446366782006921,
            0.9688581314878892,
            0.9653979238754326,
            0.972318339100346,
            0.9965397923875432,
            0.9411764705882353,
            0.9653979238754326,
            0.9550173010380623,
            0.9619377162629758,
            0.9965397923875432,
            1.0,
            0.9930795847750865,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2568,
            -2890,
            -1165,
            -2640,
            -709,
            -1369,
            -898,
            -1371,
            -630,
            -410,
            -1549,
            -2890,
            -1716,
            -1520,
            -2569,
            -2482,
            -431,
            -593,
            -1174,
            -1310,
            -996,
            -825,
            -579,
            -1134,
            -598,
            -1412,
            -640,
            -1401,
            -840,
            -487,
            -376,
            -490,
            -197
        ],
        "steps_history": [
            2890,
            2890,
            2669,
            2890,
            1266,
            2741,
            810,
            1470,
            999,
            1472,
            731,
            511,
            1650,
            2890,
            1817,
            1621,
            2670,
            2583,
            532,
            694,
            1275,
            1411,
            1097,
            926,
            680,
            1235,
            699,
            1513,
            741,
            1502,
            941,
            588,
            477,
            591,
            298
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "519/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.258748769760132,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8650519031141869,
            0.9204152249134948,
            0.8754325259515571,
            0.7854671280276817,
            0.8685121107266436,
            0.8477508650519031,
            0.8442906574394463,
            0.8719723183391004,
            0.9757785467128027,
            0.8269896193771626,
            0.8719723183391004,
            0.9134948096885813,
            0.9342560553633218,
            0.8200692041522492,
            0.9653979238754326,
            0.9134948096885813,
            0.9342560553633218,
            0.9480968858131488,
            0.9307958477508651,
            0.9584775086505191,
            0.986159169550173,
            0.986159169550173,
            0.9550173010380623,
            0.9653979238754326,
            0.9446366782006921,
            0.9515570934256056,
            0.9826989619377162,
            0.972318339100346,
            1.0,
            1.0,
            0.9965397923875432
        ],
        "reward_history": [
            -733,
            -2890,
            -876,
            -2890,
            -2890,
            -1170,
            -1780,
            -2332,
            -1595,
            -221,
            -2890,
            -985,
            -1249,
            -995,
            -2890,
            -419,
            -625,
            -1019,
            -702,
            -1556,
            -1610,
            -504,
            -267,
            -1046,
            -1992,
            -1566,
            -666,
            -652,
            -1120,
            -832,
            -263,
            -635
        ],
        "steps_history": [
            834,
            2890,
            977,
            2890,
            2890,
            1271,
            1881,
            2433,
            1696,
            322,
            2890,
            1086,
            1350,
            1096,
            2890,
            520,
            726,
            1120,
            803,
            1657,
            1711,
            605,
            368,
            1147,
            2093,
            1667,
            767,
            753,
            1221,
            933,
            364,
            736
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "520/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.743522882461548,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.8512110726643599,
            0.7854671280276817,
            0.889273356401384,
            0.8408304498269896,
            0.8131487889273357,
            0.7681660899653979,
            0.8408304498269896,
            0.9273356401384083,
            0.8512110726643599,
            0.8512110726643599,
            0.8615916955017301,
            0.8546712802768166,
            0.9446366782006921,
            0.9342560553633218,
            0.9273356401384083,
            0.8719723183391004,
            0.9377162629757786,
            0.9307958477508651,
            0.8996539792387543,
            0.9307958477508651,
            0.9411764705882353,
            0.9377162629757786,
            0.9480968858131488,
            0.9515570934256056,
            0.9446366782006921,
            0.9896193771626297,
            0.9480968858131488,
            0.9792387543252595,
            0.986159169550173,
            0.986159169550173,
            0.986159169550173,
            0.9792387543252595,
            0.9757785467128027,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -1124,
            -2201,
            -2890,
            -1601,
            -328,
            -1536,
            -1805,
            -1998,
            -1672,
            -309,
            -504,
            -558,
            -1836,
            -664,
            -745,
            -1377,
            -828,
            -1041,
            -1771,
            -912,
            -594,
            -1575,
            -361,
            -1502,
            -492,
            -804,
            -391,
            -593,
            -503,
            -401,
            -230,
            -370,
            -356
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            1225,
            2302,
            2890,
            1702,
            429,
            1637,
            1906,
            2099,
            1773,
            410,
            605,
            659,
            1937,
            765,
            846,
            1478,
            929,
            1142,
            1872,
            1013,
            695,
            1676,
            462,
            1603,
            593,
            905,
            492,
            694,
            604,
            502,
            331,
            471,
            457
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "521/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.9436211585998535,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 38,
        "policy_stability_history": [
            0.0,
            0.7647058823529411,
            0.8235294117647058,
            0.8269896193771626,
            0.7716262975778547,
            0.7889273356401384,
            0.8166089965397924,
            0.916955017301038,
            0.9134948096885813,
            0.8685121107266436,
            0.8719723183391004,
            0.8788927335640139,
            0.8823529411764706,
            0.972318339100346,
            0.8788927335640139,
            0.9238754325259516,
            0.9480968858131488,
            0.9307958477508651,
            0.9792387543252595,
            0.9757785467128027,
            0.9826989619377162,
            0.9792387543252595,
            0.972318339100346,
            0.9792387543252595,
            0.9653979238754326,
            1.0,
            0.9653979238754326,
            0.9896193771626297,
            0.9826989619377162,
            0.9411764705882353,
            0.9757785467128027,
            0.9550173010380623,
            0.9930795847750865,
            0.9757785467128027,
            0.9757785467128027,
            0.9965397923875432,
            1.0,
            0.9965397923875432,
            1.0
        ],
        "reward_history": [
            -2890,
            -2212,
            -2223,
            -2142,
            -2890,
            -2890,
            -1551,
            -751,
            -2890,
            -1895,
            -1470,
            -1569,
            -1840,
            -213,
            -2015,
            -1169,
            -1232,
            -1177,
            -541,
            -433,
            -336,
            -419,
            -676,
            -535,
            -1821,
            -115,
            -901,
            -658,
            -705,
            -1035,
            -911,
            -1183,
            -424,
            -1071,
            -881,
            -448,
            -526,
            -429,
            -419
        ],
        "steps_history": [
            2890,
            2313,
            2324,
            2243,
            2890,
            2890,
            1652,
            852,
            2890,
            1996,
            1571,
            1670,
            1941,
            314,
            2116,
            1270,
            1333,
            1278,
            642,
            534,
            437,
            520,
            777,
            636,
            1922,
            216,
            1002,
            759,
            806,
            1136,
            1012,
            1284,
            525,
            1172,
            982,
            549,
            627,
            530,
            520
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "522/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.934272289276123,
        "final_policy_stability": 0.9930795847750865,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8200692041522492,
            0.8373702422145328,
            0.7993079584775087,
            0.8269896193771626,
            0.8650519031141869,
            0.8581314878892734,
            0.9065743944636678,
            0.8719723183391004,
            0.8823529411764706,
            0.9100346020761245,
            0.903114186851211,
            0.9273356401384083,
            0.8615916955017301,
            0.9100346020761245,
            0.9584775086505191,
            0.9204152249134948,
            0.9757785467128027,
            0.9307958477508651,
            0.9238754325259516,
            0.9757785467128027,
            0.9688581314878892,
            0.9550173010380623,
            0.9584775086505191,
            0.9307958477508651,
            0.9480968858131488,
            0.9826989619377162,
            0.9619377162629758,
            0.9896193771626297,
            0.9930795847750865,
            1.0,
            0.9930795847750865
        ],
        "reward_history": [
            -2890,
            -1734,
            -2890,
            -2890,
            -1547,
            -1342,
            -2890,
            -687,
            -967,
            -1187,
            -846,
            -846,
            -601,
            -2062,
            -1321,
            -441,
            -968,
            -254,
            -851,
            -1180,
            -749,
            -408,
            -655,
            -652,
            -1095,
            -471,
            -210,
            -1876,
            -508,
            -285,
            -246,
            -500
        ],
        "steps_history": [
            2890,
            1835,
            2890,
            2890,
            1648,
            1443,
            2890,
            788,
            1068,
            1288,
            947,
            947,
            702,
            2163,
            1422,
            542,
            1069,
            355,
            952,
            1281,
            850,
            509,
            756,
            753,
            1196,
            572,
            311,
            1977,
            609,
            386,
            347,
            601
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "523/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.990542650222778,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.7923875432525952,
            0.7889273356401384,
            0.8615916955017301,
            0.8373702422145328,
            0.8442906574394463,
            0.8096885813148789,
            0.8477508650519031,
            0.8408304498269896,
            0.9273356401384083,
            0.8961937716262975,
            0.9273356401384083,
            0.9550173010380623,
            0.9065743944636678,
            0.903114186851211,
            0.9204152249134948,
            0.9273356401384083,
            0.9480968858131488,
            0.8996539792387543,
            0.9688581314878892,
            0.9065743944636678,
            0.9342560553633218,
            0.9653979238754326,
            0.9377162629757786,
            0.9411764705882353,
            0.972318339100346,
            0.972318339100346,
            0.9965397923875432,
            0.9965397923875432,
            0.9930795847750865,
            0.986159169550173,
            0.9653979238754326,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -1272,
            -2698,
            -2092,
            -2890,
            -2095,
            -2109,
            -1213,
            -2200,
            -693,
            -901,
            -950,
            -405,
            -1271,
            -1019,
            -989,
            -1057,
            -676,
            -1089,
            -460,
            -1670,
            -1224,
            -804,
            -1360,
            -1177,
            -1138,
            -625,
            -317,
            -477,
            -187,
            -657,
            -915,
            -628,
            -239
        ],
        "steps_history": [
            2890,
            1373,
            2799,
            2193,
            2890,
            2196,
            2210,
            1314,
            2301,
            794,
            1002,
            1051,
            506,
            1372,
            1120,
            1090,
            1158,
            777,
            1190,
            561,
            1771,
            1325,
            905,
            1461,
            1278,
            1239,
            726,
            418,
            578,
            288,
            758,
            1016,
            729,
            340
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "524/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.159954309463501,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.7716262975778547,
            0.903114186851211,
            0.7647058823529411,
            0.8615916955017301,
            0.8062283737024222,
            0.8477508650519031,
            0.8235294117647058,
            0.9307958477508651,
            0.8615916955017301,
            0.903114186851211,
            0.8961937716262975,
            0.9100346020761245,
            0.8235294117647058,
            0.8408304498269896,
            0.972318339100346,
            0.8546712802768166,
            0.9619377162629758,
            0.9134948096885813,
            0.9307958477508651,
            0.9619377162629758,
            0.9480968858131488,
            0.9307958477508651,
            0.9653979238754326,
            0.9515570934256056,
            0.9792387543252595,
            0.9377162629757786,
            0.986159169550173,
            0.9826989619377162,
            0.986159169550173,
            0.972318339100346,
            0.9896193771626297,
            0.9930795847750865,
            1.0,
            0.9757785467128027,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -497,
            -2890,
            -1595,
            -2724,
            -1489,
            -2890,
            -337,
            -1841,
            -933,
            -1515,
            -844,
            -1915,
            -1647,
            -236,
            -1387,
            -439,
            -1968,
            -1107,
            -427,
            -596,
            -784,
            -567,
            -661,
            -354,
            -1804,
            -426,
            -285,
            -760,
            -676,
            -486,
            -759,
            -328,
            -960,
            -333
        ],
        "steps_history": [
            2890,
            2890,
            598,
            2890,
            1696,
            2825,
            1590,
            2890,
            438,
            1942,
            1034,
            1616,
            945,
            2016,
            1748,
            337,
            1488,
            540,
            2069,
            1208,
            528,
            697,
            885,
            668,
            762,
            455,
            1905,
            527,
            386,
            861,
            777,
            587,
            860,
            429,
            1061,
            434
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "525/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.84991192817688,
        "final_policy_stability": 0.9930795847750865,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.8788927335640139,
            0.7820069204152249,
            0.8823529411764706,
            0.8650519031141869,
            0.8719723183391004,
            0.8235294117647058,
            0.8581314878892734,
            0.9065743944636678,
            0.8408304498269896,
            0.889273356401384,
            0.8927335640138409,
            0.9238754325259516,
            0.9100346020761245,
            0.9446366782006921,
            0.9065743944636678,
            0.9930795847750865,
            0.8858131487889274,
            0.9584775086505191,
            0.9757785467128027,
            0.9411764705882353,
            0.9965397923875432,
            0.889273356401384,
            0.9584775086505191,
            0.9100346020761245,
            0.986159169550173,
            0.9930795847750865,
            0.9896193771626297,
            0.9757785467128027,
            0.986159169550173,
            0.9896193771626297,
            1.0,
            0.9826989619377162,
            0.972318339100346,
            0.9965397923875432,
            1.0,
            0.9930795847750865
        ],
        "reward_history": [
            -2890,
            -2890,
            -2226,
            -2890,
            -2890,
            -1706,
            -2890,
            -2890,
            -1739,
            -2890,
            -2890,
            -1324,
            -2890,
            -1279,
            -586,
            -2890,
            -244,
            -2730,
            -681,
            -773,
            -1635,
            -200,
            -2672,
            -1503,
            -2068,
            -755,
            -1602,
            -1092,
            -1490,
            -361,
            -793,
            -1263,
            -286,
            -765,
            -716,
            -590,
            -1586
        ],
        "steps_history": [
            2890,
            2890,
            2327,
            2890,
            2890,
            1807,
            2890,
            2890,
            1840,
            2890,
            2890,
            1425,
            2890,
            1380,
            687,
            2890,
            345,
            2831,
            782,
            874,
            1736,
            301,
            2773,
            1604,
            2169,
            856,
            1703,
            1193,
            1591,
            462,
            894,
            1364,
            387,
            866,
            817,
            691,
            1687
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "526/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.359838962554932,
        "final_policy_stability": 0.9826989619377162,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.7681660899653979,
            0.903114186851211,
            0.7785467128027682,
            0.9342560553633218,
            0.8235294117647058,
            0.8823529411764706,
            0.7923875432525952,
            0.8408304498269896,
            0.8650519031141869,
            0.8339100346020761,
            0.903114186851211,
            0.8408304498269896,
            0.8823529411764706,
            0.9550173010380623,
            0.8927335640138409,
            0.9377162629757786,
            0.9688581314878892,
            0.9273356401384083,
            0.9515570934256056,
            0.9757785467128027,
            0.9688581314878892,
            1.0,
            0.9757785467128027,
            0.9480968858131488,
            0.9446366782006921,
            0.9619377162629758,
            0.9930795847750865,
            0.9930795847750865,
            0.9896193771626297,
            0.9896193771626297,
            0.9515570934256056,
            1.0,
            0.9965397923875432,
            0.9826989619377162
        ],
        "reward_history": [
            -2890,
            -2676,
            -2890,
            -2890,
            -467,
            -2890,
            -2283,
            -2309,
            -2890,
            -2890,
            -2536,
            -2131,
            -2890,
            -2563,
            -822,
            -2890,
            -1895,
            -723,
            -1880,
            -708,
            -834,
            -479,
            -303,
            -1265,
            -1208,
            -2890,
            -2081,
            -631,
            -596,
            -720,
            -1127,
            -2068,
            -547,
            -862,
            -2146
        ],
        "steps_history": [
            2890,
            2777,
            2890,
            2890,
            568,
            2890,
            2384,
            2410,
            2890,
            2890,
            2637,
            2232,
            2890,
            2664,
            923,
            2890,
            1996,
            824,
            1981,
            809,
            935,
            580,
            404,
            1366,
            1309,
            2890,
            2182,
            732,
            697,
            821,
            1228,
            2169,
            648,
            963,
            2247
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "527/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.195459604263306,
        "final_policy_stability": 0.9896193771626297,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8961937716262975,
            0.8235294117647058,
            0.8754325259515571,
            0.8096885813148789,
            0.7820069204152249,
            0.8650519031141869,
            0.8477508650519031,
            0.9377162629757786,
            0.8269896193771626,
            0.9238754325259516,
            0.8858131487889274,
            0.8442906574394463,
            0.9238754325259516,
            0.8442906574394463,
            0.8685121107266436,
            0.8788927335640139,
            0.9307958477508651,
            0.972318339100346,
            0.9411764705882353,
            0.9653979238754326,
            0.9757785467128027,
            0.9757785467128027,
            0.9377162629757786,
            1.0,
            0.9688581314878892,
            0.972318339100346,
            0.9653979238754326,
            1.0,
            0.9965397923875432,
            0.9653979238754326,
            0.9896193771626297
        ],
        "reward_history": [
            -1543,
            -2890,
            -2890,
            -1965,
            -2890,
            -2890,
            -1503,
            -2551,
            -930,
            -2759,
            -639,
            -1473,
            -2118,
            -833,
            -2890,
            -2314,
            -2890,
            -1937,
            -921,
            -1200,
            -1033,
            -1235,
            -327,
            -1473,
            -262,
            -966,
            -955,
            -1448,
            -391,
            -1071,
            -1368,
            -522
        ],
        "steps_history": [
            1644,
            2890,
            2890,
            2066,
            2890,
            2890,
            1604,
            2652,
            1031,
            2860,
            740,
            1574,
            2219,
            934,
            2890,
            2415,
            2890,
            2038,
            1022,
            1301,
            1134,
            1336,
            428,
            1574,
            363,
            1067,
            1056,
            1549,
            492,
            1172,
            1469,
            623
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "528/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.175928831100464,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.7958477508650519,
            0.8304498269896193,
            0.8546712802768166,
            0.8304498269896193,
            0.8373702422145328,
            0.7993079584775087,
            0.8615916955017301,
            0.8996539792387543,
            0.8927335640138409,
            0.8546712802768166,
            0.8961937716262975,
            0.8408304498269896,
            0.9515570934256056,
            0.9134948096885813,
            0.8615916955017301,
            0.9446366782006921,
            0.8927335640138409,
            0.9515570934256056,
            0.9342560553633218,
            0.9446366782006921,
            0.9688581314878892,
            0.9238754325259516,
            0.9480968858131488,
            0.9826989619377162,
            0.9377162629757786,
            0.9792387543252595,
            0.972318339100346,
            0.9826989619377162,
            0.9792387543252595,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -793,
            -2890,
            -1886,
            -2890,
            -1048,
            -749,
            -1236,
            -2890,
            -899,
            -2890,
            -472,
            -1344,
            -2890,
            -758,
            -2890,
            -964,
            -1249,
            -1148,
            -806,
            -1347,
            -1400,
            -1474,
            -2589,
            -1068,
            -1386,
            -1299,
            -872,
            -534,
            -440,
            -393
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            894,
            2890,
            1987,
            2890,
            1149,
            850,
            1337,
            2890,
            1000,
            2890,
            573,
            1445,
            2890,
            859,
            2890,
            1065,
            1350,
            1249,
            907,
            1448,
            1501,
            1575,
            2690,
            1169,
            1487,
            1400,
            973,
            635,
            541,
            494
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "529/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.0753092765808105,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8339100346020761,
            0.8373702422145328,
            0.8442906574394463,
            0.8685121107266436,
            0.8512110726643599,
            0.8408304498269896,
            0.9134948096885813,
            0.9273356401384083,
            0.8996539792387543,
            0.8408304498269896,
            0.8477508650519031,
            0.8615916955017301,
            0.8927335640138409,
            0.9307958477508651,
            0.889273356401384,
            0.9480968858131488,
            0.889273356401384,
            0.9688581314878892,
            0.9653979238754326,
            0.9550173010380623,
            0.9515570934256056,
            0.9896193771626297,
            0.9826989619377162,
            0.9238754325259516,
            0.9826989619377162,
            0.986159169550173,
            0.9515570934256056,
            0.9930795847750865,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -1439,
            -2890,
            -1672,
            -2890,
            -801,
            -786,
            -2890,
            -2482,
            -2658,
            -2890,
            -1098,
            -855,
            -2094,
            -505,
            -2890,
            -1275,
            -733,
            -1255,
            -1000,
            -759,
            -407,
            -1535,
            -838,
            -1555,
            -1814,
            -706,
            -594,
            -517,
            -458
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            1540,
            2890,
            1773,
            2890,
            902,
            887,
            2890,
            2583,
            2759,
            2890,
            1199,
            956,
            2195,
            606,
            2890,
            1376,
            834,
            1356,
            1101,
            860,
            508,
            1636,
            939,
            1656,
            1915,
            807,
            695,
            618,
            559
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "530/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.1959452629089355,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8062283737024222,
            0.8373702422145328,
            0.8685121107266436,
            0.8339100346020761,
            0.8927335640138409,
            0.9065743944636678,
            0.8235294117647058,
            0.8442906574394463,
            0.8304498269896193,
            0.8961937716262975,
            0.8996539792387543,
            0.8996539792387543,
            0.9273356401384083,
            0.8304498269896193,
            0.9411764705882353,
            0.9273356401384083,
            0.9515570934256056,
            0.9342560553633218,
            0.972318339100346,
            0.9619377162629758,
            0.9273356401384083,
            0.9792387543252595,
            0.9965397923875432,
            0.9619377162629758,
            0.9377162629757786,
            0.986159169550173,
            0.9480968858131488,
            0.9792387543252595,
            0.9757785467128027,
            0.9826989619377162,
            1.0,
            1.0,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -2576,
            -1907,
            -2890,
            -1959,
            -2890,
            -539,
            -2890,
            -2477,
            -2890,
            -847,
            -1337,
            -1423,
            -1134,
            -2890,
            -815,
            -1431,
            -1125,
            -1197,
            -408,
            -750,
            -2485,
            -792,
            -405,
            -1023,
            -1288,
            -292,
            -1049,
            -542,
            -1739,
            -1337,
            -341,
            -815,
            -559
        ],
        "steps_history": [
            2890,
            2677,
            2008,
            2890,
            2060,
            2890,
            640,
            2890,
            2578,
            2890,
            948,
            1438,
            1524,
            1235,
            2890,
            916,
            1532,
            1226,
            1298,
            509,
            851,
            2586,
            893,
            506,
            1124,
            1389,
            393,
            1150,
            643,
            1840,
            1438,
            442,
            916,
            660
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "531/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.058537244796753,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.7889273356401384,
            0.8304498269896193,
            0.7958477508650519,
            0.8062283737024222,
            0.8477508650519031,
            0.7923875432525952,
            0.8650519031141869,
            0.8719723183391004,
            0.8650519031141869,
            0.9100346020761245,
            0.972318339100346,
            0.916955017301038,
            0.8927335640138409,
            0.8442906574394463,
            0.9065743944636678,
            0.9307958477508651,
            0.9584775086505191,
            0.9515570934256056,
            0.9930795847750865,
            0.9619377162629758,
            0.986159169550173,
            0.9480968858131488,
            0.9446366782006921,
            0.9619377162629758,
            0.9411764705882353,
            0.9757785467128027,
            0.9584775086505191,
            0.9965397923875432,
            0.9930795847750865,
            1.0,
            0.9896193771626297,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2534,
            -2890,
            -2582,
            -2639,
            -1619,
            -2007,
            -2180,
            -2019,
            -203,
            -628,
            -1556,
            -1643,
            -1380,
            -1175,
            -606,
            -828,
            -593,
            -938,
            -297,
            -788,
            -901,
            -943,
            -1146,
            -666,
            -1694,
            -297,
            -917,
            -355,
            -879,
            -1176
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2635,
            2890,
            2683,
            2740,
            1720,
            2108,
            2281,
            2120,
            304,
            729,
            1657,
            1744,
            1481,
            1276,
            707,
            929,
            694,
            1039,
            398,
            889,
            1002,
            1044,
            1247,
            767,
            1795,
            398,
            1018,
            456,
            980,
            1277
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "532/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.335232496261597,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.7889273356401384,
            0.8166089965397924,
            0.8200692041522492,
            0.8754325259515571,
            0.8408304498269896,
            0.8788927335640139,
            0.8719723183391004,
            0.8823529411764706,
            0.9342560553633218,
            0.9584775086505191,
            0.9377162629757786,
            0.916955017301038,
            0.9653979238754326,
            0.8788927335640139,
            0.8304498269896193,
            0.9550173010380623,
            0.9584775086505191,
            0.9757785467128027,
            0.9619377162629758,
            0.9515570934256056,
            0.9792387543252595,
            0.972318339100346,
            0.9896193771626297,
            0.9792387543252595,
            0.9065743944636678,
            0.9896193771626297,
            0.9826989619377162,
            0.9930795847750865,
            0.9653979238754326,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -1458,
            -2890,
            -1893,
            -1505,
            -1271,
            -2012,
            -675,
            -225,
            -342,
            -1524,
            -256,
            -1319,
            -2890,
            -734,
            -616,
            -451,
            -587,
            -1091,
            -602,
            -888,
            -247,
            -415,
            -1986,
            -551,
            -315,
            -703,
            -718,
            -1335
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            1559,
            2890,
            1994,
            1606,
            1372,
            2113,
            776,
            326,
            443,
            1625,
            357,
            1420,
            2890,
            835,
            717,
            552,
            688,
            1192,
            703,
            989,
            348,
            516,
            2087,
            652,
            416,
            804,
            819,
            1436
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "533/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.9719085693359375,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 39,
        "policy_stability_history": [
            0.0,
            0.8927335640138409,
            0.7439446366782007,
            0.8062283737024222,
            0.889273356401384,
            0.8304498269896193,
            0.8788927335640139,
            0.8200692041522492,
            0.8269896193771626,
            0.9446366782006921,
            0.8927335640138409,
            0.8961937716262975,
            0.9550173010380623,
            0.9480968858131488,
            0.8788927335640139,
            0.903114186851211,
            0.8546712802768166,
            0.9100346020761245,
            0.986159169550173,
            0.9792387543252595,
            0.972318339100346,
            0.9480968858131488,
            0.9204152249134948,
            0.9584775086505191,
            0.9515570934256056,
            0.986159169550173,
            0.9515570934256056,
            0.9792387543252595,
            0.972318339100346,
            0.986159169550173,
            0.9965397923875432,
            0.9550173010380623,
            0.9965397923875432,
            0.9930795847750865,
            0.9965397923875432,
            0.9896193771626297,
            0.986159169550173,
            0.9965397923875432,
            0.9930795847750865,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2568,
            -2890,
            -1165,
            -2640,
            -709,
            -1838,
            -1721,
            -488,
            -1052,
            -1229,
            -480,
            -488,
            -2166,
            -1547,
            -2890,
            -1811,
            -105,
            -484,
            -531,
            -1396,
            -788,
            -941,
            -654,
            -238,
            -825,
            -657,
            -579,
            -1134,
            -598,
            -1412,
            -321,
            -537,
            -318,
            -663,
            -489,
            -250,
            -487,
            -376
        ],
        "steps_history": [
            2890,
            2890,
            2669,
            2890,
            1266,
            2741,
            810,
            1939,
            1822,
            589,
            1153,
            1330,
            581,
            589,
            2267,
            1648,
            2890,
            1912,
            206,
            585,
            632,
            1497,
            889,
            1042,
            755,
            339,
            926,
            758,
            680,
            1235,
            699,
            1513,
            422,
            638,
            419,
            764,
            590,
            351,
            588,
            477
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "534/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.3400397300720215,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.8650519031141869,
            0.9204152249134948,
            0.8754325259515571,
            0.7854671280276817,
            0.8823529411764706,
            0.8200692041522492,
            0.7923875432525952,
            0.8581314878892734,
            0.8685121107266436,
            0.8615916955017301,
            0.8408304498269896,
            0.9134948096885813,
            0.9446366782006921,
            0.9342560553633218,
            0.9065743944636678,
            0.9826989619377162,
            0.9480968858131488,
            0.9896193771626297,
            0.972318339100346,
            0.9307958477508651,
            0.9688581314878892,
            0.9653979238754326,
            0.9792387543252595,
            0.9896193771626297,
            0.9965397923875432,
            0.9965397923875432,
            1.0
        ],
        "reward_history": [
            -733,
            -2890,
            -876,
            -2890,
            -2890,
            -1250,
            -2395,
            -2890,
            -2025,
            -2890,
            -2171,
            -2890,
            -1696,
            -1271,
            -1556,
            -1610,
            -504,
            -1239,
            -466,
            -821,
            -2345,
            -666,
            -652,
            -947,
            -1208,
            -337,
            -358,
            -595
        ],
        "steps_history": [
            834,
            2890,
            977,
            2890,
            2890,
            1351,
            2496,
            2890,
            2126,
            2890,
            2272,
            2890,
            1797,
            1372,
            1657,
            1711,
            605,
            1340,
            567,
            922,
            2446,
            767,
            753,
            1048,
            1309,
            438,
            459,
            696
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "535/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.914544105529785,
        "final_policy_stability": 0.9965397923875432,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8512110726643599,
            0.7854671280276817,
            0.8719723183391004,
            0.8788927335640139,
            0.9065743944636678,
            0.8754325259515571,
            0.8546712802768166,
            0.8096885813148789,
            0.9377162629757786,
            0.8027681660899654,
            0.9619377162629758,
            0.8339100346020761,
            0.8754325259515571,
            0.9238754325259516,
            0.9411764705882353,
            0.9550173010380623,
            0.9515570934256056,
            0.9134948096885813,
            0.9480968858131488,
            0.9238754325259516,
            0.9480968858131488,
            0.9757785467128027,
            0.9688581314878892,
            0.9653979238754326,
            0.9446366782006921,
            0.9411764705882353,
            0.9653979238754326,
            0.9792387543252595,
            0.9653979238754326,
            0.9826989619377162,
            0.986159169550173,
            0.9757785467128027,
            0.9584775086505191,
            0.9757785467128027,
            0.986159169550173,
            1.0,
            0.9965397923875432
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -2890,
            -2890,
            -561,
            -1051,
            -2890,
            -2308,
            -523,
            -2350,
            -278,
            -2890,
            -1697,
            -616,
            -428,
            -416,
            -552,
            -1009,
            -662,
            -2199,
            -709,
            -365,
            -376,
            -789,
            -836,
            -1575,
            -791,
            -480,
            -491,
            -1397,
            -391,
            -296,
            -800,
            -299,
            -332,
            -370,
            -356
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            2890,
            2890,
            662,
            1152,
            2890,
            2409,
            624,
            2451,
            379,
            2890,
            1798,
            717,
            529,
            517,
            653,
            1110,
            763,
            2300,
            810,
            466,
            477,
            890,
            937,
            1676,
            892,
            581,
            592,
            1498,
            492,
            397,
            901,
            400,
            433,
            471,
            457
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "536/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.998809099197388,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8269896193771626,
            0.8304498269896193,
            0.8858131487889274,
            0.8269896193771626,
            0.8788927335640139,
            0.8442906574394463,
            0.9134948096885813,
            0.8823529411764706,
            0.8062283737024222,
            0.8235294117647058,
            0.889273356401384,
            0.8546712802768166,
            0.916955017301038,
            0.8927335640138409,
            0.889273356401384,
            0.9446366782006921,
            0.9446366782006921,
            0.9584775086505191,
            0.9619377162629758,
            0.9757785467128027,
            0.9619377162629758,
            0.9688581314878892,
            0.9757785467128027,
            0.972318339100346,
            0.9515570934256056,
            0.986159169550173,
            0.9965397923875432,
            0.9792387543252595,
            0.9930795847750865,
            0.9688581314878892,
            0.972318339100346,
            1.0,
            1.0
        ],
        "reward_history": [
            -2890,
            -2890,
            -2890,
            -1307,
            -2890,
            -794,
            -2475,
            -662,
            -847,
            -2890,
            -1669,
            -1791,
            -2199,
            -1473,
            -1781,
            -1398,
            -521,
            -2395,
            -433,
            -336,
            -419,
            -676,
            -535,
            -1074,
            -646,
            -1164,
            -611,
            -705,
            -644,
            -353,
            -848,
            -916,
            -309,
            -412
        ],
        "steps_history": [
            2890,
            2890,
            2890,
            1408,
            2890,
            895,
            2576,
            763,
            948,
            2890,
            1770,
            1892,
            2300,
            1574,
            1882,
            1499,
            622,
            2496,
            534,
            437,
            520,
            777,
            636,
            1175,
            747,
            1265,
            712,
            806,
            745,
            454,
            949,
            1017,
            410,
            513
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "537/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.0119311809539795,
        "final_policy_stability": 0.9896193771626297,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.8235294117647058,
            0.8304498269896193,
            0.7923875432525952,
            0.8269896193771626,
            0.7647058823529411,
            0.8442906574394463,
            0.8546712802768166,
            0.8961937716262975,
            0.8442906574394463,
            0.9377162629757786,
            0.9619377162629758,
            0.9307958477508651,
            0.8408304498269896,
            0.916955017301038,
            0.9134948096885813,
            0.8858131487889274,
            0.9307958477508651,
            0.9896193771626297,
            0.9273356401384083,
            0.8927335640138409,
            0.9653979238754326,
            0.916955017301038,
            0.9619377162629758,
            0.9688581314878892,
            0.9792387543252595,
            0.9688581314878892,
            0.9757785467128027,
            0.9653979238754326,
            0.9965397923875432,
            0.9792387543252595,
            0.9688581314878892,
            1.0,
            1.0,
            1.0,
            0.9896193771626297
        ],
        "reward_history": [
            -2890,
            -1734,
            -2890,
            -2890,
            -1460,
            -2890,
            -1458,
            -1789,
            -1075,
            -2319,
            -314,
            -180,
            -683,
            -1782,
            -1070,
            -786,
            -969,
            -876,
            -272,
            -917,
            -2191,
            -467,
            -1063,
            -996,
            -761,
            -577,
            -307,
            -246,
            -1213,
            -826,
            -558,
            -706,
            -265,
            -299,
            -579,
            -632
        ],
        "steps_history": [
            2890,
            1835,
            2890,
            2890,
            1561,
            2890,
            1559,
            1890,
            1176,
            2420,
            415,
            281,
            784,
            1883,
            1171,
            887,
            1070,
            977,
            373,
            1018,
            2292,
            568,
            1164,
            1097,
            862,
            678,
            408,
            347,
            1314,
            927,
            659,
            807,
            366,
            400,
            680,
            733
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "538/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.050034999847412,
        "final_policy_stability": 0.9688581314878892,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.7439446366782007,
            0.8166089965397924,
            0.8546712802768166,
            0.8615916955017301,
            0.8062283737024222,
            0.8823529411764706,
            0.9377162629757786,
            0.8512110726643599,
            0.916955017301038,
            0.8650519031141869,
            0.9273356401384083,
            0.8408304498269896,
            0.8858131487889274,
            0.9307958477508651,
            0.8996539792387543,
            0.9377162629757786,
            0.8719723183391004,
            0.8512110726643599,
            0.9411764705882353,
            0.9757785467128027,
            0.9757785467128027,
            0.9480968858131488,
            0.9273356401384083,
            0.9653979238754326,
            0.9584775086505191,
            0.986159169550173,
            0.9792387543252595,
            1.0,
            1.0,
            0.9757785467128027,
            0.9688581314878892
        ],
        "reward_history": [
            -2890,
            -2747,
            -2231,
            -1219,
            -2890,
            -2513,
            -792,
            -369,
            -1850,
            -433,
            -1231,
            -573,
            -2890,
            -1204,
            -967,
            -943,
            -1103,
            -1838,
            -2890,
            -448,
            -234,
            -715,
            -906,
            -989,
            -541,
            -1424,
            -294,
            -362,
            -477,
            -187,
            -657,
            -915
        ],
        "steps_history": [
            2890,
            2848,
            2332,
            1320,
            2890,
            2614,
            893,
            470,
            1951,
            534,
            1332,
            674,
            2890,
            1305,
            1068,
            1044,
            1204,
            1939,
            2890,
            549,
            335,
            816,
            1007,
            1090,
            642,
            1525,
            395,
            463,
            578,
            288,
            758,
            1016
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "539/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 8,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 40.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.816203832626343,
        "final_policy_stability": 0.9757785467128027,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.7716262975778547,
            0.8996539792387543,
            0.7647058823529411,
            0.8615916955017301,
            0.8062283737024222,
            0.8477508650519031,
            0.8408304498269896,
            0.8546712802768166,
            0.8096885813148789,
            0.8823529411764706,
            0.8477508650519031,
            0.8477508650519031,
            0.9342560553633218,
            0.9238754325259516,
            0.8788927335640139,
            0.9134948096885813,
            0.9307958477508651,
            0.9273356401384083,
            0.9550173010380623,
            0.9273356401384083,
            0.9653979238754326,
            0.9307958477508651,
            0.9792387543252595,
            0.9896193771626297,
            0.9584775086505191,
            0.9584775086505191,
            0.9896193771626297,
            0.986159169550173,
            0.9792387543252595,
            0.9792387543252595,
            0.9965397923875432,
            0.9930795847750865,
            1.0,
            0.9757785467128027
        ],
        "reward_history": [
            -2890,
            -2890,
            -497,
            -2890,
            -1595,
            -2724,
            -1489,
            -2890,
            -1576,
            -2568,
            -1339,
            -1217,
            -1378,
            -554,
            -737,
            -1387,
            -615,
            -554,
            -1070,
            -510,
            -1228,
            -459,
            -784,
            -280,
            -186,
            -1116,
            -1389,
            -661,
            -362,
            -863,
            -676,
            -486,
            -759,
            -328,
            -708
        ],
        "steps_history": [
            2890,
            2890,
            598,
            2890,
            1696,
            2825,
            1590,
            2890,
            1677,
            2669,
            1440,
            1318,
            1479,
            655,
            838,
            1488,
            716,
            655,
            1171,
            611,
            1329,
            560,
            885,
            381,
            287,
            1217,
            1490,
            762,
            463,
            964,
            777,
            587,
            860,
            429,
            809
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "540/810",
        "save_path": "experiments/20250131_160708/training_plots/size_8/lr0.4_df0.99_eps0.1_trial4"
    }
]