[
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.852464199066162,
        "final_policy_stability": 0.9876543209876543,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8148148148148148,
            0.8271604938271605,
            0.7407407407407407,
            0.9382716049382716,
            0.8888888888888888,
            0.9012345679012346,
            0.9259259259259259,
            0.9259259259259259,
            1.0,
            0.9506172839506173,
            0.9876543209876543
        ],
        "reward_history": [
            -199,
            -248,
            -810,
            -512,
            -56,
            -137,
            -127,
            -810,
            -564,
            -10,
            -269,
            -810
        ],
        "steps_history": [
            300,
            349,
            810,
            613,
            157,
            238,
            228,
            810,
            665,
            111,
            370,
            810
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "91/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.119729995727539,
        "final_policy_stability": 0.9259259259259259,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8395061728395061,
            0.8395061728395061,
            0.8765432098765432,
            0.8395061728395061,
            0.9259259259259259,
            0.9629629629629629,
            1.0,
            1.0,
            0.9135802469135802,
            0.9259259259259259
        ],
        "reward_history": [
            -379,
            -130,
            -810,
            -270,
            -810,
            -39,
            -7,
            -13,
            71,
            -181,
            -365
        ],
        "steps_history": [
            480,
            231,
            810,
            371,
            810,
            140,
            108,
            114,
            30,
            282,
            466
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "92/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7948925495147705,
        "final_policy_stability": 0.9753086419753086,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8765432098765432,
            0.9753086419753086,
            0.9259259259259259,
            0.7530864197530864,
            0.8271604938271605,
            0.9629629629629629,
            0.9629629629629629,
            0.9382716049382716,
            0.8765432098765432,
            0.9382716049382716,
            0.9259259259259259,
            0.9506172839506173,
            1.0,
            0.9506172839506173,
            0.9753086419753086
        ],
        "reward_history": [
            -141,
            -79,
            53,
            -22,
            -810,
            -810,
            51,
            18,
            -309,
            -810,
            -320,
            -810,
            -501,
            -48,
            -135,
            26
        ],
        "steps_history": [
            242,
            180,
            48,
            123,
            810,
            810,
            50,
            83,
            410,
            810,
            421,
            810,
            602,
            149,
            236,
            75
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "93/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.749566078186035,
        "final_policy_stability": 0.9753086419753086,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.9629629629629629,
            0.8765432098765432,
            0.9259259259259259,
            0.8395061728395061,
            0.9382716049382716,
            0.8641975308641975,
            0.9135802469135802,
            0.9382716049382716,
            0.8518518518518519,
            0.9876543209876543,
            0.8888888888888888,
            1.0,
            0.8765432098765432,
            1.0,
            0.9753086419753086
        ],
        "reward_history": [
            -682,
            48,
            -92,
            -175,
            -268,
            36,
            -810,
            -321,
            -38,
            -419,
            25,
            -409,
            46,
            -810,
            -89,
            -134
        ],
        "steps_history": [
            783,
            53,
            193,
            276,
            369,
            65,
            810,
            422,
            139,
            520,
            76,
            510,
            55,
            810,
            190,
            235
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "94/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.3789167404174805,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8395061728395061,
            0.9506172839506173,
            0.8888888888888888,
            0.9506172839506173,
            0.9135802469135802,
            0.9506172839506173,
            0.9506172839506173,
            0.9259259259259259,
            0.9135802469135802,
            0.9876543209876543,
            0.9876543209876543,
            0.9382716049382716,
            0.9753086419753086,
            0.9753086419753086,
            0.9753086419753086,
            1.0,
            0.9876543209876543,
            1.0
        ],
        "reward_history": [
            -810,
            -336,
            -18,
            -145,
            53,
            -148,
            15,
            -42,
            -244,
            -810,
            -37,
            70,
            -117,
            11,
            -348,
            -5,
            10,
            -32,
            43
        ],
        "steps_history": [
            810,
            437,
            119,
            246,
            48,
            249,
            86,
            143,
            345,
            810,
            138,
            31,
            218,
            90,
            449,
            106,
            91,
            133,
            58
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "95/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.724931001663208,
        "final_policy_stability": 0.9506172839506173,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.7407407407407407,
            0.7654320987654321,
            0.8148148148148148,
            0.9135802469135802,
            0.8271604938271605,
            0.9753086419753086,
            0.8641975308641975,
            0.9259259259259259,
            1.0,
            0.8765432098765432,
            0.9753086419753086,
            0.9876543209876543,
            1.0,
            0.9506172839506173
        ],
        "reward_history": [
            -96,
            -810,
            -402,
            -643,
            -15,
            -810,
            77,
            -448,
            -168,
            -8,
            -251,
            37,
            3,
            37,
            -112
        ],
        "steps_history": [
            197,
            810,
            503,
            744,
            116,
            810,
            24,
            549,
            269,
            109,
            352,
            64,
            98,
            64,
            213
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "96/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8036255836486816,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.9012345679012346,
            0.9629629629629629,
            0.9012345679012346,
            0.9382716049382716,
            0.8765432098765432,
            0.8518518518518519,
            0.9629629629629629,
            1.0,
            0.9876543209876543,
            0.9012345679012346,
            0.8888888888888888,
            1.0,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            -79,
            1,
            -133,
            -61,
            -655,
            -810,
            -113,
            -34,
            43,
            -810,
            -384,
            44,
            25
        ],
        "steps_history": [
            810,
            810,
            180,
            100,
            234,
            162,
            756,
            810,
            214,
            135,
            58,
            810,
            485,
            57,
            76
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "97/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7856290340423584,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8765432098765432,
            0.8395061728395061,
            0.8148148148148148,
            0.8271604938271605,
            0.9382716049382716,
            0.9753086419753086,
            0.8888888888888888,
            0.9382716049382716,
            1.0,
            1.0,
            0.9876543209876543,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -285,
            -810,
            -566,
            -810,
            -616,
            -290,
            -151,
            -253,
            -129,
            53,
            -9,
            39,
            6,
            -82,
            41,
            18
        ],
        "steps_history": [
            386,
            810,
            667,
            810,
            717,
            391,
            252,
            354,
            230,
            48,
            110,
            62,
            95,
            183,
            60,
            83
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "98/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8564555644989014,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.8395061728395061,
            0.8395061728395061,
            0.9876543209876543,
            0.9012345679012346,
            0.8765432098765432,
            0.9876543209876543,
            0.9876543209876543,
            0.9629629629629629,
            0.9876543209876543,
            0.9629629629629629,
            0.8518518518518519,
            0.9629629629629629,
            0.9506172839506173,
            1.0
        ],
        "reward_history": [
            -810,
            -614,
            -674,
            -14,
            -177,
            -294,
            -38,
            -1,
            -13,
            -35,
            -49,
            -810,
            -272,
            -108,
            16
        ],
        "steps_history": [
            810,
            715,
            775,
            115,
            278,
            395,
            139,
            102,
            114,
            136,
            150,
            810,
            373,
            209,
            85
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "99/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.9870495796203613,
        "final_policy_stability": 0.9629629629629629,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8518518518518519,
            0.8148148148148148,
            0.9629629629629629,
            0.9753086419753086,
            0.8271604938271605,
            0.9382716049382716,
            0.9753086419753086,
            0.9012345679012346,
            0.8271604938271605,
            0.9753086419753086,
            1.0,
            1.0,
            0.9629629629629629
        ],
        "reward_history": [
            -810,
            -335,
            -273,
            52,
            -69,
            -558,
            -393,
            2,
            -354,
            -810,
            15,
            -32,
            42,
            -155
        ],
        "steps_history": [
            810,
            436,
            374,
            49,
            170,
            659,
            494,
            99,
            455,
            810,
            86,
            133,
            59,
            256
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "100/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.422637462615967,
        "final_policy_stability": 0.9876543209876543,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.8271604938271605,
            0.8765432098765432,
            0.7407407407407407,
            0.8888888888888888,
            0.9135802469135802,
            0.8395061728395061,
            0.9629629629629629,
            0.9259259259259259,
            0.8641975308641975,
            0.9876543209876543,
            1.0,
            1.0,
            0.9753086419753086,
            1.0,
            1.0,
            0.9753086419753086,
            0.9876543209876543
        ],
        "reward_history": [
            -352,
            -810,
            -211,
            -588,
            -154,
            -126,
            -392,
            19,
            -357,
            -810,
            14,
            -175,
            26,
            -53,
            -2,
            -74,
            -26,
            -30
        ],
        "steps_history": [
            453,
            810,
            312,
            689,
            255,
            227,
            493,
            82,
            458,
            810,
            87,
            276,
            75,
            154,
            103,
            175,
            127,
            131
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "101/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8137142658233643,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8148148148148148,
            0.8148148148148148,
            0.9135802469135802,
            0.9876543209876543,
            0.8271604938271605,
            0.9753086419753086,
            0.9382716049382716,
            0.9876543209876543,
            0.8271604938271605,
            0.9629629629629629,
            1.0,
            0.9753086419753086,
            1.0
        ],
        "reward_history": [
            -627,
            -407,
            -466,
            -177,
            33,
            -810,
            -14,
            -137,
            66,
            -452,
            -106,
            -17,
            -158,
            -56
        ],
        "steps_history": [
            728,
            508,
            567,
            278,
            68,
            810,
            115,
            238,
            35,
            553,
            207,
            118,
            259,
            157
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "102/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8073770999908447,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8888888888888888,
            0.8395061728395061,
            0.9382716049382716,
            0.9259259259259259,
            0.8148148148148148,
            0.8395061728395061,
            0.9382716049382716,
            0.9506172839506173,
            0.9753086419753086,
            0.9753086419753086,
            0.8765432098765432,
            0.9753086419753086,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -522,
            -810,
            -124,
            -31,
            -25,
            -810,
            -810,
            -104,
            62,
            -174,
            -125,
            -145,
            -17,
            53,
            -9,
            34
        ],
        "steps_history": [
            623,
            810,
            225,
            132,
            126,
            810,
            810,
            205,
            39,
            275,
            226,
            246,
            118,
            48,
            110,
            67
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "103/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.484137773513794,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8641975308641975,
            0.7654320987654321,
            0.9506172839506173,
            0.9012345679012346,
            0.8271604938271605,
            0.9382716049382716,
            0.8888888888888888,
            0.9629629629629629,
            0.9753086419753086,
            0.8148148148148148,
            0.9876543209876543,
            0.8395061728395061,
            0.9629629629629629,
            0.8765432098765432,
            0.9876543209876543,
            0.9876543209876543,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -692,
            -166,
            -491,
            58,
            -46,
            -810,
            -24,
            -195,
            50,
            67,
            -409,
            27,
            -345,
            -13,
            -810,
            56,
            2,
            15,
            45,
            39
        ],
        "steps_history": [
            793,
            267,
            592,
            43,
            147,
            810,
            125,
            296,
            51,
            34,
            510,
            74,
            446,
            114,
            810,
            45,
            99,
            86,
            56,
            62
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "104/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4574780464172363,
        "final_policy_stability": 0.9876543209876543,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.8641975308641975,
            0.9012345679012346,
            0.8765432098765432,
            0.9629629629629629,
            0.8888888888888888,
            0.9629629629629629,
            0.9876543209876543,
            0.8765432098765432,
            0.9382716049382716,
            0.9506172839506173,
            0.9753086419753086,
            0.9753086419753086,
            0.9753086419753086,
            1.0,
            0.8888888888888888,
            1.0,
            0.9876543209876543
        ],
        "reward_history": [
            -810,
            -810,
            -48,
            -225,
            48,
            -114,
            56,
            -13,
            -138,
            -155,
            -142,
            12,
            -5,
            46,
            38,
            -253,
            20,
            -33
        ],
        "steps_history": [
            810,
            810,
            149,
            326,
            53,
            215,
            45,
            114,
            239,
            256,
            243,
            89,
            106,
            55,
            63,
            354,
            81,
            134
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "105/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8795135021209717,
        "final_policy_stability": 0.9629629629629629,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8148148148148148,
            0.8271604938271605,
            0.7407407407407407,
            0.9135802469135802,
            0.9135802469135802,
            0.9259259259259259,
            0.9135802469135802,
            0.9012345679012346,
            1.0,
            0.9629629629629629,
            0.9629629629629629
        ],
        "reward_history": [
            -199,
            -248,
            -810,
            -512,
            -56,
            -137,
            -127,
            -382,
            -810,
            -1,
            -460,
            -810
        ],
        "steps_history": [
            300,
            349,
            810,
            613,
            157,
            238,
            228,
            483,
            810,
            102,
            561,
            810
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "106/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7903573513031006,
        "final_policy_stability": 0.9876543209876543,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.8395061728395061,
            0.8395061728395061,
            0.8765432098765432,
            0.8271604938271605,
            0.9629629629629629,
            0.9876543209876543,
            1.0,
            0.9876543209876543,
            0.9382716049382716,
            0.9753086419753086,
            0.9382716049382716,
            1.0,
            1.0,
            0.9876543209876543
        ],
        "reward_history": [
            -379,
            -130,
            -810,
            -270,
            -810,
            -39,
            -7,
            -13,
            71,
            -75,
            -5,
            -98,
            18,
            -83,
            -280
        ],
        "steps_history": [
            480,
            231,
            810,
            371,
            810,
            140,
            108,
            114,
            30,
            176,
            106,
            199,
            83,
            184,
            381
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "107/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8222861289978027,
        "final_policy_stability": 0.9753086419753086,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8765432098765432,
            0.9753086419753086,
            0.9259259259259259,
            0.7530864197530864,
            0.8271604938271605,
            0.9629629629629629,
            0.9629629629629629,
            0.9382716049382716,
            0.8765432098765432,
            0.9382716049382716,
            0.9259259259259259,
            0.9506172839506173,
            1.0,
            0.9506172839506173,
            0.9753086419753086
        ],
        "reward_history": [
            -141,
            -79,
            53,
            -22,
            -810,
            -810,
            51,
            18,
            -309,
            -810,
            -320,
            -810,
            -501,
            -48,
            -135,
            26
        ],
        "steps_history": [
            242,
            180,
            48,
            123,
            810,
            810,
            50,
            83,
            410,
            810,
            421,
            810,
            602,
            149,
            236,
            75
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "108/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8076364994049072,
        "final_policy_stability": 0.9753086419753086,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.9629629629629629,
            0.8765432098765432,
            0.9259259259259259,
            0.8395061728395061,
            0.9382716049382716,
            0.8641975308641975,
            0.9135802469135802,
            0.9382716049382716,
            0.8395061728395061,
            0.9876543209876543,
            0.8888888888888888,
            1.0,
            0.8888888888888888,
            1.0,
            0.9753086419753086
        ],
        "reward_history": [
            -682,
            48,
            -92,
            -175,
            -268,
            36,
            -810,
            -321,
            -38,
            -419,
            25,
            -409,
            46,
            -810,
            -89,
            -134
        ],
        "steps_history": [
            783,
            53,
            193,
            276,
            369,
            65,
            810,
            422,
            139,
            520,
            76,
            510,
            55,
            810,
            190,
            235
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "109/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4768216609954834,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8395061728395061,
            0.9506172839506173,
            0.8888888888888888,
            0.9506172839506173,
            0.9135802469135802,
            0.9506172839506173,
            0.9506172839506173,
            0.9259259259259259,
            0.9135802469135802,
            0.9876543209876543,
            0.9876543209876543,
            0.9382716049382716,
            0.9753086419753086,
            0.9753086419753086,
            0.9753086419753086,
            1.0,
            0.9876543209876543,
            1.0
        ],
        "reward_history": [
            -810,
            -336,
            -18,
            -145,
            53,
            -148,
            15,
            -42,
            -244,
            -810,
            -37,
            70,
            -117,
            11,
            -348,
            -5,
            10,
            -32,
            43
        ],
        "steps_history": [
            810,
            437,
            119,
            246,
            48,
            249,
            86,
            143,
            345,
            810,
            138,
            31,
            218,
            90,
            449,
            106,
            91,
            133,
            58
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "110/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.793280839920044,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 20,
        "policy_stability_history": [
            0.0,
            0.7407407407407407,
            0.7654320987654321,
            0.8271604938271605,
            0.9506172839506173,
            0.9135802469135802,
            0.8395061728395061,
            0.9382716049382716,
            1.0,
            0.9135802469135802,
            0.8765432098765432,
            0.9135802469135802,
            0.9629629629629629,
            0.9876543209876543,
            0.9506172839506173,
            0.9876543209876543,
            0.9876543209876543,
            1.0,
            0.9876543209876543,
            0.9753086419753086,
            1.0
        ],
        "reward_history": [
            -96,
            -810,
            -402,
            -640,
            -18,
            -245,
            -387,
            -64,
            9,
            -191,
            -558,
            -34,
            3,
            37,
            -112,
            -14,
            -11,
            -12,
            -66,
            -151,
            37
        ],
        "steps_history": [
            197,
            810,
            503,
            741,
            119,
            346,
            488,
            165,
            92,
            292,
            659,
            135,
            98,
            64,
            213,
            115,
            112,
            113,
            167,
            252,
            64
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "111/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.79347825050354,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.9012345679012346,
            0.9629629629629629,
            0.9012345679012346,
            0.9382716049382716,
            0.8765432098765432,
            0.8518518518518519,
            0.9629629629629629,
            1.0,
            0.9876543209876543,
            0.8888888888888888,
            0.8765432098765432,
            1.0,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            -79,
            1,
            -133,
            -61,
            -655,
            -810,
            -113,
            -34,
            43,
            -810,
            -384,
            44,
            25
        ],
        "steps_history": [
            810,
            810,
            180,
            100,
            234,
            162,
            756,
            810,
            214,
            135,
            58,
            810,
            485,
            57,
            76
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "112/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.74287486076355,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8765432098765432,
            0.8395061728395061,
            0.8148148148148148,
            0.8271604938271605,
            0.9382716049382716,
            0.9753086419753086,
            0.9012345679012346,
            0.9382716049382716,
            1.0,
            1.0,
            0.9876543209876543,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -285,
            -810,
            -566,
            -810,
            -616,
            -290,
            -151,
            -253,
            -129,
            53,
            -9,
            39,
            6,
            -82,
            41,
            18
        ],
        "steps_history": [
            386,
            810,
            667,
            810,
            717,
            391,
            252,
            354,
            230,
            48,
            110,
            62,
            95,
            183,
            60,
            83
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "113/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.9138407707214355,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.8395061728395061,
            0.8395061728395061,
            0.9876543209876543,
            0.9012345679012346,
            0.8765432098765432,
            0.9876543209876543,
            0.9876543209876543,
            0.9629629629629629,
            0.9876543209876543,
            0.9629629629629629,
            0.8518518518518519,
            0.9629629629629629,
            0.9506172839506173,
            1.0
        ],
        "reward_history": [
            -810,
            -614,
            -674,
            -14,
            -177,
            -294,
            -38,
            -1,
            -13,
            -35,
            -49,
            -810,
            -272,
            -108,
            16
        ],
        "steps_history": [
            810,
            715,
            775,
            115,
            278,
            395,
            139,
            102,
            114,
            136,
            150,
            810,
            373,
            209,
            85
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "114/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.477018117904663,
        "final_policy_stability": 0.9876543209876543,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8395061728395061,
            0.9135802469135802,
            0.8888888888888888,
            0.9506172839506173,
            0.9012345679012346,
            0.8148148148148148,
            0.8395061728395061,
            0.9876543209876543,
            0.9012345679012346,
            1.0,
            0.9876543209876543,
            0.9382716049382716,
            1.0,
            0.9753086419753086,
            1.0,
            0.9629629629629629,
            0.8765432098765432,
            0.9876543209876543
        ],
        "reward_history": [
            -810,
            -328,
            -82,
            -97,
            52,
            -483,
            -548,
            -810,
            20,
            -216,
            4,
            60,
            -17,
            25,
            -32,
            42,
            -155,
            -810,
            -195
        ],
        "steps_history": [
            810,
            429,
            183,
            198,
            49,
            584,
            649,
            810,
            81,
            317,
            97,
            41,
            118,
            76,
            133,
            59,
            256,
            810,
            296
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "115/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.418468713760376,
        "final_policy_stability": 0.9876543209876543,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.8271604938271605,
            0.8765432098765432,
            0.7407407407407407,
            0.8888888888888888,
            0.9135802469135802,
            0.8395061728395061,
            0.9629629629629629,
            0.9259259259259259,
            0.8641975308641975,
            0.9876543209876543,
            1.0,
            1.0,
            0.9753086419753086,
            1.0,
            1.0,
            0.9753086419753086,
            0.9876543209876543
        ],
        "reward_history": [
            -352,
            -810,
            -211,
            -588,
            -154,
            -126,
            -392,
            19,
            -357,
            -810,
            14,
            -175,
            26,
            -53,
            -2,
            -74,
            -26,
            -30
        ],
        "steps_history": [
            453,
            810,
            312,
            689,
            255,
            227,
            493,
            82,
            458,
            810,
            87,
            276,
            75,
            154,
            103,
            175,
            127,
            131
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "116/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8521485328674316,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8148148148148148,
            0.8148148148148148,
            0.9135802469135802,
            0.9876543209876543,
            0.8271604938271605,
            0.9753086419753086,
            0.9382716049382716,
            0.9876543209876543,
            0.8271604938271605,
            0.9629629629629629,
            1.0,
            0.9753086419753086,
            1.0
        ],
        "reward_history": [
            -627,
            -407,
            -466,
            -177,
            33,
            -810,
            -14,
            -137,
            66,
            -452,
            -106,
            -17,
            -158,
            -56
        ],
        "steps_history": [
            728,
            508,
            567,
            278,
            68,
            810,
            115,
            238,
            35,
            553,
            207,
            118,
            259,
            157
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "117/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7786450386047363,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.8518518518518519,
            0.8395061728395061,
            0.9135802469135802,
            0.8888888888888888,
            0.9135802469135802,
            0.9259259259259259,
            0.8395061728395061,
            0.9876543209876543,
            0.9506172839506173,
            0.9506172839506173,
            0.9629629629629629,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -522,
            -810,
            -248,
            -33,
            -556,
            -118,
            -171,
            -614,
            -175,
            -125,
            -127,
            -35,
            53,
            -9,
            34
        ],
        "steps_history": [
            623,
            810,
            349,
            134,
            657,
            219,
            272,
            715,
            276,
            226,
            228,
            136,
            48,
            110,
            67
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "118/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.5215001106262207,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8641975308641975,
            0.7777777777777778,
            0.9506172839506173,
            0.8888888888888888,
            0.8271604938271605,
            0.9382716049382716,
            0.8888888888888888,
            0.9629629629629629,
            0.9753086419753086,
            0.8148148148148148,
            0.9876543209876543,
            0.8518518518518519,
            0.9753086419753086,
            0.8641975308641975,
            0.9876543209876543,
            0.9876543209876543,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -692,
            -166,
            -491,
            58,
            -46,
            -810,
            -24,
            -195,
            50,
            67,
            -409,
            27,
            -345,
            -13,
            -810,
            56,
            2,
            15,
            45,
            39
        ],
        "steps_history": [
            793,
            267,
            592,
            43,
            147,
            810,
            125,
            296,
            51,
            34,
            510,
            74,
            446,
            114,
            810,
            45,
            99,
            86,
            56,
            62
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "119/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.454918384552002,
        "final_policy_stability": 0.9876543209876543,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.8641975308641975,
            0.9012345679012346,
            0.8765432098765432,
            0.9629629629629629,
            0.8888888888888888,
            0.9629629629629629,
            0.9876543209876543,
            0.8765432098765432,
            0.9382716049382716,
            0.9506172839506173,
            0.9753086419753086,
            0.9753086419753086,
            0.9753086419753086,
            1.0,
            0.8888888888888888,
            1.0,
            0.9876543209876543
        ],
        "reward_history": [
            -810,
            -810,
            -48,
            -225,
            48,
            -114,
            56,
            -13,
            -138,
            -155,
            -142,
            12,
            -5,
            46,
            38,
            -253,
            20,
            -33
        ],
        "steps_history": [
            810,
            810,
            149,
            326,
            53,
            215,
            45,
            114,
            239,
            256,
            243,
            89,
            106,
            55,
            63,
            354,
            81,
            134
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "120/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.2_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.040771007537842,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8148148148148148,
            0.8271604938271605,
            0.7901234567901234,
            0.8271604938271605,
            0.9876543209876543,
            0.9135802469135802,
            1.0,
            0.9382716049382716,
            1.0,
            1.0
        ],
        "reward_history": [
            -199,
            -248,
            -810,
            -669,
            -810,
            -38,
            -810,
            -1,
            -460,
            -810,
            24
        ],
        "steps_history": [
            300,
            349,
            810,
            770,
            810,
            139,
            810,
            102,
            561,
            810,
            77
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "121/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7598023414611816,
        "final_policy_stability": 0.9135802469135802,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8641975308641975,
            0.8518518518518519,
            0.9629629629629629,
            0.9506172839506173,
            0.9012345679012346,
            1.0,
            0.9876543209876543,
            0.9382716049382716,
            1.0,
            0.9876543209876543,
            0.9135802469135802
        ],
        "reward_history": [
            -379,
            -810,
            -500,
            -2,
            48,
            -689,
            -124,
            69,
            -463,
            -83,
            -280,
            -249
        ],
        "steps_history": [
            480,
            810,
            601,
            103,
            53,
            790,
            225,
            32,
            564,
            184,
            381,
            350
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "122/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8075788021087646,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8641975308641975,
            0.9876543209876543,
            0.9012345679012346,
            0.8148148148148148,
            0.8518518518518519,
            0.8395061728395061,
            0.9506172839506173,
            1.0,
            0.9876543209876543,
            0.9259259259259259,
            1.0
        ],
        "reward_history": [
            -141,
            -79,
            53,
            -22,
            -810,
            -580,
            -810,
            -229,
            68,
            -629,
            -490,
            51
        ],
        "steps_history": [
            242,
            180,
            48,
            123,
            810,
            681,
            810,
            330,
            33,
            730,
            591,
            50
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "123/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7894082069396973,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.9753086419753086,
            0.8765432098765432,
            0.9259259259259259,
            0.8765432098765432,
            0.9506172839506173,
            0.9629629629629629,
            0.8765432098765432,
            0.8148148148148148,
            0.8641975308641975,
            1.0,
            0.9012345679012346,
            1.0,
            1.0
        ],
        "reward_history": [
            -682,
            48,
            -92,
            -175,
            -160,
            -7,
            36,
            -810,
            -810,
            -810,
            -1,
            -810,
            -89,
            -134
        ],
        "steps_history": [
            783,
            53,
            193,
            276,
            261,
            108,
            65,
            810,
            810,
            810,
            102,
            810,
            190,
            235
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "124/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8538312911987305,
        "final_policy_stability": 0.8641975308641975,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.9382716049382716,
            0.8888888888888888,
            0.9876543209876543,
            0.9506172839506173,
            0.9876543209876543,
            0.9382716049382716,
            0.8765432098765432,
            1.0,
            1.0,
            1.0,
            0.8641975308641975
        ],
        "reward_history": [
            -810,
            -336,
            -18,
            -145,
            53,
            4,
            26,
            -159,
            -810,
            29,
            45,
            57,
            -595
        ],
        "steps_history": [
            810,
            437,
            119,
            246,
            48,
            97,
            75,
            260,
            810,
            72,
            56,
            44,
            696
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "125/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6915102005004883,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.8024691358024691,
            0.7777777777777778,
            0.9506172839506173,
            0.9382716049382716,
            0.8765432098765432,
            0.8518518518518519,
            0.8395061728395061,
            1.0,
            0.9876543209876543,
            1.0,
            1.0
        ],
        "reward_history": [
            -96,
            -810,
            -402,
            -640,
            -18,
            -245,
            -387,
            -448,
            -456,
            0,
            -35,
            3,
            37
        ],
        "steps_history": [
            197,
            810,
            503,
            741,
            119,
            346,
            488,
            549,
            557,
            101,
            136,
            98,
            64
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "126/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.786346197128296,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8641975308641975,
            0.9012345679012346,
            0.9382716049382716,
            0.8765432098765432,
            0.9382716049382716,
            0.8641975308641975,
            0.9629629629629629,
            0.7777777777777778,
            0.9382716049382716,
            1.0,
            1.0,
            0.9012345679012346,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            -79,
            1,
            -133,
            -61,
            -467,
            -87,
            -810,
            -113,
            -34,
            43,
            -239,
            45
        ],
        "steps_history": [
            810,
            810,
            180,
            100,
            234,
            162,
            568,
            188,
            810,
            214,
            135,
            58,
            340,
            56
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "127/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.17634916305542,
        "final_policy_stability": 0.9753086419753086,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.7407407407407407,
            0.8024691358024691,
            0.8641975308641975,
            0.8271604938271605,
            0.9012345679012346,
            0.9506172839506173,
            0.9753086419753086,
            1.0,
            0.9629629629629629,
            0.9753086419753086
        ],
        "reward_history": [
            -285,
            -430,
            -810,
            -275,
            -810,
            -218,
            -40,
            -307,
            -151,
            -253,
            -129
        ],
        "steps_history": [
            386,
            531,
            810,
            376,
            810,
            319,
            141,
            408,
            252,
            354,
            230
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "128/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.827440023422241,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.9012345679012346,
            0.9506172839506173,
            0.8888888888888888,
            0.9135802469135802,
            0.9382716049382716,
            1.0,
            1.0,
            0.9876543209876543,
            1.0,
            1.0,
            0.9259259259259259,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            -40,
            40,
            -25,
            5,
            -79,
            14,
            0,
            -4,
            11,
            15,
            -294,
            -38
        ],
        "steps_history": [
            810,
            810,
            141,
            61,
            126,
            96,
            180,
            87,
            101,
            105,
            90,
            86,
            395,
            139
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "129/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.790215015411377,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.9259259259259259,
            0.9506172839506173,
            0.9259259259259259,
            0.8518518518518519,
            1.0,
            0.9629629629629629,
            0.9876543209876543,
            0.9753086419753086,
            0.8888888888888888,
            1.0,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            46,
            -66,
            -310,
            -548,
            11,
            41,
            61,
            -42,
            -810,
            36,
            4
        ],
        "steps_history": [
            810,
            810,
            55,
            167,
            411,
            649,
            90,
            60,
            40,
            143,
            810,
            65,
            97
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "130/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7394468784332275,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.8395061728395061,
            0.8395061728395061,
            0.8888888888888888,
            0.8765432098765432,
            0.9135802469135802,
            0.9259259259259259,
            0.9629629629629629,
            0.9753086419753086,
            0.8518518518518519,
            0.8765432098765432,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -204,
            -810,
            -359,
            -588,
            -136,
            -144,
            -119,
            -164,
            11,
            24,
            -810,
            -643,
            26,
            -53,
            -2
        ],
        "steps_history": [
            305,
            810,
            460,
            689,
            237,
            245,
            220,
            265,
            90,
            77,
            810,
            744,
            75,
            154,
            103
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "131/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.757551670074463,
        "final_policy_stability": 0.9753086419753086,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.7530864197530864,
            0.8271604938271605,
            0.9135802469135802,
            0.9382716049382716,
            0.9753086419753086,
            0.8271604938271605,
            0.8888888888888888,
            1.0,
            0.9012345679012346,
            1.0,
            0.9753086419753086
        ],
        "reward_history": [
            -627,
            -582,
            -810,
            -121,
            -59,
            53,
            -320,
            -139,
            66,
            -270,
            -81,
            -106
        ],
        "steps_history": [
            728,
            683,
            810,
            222,
            160,
            48,
            421,
            240,
            35,
            371,
            182,
            207
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "132/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7764909267425537,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.8518518518518519,
            0.8765432098765432,
            0.9259259259259259,
            0.9259259259259259,
            0.9135802469135802,
            0.9382716049382716,
            0.9506172839506173,
            0.8888888888888888,
            0.8888888888888888,
            0.8518518518518519,
            0.9876543209876543,
            0.9876543209876543,
            0.9629629629629629,
            1.0
        ],
        "reward_history": [
            -522,
            -810,
            -125,
            7,
            -19,
            -74,
            -141,
            -32,
            -268,
            -348,
            -810,
            -129,
            -127,
            -35,
            53
        ],
        "steps_history": [
            623,
            810,
            226,
            94,
            120,
            175,
            242,
            133,
            369,
            449,
            810,
            230,
            228,
            136,
            48
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "133/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7946524620056152,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.9629629629629629,
            0.8641975308641975,
            0.9506172839506173,
            0.9382716049382716,
            0.9506172839506173,
            0.9876543209876543,
            0.8888888888888888,
            0.9876543209876543,
            0.9753086419753086,
            0.9506172839506173,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -690,
            57,
            -124,
            -12,
            -91,
            -110,
            39,
            -810,
            15,
            11,
            -52,
            -54,
            22,
            39
        ],
        "steps_history": [
            791,
            44,
            225,
            113,
            192,
            211,
            62,
            810,
            86,
            90,
            153,
            155,
            79,
            62
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "134/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.85090708732605,
        "final_policy_stability": 0.9506172839506173,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8148148148148148,
            0.8765432098765432,
            0.8765432098765432,
            0.8765432098765432,
            0.9753086419753086,
            0.9135802469135802,
            0.9629629629629629,
            0.9629629629629629,
            0.9506172839506173,
            0.9629629629629629,
            0.9753086419753086,
            0.9876543209876543,
            1.0,
            1.0,
            0.9506172839506173
        ],
        "reward_history": [
            -810,
            -810,
            -48,
            -277,
            -122,
            40,
            -229,
            -63,
            -41,
            -25,
            33,
            13,
            -5,
            46,
            38,
            -253
        ],
        "steps_history": [
            810,
            810,
            149,
            378,
            223,
            61,
            330,
            164,
            142,
            126,
            68,
            88,
            106,
            55,
            63,
            354
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "135/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.158158779144287,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8148148148148148,
            0.8271604938271605,
            0.7777777777777778,
            0.8148148148148148,
            0.9876543209876543,
            0.9135802469135802,
            1.0,
            0.9382716049382716,
            1.0,
            1.0
        ],
        "reward_history": [
            -199,
            -248,
            -810,
            -669,
            -810,
            -38,
            -810,
            -1,
            -460,
            -810,
            24
        ],
        "steps_history": [
            300,
            349,
            810,
            770,
            810,
            139,
            810,
            102,
            561,
            810,
            77
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "136/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6933693885803223,
        "final_policy_stability": 0.9135802469135802,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8641975308641975,
            0.8518518518518519,
            0.9629629629629629,
            0.9506172839506173,
            0.9012345679012346,
            1.0,
            0.9876543209876543,
            0.9382716049382716,
            1.0,
            0.9876543209876543,
            0.9135802469135802
        ],
        "reward_history": [
            -379,
            -810,
            -500,
            -2,
            48,
            -689,
            -124,
            69,
            -463,
            -83,
            -280,
            -249
        ],
        "steps_history": [
            480,
            810,
            601,
            103,
            53,
            790,
            225,
            32,
            564,
            184,
            381,
            350
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "137/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7030482292175293,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8641975308641975,
            0.9876543209876543,
            0.9012345679012346,
            0.8148148148148148,
            0.8518518518518519,
            0.8395061728395061,
            0.9506172839506173,
            1.0,
            0.9876543209876543,
            0.9259259259259259,
            1.0
        ],
        "reward_history": [
            -141,
            -79,
            53,
            -22,
            -810,
            -580,
            -810,
            -229,
            68,
            -629,
            -490,
            51
        ],
        "steps_history": [
            242,
            180,
            48,
            123,
            810,
            681,
            810,
            330,
            33,
            730,
            591,
            50
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "138/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7906200885772705,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.9753086419753086,
            0.8765432098765432,
            0.9259259259259259,
            0.8765432098765432,
            0.9506172839506173,
            0.9629629629629629,
            0.8765432098765432,
            0.8148148148148148,
            0.8641975308641975,
            1.0,
            0.9012345679012346,
            1.0,
            1.0
        ],
        "reward_history": [
            -682,
            48,
            -92,
            -175,
            -160,
            -7,
            36,
            -810,
            -810,
            -810,
            -1,
            -810,
            -89,
            -134
        ],
        "steps_history": [
            783,
            53,
            193,
            276,
            261,
            108,
            65,
            810,
            810,
            810,
            102,
            810,
            190,
            235
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "139/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.939584970474243,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.9382716049382716,
            0.9259259259259259,
            0.9382716049382716,
            0.9753086419753086,
            0.9753086419753086,
            1.0,
            0.9506172839506173,
            0.9629629629629629,
            0.8765432098765432,
            0.8765432098765432,
            0.9876543209876543,
            1.0
        ],
        "reward_history": [
            -810,
            -336,
            -18,
            23,
            -67,
            53,
            4,
            26,
            -62,
            -167,
            -810,
            -506,
            11,
            -82
        ],
        "steps_history": [
            810,
            437,
            119,
            78,
            168,
            48,
            97,
            75,
            163,
            268,
            810,
            607,
            90,
            183
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "140/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.9857468605041504,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.8024691358024691,
            0.7777777777777778,
            0.9506172839506173,
            0.9382716049382716,
            0.8765432098765432,
            0.8518518518518519,
            0.8395061728395061,
            1.0,
            0.9876543209876543,
            1.0,
            1.0
        ],
        "reward_history": [
            -96,
            -810,
            -402,
            -640,
            -18,
            -245,
            -387,
            -448,
            -456,
            0,
            -35,
            3,
            37
        ],
        "steps_history": [
            197,
            810,
            503,
            741,
            119,
            346,
            488,
            549,
            557,
            101,
            136,
            98,
            64
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "141/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8346056938171387,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8641975308641975,
            0.9012345679012346,
            0.9382716049382716,
            0.8765432098765432,
            0.9382716049382716,
            0.8641975308641975,
            0.9629629629629629,
            0.7901234567901234,
            0.9382716049382716,
            1.0,
            1.0,
            0.9012345679012346,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            -79,
            1,
            -133,
            -61,
            -467,
            -87,
            -810,
            -113,
            -34,
            43,
            -239,
            45
        ],
        "steps_history": [
            810,
            810,
            180,
            100,
            234,
            162,
            568,
            188,
            810,
            214,
            135,
            58,
            340,
            56
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "142/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.076047897338867,
        "final_policy_stability": 0.9753086419753086,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.7407407407407407,
            0.7901234567901234,
            0.8765432098765432,
            0.8148148148148148,
            0.9012345679012346,
            0.9506172839506173,
            0.9753086419753086,
            1.0,
            0.9629629629629629,
            0.9753086419753086
        ],
        "reward_history": [
            -285,
            -430,
            -810,
            -275,
            -810,
            -218,
            -40,
            -307,
            -151,
            -253,
            -129
        ],
        "steps_history": [
            386,
            531,
            810,
            376,
            810,
            319,
            141,
            408,
            252,
            354,
            230
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "143/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.707008123397827,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.9012345679012346,
            0.9506172839506173,
            0.8888888888888888,
            0.9135802469135802,
            0.9382716049382716,
            1.0,
            1.0,
            0.9876543209876543,
            1.0,
            1.0,
            0.9259259259259259,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            -40,
            40,
            -25,
            5,
            -79,
            14,
            0,
            -4,
            11,
            15,
            -294,
            -38
        ],
        "steps_history": [
            810,
            810,
            141,
            61,
            126,
            96,
            180,
            87,
            101,
            105,
            90,
            86,
            395,
            139
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "144/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7677786350250244,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.9259259259259259,
            0.9506172839506173,
            0.9259259259259259,
            0.8518518518518519,
            1.0,
            0.9629629629629629,
            0.9876543209876543,
            0.9629629629629629,
            0.8765432098765432,
            1.0,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            46,
            -66,
            -310,
            -548,
            11,
            41,
            61,
            -42,
            -810,
            36,
            4
        ],
        "steps_history": [
            810,
            810,
            55,
            167,
            411,
            649,
            90,
            60,
            40,
            143,
            810,
            65,
            97
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "145/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.785433053970337,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.8395061728395061,
            0.8395061728395061,
            0.8888888888888888,
            0.8641975308641975,
            0.9259259259259259,
            0.9259259259259259,
            0.9629629629629629,
            0.9753086419753086,
            0.8518518518518519,
            0.8765432098765432,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -204,
            -810,
            -359,
            -588,
            -136,
            -144,
            -119,
            -164,
            11,
            24,
            -810,
            -643,
            26,
            -53,
            -2
        ],
        "steps_history": [
            305,
            810,
            460,
            689,
            237,
            245,
            220,
            265,
            90,
            77,
            810,
            744,
            75,
            154,
            103
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "146/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8090081214904785,
        "final_policy_stability": 0.9753086419753086,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.7530864197530864,
            0.8271604938271605,
            0.9135802469135802,
            0.9382716049382716,
            0.9753086419753086,
            0.8271604938271605,
            0.8888888888888888,
            1.0,
            0.9012345679012346,
            1.0,
            0.9753086419753086
        ],
        "reward_history": [
            -627,
            -582,
            -810,
            -121,
            -59,
            53,
            -320,
            -139,
            66,
            -270,
            -81,
            -106
        ],
        "steps_history": [
            728,
            683,
            810,
            222,
            160,
            48,
            421,
            240,
            35,
            371,
            182,
            207
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "147/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.4435911178588867,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 16,
        "policy_stability_history": [
            0.0,
            0.8518518518518519,
            0.8765432098765432,
            0.9259259259259259,
            0.9259259259259259,
            0.9135802469135802,
            0.9382716049382716,
            0.9506172839506173,
            0.8888888888888888,
            0.8641975308641975,
            0.9506172839506173,
            0.9753086419753086,
            0.9876543209876543,
            1.0,
            1.0,
            0.9753086419753086,
            1.0
        ],
        "reward_history": [
            -522,
            -810,
            -125,
            7,
            -19,
            -74,
            -141,
            -32,
            -268,
            -498,
            -249,
            63,
            -175,
            -125,
            -127,
            -35,
            53
        ],
        "steps_history": [
            623,
            810,
            226,
            94,
            120,
            175,
            242,
            133,
            369,
            599,
            350,
            38,
            276,
            226,
            228,
            136,
            48
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "148/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6992714405059814,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.9629629629629629,
            0.8641975308641975,
            0.9629629629629629,
            0.9382716049382716,
            0.9506172839506173,
            0.9135802469135802,
            1.0,
            0.9629629629629629,
            0.9753086419753086,
            0.9629629629629629,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -690,
            57,
            -125,
            -11,
            -91,
            -110,
            -810,
            57,
            -4,
            12,
            -52,
            -54,
            22,
            39
        ],
        "steps_history": [
            791,
            44,
            226,
            112,
            192,
            211,
            810,
            44,
            105,
            89,
            153,
            155,
            79,
            62
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "149/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8551039695739746,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.9135802469135802,
            0.8395061728395061,
            0.9629629629629629,
            0.9506172839506173,
            0.9259259259259259,
            0.9012345679012346,
            0.9629629629629629,
            0.9382716049382716,
            0.9876543209876543,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            -48,
            -343,
            18,
            -5,
            -19,
            -138,
            -155,
            -75,
            33,
            13,
            -5,
            46
        ],
        "steps_history": [
            810,
            810,
            149,
            444,
            83,
            106,
            120,
            239,
            256,
            176,
            68,
            88,
            106,
            55
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "150/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.3_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.0034093856811523,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.7654320987654321,
            0.8765432098765432,
            0.9629629629629629,
            0.9506172839506173,
            0.9753086419753086,
            0.8888888888888888,
            0.9876543209876543,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -199,
            -501,
            -316,
            -68,
            53,
            46,
            -298,
            20,
            -57,
            -19,
            -17
        ],
        "steps_history": [
            300,
            602,
            417,
            169,
            48,
            55,
            399,
            81,
            158,
            120,
            118
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "151/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.076338291168213,
        "final_policy_stability": 0.9753086419753086,
        "episodes_to_convergence": 7,
        "policy_stability_history": [
            0.0,
            0.8271604938271605,
            0.8024691358024691,
            0.9382716049382716,
            0.9753086419753086,
            0.9629629629629629,
            1.0,
            0.9753086419753086
        ],
        "reward_history": [
            -379,
            -130,
            -810,
            -118,
            -49,
            -3,
            48,
            -57
        ],
        "steps_history": [
            480,
            231,
            810,
            219,
            150,
            104,
            53,
            158
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "152/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.993673324584961,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8765432098765432,
            0.9753086419753086,
            0.9135802469135802,
            0.7777777777777778,
            0.8765432098765432,
            0.9876543209876543,
            1.0,
            0.9012345679012346,
            0.9259259259259259,
            1.0
        ],
        "reward_history": [
            -141,
            -79,
            53,
            -22,
            -810,
            -810,
            55,
            14,
            -308,
            -368,
            68
        ],
        "steps_history": [
            242,
            180,
            48,
            123,
            810,
            810,
            46,
            87,
            409,
            469,
            33
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "153/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.1842434406280518,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8765432098765432,
            0.9012345679012346,
            0.8765432098765432,
            0.8641975308641975,
            0.9382716049382716,
            0.8765432098765432,
            0.8888888888888888,
            1.0,
            0.9259259259259259,
            1.0
        ],
        "reward_history": [
            -810,
            -119,
            -180,
            -327,
            -810,
            -297,
            -810,
            -358,
            46,
            -384,
            54
        ],
        "steps_history": [
            810,
            220,
            281,
            428,
            810,
            398,
            810,
            459,
            55,
            485,
            47
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "154/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.774097442626953,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7654320987654321,
            0.9753086419753086,
            0.9012345679012346,
            0.9753086419753086,
            0.9876543209876543,
            1.0,
            0.9629629629629629,
            0.9506172839506173,
            0.9753086419753086,
            0.9629629629629629,
            0.9012345679012346,
            1.0
        ],
        "reward_history": [
            -810,
            -336,
            -18,
            -145,
            53,
            4,
            26,
            -62,
            -167,
            -119,
            36,
            -810,
            28
        ],
        "steps_history": [
            810,
            437,
            119,
            246,
            48,
            97,
            75,
            163,
            268,
            220,
            65,
            810,
            73
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "155/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.84183406829834,
        "final_policy_stability": 0.9382716049382716,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.8024691358024691,
            0.7654320987654321,
            0.9259259259259259,
            0.9259259259259259,
            0.9135802469135802,
            0.9135802469135802,
            0.9012345679012346,
            0.9876543209876543,
            1.0,
            0.9382716049382716
        ],
        "reward_history": [
            -337,
            -332,
            -542,
            -391,
            18,
            -64,
            -361,
            -387,
            -448,
            -168,
            -8,
            -180
        ],
        "steps_history": [
            438,
            433,
            643,
            492,
            83,
            165,
            462,
            488,
            549,
            269,
            109,
            281
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "156/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.770040988922119,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8395061728395061,
            0.9259259259259259,
            0.7777777777777778,
            0.9753086419753086,
            0.9506172839506173,
            0.9382716049382716,
            0.8518518518518519,
            0.9876543209876543,
            0.9629629629629629,
            0.9753086419753086,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            -1,
            -810,
            5,
            -135,
            -87,
            -448,
            38,
            -107,
            -103,
            -34
        ],
        "steps_history": [
            810,
            810,
            102,
            810,
            96,
            236,
            188,
            549,
            63,
            208,
            204,
            135
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "157/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6695215702056885,
        "final_policy_stability": 0.9629629629629629,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7407407407407407,
            0.8024691358024691,
            0.9012345679012346,
            0.9506172839506173,
            0.9506172839506173,
            0.8395061728395061,
            0.9629629629629629,
            0.9876543209876543,
            1.0,
            0.9876543209876543,
            1.0,
            0.9629629629629629
        ],
        "reward_history": [
            -285,
            -430,
            -810,
            -217,
            -36,
            7,
            -810,
            -45,
            49,
            -5,
            -290,
            -151,
            -483
        ],
        "steps_history": [
            386,
            531,
            810,
            318,
            137,
            94,
            810,
            146,
            52,
            106,
            391,
            252,
            584
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "158/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.777456283569336,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.7901234567901234,
            0.9259259259259259,
            0.9135802469135802,
            0.9135802469135802,
            0.9876543209876543,
            0.9876543209876543,
            0.9876543209876543,
            0.9135802469135802,
            0.9876543209876543,
            0.9876543209876543,
            1.0
        ],
        "reward_history": [
            -810,
            -409,
            -531,
            9,
            -69,
            -97,
            -4,
            11,
            15,
            -810,
            21,
            -23,
            -223
        ],
        "steps_history": [
            810,
            510,
            632,
            92,
            170,
            198,
            105,
            90,
            86,
            810,
            80,
            124,
            324
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "159/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7091896533966064,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.7654320987654321,
            0.8395061728395061,
            0.9753086419753086,
            0.9629629629629629,
            0.9753086419753086,
            0.9259259259259259,
            0.8024691358024691,
            1.0,
            0.9506172839506173,
            1.0,
            1.0
        ],
        "reward_history": [
            -810,
            -328,
            -280,
            52,
            -69,
            -35,
            -177,
            -543,
            6,
            -229,
            -29,
            70
        ],
        "steps_history": [
            810,
            429,
            381,
            49,
            170,
            136,
            278,
            644,
            95,
            330,
            130,
            31
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "160/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.677222967147827,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.8271604938271605,
            0.8148148148148148,
            0.8395061728395061,
            0.8888888888888888,
            0.8888888888888888,
            0.9876543209876543,
            0.9876543209876543,
            1.0,
            0.9753086419753086,
            1.0,
            1.0
        ],
        "reward_history": [
            -215,
            -469,
            -588,
            -337,
            -384,
            -147,
            -119,
            -164,
            11,
            24,
            -187,
            59,
            49
        ],
        "steps_history": [
            316,
            570,
            689,
            438,
            485,
            248,
            220,
            265,
            90,
            77,
            288,
            42,
            52
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "161/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.1532623767852783,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.9012345679012346,
            0.8024691358024691,
            0.9259259259259259,
            0.9629629629629629,
            0.9382716049382716,
            0.9629629629629629,
            1.0,
            0.9259259259259259,
            0.9629629629629629,
            1.0
        ],
        "reward_history": [
            -627,
            -810,
            -364,
            -45,
            14,
            -106,
            -59,
            53,
            -320,
            -138,
            65
        ],
        "steps_history": [
            728,
            810,
            465,
            146,
            87,
            207,
            160,
            48,
            421,
            239,
            36
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "162/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.735898494720459,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.9753086419753086,
            0.9506172839506173,
            0.9506172839506173,
            0.8518518518518519,
            0.9382716049382716,
            0.9382716049382716,
            0.9012345679012346,
            0.9629629629629629,
            0.9382716049382716,
            0.9876543209876543,
            0.9629629629629629,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -522,
            -810,
            56,
            -73,
            -8,
            -432,
            -28,
            -90,
            -77,
            -171,
            -97,
            61,
            7,
            -244,
            63,
            -175
        ],
        "steps_history": [
            623,
            810,
            45,
            174,
            109,
            533,
            129,
            191,
            178,
            272,
            198,
            40,
            94,
            345,
            38,
            276
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "163/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7531533241271973,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8888888888888888,
            0.8888888888888888,
            0.9629629629629629,
            0.9753086419753086,
            0.8641975308641975,
            0.9382716049382716,
            0.9506172839506173,
            0.9753086419753086,
            0.9876543209876543,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -810,
            -150,
            -490,
            58,
            -46,
            -810,
            -24,
            -54,
            46,
            15,
            50,
            67,
            -93,
            -14
        ],
        "steps_history": [
            810,
            251,
            591,
            43,
            147,
            810,
            125,
            155,
            55,
            86,
            51,
            34,
            194,
            115
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "164/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.0138065814971924,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.8641975308641975,
            0.9259259259259259,
            0.9012345679012346,
            0.9876543209876543,
            0.9629629629629629,
            0.9876543209876543,
            0.9753086419753086,
            0.9753086419753086,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            -48,
            -120,
            -4,
            48,
            37,
            17,
            27,
            40,
            10
        ],
        "steps_history": [
            810,
            810,
            149,
            221,
            105,
            53,
            64,
            84,
            74,
            61,
            91
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "165/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.087130308151245,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.7654320987654321,
            0.8765432098765432,
            0.9629629629629629,
            0.9506172839506173,
            0.9753086419753086,
            0.8888888888888888,
            0.9876543209876543,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -199,
            -501,
            -316,
            -68,
            53,
            46,
            -298,
            20,
            -57,
            -19,
            -17
        ],
        "steps_history": [
            300,
            602,
            417,
            169,
            48,
            55,
            399,
            81,
            158,
            120,
            118
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "166/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.0169878005981445,
        "final_policy_stability": 0.9753086419753086,
        "episodes_to_convergence": 7,
        "policy_stability_history": [
            0.0,
            0.8271604938271605,
            0.8024691358024691,
            0.9382716049382716,
            0.9753086419753086,
            0.9629629629629629,
            1.0,
            0.9753086419753086
        ],
        "reward_history": [
            -379,
            -130,
            -810,
            -118,
            -49,
            -3,
            48,
            -57
        ],
        "steps_history": [
            480,
            231,
            810,
            219,
            150,
            104,
            53,
            158
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "167/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9661128520965576,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8765432098765432,
            0.9753086419753086,
            0.9135802469135802,
            0.7777777777777778,
            0.8888888888888888,
            0.9876543209876543,
            1.0,
            0.9135802469135802,
            0.9259259259259259,
            1.0
        ],
        "reward_history": [
            -141,
            -79,
            53,
            -22,
            -810,
            -810,
            55,
            14,
            -308,
            -368,
            68
        ],
        "steps_history": [
            242,
            180,
            48,
            123,
            810,
            810,
            46,
            87,
            409,
            469,
            33
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "168/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.1669836044311523,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8765432098765432,
            0.9012345679012346,
            0.8765432098765432,
            0.8641975308641975,
            0.9382716049382716,
            0.8765432098765432,
            0.8888888888888888,
            1.0,
            0.9259259259259259,
            1.0
        ],
        "reward_history": [
            -810,
            -119,
            -180,
            -327,
            -810,
            -297,
            -810,
            -358,
            46,
            -384,
            54
        ],
        "steps_history": [
            810,
            220,
            281,
            428,
            810,
            398,
            810,
            459,
            55,
            485,
            47
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "169/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8126413822174072,
        "final_policy_stability": 0.9876543209876543,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7654320987654321,
            0.9753086419753086,
            0.8148148148148148,
            0.9506172839506173,
            0.9629629629629629,
            0.9629629629629629,
            0.8518518518518519,
            0.9876543209876543,
            1.0,
            0.9876543209876543,
            1.0,
            0.9876543209876543
        ],
        "reward_history": [
            -810,
            -336,
            -18,
            -671,
            -24,
            -119,
            36,
            -810,
            28,
            70,
            -117,
            11,
            -82
        ],
        "steps_history": [
            810,
            437,
            119,
            772,
            125,
            220,
            65,
            810,
            73,
            31,
            218,
            90,
            183
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "170/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6026058197021484,
        "final_policy_stability": 0.9382716049382716,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.8024691358024691,
            0.7654320987654321,
            0.9259259259259259,
            0.9259259259259259,
            0.9135802469135802,
            0.9135802469135802,
            0.9012345679012346,
            0.9876543209876543,
            1.0,
            0.9382716049382716
        ],
        "reward_history": [
            -337,
            -332,
            -542,
            -391,
            18,
            -64,
            -361,
            -387,
            -448,
            -168,
            -8,
            -180
        ],
        "steps_history": [
            438,
            433,
            643,
            492,
            83,
            165,
            462,
            488,
            549,
            269,
            109,
            281
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "171/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7287518978118896,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8395061728395061,
            0.9259259259259259,
            0.7777777777777778,
            0.9753086419753086,
            0.9506172839506173,
            0.9382716049382716,
            0.8518518518518519,
            0.9876543209876543,
            0.9629629629629629,
            0.9753086419753086,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            -1,
            -810,
            5,
            -135,
            -87,
            -448,
            38,
            -107,
            -103,
            -34
        ],
        "steps_history": [
            810,
            810,
            102,
            810,
            96,
            236,
            188,
            549,
            63,
            208,
            204,
            135
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "172/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.777463436126709,
        "final_policy_stability": 0.9629629629629629,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7407407407407407,
            0.8024691358024691,
            0.9012345679012346,
            0.9506172839506173,
            0.9506172839506173,
            0.8395061728395061,
            0.9629629629629629,
            0.9876543209876543,
            1.0,
            0.9876543209876543,
            1.0,
            0.9629629629629629
        ],
        "reward_history": [
            -285,
            -430,
            -810,
            -217,
            -36,
            7,
            -810,
            -45,
            49,
            -5,
            -290,
            -151,
            -483
        ],
        "steps_history": [
            386,
            531,
            810,
            318,
            137,
            94,
            810,
            146,
            52,
            106,
            391,
            252,
            584
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "173/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.798430919647217,
        "final_policy_stability": 0.9506172839506173,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.8888888888888888,
            0.9135802469135802,
            0.9259259259259259,
            0.9259259259259259,
            0.9753086419753086,
            0.9753086419753086,
            0.9629629629629629,
            1.0,
            1.0,
            0.9753086419753086,
            0.9506172839506173
        ],
        "reward_history": [
            -810,
            -810,
            -40,
            -68,
            31,
            -51,
            39,
            4,
            -8,
            4,
            11,
            15,
            -294
        ],
        "steps_history": [
            810,
            810,
            141,
            169,
            70,
            152,
            62,
            97,
            109,
            97,
            90,
            86,
            395
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "174/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.881967306137085,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.7654320987654321,
            0.8395061728395061,
            0.9753086419753086,
            0.9506172839506173,
            0.9629629629629629,
            0.9259259259259259,
            0.8024691358024691,
            1.0,
            0.9506172839506173,
            1.0,
            1.0
        ],
        "reward_history": [
            -810,
            -328,
            -280,
            52,
            -69,
            -35,
            -177,
            -543,
            6,
            -229,
            -29,
            70
        ],
        "steps_history": [
            810,
            429,
            381,
            49,
            170,
            136,
            278,
            644,
            95,
            330,
            130,
            31
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "175/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.708486318588257,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.8271604938271605,
            0.8271604938271605,
            0.8641975308641975,
            0.9506172839506173,
            0.9382716049382716,
            0.9876543209876543,
            1.0,
            0.9753086419753086,
            1.0,
            1.0
        ],
        "reward_history": [
            -215,
            -469,
            -588,
            -337,
            -810,
            -42,
            -164,
            11,
            24,
            -187,
            59,
            49
        ],
        "steps_history": [
            316,
            570,
            689,
            438,
            810,
            143,
            265,
            90,
            77,
            288,
            42,
            52
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "176/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9244320392608643,
        "final_policy_stability": 0.9629629629629629,
        "episodes_to_convergence": 9,
        "policy_stability_history": [
            0.0,
            0.9012345679012346,
            0.8024691358024691,
            0.9012345679012346,
            0.9876543209876543,
            0.9382716049382716,
            0.9753086419753086,
            1.0,
            0.8888888888888888,
            0.9629629629629629
        ],
        "reward_history": [
            -627,
            -810,
            -364,
            -45,
            14,
            -106,
            -59,
            53,
            -320,
            -138
        ],
        "steps_history": [
            728,
            810,
            465,
            146,
            87,
            207,
            160,
            48,
            421,
            239
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "177/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.62895131111145,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8024691358024691,
            0.9753086419753086,
            0.9506172839506173,
            0.9506172839506173,
            0.8518518518518519,
            0.9382716049382716,
            0.9382716049382716,
            0.9012345679012346,
            0.9629629629629629,
            0.9259259259259259,
            0.9876543209876543,
            0.9629629629629629,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -522,
            -810,
            56,
            -73,
            -8,
            -432,
            -28,
            -90,
            -77,
            -171,
            -97,
            61,
            7,
            -244,
            63,
            -175
        ],
        "steps_history": [
            623,
            810,
            45,
            174,
            109,
            533,
            129,
            191,
            178,
            272,
            198,
            40,
            94,
            345,
            38,
            276
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "178/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.9040138721466064,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8888888888888888,
            0.8888888888888888,
            0.9629629629629629,
            0.9753086419753086,
            0.8641975308641975,
            0.9382716049382716,
            0.9506172839506173,
            0.9753086419753086,
            0.9876543209876543,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -810,
            -150,
            -490,
            58,
            -46,
            -810,
            -24,
            -54,
            46,
            15,
            50,
            67,
            -93,
            -14
        ],
        "steps_history": [
            810,
            251,
            591,
            43,
            147,
            810,
            125,
            155,
            55,
            86,
            51,
            34,
            194,
            115
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "179/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 4,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.180424451828003,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.8888888888888888,
            0.9135802469135802,
            0.9012345679012346,
            0.9876543209876543,
            0.9629629629629629,
            0.9876543209876543,
            0.9876543209876543,
            0.9876543209876543,
            1.0
        ],
        "reward_history": [
            -810,
            -810,
            -48,
            -120,
            -4,
            48,
            37,
            17,
            27,
            40,
            10
        ],
        "steps_history": [
            810,
            810,
            149,
            221,
            105,
            53,
            64,
            84,
            74,
            61,
            91
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "180/810",
        "save_path": "experiments/20250131_160708/training_plots/size_4/lr0.4_df0.99_eps0.1_trial4"
    }
]