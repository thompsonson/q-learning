[
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.530970096588135,
        "final_policy_stability": 0.9365079365079365,
        "episodes_to_convergence": 47,
        "policy_stability_history": [
            0.0,
            0.8321995464852607,
            0.8049886621315193,
            0.9229024943310657,
            0.8526077097505669,
            0.8639455782312925,
            0.8956916099773242,
            0.9569160997732427,
            0.9138321995464853,
            0.9002267573696145,
            0.8458049886621315,
            0.8820861678004536,
            0.9569160997732427,
            0.8934240362811792,
            0.909297052154195,
            0.854875283446712,
            0.8888888888888888,
            0.9727891156462585,
            0.9501133786848073,
            0.9773242630385488,
            0.9138321995464853,
            0.8639455782312925,
            0.9002267573696145,
            0.9229024943310657,
            0.9410430839002267,
            0.9206349206349206,
            0.9841269841269841,
            0.9319727891156463,
            0.9478458049886621,
            0.9886621315192744,
            0.9841269841269841,
            0.9546485260770975,
            0.9909297052154195,
            0.9138321995464853,
            0.9841269841269841,
            0.927437641723356,
            0.9115646258503401,
            0.9886621315192744,
            0.9455782312925171,
            0.9591836734693877,
            0.9909297052154195,
            0.9138321995464853,
            0.9183673469387755,
            0.9002267573696145,
            0.9773242630385488,
            0.9297052154195011,
            0.9727891156462585,
            0.9365079365079365
        ],
        "reward_history": [
            -874,
            -4410,
            -4410,
            -4410,
            -4410,
            -3372,
            -4410,
            -1256,
            -2005,
            -4410,
            -4410,
            -4410,
            -665,
            -4410,
            -4410,
            -4410,
            -4410,
            -973,
            -1802,
            -643,
            -4410,
            -4410,
            -4410,
            -2291,
            -850,
            -3378,
            -701,
            -2291,
            -1593,
            -902,
            -558,
            -882,
            -1813,
            -4410,
            -673,
            -2670,
            -4410,
            -815,
            -1582,
            -1600,
            -603,
            -4410,
            -4410,
            -4410,
            -1206,
            -2855,
            -1452,
            -3291
        ],
        "steps_history": [
            975,
            4410,
            4410,
            4410,
            4410,
            3473,
            4410,
            1357,
            2106,
            4410,
            4410,
            4410,
            766,
            4410,
            4410,
            4410,
            4410,
            1074,
            1903,
            744,
            4410,
            4410,
            4410,
            2392,
            951,
            3479,
            802,
            2392,
            1694,
            1003,
            659,
            983,
            1914,
            4410,
            774,
            2771,
            4410,
            916,
            1683,
            1701,
            704,
            4410,
            4410,
            4410,
            1307,
            2956,
            1553,
            3392
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "631/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.701181888580322,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 45,
        "policy_stability_history": [
            0.0,
            0.9206349206349206,
            0.8979591836734694,
            0.8934240362811792,
            0.8344671201814059,
            0.8934240362811792,
            0.9501133786848073,
            0.8956916099773242,
            0.9047619047619048,
            0.8594104308390023,
            0.9365079365079365,
            0.9387755102040817,
            0.9070294784580499,
            0.9002267573696145,
            0.9365079365079365,
            0.9342403628117913,
            0.8684807256235828,
            0.8820861678004536,
            0.9705215419501134,
            0.8707482993197279,
            0.9387755102040817,
            0.9705215419501134,
            0.8662131519274376,
            0.9047619047619048,
            0.9750566893424036,
            0.9659863945578231,
            0.981859410430839,
            0.8866213151927438,
            0.9024943310657596,
            0.9727891156462585,
            0.8979591836734694,
            0.9750566893424036,
            0.8956916099773242,
            0.9909297052154195,
            0.9909297052154195,
            0.981859410430839,
            0.9750566893424036,
            0.9433106575963719,
            0.9863945578231292,
            0.9863945578231292,
            0.9954648526077098,
            0.9750566893424036,
            0.9886621315192744,
            0.9183673469387755,
            0.9455782312925171,
            0.9977324263038548
        ],
        "reward_history": [
            -4410,
            -388,
            -4410,
            -2691,
            -3806,
            -4410,
            -800,
            -4410,
            -4410,
            -4410,
            -1490,
            -1084,
            -4410,
            -4410,
            -2221,
            -2643,
            -4410,
            -4410,
            -725,
            -4410,
            -2283,
            -922,
            -4410,
            -4410,
            -816,
            -794,
            -544,
            -4410,
            -4410,
            -591,
            -4410,
            -1454,
            -4410,
            -944,
            -428,
            -536,
            -1056,
            -3214,
            -608,
            -658,
            -420,
            -1303,
            -1214,
            -4410,
            -1345,
            -881
        ],
        "steps_history": [
            4410,
            489,
            4410,
            2792,
            3907,
            4410,
            901,
            4410,
            4410,
            4410,
            1591,
            1185,
            4410,
            4410,
            2322,
            2744,
            4410,
            4410,
            826,
            4410,
            2384,
            1023,
            4410,
            4410,
            917,
            895,
            645,
            4410,
            4410,
            692,
            4410,
            1555,
            4410,
            1045,
            529,
            637,
            1157,
            3315,
            709,
            759,
            521,
            1404,
            1315,
            4410,
            1446,
            982
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "632/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.46336841583252,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 41,
        "policy_stability_history": [
            0.0,
            0.8752834467120182,
            0.8775510204081632,
            0.8616780045351474,
            0.8820861678004536,
            0.8820861678004536,
            0.8979591836734694,
            0.9115646258503401,
            0.9365079365079365,
            0.909297052154195,
            0.9251700680272109,
            0.9138321995464853,
            0.9478458049886621,
            0.909297052154195,
            0.9070294784580499,
            0.9115646258503401,
            0.8684807256235828,
            0.9115646258503401,
            0.9501133786848073,
            0.8888888888888888,
            0.9682539682539683,
            0.9863945578231292,
            0.9773242630385488,
            0.9455782312925171,
            0.9954648526077098,
            0.9002267573696145,
            0.891156462585034,
            0.9841269841269841,
            0.9863945578231292,
            0.8616780045351474,
            0.9727891156462585,
            0.9705215419501134,
            0.9569160997732427,
            0.927437641723356,
            0.9546485260770975,
            0.9024943310657596,
            0.9909297052154195,
            0.9183673469387755,
            0.9002267573696145,
            0.9614512471655329,
            0.9977324263038548,
            1.0
        ],
        "reward_history": [
            -1441,
            -4410,
            -3497,
            -4410,
            -4410,
            -4410,
            -4410,
            -1165,
            -2287,
            -4410,
            -1568,
            -4410,
            -1665,
            -4410,
            -4410,
            -3439,
            -4410,
            -4410,
            -702,
            -4410,
            -1223,
            -191,
            -447,
            -1507,
            -179,
            -4410,
            -4410,
            -191,
            -784,
            -4410,
            -1026,
            -1000,
            -943,
            -4410,
            -848,
            -4410,
            -516,
            -3051,
            -4410,
            -1984,
            -528,
            -480
        ],
        "steps_history": [
            1542,
            4410,
            3598,
            4410,
            4410,
            4410,
            4410,
            1266,
            2388,
            4410,
            1669,
            4410,
            1766,
            4410,
            4410,
            3540,
            4410,
            4410,
            803,
            4410,
            1324,
            292,
            548,
            1608,
            280,
            4410,
            4410,
            292,
            885,
            4410,
            1127,
            1101,
            1044,
            4410,
            949,
            4410,
            617,
            3152,
            4410,
            2085,
            629,
            581
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "633/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.037685632705688,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 54,
        "policy_stability_history": [
            0.0,
            0.8775510204081632,
            0.9070294784580499,
            0.8752834467120182,
            0.9115646258503401,
            0.8458049886621315,
            0.8662131519274376,
            0.8526077097505669,
            0.9047619047619048,
            0.8820861678004536,
            0.9047619047619048,
            0.9047619047619048,
            0.9478458049886621,
            0.9024943310657596,
            0.9365079365079365,
            0.8843537414965986,
            0.8798185941043084,
            0.9365079365079365,
            0.9002267573696145,
            0.8934240362811792,
            0.9705215419501134,
            0.9773242630385488,
            0.8798185941043084,
            0.9183673469387755,
            0.9795918367346939,
            0.963718820861678,
            0.891156462585034,
            0.9183673469387755,
            0.8934240362811792,
            0.9931972789115646,
            0.9750566893424036,
            0.9659863945578231,
            0.8752834467120182,
            0.9659863945578231,
            0.9750566893424036,
            0.9705215419501134,
            0.981859410430839,
            0.981859410430839,
            0.891156462585034,
            0.9591836734693877,
            0.9863945578231292,
            0.9886621315192744,
            0.8956916099773242,
            0.9501133786848073,
            0.9478458049886621,
            0.9931972789115646,
            0.9886621315192744,
            1.0,
            0.9909297052154195,
            0.9727891156462585,
            0.9931972789115646,
            1.0,
            0.9433106575963719,
            0.9569160997732427,
            1.0
        ],
        "reward_history": [
            -3710,
            -4410,
            -1988,
            -4410,
            -1708,
            -4410,
            -3612,
            -4410,
            -4410,
            -4410,
            -2660,
            -3555,
            -998,
            -4410,
            -2615,
            -4410,
            -4410,
            -1511,
            -4410,
            -2519,
            -565,
            -331,
            -4410,
            -4410,
            -878,
            -1216,
            -4410,
            -4410,
            -4410,
            -473,
            -429,
            -941,
            -4410,
            -936,
            -399,
            -421,
            -240,
            -809,
            -4410,
            -1627,
            -1087,
            -692,
            -4410,
            -1420,
            -3264,
            -388,
            -474,
            -285,
            -1035,
            -937,
            -877,
            -459,
            -4410,
            -2549,
            -318
        ],
        "steps_history": [
            3811,
            4410,
            2089,
            4410,
            1809,
            4410,
            3713,
            4410,
            4410,
            4410,
            2761,
            3656,
            1099,
            4410,
            2716,
            4410,
            4410,
            1612,
            4410,
            2620,
            666,
            432,
            4410,
            4410,
            979,
            1317,
            4410,
            4410,
            4410,
            574,
            530,
            1042,
            4410,
            1037,
            500,
            522,
            341,
            910,
            4410,
            1728,
            1188,
            793,
            4410,
            1521,
            3365,
            489,
            575,
            386,
            1136,
            1038,
            978,
            560,
            4410,
            2650,
            419
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "634/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.54482889175415,
        "final_policy_stability": 0.9886621315192744,
        "episodes_to_convergence": 42,
        "policy_stability_history": [
            0.0,
            0.8140589569160998,
            0.8798185941043084,
            0.8571428571428571,
            0.8480725623582767,
            0.8684807256235828,
            0.891156462585034,
            0.8934240362811792,
            0.8843537414965986,
            0.9614512471655329,
            0.8843537414965986,
            0.8979591836734694,
            0.891156462585034,
            0.9365079365079365,
            0.9024943310657596,
            0.8798185941043084,
            0.9841269841269841,
            0.9682539682539683,
            0.9841269841269841,
            0.8662131519274376,
            0.8979591836734694,
            0.8866213151927438,
            0.9138321995464853,
            0.891156462585034,
            0.9682539682539683,
            0.9591836734693877,
            0.9546485260770975,
            0.9750566893424036,
            0.9773242630385488,
            0.8752834467120182,
            0.9569160997732427,
            0.9909297052154195,
            0.9954648526077098,
            0.9841269841269841,
            0.9523809523809523,
            0.8979591836734694,
            0.8956916099773242,
            0.8775510204081632,
            0.9773242630385488,
            0.9954648526077098,
            0.8956916099773242,
            1.0,
            0.9886621315192744
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -3298,
            -4410,
            -3403,
            -4410,
            -686,
            -4410,
            -4410,
            -4410,
            -1424,
            -4410,
            -4410,
            -494,
            -1452,
            -711,
            -4410,
            -3064,
            -4410,
            -4410,
            -4410,
            -1040,
            -939,
            -2141,
            -577,
            -758,
            -4118,
            -1462,
            -304,
            -210,
            -860,
            -3117,
            -4410,
            -4410,
            -4410,
            -876,
            -308,
            -3614,
            -808,
            -1158
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            4410,
            3399,
            4410,
            3504,
            4410,
            787,
            4410,
            4410,
            4410,
            1525,
            4410,
            4410,
            595,
            1553,
            812,
            4410,
            3165,
            4410,
            4410,
            4410,
            1141,
            1040,
            2242,
            678,
            859,
            4219,
            1563,
            405,
            311,
            961,
            3218,
            4410,
            4410,
            4410,
            977,
            409,
            3715,
            909,
            1259
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "635/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.332296133041382,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 62,
        "policy_stability_history": [
            0.0,
            0.8412698412698413,
            0.8934240362811792,
            0.8049886621315193,
            0.891156462585034,
            0.9115646258503401,
            0.9138321995464853,
            0.8503401360544217,
            0.8888888888888888,
            0.8775510204081632,
            0.8616780045351474,
            0.9546485260770975,
            0.8684807256235828,
            0.8888888888888888,
            0.9659863945578231,
            0.9047619047619048,
            0.963718820861678,
            0.8526077097505669,
            0.9433106575963719,
            0.9183673469387755,
            0.9206349206349206,
            0.9410430839002267,
            0.9841269841269841,
            0.8866213151927438,
            0.9183673469387755,
            0.8866213151927438,
            0.8934240362811792,
            0.9773242630385488,
            0.9659863945578231,
            0.9705215419501134,
            0.9047619047619048,
            0.8888888888888888,
            0.9523809523809523,
            0.9160997732426304,
            0.8639455782312925,
            0.9705215419501134,
            0.8866213151927438,
            0.9773242630385488,
            0.9909297052154195,
            0.9795918367346939,
            0.9727891156462585,
            0.9614512471655329,
            0.9931972789115646,
            0.9682539682539683,
            0.9229024943310657,
            0.9931972789115646,
            0.9795918367346939,
            0.9795918367346939,
            0.981859410430839,
            0.9002267573696145,
            0.9931972789115646,
            0.9546485260770975,
            0.9931972789115646,
            0.873015873015873,
            0.9795918367346939,
            0.9569160997732427,
            0.9977324263038548,
            0.9909297052154195,
            0.9591836734693877,
            0.9954648526077098,
            0.9886621315192744,
            0.9047619047619048,
            0.9977324263038548
        ],
        "reward_history": [
            -4410,
            -3856,
            -4410,
            -4410,
            -2392,
            -1699,
            -1698,
            -4410,
            -4410,
            -4410,
            -4410,
            -321,
            -4410,
            -4410,
            -257,
            -1563,
            -429,
            -4410,
            -1058,
            -2091,
            -1438,
            -965,
            -271,
            -4410,
            -1395,
            -4410,
            -2991,
            -468,
            -813,
            -555,
            -2339,
            -4410,
            -1314,
            -4410,
            -3980,
            -419,
            -4410,
            -372,
            -321,
            -632,
            -531,
            -997,
            -93,
            -708,
            -4410,
            -210,
            -747,
            -680,
            -568,
            -4410,
            -84,
            -1295,
            -374,
            -4410,
            -492,
            -868,
            -115,
            -528,
            -1026,
            -174,
            -625,
            -4410,
            -758
        ],
        "steps_history": [
            4410,
            3957,
            4410,
            4410,
            2493,
            1800,
            1799,
            4410,
            4410,
            4410,
            4410,
            422,
            4410,
            4410,
            358,
            1664,
            530,
            4410,
            1159,
            2192,
            1539,
            1066,
            372,
            4410,
            1496,
            4410,
            3092,
            569,
            914,
            656,
            2440,
            4410,
            1415,
            4410,
            4081,
            520,
            4410,
            473,
            422,
            733,
            632,
            1098,
            194,
            809,
            4410,
            311,
            848,
            781,
            669,
            4410,
            185,
            1396,
            475,
            4410,
            593,
            969,
            216,
            629,
            1127,
            275,
            726,
            4410,
            859
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "636/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.3271164894104,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 46,
        "policy_stability_history": [
            0.0,
            0.873015873015873,
            0.9115646258503401,
            0.9160997732426304,
            0.8820861678004536,
            0.8684807256235828,
            0.9297052154195011,
            0.8956916099773242,
            0.8798185941043084,
            0.8798185941043084,
            0.8503401360544217,
            0.9115646258503401,
            0.9365079365079365,
            0.8662131519274376,
            0.909297052154195,
            0.9229024943310657,
            0.8956916099773242,
            0.9750566893424036,
            0.8775510204081632,
            0.8866213151927438,
            0.8752834467120182,
            0.9773242630385488,
            0.9682539682539683,
            0.963718820861678,
            0.9229024943310657,
            0.891156462585034,
            0.854875283446712,
            0.9002267573696145,
            0.8798185941043084,
            0.9523809523809523,
            0.8707482993197279,
            0.9863945578231292,
            0.8594104308390023,
            0.9841269841269841,
            0.9727891156462585,
            0.9931972789115646,
            0.9750566893424036,
            0.9002267573696145,
            0.8866213151927438,
            0.8956916099773242,
            0.963718820861678,
            0.9682539682539683,
            0.9931972789115646,
            0.9795918367346939,
            0.9977324263038548,
            0.9863945578231292,
            0.9977324263038548
        ],
        "reward_history": [
            -1420,
            -4410,
            -1704,
            -1607,
            -4410,
            -4410,
            -1873,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -1320,
            -4410,
            -2377,
            -2424,
            -4410,
            -385,
            -2374,
            -3518,
            -3273,
            -436,
            -437,
            -535,
            -2660,
            -4410,
            -4410,
            -4410,
            -4410,
            -1012,
            -4410,
            -800,
            -4410,
            -166,
            -691,
            -426,
            -660,
            -4410,
            -3358,
            -4410,
            -805,
            -791,
            -623,
            -543,
            -171,
            -953,
            -506
        ],
        "steps_history": [
            1521,
            4410,
            1805,
            1708,
            4410,
            4410,
            1974,
            4410,
            4410,
            4410,
            4410,
            4410,
            1421,
            4410,
            2478,
            2525,
            4410,
            486,
            2475,
            3619,
            3374,
            537,
            538,
            636,
            2761,
            4410,
            4410,
            4410,
            4410,
            1113,
            4410,
            901,
            4410,
            267,
            792,
            527,
            761,
            4410,
            3459,
            4410,
            906,
            892,
            724,
            644,
            272,
            1054,
            607
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "637/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.620910882949829,
        "final_policy_stability": 0.8843537414965986,
        "episodes_to_convergence": 52,
        "policy_stability_history": [
            0.0,
            0.8435374149659864,
            0.8027210884353742,
            0.8616780045351474,
            0.8594104308390023,
            0.8866213151927438,
            0.9705215419501134,
            0.8662131519274376,
            0.9115646258503401,
            0.8526077097505669,
            0.9659863945578231,
            0.8594104308390023,
            0.8934240362811792,
            0.891156462585034,
            0.891156462585034,
            0.927437641723356,
            0.9727891156462585,
            0.8979591836734694,
            0.9160997732426304,
            0.9115646258503401,
            0.9229024943310657,
            0.9501133786848073,
            0.9183673469387755,
            0.873015873015873,
            0.9591836734693877,
            0.8866213151927438,
            0.9659863945578231,
            0.8480725623582767,
            0.9546485260770975,
            0.9002267573696145,
            0.8798185941043084,
            0.9931972789115646,
            0.8979591836734694,
            0.9569160997732427,
            0.9909297052154195,
            0.9863945578231292,
            0.9319727891156463,
            0.9727891156462585,
            0.8956916099773242,
            0.891156462585034,
            0.981859410430839,
            0.9909297052154195,
            1.0,
            0.891156462585034,
            0.9841269841269841,
            0.9773242630385488,
            0.9886621315192744,
            0.9795918367346939,
            0.8888888888888888,
            0.9863945578231292,
            1.0,
            1.0,
            0.8843537414965986
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -2818,
            -4410,
            -179,
            -4410,
            -1970,
            -4410,
            -560,
            -4410,
            -1816,
            -3215,
            -4410,
            -1271,
            -335,
            -4410,
            -4410,
            -2390,
            -1202,
            -1194,
            -4410,
            -2907,
            -552,
            -4410,
            -582,
            -4410,
            -995,
            -4410,
            -3441,
            -218,
            -2568,
            -1845,
            -416,
            -652,
            -1594,
            -805,
            -2919,
            -2433,
            -527,
            -505,
            -403,
            -4410,
            -396,
            -567,
            -431,
            -527,
            -4410,
            -1310,
            -732,
            -286,
            -4410
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            2919,
            4410,
            280,
            4410,
            2071,
            4410,
            661,
            4410,
            1917,
            3316,
            4410,
            1372,
            436,
            4410,
            4410,
            2491,
            1303,
            1295,
            4410,
            3008,
            653,
            4410,
            683,
            4410,
            1096,
            4410,
            3542,
            319,
            2669,
            1946,
            517,
            753,
            1695,
            906,
            3020,
            2534,
            628,
            606,
            504,
            4410,
            497,
            668,
            532,
            628,
            4410,
            1411,
            833,
            387,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "638/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.23646855354309,
        "final_policy_stability": 0.9410430839002267,
        "episodes_to_convergence": 61,
        "policy_stability_history": [
            0.0,
            0.8843537414965986,
            0.8458049886621315,
            0.9229024943310657,
            0.8503401360544217,
            0.8888888888888888,
            0.7755102040816326,
            0.9387755102040817,
            0.8435374149659864,
            0.8888888888888888,
            0.9115646258503401,
            0.8820861678004536,
            0.8956916099773242,
            0.9024943310657596,
            0.9410430839002267,
            0.9251700680272109,
            0.873015873015873,
            0.9614512471655329,
            0.9433106575963719,
            0.8458049886621315,
            0.9501133786848073,
            0.873015873015873,
            0.8956916099773242,
            0.9229024943310657,
            0.8707482993197279,
            0.8435374149659864,
            0.9750566893424036,
            0.8707482993197279,
            0.9591836734693877,
            0.9115646258503401,
            0.8503401360544217,
            0.9750566893424036,
            0.9682539682539683,
            0.9546485260770975,
            0.981859410430839,
            0.9659863945578231,
            0.981859410430839,
            0.9682539682539683,
            0.8843537414965986,
            0.9954648526077098,
            0.9750566893424036,
            0.9659863945578231,
            0.9659863945578231,
            0.8526077097505669,
            0.9886621315192744,
            0.9773242630385488,
            0.9863945578231292,
            0.9977324263038548,
            0.9659863945578231,
            0.9795918367346939,
            0.981859410430839,
            0.9523809523809523,
            0.9954648526077098,
            0.9954648526077098,
            0.9977324263038548,
            0.9977324263038548,
            0.9886621315192744,
            0.9954648526077098,
            1.0,
            0.909297052154195,
            0.9954648526077098,
            0.9410430839002267
        ],
        "reward_history": [
            -4410,
            -1582,
            -4410,
            -1056,
            -4410,
            -4410,
            -4410,
            -1389,
            -4410,
            -3069,
            -3120,
            -2120,
            -4410,
            -4410,
            -1087,
            -1187,
            -4410,
            -525,
            -1224,
            -4410,
            -783,
            -2749,
            -2486,
            -4410,
            -4410,
            -4410,
            -338,
            -4410,
            -633,
            -4410,
            -4410,
            -674,
            -492,
            -1311,
            -410,
            -637,
            -541,
            -317,
            -4410,
            -130,
            -521,
            -885,
            -695,
            -4410,
            -258,
            -963,
            -286,
            -203,
            -973,
            -907,
            -405,
            -1285,
            -992,
            -444,
            -588,
            -143,
            -331,
            -347,
            -315,
            -4410,
            -591,
            -1815
        ],
        "steps_history": [
            4410,
            1683,
            4410,
            1157,
            4410,
            4410,
            4410,
            1490,
            4410,
            3170,
            3221,
            2221,
            4410,
            4410,
            1188,
            1288,
            4410,
            626,
            1325,
            4410,
            884,
            2850,
            2587,
            4410,
            4410,
            4410,
            439,
            4410,
            734,
            4410,
            4410,
            775,
            593,
            1412,
            511,
            738,
            642,
            418,
            4410,
            231,
            622,
            986,
            796,
            4410,
            359,
            1064,
            387,
            304,
            1074,
            1008,
            506,
            1386,
            1093,
            545,
            689,
            244,
            432,
            448,
            416,
            4410,
            692,
            1916
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "639/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.664705753326416,
        "final_policy_stability": 0.9727891156462585,
        "episodes_to_convergence": 59,
        "policy_stability_history": [
            0.0,
            0.8820861678004536,
            0.8775510204081632,
            0.8072562358276644,
            0.9160997732426304,
            0.9047619047619048,
            0.9478458049886621,
            0.9297052154195011,
            0.8707482993197279,
            0.9024943310657596,
            0.8480725623582767,
            0.854875283446712,
            0.9160997732426304,
            0.8662131519274376,
            0.9569160997732427,
            0.9115646258503401,
            0.9387755102040817,
            0.9387755102040817,
            0.8639455782312925,
            0.8956916099773242,
            0.8866213151927438,
            0.8276643990929705,
            0.909297052154195,
            0.9365079365079365,
            0.9319727891156463,
            0.891156462585034,
            0.981859410430839,
            0.9750566893424036,
            0.9070294784580499,
            0.9455782312925171,
            0.9705215419501134,
            0.9659863945578231,
            0.8843537414965986,
            0.9002267573696145,
            0.9523809523809523,
            0.8934240362811792,
            0.9886621315192744,
            0.9795918367346939,
            0.927437641723356,
            0.873015873015873,
            0.9863945578231292,
            0.9455782312925171,
            0.9614512471655329,
            0.9909297052154195,
            0.8616780045351474,
            0.9931972789115646,
            0.9909297052154195,
            0.909297052154195,
            0.9931972789115646,
            0.9115646258503401,
            0.9977324263038548,
            0.9977324263038548,
            0.8843537414965986,
            0.8775510204081632,
            0.9863945578231292,
            0.9727891156462585,
            0.9841269841269841,
            0.981859410430839,
            0.9795918367346939,
            0.9727891156462585
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -2001,
            -2043,
            -613,
            -903,
            -4410,
            -4410,
            -4410,
            -2655,
            -2424,
            -4410,
            -690,
            -914,
            -940,
            -679,
            -4410,
            -4410,
            -4410,
            -4410,
            -1575,
            -1561,
            -1442,
            -4410,
            -624,
            -318,
            -2506,
            -1084,
            -588,
            -1147,
            -4410,
            -4410,
            -1577,
            -4410,
            -170,
            -571,
            -1779,
            -3931,
            -349,
            -1176,
            -918,
            -448,
            -4410,
            -322,
            -259,
            -4410,
            -452,
            -3138,
            -169,
            -341,
            -4410,
            -4410,
            -525,
            -1081,
            -898,
            -881,
            -794,
            -772
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            2102,
            2144,
            714,
            1004,
            4410,
            4410,
            4410,
            2756,
            2525,
            4410,
            791,
            1015,
            1041,
            780,
            4410,
            4410,
            4410,
            4410,
            1676,
            1662,
            1543,
            4410,
            725,
            419,
            2607,
            1185,
            689,
            1248,
            4410,
            4410,
            1678,
            4410,
            271,
            672,
            1880,
            4032,
            450,
            1277,
            1019,
            549,
            4410,
            423,
            360,
            4410,
            553,
            3239,
            270,
            442,
            4410,
            4410,
            626,
            1182,
            999,
            982,
            895,
            873
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "640/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.500095129013062,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 59,
        "policy_stability_history": [
            0.0,
            0.8503401360544217,
            0.8299319727891157,
            0.8820861678004536,
            0.8888888888888888,
            0.891156462585034,
            0.8866213151927438,
            0.8140589569160998,
            0.9138321995464853,
            0.8458049886621315,
            0.9319727891156463,
            0.9523809523809523,
            0.8662131519274376,
            0.9365079365079365,
            0.9546485260770975,
            0.8639455782312925,
            0.891156462585034,
            0.9523809523809523,
            0.8752834467120182,
            0.8775510204081632,
            0.891156462585034,
            0.927437641723356,
            0.8390022675736961,
            0.8571428571428571,
            0.8616780045351474,
            0.8798185941043084,
            0.8594104308390023,
            0.9501133786848073,
            0.9682539682539683,
            0.9773242630385488,
            0.9455782312925171,
            0.9591836734693877,
            0.9863945578231292,
            0.891156462585034,
            0.981859410430839,
            0.9614512471655329,
            0.9501133786848073,
            0.9863945578231292,
            0.9841269841269841,
            0.8820861678004536,
            0.9841269841269841,
            0.9705215419501134,
            0.9682539682539683,
            0.9047619047619048,
            0.9727891156462585,
            0.9705215419501134,
            0.9886621315192744,
            0.981859410430839,
            0.9863945578231292,
            0.9727891156462585,
            0.9002267573696145,
            0.9160997732426304,
            0.9886621315192744,
            0.9297052154195011,
            0.9410430839002267,
            0.981859410430839,
            0.9931972789115646,
            0.9954648526077098,
            1.0,
            1.0
        ],
        "reward_history": [
            -4410,
            -4410,
            -3254,
            -4410,
            -2215,
            -2237,
            -4410,
            -4410,
            -1715,
            -3205,
            -1003,
            -420,
            -4410,
            -1031,
            -634,
            -4410,
            -4410,
            -764,
            -4410,
            -2649,
            -4410,
            -2215,
            -3486,
            -4410,
            -4410,
            -4410,
            -3166,
            -832,
            -510,
            -297,
            -1241,
            -276,
            -408,
            -4410,
            -288,
            -563,
            -1212,
            -549,
            -357,
            -4410,
            -370,
            -1002,
            -610,
            -3569,
            -446,
            -745,
            -214,
            -787,
            -309,
            -885,
            -3899,
            -4410,
            -423,
            -4410,
            -2548,
            -695,
            -480,
            -436,
            -295,
            -356
        ],
        "steps_history": [
            4410,
            4410,
            3355,
            4410,
            2316,
            2338,
            4410,
            4410,
            1816,
            3306,
            1104,
            521,
            4410,
            1132,
            735,
            4410,
            4410,
            865,
            4410,
            2750,
            4410,
            2316,
            3587,
            4410,
            4410,
            4410,
            3267,
            933,
            611,
            398,
            1342,
            377,
            509,
            4410,
            389,
            664,
            1313,
            650,
            458,
            4410,
            471,
            1103,
            711,
            3670,
            547,
            846,
            315,
            888,
            410,
            986,
            4000,
            4410,
            524,
            4410,
            2649,
            796,
            581,
            537,
            396,
            457
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "641/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.300557374954224,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 52,
        "policy_stability_history": [
            0.0,
            0.8049886621315193,
            0.873015873015873,
            0.9501133786848073,
            0.927437641723356,
            0.9115646258503401,
            0.8888888888888888,
            0.9478458049886621,
            0.8571428571428571,
            0.873015873015873,
            0.8616780045351474,
            0.9569160997732427,
            0.8571428571428571,
            0.891156462585034,
            0.891156462585034,
            0.9433106575963719,
            0.8458049886621315,
            0.8866213151927438,
            0.9138321995464853,
            0.8412698412698413,
            0.9229024943310657,
            0.9387755102040817,
            0.8662131519274376,
            0.8390022675736961,
            0.9070294784580499,
            0.963718820861678,
            0.9863945578231292,
            0.9160997732426304,
            0.8866213151927438,
            0.9682539682539683,
            0.9002267573696145,
            0.9841269841269841,
            0.8458049886621315,
            0.9886621315192744,
            0.8684807256235828,
            0.9841269841269841,
            0.9886621315192744,
            0.9659863945578231,
            0.9478458049886621,
            0.9954648526077098,
            0.9954648526077098,
            0.9931972789115646,
            0.9705215419501134,
            0.9795918367346939,
            0.9863945578231292,
            0.9297052154195011,
            0.9954648526077098,
            0.9841269841269841,
            0.9841269841269841,
            1.0,
            0.9863945578231292,
            0.9931972789115646,
            1.0
        ],
        "reward_history": [
            -3020,
            -4410,
            -4410,
            -398,
            -1689,
            -1868,
            -4410,
            -813,
            -4410,
            -4410,
            -2494,
            -581,
            -4410,
            -4410,
            -4410,
            -870,
            -4410,
            -2617,
            -1503,
            -4410,
            -1271,
            -570,
            -4410,
            -4410,
            -2758,
            -971,
            -267,
            -1814,
            -3293,
            -953,
            -2446,
            -261,
            -4410,
            -345,
            -4410,
            -344,
            -197,
            -716,
            -1028,
            -419,
            -269,
            -216,
            -895,
            -374,
            -318,
            -2391,
            -372,
            -767,
            -714,
            -254,
            -297,
            -278,
            -147
        ],
        "steps_history": [
            3121,
            4410,
            4410,
            499,
            1790,
            1969,
            4410,
            914,
            4410,
            4410,
            2595,
            682,
            4410,
            4410,
            4410,
            971,
            4410,
            2718,
            1604,
            4410,
            1372,
            671,
            4410,
            4410,
            2859,
            1072,
            368,
            1915,
            3394,
            1054,
            2547,
            362,
            4410,
            446,
            4410,
            445,
            298,
            817,
            1129,
            520,
            370,
            317,
            996,
            475,
            419,
            2492,
            473,
            868,
            815,
            355,
            398,
            379,
            248
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "642/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.412500143051147,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 46,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.8684807256235828,
            0.9523809523809523,
            0.8117913832199547,
            0.8639455782312925,
            0.8798185941043084,
            0.9501133786848073,
            0.9138321995464853,
            0.891156462585034,
            0.9365079365079365,
            0.8707482993197279,
            0.9523809523809523,
            0.8843537414965986,
            0.8820861678004536,
            0.9297052154195011,
            0.8866213151927438,
            0.9727891156462585,
            0.9024943310657596,
            0.963718820861678,
            0.8662131519274376,
            0.9183673469387755,
            0.9591836734693877,
            0.927437641723356,
            0.9773242630385488,
            0.8571428571428571,
            0.8684807256235828,
            0.8390022675736961,
            0.9591836734693877,
            0.8843537414965986,
            0.9841269841269841,
            0.9659863945578231,
            0.8843537414965986,
            0.9886621315192744,
            0.927437641723356,
            0.9795918367346939,
            0.9750566893424036,
            0.9614512471655329,
            0.981859410430839,
            0.9750566893424036,
            0.873015873015873,
            0.9931972789115646,
            0.9410430839002267,
            0.9977324263038548,
            0.9977324263038548,
            0.9977324263038548,
            1.0
        ],
        "reward_history": [
            -4410,
            -4410,
            -2946,
            -559,
            -4410,
            -4410,
            -4410,
            -619,
            -1687,
            -4410,
            -648,
            -4410,
            -709,
            -4410,
            -4410,
            -1079,
            -2924,
            -489,
            -4410,
            -400,
            -4410,
            -1311,
            -339,
            -1396,
            -540,
            -4410,
            -4410,
            -3965,
            -858,
            -4269,
            -583,
            -595,
            -4410,
            -250,
            -1729,
            -616,
            -707,
            -543,
            -440,
            -475,
            -3976,
            -387,
            -1734,
            -361,
            -380,
            -487,
            -214
        ],
        "steps_history": [
            4410,
            4410,
            3047,
            660,
            4410,
            4410,
            4410,
            720,
            1788,
            4410,
            749,
            4410,
            810,
            4410,
            4410,
            1180,
            3025,
            590,
            4410,
            501,
            4410,
            1412,
            440,
            1497,
            641,
            4410,
            4410,
            4066,
            959,
            4370,
            684,
            696,
            4410,
            351,
            1830,
            717,
            808,
            644,
            541,
            576,
            4077,
            488,
            1835,
            462,
            481,
            588,
            315
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "643/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.612926959991455,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 59,
        "policy_stability_history": [
            0.0,
            0.8684807256235828,
            0.8820861678004536,
            0.8662131519274376,
            0.8639455782312925,
            0.9115646258503401,
            0.8798185941043084,
            0.9342403628117913,
            0.8707482993197279,
            0.9591836734693877,
            0.7891156462585034,
            0.9455782312925171,
            0.8616780045351474,
            0.9024943310657596,
            0.8639455782312925,
            0.927437641723356,
            0.9070294784580499,
            0.927437641723356,
            0.8571428571428571,
            0.8299319727891157,
            0.9569160997732427,
            0.8843537414965986,
            0.9614512471655329,
            0.8843537414965986,
            0.9433106575963719,
            0.8820861678004536,
            0.9478458049886621,
            0.9433106575963719,
            0.963718820861678,
            0.9863945578231292,
            0.8662131519274376,
            0.8979591836734694,
            0.9569160997732427,
            0.873015873015873,
            0.8752834467120182,
            0.9659863945578231,
            0.9886621315192744,
            0.981859410430839,
            0.9909297052154195,
            0.9501133786848073,
            0.9886621315192744,
            0.9206349206349206,
            0.981859410430839,
            0.8956916099773242,
            0.9206349206349206,
            0.9795918367346939,
            0.9795918367346939,
            0.9909297052154195,
            0.9954648526077098,
            0.9841269841269841,
            0.9841269841269841,
            1.0,
            0.9954648526077098,
            0.9863945578231292,
            0.9750566893424036,
            0.9977324263038548,
            0.9977324263038548,
            0.9931972789115646,
            1.0,
            0.9977324263038548
        ],
        "reward_history": [
            -3920,
            -4410,
            -2733,
            -4410,
            -3740,
            -2365,
            -2450,
            -1542,
            -4410,
            -569,
            -4410,
            -719,
            -4410,
            -4410,
            -4410,
            -1481,
            -4410,
            -791,
            -2731,
            -3092,
            -457,
            -4410,
            -453,
            -2807,
            -574,
            -4410,
            -921,
            -1174,
            -530,
            -200,
            -4410,
            -4410,
            -619,
            -3071,
            -4410,
            -586,
            -216,
            -201,
            -166,
            -1251,
            -366,
            -2437,
            -449,
            -4410,
            -2985,
            -428,
            -618,
            -784,
            -261,
            -528,
            -529,
            -231,
            -189,
            -346,
            -555,
            -242,
            -304,
            -295,
            -219,
            -147
        ],
        "steps_history": [
            4021,
            4410,
            2834,
            4410,
            3841,
            2466,
            2551,
            1643,
            4410,
            670,
            4410,
            820,
            4410,
            4410,
            4410,
            1582,
            4410,
            892,
            2832,
            3193,
            558,
            4410,
            554,
            2908,
            675,
            4410,
            1022,
            1275,
            631,
            301,
            4410,
            4410,
            720,
            3172,
            4410,
            687,
            317,
            302,
            267,
            1352,
            467,
            2538,
            550,
            4410,
            3086,
            529,
            719,
            885,
            362,
            629,
            630,
            332,
            290,
            447,
            656,
            343,
            405,
            396,
            320,
            248
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "644/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.211816787719727,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 48,
        "policy_stability_history": [
            0.0,
            0.8752834467120182,
            0.8095238095238095,
            0.9387755102040817,
            0.8934240362811792,
            0.8775510204081632,
            0.8979591836734694,
            0.8140589569160998,
            0.8344671201814059,
            0.9433106575963719,
            0.9501133786848073,
            0.8662131519274376,
            0.9342403628117913,
            0.9047619047619048,
            0.9410430839002267,
            0.8707482993197279,
            0.9024943310657596,
            0.8571428571428571,
            0.9206349206349206,
            0.8956916099773242,
            0.9569160997732427,
            0.9183673469387755,
            0.9546485260770975,
            0.9455782312925171,
            0.9773242630385488,
            0.81859410430839,
            0.9682539682539683,
            0.927437641723356,
            0.9773242630385488,
            0.9705215419501134,
            0.9750566893424036,
            0.9659863945578231,
            0.8956916099773242,
            0.9659863945578231,
            0.9909297052154195,
            0.9138321995464853,
            0.9750566893424036,
            0.9591836734693877,
            0.9659863945578231,
            0.8866213151927438,
            0.9863945578231292,
            0.9931972789115646,
            0.9750566893424036,
            0.9115646258503401,
            0.9886621315192744,
            0.9954648526077098,
            0.9795918367346939,
            0.9931972789115646,
            1.0
        ],
        "reward_history": [
            -573,
            -4410,
            -3959,
            -955,
            -4410,
            -3801,
            -4410,
            -4410,
            -4410,
            -497,
            -638,
            -4410,
            -822,
            -4410,
            -739,
            -4410,
            -4410,
            -4410,
            -1306,
            -4410,
            -841,
            -2962,
            -722,
            -1209,
            -352,
            -4410,
            -996,
            -1316,
            -490,
            -799,
            -534,
            -697,
            -2965,
            -682,
            -236,
            -4410,
            -589,
            -764,
            -535,
            -3084,
            -560,
            -298,
            -576,
            -4120,
            -364,
            -285,
            -669,
            -381,
            -182
        ],
        "steps_history": [
            674,
            4410,
            4060,
            1056,
            4410,
            3902,
            4410,
            4410,
            4410,
            598,
            739,
            4410,
            923,
            4410,
            840,
            4410,
            4410,
            4410,
            1407,
            4410,
            942,
            3063,
            823,
            1310,
            453,
            4410,
            1097,
            1417,
            591,
            900,
            635,
            798,
            3066,
            783,
            337,
            4410,
            690,
            865,
            636,
            3185,
            661,
            399,
            677,
            4221,
            465,
            386,
            770,
            482,
            283
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "645/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.754313468933105,
        "final_policy_stability": 0.9750566893424036,
        "episodes_to_convergence": 45,
        "policy_stability_history": [
            0.0,
            0.8321995464852607,
            0.8072562358276644,
            0.9206349206349206,
            0.8526077097505669,
            0.8594104308390023,
            0.8843537414965986,
            0.9501133786848073,
            0.8775510204081632,
            0.8752834467120182,
            0.9546485260770975,
            0.891156462585034,
            0.8571428571428571,
            0.9024943310657596,
            0.8979591836734694,
            0.9229024943310657,
            0.9501133786848073,
            0.8798185941043084,
            0.9387755102040817,
            0.8752834467120182,
            0.9047619047619048,
            0.9841269841269841,
            0.9229024943310657,
            0.9455782312925171,
            0.9047619047619048,
            0.9931972789115646,
            0.8662131519274376,
            0.9659863945578231,
            0.9501133786848073,
            0.9591836734693877,
            0.9659863945578231,
            0.8594104308390023,
            0.9954648526077098,
            0.981859410430839,
            0.9705215419501134,
            0.9931972789115646,
            0.8639455782312925,
            0.8775510204081632,
            0.9773242630385488,
            0.9614512471655329,
            0.891156462585034,
            0.9047619047619048,
            0.9841269841269841,
            0.9841269841269841,
            0.9160997732426304,
            0.9750566893424036
        ],
        "reward_history": [
            -874,
            -4410,
            -4410,
            -4410,
            -4410,
            -3296,
            -4410,
            -1332,
            -4410,
            -4410,
            -1575,
            -4410,
            -4410,
            -4410,
            -4410,
            -1599,
            -1190,
            -4410,
            -1460,
            -4410,
            -4410,
            -274,
            -2661,
            -2562,
            -4410,
            -233,
            -3803,
            -856,
            -1623,
            -917,
            -1221,
            -3948,
            -408,
            -471,
            -1365,
            -250,
            -4410,
            -4410,
            -671,
            -1126,
            -4410,
            -4410,
            -654,
            -742,
            -4410,
            -918
        ],
        "steps_history": [
            975,
            4410,
            4410,
            4410,
            4410,
            3397,
            4410,
            1433,
            4410,
            4410,
            1676,
            4410,
            4410,
            4410,
            4410,
            1700,
            1291,
            4410,
            1561,
            4410,
            4410,
            375,
            2762,
            2663,
            4410,
            334,
            3904,
            957,
            1724,
            1018,
            1322,
            4049,
            509,
            572,
            1466,
            351,
            4410,
            4410,
            772,
            1227,
            4410,
            4410,
            755,
            843,
            4410,
            1019
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "646/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.45941710472107,
        "final_policy_stability": 0.9750566893424036,
        "episodes_to_convergence": 50,
        "policy_stability_history": [
            0.0,
            0.9206349206349206,
            0.8866213151927438,
            0.891156462585034,
            0.8344671201814059,
            0.8934240362811792,
            0.9523809523809523,
            0.9047619047619048,
            0.9183673469387755,
            0.8820861678004536,
            0.9365079365079365,
            0.9183673469387755,
            0.8934240362811792,
            0.8866213151927438,
            0.9229024943310657,
            0.8866213151927438,
            0.9002267573696145,
            0.9569160997732427,
            0.8956916099773242,
            0.8820861678004536,
            0.8798185941043084,
            0.9229024943310657,
            0.9591836734693877,
            0.9115646258503401,
            0.891156462585034,
            0.9523809523809523,
            0.9410430839002267,
            0.891156462585034,
            0.9682539682539683,
            0.8775510204081632,
            0.9614512471655329,
            0.8594104308390023,
            0.8798185941043084,
            0.9455782312925171,
            0.891156462585034,
            0.9546485260770975,
            0.9795918367346939,
            0.9841269841269841,
            0.9682539682539683,
            0.9070294784580499,
            0.9954648526077098,
            0.9909297052154195,
            0.9954648526077098,
            0.9115646258503401,
            0.9863945578231292,
            0.9909297052154195,
            0.9795918367346939,
            0.9841269841269841,
            0.9931972789115646,
            1.0,
            0.9750566893424036
        ],
        "reward_history": [
            -4410,
            -388,
            -4410,
            -2691,
            -3806,
            -4410,
            -800,
            -4410,
            -1551,
            -4410,
            -1597,
            -4410,
            -4410,
            -4410,
            -1104,
            -1985,
            -4410,
            -762,
            -2112,
            -4410,
            -3441,
            -4410,
            -860,
            -2911,
            -2794,
            -1447,
            -1863,
            -4410,
            -1002,
            -3611,
            -543,
            -4410,
            -4410,
            -1855,
            -4410,
            -1461,
            -1004,
            -449,
            -1073,
            -4410,
            -125,
            -449,
            -500,
            -4410,
            -794,
            -901,
            -2274,
            -949,
            -673,
            -260,
            -1117
        ],
        "steps_history": [
            4410,
            489,
            4410,
            2792,
            3907,
            4410,
            901,
            4410,
            1652,
            4410,
            1698,
            4410,
            4410,
            4410,
            1205,
            2086,
            4410,
            863,
            2213,
            4410,
            3542,
            4410,
            961,
            3012,
            2895,
            1548,
            1964,
            4410,
            1103,
            3712,
            644,
            4410,
            4410,
            1956,
            4410,
            1562,
            1105,
            550,
            1174,
            4410,
            226,
            550,
            601,
            4410,
            895,
            1002,
            2375,
            1050,
            774,
            361,
            1218
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "647/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.725655555725098,
        "final_policy_stability": 0.9206349206349206,
        "episodes_to_convergence": 42,
        "policy_stability_history": [
            0.0,
            0.8752834467120182,
            0.8752834467120182,
            0.8616780045351474,
            0.8820861678004536,
            0.9002267573696145,
            0.8956916099773242,
            0.8662131519274376,
            0.9206349206349206,
            0.8979591836734694,
            0.9002267573696145,
            0.9387755102040817,
            0.8979591836734694,
            0.8956916099773242,
            0.8639455782312925,
            0.9002267573696145,
            0.9024943310657596,
            0.9138321995464853,
            0.9591836734693877,
            0.909297052154195,
            0.8503401360544217,
            0.9795918367346939,
            0.9115646258503401,
            0.9795918367346939,
            0.9841269841269841,
            0.9705215419501134,
            0.8684807256235828,
            0.927437641723356,
            0.9909297052154195,
            0.9795918367346939,
            0.9523809523809523,
            0.9569160997732427,
            0.9138321995464853,
            0.9455782312925171,
            0.9365079365079365,
            0.9841269841269841,
            0.9886621315192744,
            0.9886621315192744,
            0.9750566893424036,
            0.9342403628117913,
            0.9773242630385488,
            0.9954648526077098,
            0.9206349206349206
        ],
        "reward_history": [
            -1441,
            -4410,
            -3497,
            -4410,
            -4410,
            -2548,
            -4410,
            -3863,
            -1665,
            -4410,
            -4410,
            -1142,
            -3639,
            -4410,
            -4410,
            -2791,
            -4410,
            -2205,
            -582,
            -4410,
            -4410,
            -387,
            -4410,
            -362,
            -1313,
            -572,
            -3099,
            -4410,
            -359,
            -761,
            -1074,
            -1594,
            -4410,
            -1730,
            -4410,
            -616,
            -669,
            -674,
            -1145,
            -3068,
            -1172,
            -1621,
            -4410
        ],
        "steps_history": [
            1542,
            4410,
            3598,
            4410,
            4410,
            2649,
            4410,
            3964,
            1766,
            4410,
            4410,
            1243,
            3740,
            4410,
            4410,
            2892,
            4410,
            2306,
            683,
            4410,
            4410,
            488,
            4410,
            463,
            1414,
            673,
            3200,
            4410,
            460,
            862,
            1175,
            1695,
            4410,
            1831,
            4410,
            717,
            770,
            775,
            1246,
            3169,
            1273,
            1722,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "648/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.917649984359741,
        "final_policy_stability": 0.9931972789115646,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.873015873015873,
            0.8503401360544217,
            0.8820861678004536,
            0.9229024943310657,
            0.891156462585034,
            0.9160997732426304,
            0.8956916099773242,
            0.9206349206349206,
            0.9410430839002267,
            0.9251700680272109,
            0.9070294784580499,
            0.9047619047619048,
            0.9455782312925171,
            0.8956916099773242,
            0.8820861678004536,
            0.9206349206349206,
            0.8752834467120182,
            0.8934240362811792,
            0.9115646258503401,
            0.8820861678004536,
            0.8888888888888888,
            0.9659863945578231,
            0.8956916099773242,
            0.9523809523809523,
            0.9160997732426304,
            0.8820861678004536,
            0.9569160997732427,
            0.9024943310657596,
            0.8752834467120182,
            0.9773242630385488,
            0.9160997732426304,
            0.9206349206349206,
            0.9433106575963719,
            0.9954648526077098,
            0.9160997732426304,
            0.9160997732426304,
            1.0,
            0.9863945578231292,
            0.9954648526077098,
            0.9931972789115646
        ],
        "reward_history": [
            -3710,
            -4410,
            -2581,
            -3198,
            -1467,
            -4410,
            -4410,
            -4410,
            -1363,
            -1353,
            -4410,
            -3225,
            -4410,
            -1603,
            -4410,
            -4410,
            -1627,
            -3491,
            -4410,
            -3341,
            -4410,
            -4410,
            -1298,
            -4410,
            -1306,
            -4410,
            -4410,
            -993,
            -4410,
            -4410,
            -723,
            -4410,
            -1560,
            -3096,
            -385,
            -4410,
            -4410,
            -97,
            -933,
            -514,
            -453
        ],
        "steps_history": [
            3811,
            4410,
            2682,
            3299,
            1568,
            4410,
            4410,
            4410,
            1464,
            1454,
            4410,
            3326,
            4410,
            1704,
            4410,
            4410,
            1728,
            3592,
            4410,
            3442,
            4410,
            4410,
            1399,
            4410,
            1407,
            4410,
            4410,
            1094,
            4410,
            4410,
            824,
            4410,
            1661,
            3197,
            486,
            4410,
            4410,
            198,
            1034,
            615,
            554
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "649/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.374809265136719,
        "final_policy_stability": 0.8843537414965986,
        "episodes_to_convergence": 48,
        "policy_stability_history": [
            0.0,
            0.8140589569160998,
            0.8798185941043084,
            0.8594104308390023,
            0.8526077097505669,
            0.8707482993197279,
            0.8843537414965986,
            0.8798185941043084,
            0.8979591836734694,
            0.8934240362811792,
            0.9206349206349206,
            0.8616780045351474,
            0.9478458049886621,
            0.9160997732426304,
            0.9614512471655329,
            0.9682539682539683,
            0.9115646258503401,
            0.8616780045351474,
            0.9546485260770975,
            0.9591836734693877,
            0.9705215419501134,
            0.909297052154195,
            0.9070294784580499,
            0.8888888888888888,
            0.891156462585034,
            0.927437641723356,
            0.9727891156462585,
            0.9138321995464853,
            0.9909297052154195,
            0.9659863945578231,
            0.9750566893424036,
            0.9727891156462585,
            0.8775510204081632,
            0.981859410430839,
            0.9886621315192744,
            0.9954648526077098,
            0.9115646258503401,
            0.9183673469387755,
            0.8888888888888888,
            0.9931972789115646,
            0.9183673469387755,
            0.9977324263038548,
            0.9433106575963719,
            0.9773242630385488,
            0.9909297052154195,
            1.0,
            0.9954648526077098,
            0.9909297052154195,
            0.8843537414965986
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -3298,
            -4410,
            -4410,
            -2740,
            -1965,
            -4410,
            -4410,
            -990,
            -1670,
            -713,
            -799,
            -4410,
            -4410,
            -847,
            -652,
            -659,
            -2005,
            -4410,
            -4410,
            -4410,
            -2181,
            -435,
            -4410,
            -121,
            -1341,
            -2141,
            -1436,
            -4410,
            -520,
            -650,
            -455,
            -4410,
            -2493,
            -4410,
            -564,
            -4410,
            -672,
            -3164,
            -1190,
            -751,
            -808,
            -491,
            -968,
            -4410
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            4410,
            3399,
            4410,
            4410,
            2841,
            2066,
            4410,
            4410,
            1091,
            1771,
            814,
            900,
            4410,
            4410,
            948,
            753,
            760,
            2106,
            4410,
            4410,
            4410,
            2282,
            536,
            4410,
            222,
            1442,
            2242,
            1537,
            4410,
            621,
            751,
            556,
            4410,
            2594,
            4410,
            665,
            4410,
            773,
            3265,
            1291,
            852,
            909,
            592,
            1069,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "650/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.870588302612305,
        "final_policy_stability": 0.9206349206349206,
        "episodes_to_convergence": 53,
        "policy_stability_history": [
            0.0,
            0.8276643990929705,
            0.8820861678004536,
            0.873015873015873,
            0.8571428571428571,
            0.909297052154195,
            0.891156462585034,
            0.9478458049886621,
            0.873015873015873,
            0.9659863945578231,
            0.8390022675736961,
            0.8390022675736961,
            0.8684807256235828,
            0.891156462585034,
            0.9501133786848073,
            0.9365079365079365,
            0.8979591836734694,
            0.873015873015873,
            0.9047619047619048,
            0.8798185941043084,
            0.8594104308390023,
            0.891156462585034,
            0.9319727891156463,
            0.9659863945578231,
            0.8888888888888888,
            0.9795918367346939,
            0.963718820861678,
            0.9727891156462585,
            0.8888888888888888,
            0.981859410430839,
            0.9659863945578231,
            0.9750566893424036,
            0.9682539682539683,
            0.981859410430839,
            0.8956916099773242,
            0.9705215419501134,
            0.9727891156462585,
            0.9705215419501134,
            0.9977324263038548,
            0.9024943310657596,
            0.9954648526077098,
            0.9229024943310657,
            0.9002267573696145,
            0.9841269841269841,
            0.9591836734693877,
            0.9795918367346939,
            0.9433106575963719,
            0.9591836734693877,
            0.9909297052154195,
            0.9931972789115646,
            1.0,
            0.9841269841269841,
            1.0,
            0.9206349206349206
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -3739,
            -3049,
            -4410,
            -483,
            -4410,
            -449,
            -4410,
            -4410,
            -4410,
            -4410,
            -502,
            -1346,
            -3262,
            -4410,
            -4410,
            -2799,
            -4410,
            -4410,
            -1540,
            -512,
            -4410,
            -252,
            -486,
            -519,
            -3089,
            -227,
            -1398,
            -532,
            -667,
            -618,
            -4410,
            -721,
            -906,
            -580,
            -158,
            -4410,
            -578,
            -4410,
            -3685,
            -361,
            -1442,
            -458,
            -1269,
            -1310,
            -343,
            -270,
            -315,
            -979,
            -854,
            -4410
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            3840,
            3150,
            4410,
            584,
            4410,
            550,
            4410,
            4410,
            4410,
            4410,
            603,
            1447,
            3363,
            4410,
            4410,
            2900,
            4410,
            4410,
            1641,
            613,
            4410,
            353,
            587,
            620,
            3190,
            328,
            1499,
            633,
            768,
            719,
            4410,
            822,
            1007,
            681,
            259,
            4410,
            679,
            4410,
            3786,
            462,
            1543,
            559,
            1370,
            1411,
            444,
            371,
            416,
            1080,
            955,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "651/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.605917930603027,
        "final_policy_stability": 0.981859410430839,
        "episodes_to_convergence": 50,
        "policy_stability_history": [
            0.0,
            0.873015873015873,
            0.9115646258503401,
            0.9160997732426304,
            0.8888888888888888,
            0.8684807256235828,
            0.927437641723356,
            0.8843537414965986,
            0.8888888888888888,
            0.8276643990929705,
            0.8458049886621315,
            0.9523809523809523,
            0.8752834467120182,
            0.9501133786848073,
            0.9024943310657596,
            0.8662131519274376,
            0.9773242630385488,
            0.8752834467120182,
            0.9478458049886621,
            0.9138321995464853,
            0.8775510204081632,
            0.9002267573696145,
            0.963718820861678,
            0.9319727891156463,
            0.891156462585034,
            0.8888888888888888,
            0.8390022675736961,
            0.9546485260770975,
            0.909297052154195,
            0.963718820861678,
            0.8934240362811792,
            0.9659863945578231,
            0.963718820861678,
            0.909297052154195,
            0.9863945578231292,
            0.9909297052154195,
            0.9319727891156463,
            0.9931972789115646,
            0.9773242630385488,
            0.9909297052154195,
            0.9954648526077098,
            0.9909297052154195,
            1.0,
            0.9002267573696145,
            0.927437641723356,
            0.9863945578231292,
            0.9954648526077098,
            0.9297052154195011,
            0.9954648526077098,
            0.9795918367346939,
            0.981859410430839
        ],
        "reward_history": [
            -1420,
            -4410,
            -1704,
            -1607,
            -4410,
            -4410,
            -1873,
            -4410,
            -4410,
            -4410,
            -4410,
            -1573,
            -3498,
            -730,
            -4410,
            -3913,
            -308,
            -4410,
            -824,
            -4410,
            -4410,
            -4410,
            -785,
            -2239,
            -4410,
            -4410,
            -4240,
            -1048,
            -4410,
            -445,
            -3286,
            -661,
            -688,
            -4410,
            -641,
            -348,
            -2322,
            -331,
            -865,
            -394,
            -297,
            -724,
            -331,
            -4410,
            -2397,
            -530,
            -149,
            -1362,
            -166,
            -813,
            -450
        ],
        "steps_history": [
            1521,
            4410,
            1805,
            1708,
            4410,
            4410,
            1974,
            4410,
            4410,
            4410,
            4410,
            1674,
            3599,
            831,
            4410,
            4014,
            409,
            4410,
            925,
            4410,
            4410,
            4410,
            886,
            2340,
            4410,
            4410,
            4341,
            1149,
            4410,
            546,
            3387,
            762,
            789,
            4410,
            742,
            449,
            2423,
            432,
            966,
            495,
            398,
            825,
            432,
            4410,
            2498,
            631,
            250,
            1463,
            267,
            914,
            551
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "652/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.178295850753784,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 47,
        "policy_stability_history": [
            0.0,
            0.8095238095238095,
            0.9002267573696145,
            0.9160997732426304,
            0.8639455782312925,
            0.8594104308390023,
            0.9047619047619048,
            0.891156462585034,
            0.8798185941043084,
            0.9002267573696145,
            0.8707482993197279,
            0.9002267573696145,
            0.8526077097505669,
            0.8956916099773242,
            0.9727891156462585,
            0.9206349206349206,
            0.8412698412698413,
            0.9773242630385488,
            0.9047619047619048,
            0.909297052154195,
            0.8526077097505669,
            0.9591836734693877,
            0.9523809523809523,
            0.8956916099773242,
            0.8866213151927438,
            0.8662131519274376,
            0.9024943310657596,
            0.9591836734693877,
            0.9569160997732427,
            0.8956916099773242,
            0.9863945578231292,
            0.9750566893424036,
            0.9954648526077098,
            0.9931972789115646,
            0.9682539682539683,
            0.8843537414965986,
            0.9024943310657596,
            0.9886621315192744,
            0.9115646258503401,
            0.981859410430839,
            0.9931972789115646,
            0.9909297052154195,
            0.9954648526077098,
            0.9750566893424036,
            0.9886621315192744,
            0.9977324263038548,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -1836,
            -4410,
            -4410,
            -2531,
            -2044,
            -2303,
            -4410,
            -4410,
            -4410,
            -3916,
            -4410,
            -190,
            -1791,
            -3877,
            -370,
            -4410,
            -1308,
            -4410,
            -1304,
            -1750,
            -4410,
            -4410,
            -3831,
            -4410,
            -600,
            -964,
            -4410,
            -570,
            -1204,
            -408,
            -531,
            -1190,
            -4410,
            -4410,
            -502,
            -3497,
            -387,
            -567,
            -404,
            -554,
            -805,
            -1084,
            -1212,
            -890,
            -1325
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            1937,
            4410,
            4410,
            2632,
            2145,
            2404,
            4410,
            4410,
            4410,
            4017,
            4410,
            291,
            1892,
            3978,
            471,
            4410,
            1409,
            4410,
            1405,
            1851,
            4410,
            4410,
            3932,
            4410,
            701,
            1065,
            4410,
            671,
            1305,
            509,
            632,
            1291,
            4410,
            4410,
            603,
            3598,
            488,
            668,
            505,
            655,
            906,
            1185,
            1313,
            991,
            1426
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "653/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.7433180809021,
        "final_policy_stability": 0.9841269841269841,
        "episodes_to_convergence": 55,
        "policy_stability_history": [
            0.0,
            0.8662131519274376,
            0.9342403628117913,
            0.9002267573696145,
            0.891156462585034,
            0.9115646258503401,
            0.8707482993197279,
            0.8662131519274376,
            0.9591836734693877,
            0.8752834467120182,
            0.9047619047619048,
            0.9433106575963719,
            0.8639455782312925,
            0.8934240362811792,
            0.8979591836734694,
            0.9433106575963719,
            0.9206349206349206,
            0.9070294784580499,
            0.9229024943310657,
            0.9206349206349206,
            0.8775510204081632,
            0.8888888888888888,
            0.9160997732426304,
            0.8775510204081632,
            0.8979591836734694,
            0.9659863945578231,
            0.9251700680272109,
            0.9410430839002267,
            0.9070294784580499,
            0.9160997732426304,
            0.981859410430839,
            0.9478458049886621,
            0.9478458049886621,
            0.9365079365079365,
            0.8979591836734694,
            0.9841269841269841,
            0.9841269841269841,
            0.9773242630385488,
            0.9841269841269841,
            0.8956916099773242,
            0.9614512471655329,
            0.9863945578231292,
            0.9909297052154195,
            0.9931972789115646,
            0.9909297052154195,
            0.9682539682539683,
            0.9977324263038548,
            0.9705215419501134,
            0.9931972789115646,
            0.8934240362811792,
            0.9909297052154195,
            0.9931972789115646,
            0.9841269841269841,
            0.9931972789115646,
            0.9727891156462585,
            0.9841269841269841
        ],
        "reward_history": [
            -4410,
            -2906,
            -757,
            -4410,
            -4410,
            -4410,
            -4410,
            -3419,
            -332,
            -4410,
            -1368,
            -535,
            -4410,
            -4410,
            -1220,
            -1069,
            -1654,
            -4410,
            -1679,
            -1584,
            -4183,
            -4410,
            -1908,
            -4410,
            -4410,
            -894,
            -2236,
            -1179,
            -4410,
            -3381,
            -100,
            -908,
            -959,
            -1756,
            -4410,
            -250,
            -374,
            -313,
            -199,
            -3714,
            -715,
            -206,
            -470,
            -115,
            -521,
            -398,
            -386,
            -935,
            -311,
            -4410,
            -270,
            -686,
            -304,
            -275,
            -716,
            -687
        ],
        "steps_history": [
            4410,
            3007,
            858,
            4410,
            4410,
            4410,
            4410,
            3520,
            433,
            4410,
            1469,
            636,
            4410,
            4410,
            1321,
            1170,
            1755,
            4410,
            1780,
            1685,
            4284,
            4410,
            2009,
            4410,
            4410,
            995,
            2337,
            1280,
            4410,
            3482,
            201,
            1009,
            1060,
            1857,
            4410,
            351,
            475,
            414,
            300,
            3815,
            816,
            307,
            571,
            216,
            622,
            499,
            487,
            1036,
            412,
            4410,
            371,
            787,
            405,
            376,
            817,
            788
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "654/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.874574661254883,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 54,
        "policy_stability_history": [
            0.0,
            0.8798185941043084,
            0.8458049886621315,
            0.8843537414965986,
            0.9024943310657596,
            0.9138321995464853,
            0.9478458049886621,
            0.9070294784580499,
            0.8707482993197279,
            0.8140589569160998,
            0.8707482993197279,
            0.8843537414965986,
            0.9501133786848073,
            0.8934240362811792,
            0.8616780045351474,
            0.9433106575963719,
            0.8684807256235828,
            0.963718820861678,
            0.9433106575963719,
            0.8843537414965986,
            0.9365079365079365,
            0.9229024943310657,
            0.9682539682539683,
            0.8798185941043084,
            0.9478458049886621,
            0.9523809523809523,
            0.8956916099773242,
            0.8866213151927438,
            0.9319727891156463,
            0.927437641723356,
            0.9841269841269841,
            0.9591836734693877,
            0.9024943310657596,
            0.9455782312925171,
            0.9342403628117913,
            0.9659863945578231,
            0.9886621315192744,
            0.9773242630385488,
            0.9931972789115646,
            0.9931972789115646,
            0.9773242630385488,
            0.9977324263038548,
            0.9727891156462585,
            0.9614512471655329,
            0.9773242630385488,
            0.9070294784580499,
            0.891156462585034,
            0.981859410430839,
            1.0,
            0.9342403628117913,
            0.9954648526077098,
            1.0,
            0.9977324263038548,
            0.9909297052154195,
            1.0
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -2592,
            -1072,
            -439,
            -1582,
            -4410,
            -4410,
            -4410,
            -4410,
            -645,
            -4410,
            -4410,
            -699,
            -4410,
            -433,
            -581,
            -4410,
            -885,
            -2115,
            -544,
            -4410,
            -892,
            -1129,
            -4410,
            -4410,
            -1368,
            -4410,
            -168,
            -1148,
            -4410,
            -1475,
            -1700,
            -780,
            -637,
            -841,
            -218,
            -349,
            -625,
            -555,
            -683,
            -1327,
            -451,
            -4410,
            -4035,
            -887,
            -243,
            -4410,
            -447,
            -502,
            -1296,
            -540,
            -275
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            2693,
            1173,
            540,
            1683,
            4410,
            4410,
            4410,
            4410,
            746,
            4410,
            4410,
            800,
            4410,
            534,
            682,
            4410,
            986,
            2216,
            645,
            4410,
            993,
            1230,
            4410,
            4410,
            1469,
            4410,
            269,
            1249,
            4410,
            1576,
            1801,
            881,
            738,
            942,
            319,
            450,
            726,
            656,
            784,
            1428,
            552,
            4410,
            4136,
            988,
            344,
            4410,
            548,
            603,
            1397,
            641,
            376
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "655/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.44610857963562,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 51,
        "policy_stability_history": [
            0.0,
            0.780045351473923,
            0.8253968253968254,
            0.8866213151927438,
            0.9024943310657596,
            0.9138321995464853,
            0.9070294784580499,
            0.8775510204081632,
            0.8888888888888888,
            0.8616780045351474,
            0.8866213151927438,
            0.8639455782312925,
            0.9251700680272109,
            0.9160997732426304,
            0.8752834467120182,
            0.9455782312925171,
            0.9569160997732427,
            0.9682539682539683,
            0.9410430839002267,
            0.891156462585034,
            0.9410430839002267,
            0.854875283446712,
            0.8775510204081632,
            0.8956916099773242,
            0.9229024943310657,
            0.9705215419501134,
            0.9931972789115646,
            0.8843537414965986,
            0.9206349206349206,
            0.9523809523809523,
            0.9841269841269841,
            0.8843537414965986,
            0.9841269841269841,
            0.9727891156462585,
            0.8843537414965986,
            0.9886621315192744,
            0.9795918367346939,
            0.981859410430839,
            0.9795918367346939,
            0.9410430839002267,
            0.981859410430839,
            0.9909297052154195,
            0.9909297052154195,
            0.8956916099773242,
            1.0,
            0.9909297052154195,
            0.9909297052154195,
            0.9863945578231292,
            0.9977324263038548,
            0.9977324263038548,
            1.0,
            1.0
        ],
        "reward_history": [
            -4410,
            -3861,
            -4410,
            -4410,
            -1943,
            -4410,
            -1775,
            -2423,
            -2196,
            -4410,
            -4410,
            -4410,
            -1254,
            -1675,
            -4410,
            -646,
            -593,
            -361,
            -891,
            -4410,
            -819,
            -4410,
            -2804,
            -4410,
            -2164,
            -285,
            -86,
            -4410,
            -1334,
            -2182,
            -540,
            -4410,
            -340,
            -523,
            -3915,
            -349,
            -331,
            -299,
            -605,
            -1456,
            -1009,
            -406,
            -536,
            -3174,
            -89,
            -264,
            -408,
            -338,
            -539,
            -193,
            -385,
            -351
        ],
        "steps_history": [
            4410,
            3962,
            4410,
            4410,
            2044,
            4410,
            1876,
            2524,
            2297,
            4410,
            4410,
            4410,
            1355,
            1776,
            4410,
            747,
            694,
            462,
            992,
            4410,
            920,
            4410,
            2905,
            4410,
            2265,
            386,
            187,
            4410,
            1435,
            2283,
            641,
            4410,
            441,
            624,
            4016,
            450,
            432,
            400,
            706,
            1557,
            1110,
            507,
            637,
            3275,
            190,
            365,
            509,
            439,
            640,
            294,
            486,
            452
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "656/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.71226167678833,
        "final_policy_stability": 0.9909297052154195,
        "episodes_to_convergence": 63,
        "policy_stability_history": [
            0.0,
            0.8639455782312925,
            0.9070294784580499,
            0.891156462585034,
            0.9433106575963719,
            0.8390022675736961,
            0.8888888888888888,
            0.9478458049886621,
            0.8820861678004536,
            0.8888888888888888,
            0.891156462585034,
            0.8526077097505669,
            0.8798185941043084,
            0.9591836734693877,
            0.8888888888888888,
            0.8435374149659864,
            0.8820861678004536,
            0.9229024943310657,
            0.9342403628117913,
            0.9591836734693877,
            0.9614512471655329,
            0.9070294784580499,
            0.8843537414965986,
            0.8934240362811792,
            0.8934240362811792,
            0.9614512471655329,
            0.9727891156462585,
            0.963718820861678,
            0.9002267573696145,
            0.9591836734693877,
            0.8412698412698413,
            0.9727891156462585,
            0.8866213151927438,
            0.9342403628117913,
            0.873015873015873,
            0.9705215419501134,
            0.9569160997732427,
            0.963718820861678,
            0.9841269841269841,
            0.9591836734693877,
            0.8866213151927438,
            0.9886621315192744,
            0.9886621315192744,
            0.9886621315192744,
            0.9773242630385488,
            0.9773242630385488,
            0.9909297052154195,
            0.9773242630385488,
            0.9931972789115646,
            1.0,
            0.8956916099773242,
            0.8390022675736961,
            0.873015873015873,
            0.9977324263038548,
            0.9863945578231292,
            0.9909297052154195,
            0.9931972789115646,
            0.8775510204081632,
            0.8820861678004536,
            0.9954648526077098,
            0.9863945578231292,
            0.9954648526077098,
            1.0,
            0.9909297052154195
        ],
        "reward_history": [
            -3020,
            -2894,
            -1868,
            -4410,
            -1050,
            -4410,
            -4410,
            -1122,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -466,
            -4410,
            -4410,
            -3791,
            -1282,
            -1312,
            -749,
            -1058,
            -1557,
            -4410,
            -4410,
            -4410,
            -1182,
            -589,
            -649,
            -2912,
            -837,
            -4410,
            -751,
            -3542,
            -1832,
            -3684,
            -340,
            -774,
            -1075,
            -479,
            -1101,
            -4410,
            -397,
            -260,
            -445,
            -730,
            -533,
            -252,
            -566,
            -240,
            -527,
            -3174,
            -3289,
            -4119,
            -546,
            -630,
            -298,
            -203,
            -4410,
            -4410,
            -215,
            -566,
            -519,
            -108,
            -233
        ],
        "steps_history": [
            3121,
            2995,
            1969,
            4410,
            1151,
            4410,
            4410,
            1223,
            4410,
            4410,
            4410,
            4410,
            4410,
            567,
            4410,
            4410,
            3892,
            1383,
            1413,
            850,
            1159,
            1658,
            4410,
            4410,
            4410,
            1283,
            690,
            750,
            3013,
            938,
            4410,
            852,
            3643,
            1933,
            3785,
            441,
            875,
            1176,
            580,
            1202,
            4410,
            498,
            361,
            546,
            831,
            634,
            353,
            667,
            341,
            628,
            3275,
            3390,
            4220,
            647,
            731,
            399,
            304,
            4410,
            4410,
            316,
            667,
            620,
            209,
            334
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "657/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.909041404724121,
        "final_policy_stability": 0.9863945578231292,
        "episodes_to_convergence": 51,
        "policy_stability_history": [
            0.0,
            0.7981859410430839,
            0.8503401360544217,
            0.8571428571428571,
            0.9591836734693877,
            0.9183673469387755,
            0.9002267573696145,
            0.8616780045351474,
            0.8276643990929705,
            0.9229024943310657,
            0.8956916099773242,
            0.8231292517006803,
            0.927437641723356,
            0.9047619047619048,
            0.9478458049886621,
            0.9297052154195011,
            0.9523809523809523,
            0.9478458049886621,
            0.782312925170068,
            0.981859410430839,
            0.8843537414965986,
            0.9659863945578231,
            0.9705215419501134,
            0.9682539682539683,
            0.981859410430839,
            0.8798185941043084,
            0.8526077097505669,
            0.8639455782312925,
            0.8412698412698413,
            0.9342403628117913,
            0.9229024943310657,
            0.963718820861678,
            0.9070294784580499,
            0.9727891156462585,
            0.8662131519274376,
            0.9251700680272109,
            0.963718820861678,
            0.9795918367346939,
            0.9931972789115646,
            0.9795918367346939,
            0.9501133786848073,
            0.9160997732426304,
            0.8843537414965986,
            0.9750566893424036,
            0.9750566893424036,
            0.9977324263038548,
            0.9024943310657596,
            0.9954648526077098,
            0.9614512471655329,
            1.0,
            0.9886621315192744,
            0.9863945578231292
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -811,
            -1294,
            -4410,
            -4410,
            -4410,
            -896,
            -4410,
            -4410,
            -1204,
            -1393,
            -611,
            -882,
            -538,
            -560,
            -4410,
            -442,
            -4410,
            -438,
            -465,
            -960,
            -458,
            -4410,
            -4410,
            -4110,
            -3809,
            -907,
            -1336,
            -1113,
            -3208,
            -279,
            -4410,
            -1214,
            -436,
            -228,
            -103,
            -330,
            -1177,
            -4410,
            -3286,
            -539,
            -782,
            -198,
            -4410,
            -345,
            -1222,
            -318,
            -475,
            -309
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            912,
            1395,
            4410,
            4410,
            4410,
            997,
            4410,
            4410,
            1305,
            1494,
            712,
            983,
            639,
            661,
            4410,
            543,
            4410,
            539,
            566,
            1061,
            559,
            4410,
            4410,
            4211,
            3910,
            1008,
            1437,
            1214,
            3309,
            380,
            4410,
            1315,
            537,
            329,
            204,
            431,
            1278,
            4410,
            3387,
            640,
            883,
            299,
            4410,
            446,
            1323,
            419,
            576,
            410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "658/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.656991243362427,
        "final_policy_stability": 0.9954648526077098,
        "episodes_to_convergence": 52,
        "policy_stability_history": [
            0.0,
            0.8684807256235828,
            0.8820861678004536,
            0.8458049886621315,
            0.8321995464852607,
            0.9478458049886621,
            0.909297052154195,
            0.9160997732426304,
            0.891156462585034,
            0.891156462585034,
            0.891156462585034,
            0.927437641723356,
            0.909297052154195,
            0.9478458049886621,
            0.8775510204081632,
            0.9183673469387755,
            0.9501133786848073,
            0.9047619047619048,
            0.8344671201814059,
            0.9614512471655329,
            0.8321995464852607,
            0.8888888888888888,
            0.8662131519274376,
            0.8616780045351474,
            0.8662131519274376,
            0.8866213151927438,
            0.9727891156462585,
            0.9319727891156463,
            0.9523809523809523,
            0.9410430839002267,
            0.8979591836734694,
            0.9750566893424036,
            0.9954648526077098,
            0.9002267573696145,
            0.9863945578231292,
            0.9909297052154195,
            0.8503401360544217,
            0.9954648526077098,
            0.981859410430839,
            0.9024943310657596,
            0.9931972789115646,
            0.9909297052154195,
            0.9931972789115646,
            0.9931972789115646,
            0.9863945578231292,
            0.9863945578231292,
            0.9886621315192744,
            0.9727891156462585,
            0.9433106575963719,
            1.0,
            1.0,
            0.9977324263038548,
            0.9954648526077098
        ],
        "reward_history": [
            -3920,
            -4410,
            -2733,
            -4410,
            -4410,
            -545,
            -3840,
            -1247,
            -4410,
            -4410,
            -4410,
            -1370,
            -3011,
            -1291,
            -4410,
            -4410,
            -703,
            -4410,
            -4410,
            -651,
            -4410,
            -1760,
            -4410,
            -4166,
            -4410,
            -4410,
            -303,
            -1381,
            -731,
            -1400,
            -4410,
            -576,
            -201,
            -4410,
            -431,
            -159,
            -4131,
            -304,
            -550,
            -3230,
            -407,
            -373,
            -261,
            -421,
            -583,
            -201,
            -293,
            -489,
            -1535,
            -219,
            -147,
            -378,
            -431
        ],
        "steps_history": [
            4021,
            4410,
            2834,
            4410,
            4410,
            646,
            3941,
            1348,
            4410,
            4410,
            4410,
            1471,
            3112,
            1392,
            4410,
            4410,
            804,
            4410,
            4410,
            752,
            4410,
            1861,
            4410,
            4267,
            4410,
            4410,
            404,
            1482,
            832,
            1501,
            4410,
            677,
            302,
            4410,
            532,
            260,
            4232,
            405,
            651,
            3331,
            508,
            474,
            362,
            522,
            684,
            302,
            394,
            590,
            1636,
            320,
            248,
            479,
            532
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "659/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.788868427276611,
        "final_policy_stability": 0.9841269841269841,
        "episodes_to_convergence": 52,
        "policy_stability_history": [
            0.0,
            0.8480725623582767,
            0.8140589569160998,
            0.873015873015873,
            0.9319727891156463,
            0.8526077097505669,
            0.9433106575963719,
            0.8934240362811792,
            0.8775510204081632,
            0.8662131519274376,
            0.8390022675736961,
            0.9365079365079365,
            0.8321995464852607,
            0.8571428571428571,
            0.8412698412698413,
            0.9433106575963719,
            0.9229024943310657,
            0.9727891156462585,
            0.9024943310657596,
            0.8956916099773242,
            0.8775510204081632,
            0.9433106575963719,
            0.9251700680272109,
            0.9455782312925171,
            0.9160997732426304,
            0.9206349206349206,
            0.8662131519274376,
            0.9569160997732427,
            0.9750566893424036,
            0.9773242630385488,
            0.9433106575963719,
            0.8843537414965986,
            0.8662131519274376,
            0.9682539682539683,
            0.9773242630385488,
            0.9410430839002267,
            0.8843537414965986,
            0.9954648526077098,
            0.8775510204081632,
            0.9795918367346939,
            0.9523809523809523,
            0.9795918367346939,
            0.9773242630385488,
            0.8820861678004536,
            0.9841269841269841,
            0.8752834467120182,
            0.9591836734693877,
            0.981859410430839,
            0.9954648526077098,
            0.9841269841269841,
            1.0,
            1.0,
            0.9841269841269841
        ],
        "reward_history": [
            -573,
            -4410,
            -4410,
            -4410,
            -691,
            -3659,
            -804,
            -4410,
            -4410,
            -2158,
            -4100,
            -1130,
            -4410,
            -4410,
            -4410,
            -357,
            -1224,
            -295,
            -4410,
            -4410,
            -4410,
            -568,
            -1454,
            -735,
            -1721,
            -1211,
            -4410,
            -1008,
            -376,
            -481,
            -1231,
            -4410,
            -4410,
            -387,
            -410,
            -719,
            -4060,
            -88,
            -4410,
            -415,
            -912,
            -285,
            -470,
            -4410,
            -271,
            -4410,
            -1570,
            -528,
            -182,
            -648,
            -139,
            -203,
            -737
        ],
        "steps_history": [
            674,
            4410,
            4410,
            4410,
            792,
            3760,
            905,
            4410,
            4410,
            2259,
            4201,
            1231,
            4410,
            4410,
            4410,
            458,
            1325,
            396,
            4410,
            4410,
            4410,
            669,
            1555,
            836,
            1822,
            1312,
            4410,
            1109,
            477,
            582,
            1332,
            4410,
            4410,
            488,
            511,
            820,
            4161,
            189,
            4410,
            516,
            1013,
            386,
            571,
            4410,
            372,
            4410,
            1671,
            629,
            283,
            749,
            240,
            304,
            838
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "660/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.2_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.923820972442627,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8344671201814059,
            0.7619047619047619,
            0.8934240362811792,
            0.8934240362811792,
            0.8775510204081632,
            0.8707482993197279,
            0.9569160997732427,
            0.854875283446712,
            0.9047619047619048,
            0.8662131519274376,
            0.9070294784580499,
            0.8503401360544217,
            0.9569160997732427,
            0.9659863945578231,
            0.8594104308390023,
            0.9024943310657596,
            0.8956916099773242,
            0.9183673469387755,
            0.8979591836734694,
            0.8480725623582767,
            0.9931972789115646,
            0.9002267573696145,
            0.9183673469387755,
            0.9727891156462585,
            0.9863945578231292,
            0.8843537414965986,
            0.9115646258503401,
            0.9750566893424036,
            0.9977324263038548,
            1.0,
            0.9977324263038548,
            0.9387755102040817,
            1.0
        ],
        "reward_history": [
            -874,
            -4410,
            -4410,
            -4410,
            -4410,
            -3785,
            -4410,
            -843,
            -3297,
            -4410,
            -3552,
            -2607,
            -4410,
            -767,
            -884,
            -4410,
            -4410,
            -4410,
            -2276,
            -4410,
            -4410,
            -482,
            -3512,
            -3365,
            -463,
            -1347,
            -4410,
            -4410,
            -611,
            -882,
            -1813,
            -583,
            -2198,
            -471
        ],
        "steps_history": [
            975,
            4410,
            4410,
            4410,
            4410,
            3886,
            4410,
            944,
            3398,
            4410,
            3653,
            2708,
            4410,
            868,
            985,
            4410,
            4410,
            4410,
            2377,
            4410,
            4410,
            583,
            3613,
            3466,
            564,
            1448,
            4410,
            4410,
            712,
            983,
            1914,
            684,
            2299,
            572
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "661/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.817262649536133,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.927437641723356,
            0.8344671201814059,
            0.873015873015873,
            0.8458049886621315,
            0.8276643990929705,
            0.9433106575963719,
            0.8616780045351474,
            0.8956916099773242,
            0.8662131519274376,
            0.9070294784580499,
            0.9115646258503401,
            0.9455782312925171,
            0.8321995464852607,
            0.8435374149659864,
            0.9410430839002267,
            0.963718820861678,
            0.9002267573696145,
            0.9546485260770975,
            0.9047619047619048,
            0.8798185941043084,
            0.9501133786848073,
            0.9705215419501134,
            0.8866213151927438,
            0.9931972789115646,
            0.927437641723356,
            0.9773242630385488,
            0.9931972789115646,
            0.9863945578231292,
            0.981859410430839,
            0.9614512471655329,
            0.9886621315192744,
            1.0,
            1.0
        ],
        "reward_history": [
            -4410,
            -816,
            -4410,
            -4410,
            -4410,
            -4410,
            -936,
            -3101,
            -4410,
            -4410,
            -4410,
            -4410,
            -842,
            -4410,
            -4410,
            -1180,
            -656,
            -4410,
            -1400,
            -4410,
            -4410,
            -1536,
            -2008,
            -4410,
            -316,
            -3925,
            -921,
            -392,
            -337,
            -1090,
            -1779,
            -1005,
            -1366,
            -602
        ],
        "steps_history": [
            4410,
            917,
            4410,
            4410,
            4410,
            4410,
            1037,
            3202,
            4410,
            4410,
            4410,
            4410,
            943,
            4410,
            4410,
            1281,
            757,
            4410,
            1501,
            4410,
            4410,
            1637,
            2109,
            4410,
            417,
            4026,
            1022,
            493,
            438,
            1191,
            1880,
            1106,
            1467,
            703
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "662/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.9581217765808105,
        "final_policy_stability": 0.9387755102040817,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8888888888888888,
            0.891156462585034,
            0.8140589569160998,
            0.9433106575963719,
            0.8662131519274376,
            0.8934240362811792,
            0.9024943310657596,
            0.9047619047619048,
            0.9206349206349206,
            0.9229024943310657,
            0.9002267573696145,
            0.891156462585034,
            0.9183673469387755,
            0.9455782312925171,
            0.9433106575963719,
            0.8820861678004536,
            0.927437641723356,
            0.9047619047619048,
            0.9478458049886621,
            0.9501133786848073,
            0.9160997732426304,
            0.8276643990929705,
            0.9070294784580499,
            0.9591836734693877,
            0.8344671201814059,
            0.8820861678004536,
            0.8979591836734694,
            0.9569160997732427,
            0.9659863945578231,
            0.9954648526077098,
            0.981859410430839,
            0.9387755102040817
        ],
        "reward_history": [
            -1031,
            -4410,
            -4410,
            -4410,
            -868,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -3665,
            -1604,
            -1224,
            -987,
            -4410,
            -2187,
            -2348,
            -2480,
            -2026,
            -4410,
            -4410,
            -2807,
            -572,
            -4410,
            -4410,
            -2881,
            -920,
            -1687,
            -1218,
            -1552,
            -2246
        ],
        "steps_history": [
            1132,
            4410,
            4410,
            4410,
            969,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            3766,
            1705,
            1325,
            1088,
            4410,
            2288,
            2449,
            2581,
            2127,
            4410,
            4410,
            2908,
            673,
            4410,
            4410,
            2982,
            1021,
            1788,
            1319,
            1653,
            2347
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "663/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.7152321338653564,
        "final_policy_stability": 0.9002267573696145,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8707482993197279,
            0.9206349206349206,
            0.8503401360544217,
            0.9433106575963719,
            0.8956916099773242,
            0.8662131519274376,
            0.891156462585034,
            0.9455782312925171,
            0.854875283446712,
            0.8956916099773242,
            0.8956916099773242,
            0.8684807256235828,
            0.9614512471655329,
            0.8707482993197279,
            0.9569160997732427,
            0.9750566893424036,
            0.9727891156462585,
            0.9410430839002267,
            0.9727891156462585,
            0.9410430839002267,
            0.9024943310657596,
            0.9659863945578231,
            0.8253968253968254,
            0.9727891156462585,
            0.9705215419501134,
            0.9886621315192744,
            0.9002267573696145,
            0.8866213151927438,
            0.9886621315192744,
            0.9931972789115646,
            0.9863945578231292,
            0.9024943310657596,
            0.9501133786848073,
            0.9183673469387755,
            0.9002267573696145,
            0.9886621315192744,
            0.9002267573696145
        ],
        "reward_history": [
            -4410,
            -4410,
            -1327,
            -4410,
            -1030,
            -3799,
            -4410,
            -4410,
            -1041,
            -4410,
            -4410,
            -4410,
            -4410,
            -691,
            -4410,
            -645,
            -431,
            -313,
            -883,
            -551,
            -1004,
            -4410,
            -637,
            -4410,
            -971,
            -1609,
            -505,
            -4410,
            -2772,
            -723,
            -434,
            -1382,
            -4410,
            -1457,
            -3460,
            -3564,
            -418,
            -4410
        ],
        "steps_history": [
            4410,
            4410,
            1428,
            4410,
            1131,
            3900,
            4410,
            4410,
            1142,
            4410,
            4410,
            4410,
            4410,
            792,
            4410,
            746,
            532,
            414,
            984,
            652,
            1105,
            4410,
            738,
            4410,
            1072,
            1710,
            606,
            4410,
            2873,
            824,
            535,
            1483,
            4410,
            1558,
            3561,
            3665,
            519,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "664/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.589356422424316,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8231292517006803,
            0.8820861678004536,
            0.891156462585034,
            0.8684807256235828,
            0.8367346938775511,
            0.9024943310657596,
            0.873015873015873,
            0.9591836734693877,
            0.8571428571428571,
            0.963718820861678,
            0.8594104308390023,
            0.8956916099773242,
            0.8956916099773242,
            0.9433106575963719,
            0.8979591836734694,
            0.8956916099773242,
            0.8956916099773242,
            0.9297052154195011,
            0.9682539682539683,
            0.9591836734693877,
            0.9773242630385488,
            0.9387755102040817,
            0.9863945578231292,
            0.9841269841269841,
            0.9546485260770975,
            0.9705215419501134,
            0.9410430839002267,
            0.9229024943310657,
            0.9909297052154195,
            0.9863945578231292,
            0.9931972789115646,
            0.9886621315192744,
            0.9863945578231292,
            1.0,
            0.9886621315192744,
            0.8934240362811792,
            0.9115646258503401,
            0.9931972789115646,
            0.8979591836734694,
            1.0
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -3998,
            -4410,
            -4410,
            -3187,
            -607,
            -3403,
            -671,
            -4410,
            -4410,
            -4410,
            -1320,
            -4410,
            -4410,
            -4410,
            -1478,
            -704,
            -1328,
            -420,
            -1589,
            -980,
            -527,
            -641,
            -482,
            -1685,
            -2687,
            -1026,
            -471,
            -860,
            -421,
            -362,
            -214,
            -703,
            -4410,
            -2876,
            -313,
            -4410,
            -309
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            4099,
            4410,
            4410,
            3288,
            708,
            3504,
            772,
            4410,
            4410,
            4410,
            1421,
            4410,
            4410,
            4410,
            1579,
            805,
            1429,
            521,
            1690,
            1081,
            628,
            742,
            583,
            1786,
            2788,
            1127,
            572,
            961,
            522,
            463,
            315,
            804,
            4410,
            2977,
            414,
            4410,
            410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "665/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.042287588119507,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 48,
        "policy_stability_history": [
            0.0,
            0.8775510204081632,
            0.8662131519274376,
            0.8979591836734694,
            0.8344671201814059,
            0.8503401360544217,
            0.8594104308390023,
            0.8571428571428571,
            0.8775510204081632,
            0.8775510204081632,
            0.8321995464852607,
            0.8956916099773242,
            0.8707482993197279,
            0.9455782312925171,
            0.963718820861678,
            0.9591836734693877,
            0.9410430839002267,
            0.9047619047619048,
            0.9705215419501134,
            0.8775510204081632,
            0.8956916099773242,
            0.9183673469387755,
            0.9433106575963719,
            0.8820861678004536,
            0.9773242630385488,
            0.9160997732426304,
            0.9841269841269841,
            0.9841269841269841,
            0.9795918367346939,
            0.9591836734693877,
            0.9750566893424036,
            0.981859410430839,
            0.8956916099773242,
            0.9047619047619048,
            0.9863945578231292,
            0.9773242630385488,
            0.9886621315192744,
            0.9886621315192744,
            0.9863945578231292,
            0.9750566893424036,
            0.9909297052154195,
            0.9909297052154195,
            1.0,
            0.981859410430839,
            0.9886621315192744,
            0.9954648526077098,
            0.9909297052154195,
            0.9977324263038548,
            1.0
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -1488,
            -4410,
            -1450,
            -486,
            -889,
            -1157,
            -4410,
            -758,
            -4410,
            -2262,
            -2054,
            -2279,
            -4410,
            -435,
            -1905,
            -400,
            -1006,
            -591,
            -455,
            -400,
            -288,
            -4410,
            -3808,
            -405,
            -944,
            -216,
            -544,
            -868,
            -698,
            -906,
            -580,
            -180,
            -739,
            -348,
            -687,
            -563,
            -461,
            -93
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            1589,
            4410,
            1551,
            587,
            990,
            1258,
            4410,
            859,
            4410,
            2363,
            2155,
            2380,
            4410,
            536,
            2006,
            501,
            1107,
            692,
            556,
            501,
            389,
            4410,
            3909,
            506,
            1045,
            317,
            645,
            969,
            799,
            1007,
            681,
            281,
            840,
            449,
            788,
            664,
            562,
            194
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "666/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.677887439727783,
        "final_policy_stability": 0.981859410430839,
        "episodes_to_convergence": 42,
        "policy_stability_history": [
            0.0,
            0.854875283446712,
            0.8480725623582767,
            0.8412698412698413,
            0.873015873015873,
            0.9229024943310657,
            0.9115646258503401,
            0.9070294784580499,
            0.8956916099773242,
            0.909297052154195,
            0.9138321995464853,
            0.873015873015873,
            0.8412698412698413,
            0.8775510204081632,
            0.8526077097505669,
            0.891156462585034,
            0.9410430839002267,
            0.9795918367346939,
            0.9478458049886621,
            0.8639455782312925,
            0.9886621315192744,
            0.9138321995464853,
            0.8979591836734694,
            0.8321995464852607,
            0.8820861678004536,
            0.9206349206349206,
            0.9863945578231292,
            0.9501133786848073,
            0.9863945578231292,
            0.9024943310657596,
            0.9319727891156463,
            0.9727891156462585,
            0.8956916099773242,
            0.9727891156462585,
            0.981859410430839,
            0.9977324263038548,
            0.9841269841269841,
            0.9863945578231292,
            0.9863945578231292,
            0.9909297052154195,
            0.9863945578231292,
            0.9977324263038548,
            0.981859410430839
        ],
        "reward_history": [
            -3375,
            -2038,
            -4410,
            -3773,
            -4410,
            -760,
            -2129,
            -4410,
            -2876,
            -4410,
            -4410,
            -4410,
            -4410,
            -4069,
            -3648,
            -4410,
            -1362,
            -275,
            -733,
            -4410,
            -320,
            -4410,
            -4410,
            -4410,
            -4410,
            -1958,
            -385,
            -1248,
            -759,
            -3312,
            -1473,
            -656,
            -4410,
            -712,
            -977,
            -762,
            -373,
            -642,
            -734,
            -300,
            -473,
            -398,
            -706
        ],
        "steps_history": [
            3476,
            2139,
            4410,
            3874,
            4410,
            861,
            2230,
            4410,
            2977,
            4410,
            4410,
            4410,
            4410,
            4170,
            3749,
            4410,
            1463,
            376,
            834,
            4410,
            421,
            4410,
            4410,
            4410,
            4410,
            2059,
            486,
            1349,
            860,
            3413,
            1574,
            757,
            4410,
            813,
            1078,
            863,
            474,
            743,
            835,
            401,
            574,
            499,
            807
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "667/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.229438781738281,
        "final_policy_stability": 0.9410430839002267,
        "episodes_to_convergence": 46,
        "policy_stability_history": [
            0.0,
            0.8526077097505669,
            0.8571428571428571,
            0.8888888888888888,
            0.8571428571428571,
            0.8707482993197279,
            0.8594104308390023,
            0.9115646258503401,
            0.9115646258503401,
            0.8866213151927438,
            0.8888888888888888,
            0.8435374149659864,
            0.9478458049886621,
            0.873015873015873,
            0.8843537414965986,
            0.9501133786848073,
            0.9659863945578231,
            0.9569160997732427,
            0.9546485260770975,
            0.8888888888888888,
            0.9591836734693877,
            0.9773242630385488,
            0.9750566893424036,
            0.9841269841269841,
            0.8503401360544217,
            0.9183673469387755,
            0.9682539682539683,
            0.8707482993197279,
            0.9886621315192744,
            0.8979591836734694,
            0.9773242630385488,
            0.9002267573696145,
            0.9909297052154195,
            0.9954648526077098,
            0.9705215419501134,
            0.9024943310657596,
            0.9886621315192744,
            0.9931972789115646,
            0.9297052154195011,
            0.9705215419501134,
            0.9977324263038548,
            0.9931972789115646,
            0.9977324263038548,
            0.9229024943310657,
            1.0,
            0.9954648526077098,
            0.9410430839002267
        ],
        "reward_history": [
            -2988,
            -4410,
            -4410,
            -4410,
            -3403,
            -3925,
            -4410,
            -1382,
            -2084,
            -4410,
            -4410,
            -4410,
            -971,
            -4410,
            -4410,
            -900,
            -337,
            -508,
            -864,
            -4410,
            -794,
            -351,
            -522,
            -315,
            -4410,
            -1722,
            -960,
            -4410,
            -459,
            -4410,
            -1055,
            -3512,
            -342,
            -362,
            -469,
            -4410,
            -350,
            -773,
            -1372,
            -770,
            -762,
            -1436,
            -287,
            -2237,
            -151,
            -553,
            -2074
        ],
        "steps_history": [
            3089,
            4410,
            4410,
            4410,
            3504,
            4026,
            4410,
            1483,
            2185,
            4410,
            4410,
            4410,
            1072,
            4410,
            4410,
            1001,
            438,
            609,
            965,
            4410,
            895,
            452,
            623,
            416,
            4410,
            1823,
            1061,
            4410,
            560,
            4410,
            1156,
            3613,
            443,
            463,
            570,
            4410,
            451,
            874,
            1473,
            871,
            863,
            1537,
            388,
            2338,
            252,
            654,
            2175
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "668/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.648751258850098,
        "final_policy_stability": 0.9138321995464853,
        "episodes_to_convergence": 43,
        "policy_stability_history": [
            0.0,
            0.9206349206349206,
            0.8140589569160998,
            0.8571428571428571,
            0.9297052154195011,
            0.8979591836734694,
            0.9251700680272109,
            0.8594104308390023,
            0.9342403628117913,
            0.8684807256235828,
            0.9410430839002267,
            0.8843537414965986,
            0.9455782312925171,
            0.9160997732426304,
            0.8820861678004536,
            0.9024943310657596,
            0.8934240362811792,
            0.9727891156462585,
            0.963718820861678,
            0.9070294784580499,
            0.9546485260770975,
            0.9750566893424036,
            0.9251700680272109,
            0.9297052154195011,
            0.9523809523809523,
            0.9682539682539683,
            0.9478458049886621,
            0.9591836734693877,
            0.9863945578231292,
            0.9841269841269841,
            0.8979591836734694,
            0.9886621315192744,
            0.9954648526077098,
            0.8843537414965986,
            0.9002267573696145,
            0.9863945578231292,
            0.9410430839002267,
            0.9931972789115646,
            0.9863945578231292,
            0.9954648526077098,
            1.0,
            0.9931972789115646,
            0.9841269841269841,
            0.9138321995464853
        ],
        "reward_history": [
            -3800,
            -1486,
            -4410,
            -4410,
            -4410,
            -4410,
            -1156,
            -4410,
            -927,
            -4410,
            -897,
            -3427,
            -1944,
            -2602,
            -4160,
            -4410,
            -4410,
            -370,
            -680,
            -4410,
            -760,
            -517,
            -1079,
            -1442,
            -910,
            -975,
            -1451,
            -1334,
            -314,
            -234,
            -4410,
            -911,
            -95,
            -4410,
            -4410,
            -362,
            -2216,
            -443,
            -650,
            -226,
            -119,
            -764,
            -1147,
            -3544
        ],
        "steps_history": [
            3901,
            1587,
            4410,
            4410,
            4410,
            4410,
            1257,
            4410,
            1028,
            4410,
            998,
            3528,
            2045,
            2703,
            4261,
            4410,
            4410,
            471,
            781,
            4410,
            861,
            618,
            1180,
            1543,
            1011,
            1076,
            1552,
            1435,
            415,
            335,
            4410,
            1012,
            196,
            4410,
            4410,
            463,
            2317,
            544,
            751,
            327,
            220,
            865,
            1248,
            3645
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "669/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.682082653045654,
        "final_policy_stability": 0.8866213151927438,
        "episodes_to_convergence": 39,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.9002267573696145,
            0.8775510204081632,
            0.8594104308390023,
            0.8684807256235828,
            0.9387755102040817,
            0.8752834467120182,
            0.8140589569160998,
            0.9433106575963719,
            0.8798185941043084,
            0.8843537414965986,
            0.9206349206349206,
            0.8798185941043084,
            0.8956916099773242,
            0.927437641723356,
            0.9138321995464853,
            0.9614512471655329,
            0.981859410430839,
            0.8435374149659864,
            0.8934240362811792,
            0.8843537414965986,
            0.9070294784580499,
            0.9682539682539683,
            0.9319727891156463,
            0.963718820861678,
            0.9727891156462585,
            0.9773242630385488,
            0.9931972789115646,
            0.9931972789115646,
            0.9931972789115646,
            0.9047619047619048,
            0.9931972789115646,
            0.9795918367346939,
            0.9795918367346939,
            0.9115646258503401,
            0.9795918367346939,
            0.8934240362811792,
            0.9795918367346939,
            0.8866213151927438
        ],
        "reward_history": [
            -4410,
            -3124,
            -4410,
            -4410,
            -2159,
            -4410,
            -1115,
            -4410,
            -4410,
            -1424,
            -3345,
            -2553,
            -920,
            -4410,
            -4410,
            -1407,
            -2239,
            -437,
            -308,
            -4410,
            -4410,
            -4083,
            -2096,
            -406,
            -1824,
            -1220,
            -395,
            -720,
            -482,
            -394,
            -306,
            -4410,
            -459,
            -514,
            -693,
            -4161,
            -1368,
            -3928,
            -1036,
            -4410
        ],
        "steps_history": [
            4410,
            3225,
            4410,
            4410,
            2260,
            4410,
            1216,
            4410,
            4410,
            1525,
            3446,
            2654,
            1021,
            4410,
            4410,
            1508,
            2340,
            538,
            409,
            4410,
            4410,
            4184,
            2197,
            507,
            1925,
            1321,
            496,
            821,
            583,
            495,
            407,
            4410,
            560,
            615,
            794,
            4262,
            1469,
            4029,
            1137,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "670/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.715348720550537,
        "final_policy_stability": 0.9863945578231292,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8616780045351474,
            0.8299319727891157,
            0.8979591836734694,
            0.9160997732426304,
            0.8639455782312925,
            0.8843537414965986,
            0.8435374149659864,
            0.9501133786848073,
            0.8843537414965986,
            0.9342403628117913,
            0.8843537414965986,
            0.8321995464852607,
            0.9569160997732427,
            0.8458049886621315,
            0.9569160997732427,
            0.9682539682539683,
            0.8888888888888888,
            0.9251700680272109,
            0.9705215419501134,
            0.9841269841269841,
            0.9773242630385488,
            0.8888888888888888,
            0.8775510204081632,
            0.9909297052154195,
            0.9909297052154195,
            0.9727891156462585,
            0.8866213151927438,
            0.8934240362811792,
            0.9909297052154195,
            0.9954648526077098,
            0.963718820861678,
            1.0,
            0.9863945578231292
        ],
        "reward_history": [
            -4410,
            -4410,
            -3253,
            -1366,
            -4410,
            -3652,
            -2906,
            -4410,
            -337,
            -4410,
            -1112,
            -4410,
            -4242,
            -1215,
            -3271,
            -881,
            -493,
            -4410,
            -1558,
            -624,
            -142,
            -351,
            -4410,
            -4410,
            -190,
            -74,
            -558,
            -4410,
            -4410,
            -261,
            -108,
            -793,
            -182,
            -396
        ],
        "steps_history": [
            4410,
            4410,
            3354,
            1467,
            4410,
            3753,
            3007,
            4410,
            438,
            4410,
            1213,
            4410,
            4343,
            1316,
            3372,
            982,
            594,
            4410,
            1659,
            725,
            243,
            452,
            4410,
            4410,
            291,
            175,
            659,
            4410,
            4410,
            362,
            209,
            894,
            283,
            497
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "671/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.444262981414795,
        "final_policy_stability": 0.9954648526077098,
        "episodes_to_convergence": 38,
        "policy_stability_history": [
            0.0,
            0.8798185941043084,
            0.8253968253968254,
            0.8526077097505669,
            0.8616780045351474,
            0.9319727891156463,
            0.891156462585034,
            0.8390022675736961,
            0.9160997732426304,
            0.9410430839002267,
            0.8639455782312925,
            0.8526077097505669,
            0.9478458049886621,
            0.8956916099773242,
            0.9115646258503401,
            0.8594104308390023,
            0.8458049886621315,
            0.8526077097505669,
            0.9342403628117913,
            0.9750566893424036,
            0.8752834467120182,
            0.9319727891156463,
            0.9750566893424036,
            0.8616780045351474,
            0.9795918367346939,
            0.9795918367346939,
            0.8866213151927438,
            0.873015873015873,
            0.9931972789115646,
            0.9659863945578231,
            0.9863945578231292,
            0.9115646258503401,
            0.8934240362811792,
            0.9795918367346939,
            0.9727891156462585,
            0.9954648526077098,
            0.981859410430839,
            0.9977324263038548,
            0.9954648526077098
        ],
        "reward_history": [
            -3020,
            -4410,
            -4410,
            -2409,
            -4410,
            -324,
            -2462,
            -4410,
            -2231,
            -828,
            -4410,
            -4410,
            -579,
            -4410,
            -1371,
            -2488,
            -2903,
            -4410,
            -1034,
            -453,
            -4410,
            -1300,
            -329,
            -4410,
            -226,
            -271,
            -4410,
            -3525,
            -93,
            -738,
            -535,
            -4410,
            -4410,
            -536,
            -870,
            -216,
            -941,
            -243,
            -323
        ],
        "steps_history": [
            3121,
            4410,
            4410,
            2510,
            4410,
            425,
            2563,
            4410,
            2332,
            929,
            4410,
            4410,
            680,
            4410,
            1472,
            2589,
            3004,
            4410,
            1135,
            554,
            4410,
            1401,
            430,
            4410,
            327,
            372,
            4410,
            3626,
            194,
            839,
            636,
            4410,
            4410,
            637,
            971,
            317,
            1042,
            344,
            424
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "672/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.374799013137817,
        "final_policy_stability": 0.9387755102040817,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.8480725623582767,
            0.8662131519274376,
            0.8412698412698413,
            0.8775510204081632,
            0.8344671201814059,
            0.8503401360544217,
            0.8934240362811792,
            0.8208616780045351,
            0.9342403628117913,
            0.8956916099773242,
            0.927437641723356,
            0.8594104308390023,
            0.8775510204081632,
            0.8934240362811792,
            0.9569160997732427,
            0.9659863945578231,
            0.9319727891156463,
            0.8684807256235828,
            0.9750566893424036,
            0.8866213151927438,
            0.9705215419501134,
            0.963718820861678,
            0.9682539682539683,
            0.9546485260770975,
            0.8458049886621315,
            0.9659863945578231,
            0.9795918367346939,
            0.9863945578231292,
            0.854875283446712,
            0.9070294784580499,
            0.854875283446712,
            0.9886621315192744,
            1.0,
            0.891156462585034,
            0.9591836734693877,
            0.9387755102040817
        ],
        "reward_history": [
            -3172,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -2490,
            -4410,
            -4410,
            -1077,
            -1619,
            -1395,
            -4410,
            -4410,
            -1620,
            -435,
            -258,
            -885,
            -4410,
            -140,
            -4410,
            -148,
            -632,
            -424,
            -710,
            -3543,
            -318,
            -254,
            -188,
            -4410,
            -1806,
            -4410,
            -344,
            -280,
            -4410,
            -1077,
            -2009
        ],
        "steps_history": [
            3273,
            4410,
            4410,
            4410,
            4410,
            4410,
            2591,
            4410,
            4410,
            1178,
            1720,
            1496,
            4410,
            4410,
            1721,
            536,
            359,
            986,
            4410,
            241,
            4410,
            249,
            733,
            525,
            811,
            3644,
            419,
            355,
            289,
            4410,
            1907,
            4410,
            445,
            381,
            4410,
            1178,
            2110
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "673/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.507423400878906,
        "final_policy_stability": 0.9954648526077098,
        "episodes_to_convergence": 42,
        "policy_stability_history": [
            0.0,
            0.9251700680272109,
            0.800453514739229,
            0.8639455782312925,
            0.8798185941043084,
            0.8775510204081632,
            0.8934240362811792,
            0.9138321995464853,
            0.927437641723356,
            0.8662131519274376,
            0.854875283446712,
            0.9115646258503401,
            0.9160997732426304,
            0.927437641723356,
            0.854875283446712,
            0.9387755102040817,
            0.9546485260770975,
            0.927437641723356,
            0.9455782312925171,
            0.873015873015873,
            0.9841269841269841,
            0.9410430839002267,
            0.9002267573696145,
            0.9478458049886621,
            0.9297052154195011,
            0.9750566893424036,
            0.8843537414965986,
            0.963718820861678,
            0.8934240362811792,
            0.9297052154195011,
            0.873015873015873,
            0.9909297052154195,
            0.9773242630385488,
            0.9886621315192744,
            0.9002267573696145,
            0.9002267573696145,
            0.8843537414965986,
            0.9886621315192744,
            0.9523809523809523,
            1.0,
            0.9909297052154195,
            0.9841269841269841,
            0.9954648526077098
        ],
        "reward_history": [
            -4410,
            -1036,
            -4410,
            -4410,
            -4410,
            -4410,
            -2244,
            -2128,
            -975,
            -4410,
            -4410,
            -1557,
            -1615,
            -1065,
            -4410,
            -1101,
            -189,
            -1349,
            -832,
            -4410,
            -173,
            -896,
            -4410,
            -1386,
            -1265,
            -318,
            -4410,
            -787,
            -4410,
            -953,
            -3741,
            -223,
            -497,
            -234,
            -4410,
            -2941,
            -4410,
            -739,
            -1137,
            -304,
            -516,
            -875,
            -618
        ],
        "steps_history": [
            4410,
            1137,
            4410,
            4410,
            4410,
            4410,
            2345,
            2229,
            1076,
            4410,
            4410,
            1658,
            1716,
            1166,
            4410,
            1202,
            290,
            1450,
            933,
            4410,
            274,
            997,
            4410,
            1487,
            1366,
            419,
            4410,
            888,
            4410,
            1054,
            3842,
            324,
            598,
            335,
            4410,
            3042,
            4410,
            840,
            1238,
            405,
            617,
            976,
            719
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "674/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.32895803451538,
        "final_policy_stability": 0.9931972789115646,
        "episodes_to_convergence": 44,
        "policy_stability_history": [
            0.0,
            0.8594104308390023,
            0.8526077097505669,
            0.8049886621315193,
            0.8526077097505669,
            0.8866213151927438,
            0.8866213151927438,
            0.8866213151927438,
            0.9206349206349206,
            0.8843537414965986,
            0.8798185941043084,
            0.9682539682539683,
            0.927437641723356,
            0.9070294784580499,
            0.891156462585034,
            0.8503401360544217,
            0.8820861678004536,
            0.9659863945578231,
            0.8684807256235828,
            0.9365079365079365,
            0.9183673469387755,
            0.8231292517006803,
            0.9501133786848073,
            0.9478458049886621,
            0.9795918367346939,
            0.9365079365079365,
            0.9795918367346939,
            0.9682539682539683,
            0.963718820861678,
            0.9727891156462585,
            0.9863945578231292,
            0.9773242630385488,
            0.9909297052154195,
            0.9024943310657596,
            0.9795918367346939,
            0.9319727891156463,
            0.9977324263038548,
            0.9909297052154195,
            0.8843537414965986,
            0.9546485260770975,
            0.963718820861678,
            1.0,
            1.0,
            1.0,
            0.9931972789115646
        ],
        "reward_history": [
            -573,
            -4410,
            -2148,
            -4410,
            -4410,
            -2258,
            -2422,
            -4410,
            -1111,
            -4410,
            -2088,
            -320,
            -1977,
            -4410,
            -2602,
            -4410,
            -4410,
            -390,
            -4410,
            -492,
            -4410,
            -4410,
            -774,
            -1462,
            -306,
            -1165,
            -695,
            -874,
            -615,
            -487,
            -282,
            -162,
            -401,
            -4410,
            -400,
            -2488,
            -232,
            -307,
            -4410,
            -1004,
            -901,
            -235,
            -294,
            -71,
            -449
        ],
        "steps_history": [
            674,
            4410,
            2249,
            4410,
            4410,
            2359,
            2523,
            4410,
            1212,
            4410,
            2189,
            421,
            2078,
            4410,
            2703,
            4410,
            4410,
            491,
            4410,
            593,
            4410,
            4410,
            875,
            1563,
            407,
            1266,
            796,
            975,
            716,
            588,
            383,
            263,
            502,
            4410,
            501,
            2589,
            333,
            408,
            4410,
            1105,
            1002,
            336,
            395,
            172,
            550
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "675/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.910562753677368,
        "final_policy_stability": 0.9841269841269841,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.8344671201814059,
            0.7619047619047619,
            0.8934240362811792,
            0.8934240362811792,
            0.8639455782312925,
            0.9002267573696145,
            0.891156462585034,
            0.8866213151927438,
            0.891156462585034,
            0.8458049886621315,
            0.9342403628117913,
            0.9047619047619048,
            0.9682539682539683,
            0.9909297052154195,
            0.8662131519274376,
            0.9229024943310657,
            0.9070294784580499,
            0.9659863945578231,
            0.8684807256235828,
            0.9002267573696145,
            0.9070294784580499,
            0.9863945578231292,
            0.9863945578231292,
            0.9319727891156463,
            0.8979591836734694,
            0.963718820861678,
            0.927437641723356,
            0.9954648526077098,
            0.9954648526077098,
            0.9886621315192744,
            0.9115646258503401,
            0.9954648526077098,
            1.0,
            0.9682539682539683,
            0.9841269841269841
        ],
        "reward_history": [
            -874,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -3082,
            -4410,
            -4410,
            -1536,
            -4188,
            -856,
            -165,
            -3698,
            -4410,
            -4410,
            -971,
            -4410,
            -4410,
            -4410,
            -941,
            -539,
            -2687,
            -4410,
            -1484,
            -2777,
            -541,
            -899,
            -1813,
            -4410,
            -510,
            -250,
            -2482,
            -661
        ],
        "steps_history": [
            975,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            3183,
            4410,
            4410,
            1637,
            4289,
            957,
            266,
            3799,
            4410,
            4410,
            1072,
            4410,
            4410,
            4410,
            1042,
            640,
            2788,
            4410,
            1585,
            2878,
            642,
            1000,
            1914,
            4410,
            611,
            351,
            2583,
            762
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "676/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.74623417854309,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 42,
        "policy_stability_history": [
            0.0,
            0.927437641723356,
            0.8344671201814059,
            0.8820861678004536,
            0.9070294784580499,
            0.9319727891156463,
            0.8662131519274376,
            0.8866213151927438,
            0.9501133786848073,
            0.8639455782312925,
            0.8480725623582767,
            0.9047619047619048,
            0.9183673469387755,
            0.8934240362811792,
            0.9319727891156463,
            0.8798185941043084,
            0.981859410430839,
            0.873015873015873,
            0.8934240362811792,
            0.9841269841269841,
            0.9047619047619048,
            0.9523809523809523,
            0.909297052154195,
            0.9863945578231292,
            0.9841269841269841,
            0.9705215419501134,
            0.9614512471655329,
            0.909297052154195,
            0.9795918367346939,
            0.8888888888888888,
            0.9433106575963719,
            0.854875283446712,
            0.9569160997732427,
            0.9886621315192744,
            0.9795918367346939,
            0.927437641723356,
            0.9138321995464853,
            0.9954648526077098,
            0.9977324263038548,
            1.0,
            0.9160997732426304,
            0.9954648526077098,
            1.0
        ],
        "reward_history": [
            -4410,
            -816,
            -4410,
            -4410,
            -1559,
            -1781,
            -4410,
            -4410,
            -994,
            -3966,
            -4410,
            -4410,
            -942,
            -2683,
            -1066,
            -3658,
            -665,
            -4410,
            -4410,
            -649,
            -4410,
            -1357,
            -4410,
            -587,
            -447,
            -895,
            -1640,
            -3908,
            -470,
            -4410,
            -1736,
            -4242,
            -1005,
            -1366,
            -602,
            -4410,
            -4095,
            -895,
            -338,
            -787,
            -3923,
            -629,
            -449
        ],
        "steps_history": [
            4410,
            917,
            4410,
            4410,
            1660,
            1882,
            4410,
            4410,
            1095,
            4067,
            4410,
            4410,
            1043,
            2784,
            1167,
            3759,
            766,
            4410,
            4410,
            750,
            4410,
            1458,
            4410,
            688,
            548,
            996,
            1741,
            4009,
            571,
            4410,
            1837,
            4343,
            1106,
            1467,
            703,
            4410,
            4196,
            996,
            439,
            888,
            4024,
            730,
            550
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "677/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.625218152999878,
        "final_policy_stability": 0.9886621315192744,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.8888888888888888,
            0.891156462585034,
            0.8140589569160998,
            0.9433106575963719,
            0.8662131519274376,
            0.891156462585034,
            0.9024943310657596,
            0.9047619047619048,
            0.9160997732426304,
            0.9206349206349206,
            0.9024943310657596,
            0.8820861678004536,
            0.9614512471655329,
            0.8820861678004536,
            0.9773242630385488,
            0.963718820861678,
            0.9455782312925171,
            0.9070294784580499,
            0.9047619047619048,
            0.9365079365079365,
            0.8775510204081632,
            0.9523809523809523,
            0.9841269841269841,
            0.9365079365079365,
            0.9705215419501134,
            0.9795918367346939,
            0.9727891156462585,
            0.9773242630385488,
            0.9614512471655329,
            0.8979591836734694,
            0.981859410430839,
            0.9478458049886621,
            0.9931972789115646,
            0.9886621315192744,
            0.909297052154195,
            0.9886621315192744
        ],
        "reward_history": [
            -1031,
            -4410,
            -4410,
            -4410,
            -868,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -938,
            -4410,
            -239,
            -506,
            -1115,
            -2066,
            -4410,
            -2414,
            -4121,
            -2307,
            -530,
            -1477,
            -1418,
            -480,
            -1313,
            -299,
            -1276,
            -4410,
            -523,
            -1200,
            -530,
            -761,
            -4410,
            -534
        ],
        "steps_history": [
            1132,
            4410,
            4410,
            4410,
            969,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            1039,
            4410,
            340,
            607,
            1216,
            2167,
            4410,
            2515,
            4222,
            2408,
            631,
            1578,
            1519,
            581,
            1414,
            400,
            1377,
            4410,
            624,
            1301,
            631,
            862,
            4410,
            635
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "678/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.7076945304870605,
        "final_policy_stability": 0.9410430839002267,
        "episodes_to_convergence": 39,
        "policy_stability_history": [
            0.0,
            0.8707482993197279,
            0.9206349206349206,
            0.8503401360544217,
            0.9478458049886621,
            0.8662131519274376,
            0.8616780045351474,
            0.873015873015873,
            0.854875283446712,
            0.9523809523809523,
            0.8503401360544217,
            0.8934240362811792,
            0.9365079365079365,
            0.9478458049886621,
            0.9410430839002267,
            0.9410430839002267,
            0.9591836734693877,
            0.963718820861678,
            0.9024943310657596,
            0.9024943310657596,
            0.8707482993197279,
            0.8662131519274376,
            0.9410430839002267,
            0.9795918367346939,
            0.8616780045351474,
            0.963718820861678,
            0.9659863945578231,
            0.981859410430839,
            0.8820861678004536,
            0.9387755102040817,
            0.9002267573696145,
            0.9841269841269841,
            0.9614512471655329,
            0.9909297052154195,
            0.9909297052154195,
            0.9342403628117913,
            0.9954648526077098,
            0.9886621315192744,
            1.0,
            0.9410430839002267
        ],
        "reward_history": [
            -4410,
            -4410,
            -1327,
            -4410,
            -1057,
            -4410,
            -4410,
            -4410,
            -4410,
            -676,
            -4152,
            -4410,
            -2446,
            -931,
            -956,
            -1519,
            -1288,
            -596,
            -4410,
            -4410,
            -4410,
            -4410,
            -1791,
            -459,
            -4090,
            -1249,
            -1391,
            -1533,
            -4410,
            -1085,
            -4410,
            -776,
            -2210,
            -418,
            -537,
            -2085,
            -1101,
            -622,
            -199,
            -2665
        ],
        "steps_history": [
            4410,
            4410,
            1428,
            4410,
            1158,
            4410,
            4410,
            4410,
            4410,
            777,
            4253,
            4410,
            2547,
            1032,
            1057,
            1620,
            1389,
            697,
            4410,
            4410,
            4410,
            4410,
            1892,
            560,
            4191,
            1350,
            1492,
            1634,
            4410,
            1186,
            4410,
            877,
            2311,
            519,
            638,
            2186,
            1202,
            723,
            300,
            2766
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "679/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.028862237930298,
        "final_policy_stability": 0.8820861678004536,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.8367346938775511,
            0.8639455782312925,
            0.9047619047619048,
            0.8820861678004536,
            0.854875283446712,
            0.909297052154195,
            0.9682539682539683,
            0.9342403628117913,
            0.9024943310657596,
            0.9387755102040817,
            0.8299319727891157,
            0.891156462585034,
            0.9433106575963719,
            0.8956916099773242,
            0.8798185941043084,
            0.8684807256235828,
            0.9251700680272109,
            0.8956916099773242,
            0.891156462585034,
            0.9002267573696145,
            0.9795918367346939,
            0.891156462585034,
            0.9523809523809523,
            0.9705215419501134,
            0.9931972789115646,
            0.9841269841269841,
            0.8934240362811792,
            0.9659863945578231,
            0.9433106575963719,
            1.0,
            0.8820861678004536
        ],
        "reward_history": [
            -4410,
            -2839,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -3269,
            -469,
            -2982,
            -2731,
            -1843,
            -4410,
            -4410,
            -738,
            -4410,
            -4410,
            -4410,
            -2694,
            -4410,
            -4410,
            -4410,
            -1257,
            -4410,
            -1253,
            -745,
            -624,
            -1416,
            -4410,
            -1019,
            -1688,
            -455,
            -4410
        ],
        "steps_history": [
            4410,
            2940,
            4410,
            4410,
            4410,
            4410,
            4410,
            3370,
            570,
            3083,
            2832,
            1944,
            4410,
            4410,
            839,
            4410,
            4410,
            4410,
            2795,
            4410,
            4410,
            4410,
            1358,
            4410,
            1354,
            846,
            725,
            1517,
            4410,
            1120,
            1789,
            556,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "680/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.539951801300049,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 43,
        "policy_stability_history": [
            0.0,
            0.8752834467120182,
            0.8684807256235828,
            0.8956916099773242,
            0.8276643990929705,
            0.854875283446712,
            0.9297052154195011,
            0.8843537414965986,
            0.8798185941043084,
            0.9342403628117913,
            0.9387755102040817,
            0.9297052154195011,
            0.854875283446712,
            0.9659863945578231,
            0.8707482993197279,
            0.8866213151927438,
            0.8843537414965986,
            0.9501133786848073,
            0.927437641723356,
            0.8798185941043084,
            0.9501133786848073,
            0.9433106575963719,
            0.9115646258503401,
            0.854875283446712,
            0.9659863945578231,
            0.9863945578231292,
            0.9206349206349206,
            0.8390022675736961,
            0.9727891156462585,
            0.9160997732426304,
            0.9750566893424036,
            0.9841269841269841,
            0.9569160997732427,
            0.9886621315192744,
            0.9546485260770975,
            0.9954648526077098,
            0.9886621315192744,
            0.9886621315192744,
            0.9886621315192744,
            0.9977324263038548,
            0.9727891156462585,
            0.9863945578231292,
            0.9977324263038548,
            0.9977324263038548
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -923,
            -1965,
            -4410,
            -616,
            -488,
            -784,
            -4410,
            -567,
            -4410,
            -4410,
            -4410,
            -570,
            -1682,
            -4410,
            -895,
            -659,
            -1106,
            -4410,
            -358,
            -354,
            -1743,
            -4410,
            -894,
            -4410,
            -925,
            -552,
            -983,
            -103,
            -1368,
            -320,
            -299,
            -613,
            -415,
            -201,
            -707,
            -359,
            -148,
            -243
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            1024,
            2066,
            4410,
            717,
            589,
            885,
            4410,
            668,
            4410,
            4410,
            4410,
            671,
            1783,
            4410,
            996,
            760,
            1207,
            4410,
            459,
            455,
            1844,
            4410,
            995,
            4410,
            1026,
            653,
            1084,
            204,
            1469,
            421,
            400,
            714,
            516,
            302,
            808,
            460,
            249,
            344
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "681/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.638576030731201,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 39,
        "policy_stability_history": [
            0.0,
            0.854875283446712,
            0.8526077097505669,
            0.8934240362811792,
            0.8866213151927438,
            0.9319727891156463,
            0.782312925170068,
            0.8843537414965986,
            0.8956916099773242,
            0.873015873015873,
            0.873015873015873,
            0.9591836734693877,
            0.891156462585034,
            0.8752834467120182,
            0.9251700680272109,
            0.9501133786848073,
            0.9614512471655329,
            0.9138321995464853,
            0.873015873015873,
            0.9614512471655329,
            0.9705215419501134,
            0.8888888888888888,
            0.7936507936507936,
            0.9727891156462585,
            0.9727891156462585,
            0.909297052154195,
            0.9954648526077098,
            0.8616780045351474,
            0.8888888888888888,
            0.9727891156462585,
            0.909297052154195,
            0.9659863945578231,
            0.9977324263038548,
            0.8866213151927438,
            0.9115646258503401,
            0.9909297052154195,
            0.963718820861678,
            0.9977324263038548,
            0.9727891156462585,
            0.9977324263038548
        ],
        "reward_history": [
            -3375,
            -2038,
            -4410,
            -1772,
            -4410,
            -840,
            -4410,
            -4410,
            -2247,
            -4410,
            -4410,
            -1681,
            -4410,
            -4410,
            -1531,
            -1025,
            -553,
            -1463,
            -4410,
            -824,
            -403,
            -4410,
            -4410,
            -986,
            -521,
            -2341,
            -232,
            -3608,
            -4410,
            -632,
            -4410,
            -735,
            -316,
            -4410,
            -2888,
            -333,
            -713,
            -801,
            -734,
            -301
        ],
        "steps_history": [
            3476,
            2139,
            4410,
            1873,
            4410,
            941,
            4410,
            4410,
            2348,
            4410,
            4410,
            1782,
            4410,
            4410,
            1632,
            1126,
            654,
            1564,
            4410,
            925,
            504,
            4410,
            4410,
            1087,
            622,
            2442,
            333,
            3709,
            4410,
            733,
            4410,
            836,
            417,
            4410,
            2989,
            434,
            814,
            902,
            835,
            402
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "682/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.153169870376587,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8526077097505669,
            0.8594104308390023,
            0.8888888888888888,
            0.8571428571428571,
            0.8707482993197279,
            0.8594104308390023,
            0.8820861678004536,
            0.8571428571428571,
            0.8934240362811792,
            0.8866213151927438,
            0.9002267573696145,
            0.8435374149659864,
            0.9160997732426304,
            0.9614512471655329,
            0.8934240362811792,
            0.9387755102040817,
            0.8934240362811792,
            0.9569160997732427,
            0.9591836734693877,
            0.9501133786848073,
            0.9002267573696145,
            0.9795918367346939,
            0.9750566893424036,
            0.8526077097505669,
            0.9523809523809523,
            0.8866213151927438,
            0.9047619047619048,
            0.9841269841269841,
            0.8843537414965986,
            0.9047619047619048,
            0.9024943310657596,
            0.8707482993197279,
            0.9954648526077098,
            0.9229024943310657,
            0.9727891156462585,
            0.909297052154195,
            0.9705215419501134,
            0.9909297052154195,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            -2988,
            -4410,
            -4410,
            -4410,
            -3403,
            -3925,
            -4410,
            -4410,
            -4410,
            -3050,
            -1914,
            -4410,
            -4165,
            -1066,
            -473,
            -4410,
            -1269,
            -2478,
            -804,
            -935,
            -635,
            -2772,
            -416,
            -619,
            -4410,
            -1275,
            -4410,
            -2161,
            -353,
            -4410,
            -4410,
            -4410,
            -4205,
            -287,
            -2258,
            -784,
            -3029,
            -1015,
            -818,
            -734,
            -664
        ],
        "steps_history": [
            3089,
            4410,
            4410,
            4410,
            3504,
            4026,
            4410,
            4410,
            4410,
            3151,
            2015,
            4410,
            4266,
            1167,
            574,
            4410,
            1370,
            2579,
            905,
            1036,
            736,
            2873,
            517,
            720,
            4410,
            1376,
            4410,
            2262,
            454,
            4410,
            4410,
            4410,
            4306,
            388,
            2359,
            885,
            3130,
            1116,
            919,
            835,
            765
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "683/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.311821222305298,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 43,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.81859410430839,
            0.8979591836734694,
            0.8503401360544217,
            0.9523809523809523,
            0.8798185941043084,
            0.8662131519274376,
            0.873015873015873,
            0.891156462585034,
            0.873015873015873,
            0.9297052154195011,
            0.9410430839002267,
            0.9727891156462585,
            0.9387755102040817,
            0.8934240362811792,
            0.9569160997732427,
            0.9614512471655329,
            0.9410430839002267,
            0.9795918367346939,
            0.8956916099773242,
            0.963718820861678,
            0.9297052154195011,
            0.9682539682539683,
            0.909297052154195,
            0.8616780045351474,
            0.9705215419501134,
            0.9410430839002267,
            0.9569160997732427,
            0.9863945578231292,
            0.963718820861678,
            0.8662131519274376,
            0.9705215419501134,
            0.8662131519274376,
            0.9183673469387755,
            0.9569160997732427,
            1.0,
            0.891156462585034,
            0.9977324263038548,
            0.9229024943310657,
            0.8752834467120182,
            0.9977324263038548,
            1.0,
            0.9977324263038548
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -806,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -1171,
            -941,
            -346,
            -1266,
            -4410,
            -345,
            -486,
            -960,
            -110,
            -4410,
            -710,
            -1362,
            -722,
            -1451,
            -3069,
            -530,
            -1661,
            -1368,
            -234,
            -654,
            -4410,
            -352,
            -4410,
            -4410,
            -2679,
            -443,
            -4410,
            -442,
            -4410,
            -4219,
            -451,
            -563,
            -115
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            4410,
            907,
            4410,
            4410,
            4410,
            4410,
            4410,
            1272,
            1042,
            447,
            1367,
            4410,
            446,
            587,
            1061,
            211,
            4410,
            811,
            1463,
            823,
            1552,
            3170,
            631,
            1762,
            1469,
            335,
            755,
            4410,
            453,
            4410,
            4410,
            2780,
            544,
            4410,
            543,
            4410,
            4320,
            552,
            664,
            216
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "684/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.488924741744995,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 44,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.9002267573696145,
            0.8752834467120182,
            0.8594104308390023,
            0.8684807256235828,
            0.9387755102040817,
            0.8752834467120182,
            0.8956916099773242,
            0.8594104308390023,
            0.8798185941043084,
            0.9591836734693877,
            0.9705215419501134,
            0.891156462585034,
            0.9342403628117913,
            0.981859410430839,
            0.9251700680272109,
            0.9614512471655329,
            0.8707482993197279,
            0.9523809523809523,
            0.9886621315192744,
            0.909297052154195,
            0.9546485260770975,
            0.9138321995464853,
            0.981859410430839,
            0.963718820861678,
            0.9773242630385488,
            0.9773242630385488,
            0.9931972789115646,
            0.9115646258503401,
            0.9682539682539683,
            0.9977324263038548,
            0.9047619047619048,
            0.9909297052154195,
            0.9160997732426304,
            0.9863945578231292,
            0.9138321995464853,
            0.9410430839002267,
            0.9795918367346939,
            0.9977324263038548,
            0.9160997732426304,
            0.9206349206349206,
            0.9931972789115646,
            0.9773242630385488,
            1.0
        ],
        "reward_history": [
            -4410,
            -3124,
            -4410,
            -4410,
            -2159,
            -4410,
            -1115,
            -4410,
            -4410,
            -4410,
            -4410,
            -1145,
            -463,
            -2745,
            -1254,
            -360,
            -1482,
            -703,
            -4410,
            -886,
            -323,
            -4410,
            -965,
            -2357,
            -202,
            -942,
            -575,
            -808,
            -479,
            -2408,
            -1126,
            -308,
            -4410,
            -416,
            -4410,
            -459,
            -4299,
            -1905,
            -714,
            -294,
            -3253,
            -3558,
            -592,
            -1245,
            -218
        ],
        "steps_history": [
            4410,
            3225,
            4410,
            4410,
            2260,
            4410,
            1216,
            4410,
            4410,
            4410,
            4410,
            1246,
            564,
            2846,
            1355,
            461,
            1583,
            804,
            4410,
            987,
            424,
            4410,
            1066,
            2458,
            303,
            1043,
            676,
            909,
            580,
            2509,
            1227,
            409,
            4410,
            517,
            4410,
            560,
            4400,
            2006,
            815,
            395,
            3354,
            3659,
            693,
            1346,
            319
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "685/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.956525325775146,
        "final_policy_stability": 0.9931972789115646,
        "episodes_to_convergence": 51,
        "policy_stability_history": [
            0.0,
            0.8639455782312925,
            0.7959183673469388,
            0.8594104308390023,
            0.8820861678004536,
            0.81859410430839,
            0.8299319727891157,
            0.8798185941043084,
            0.8344671201814059,
            0.8775510204081632,
            0.9591836734693877,
            0.9591836734693877,
            0.9024943310657596,
            0.8684807256235828,
            0.81859410430839,
            0.8866213151927438,
            0.9659863945578231,
            0.9569160997732427,
            0.9727891156462585,
            0.909297052154195,
            0.9773242630385488,
            0.9546485260770975,
            0.909297052154195,
            0.8798185941043084,
            0.9727891156462585,
            0.8866213151927438,
            0.8843537414965986,
            0.9614512471655329,
            0.927437641723356,
            0.9705215419501134,
            0.927437641723356,
            0.9931972789115646,
            0.9909297052154195,
            0.9546485260770975,
            0.9977324263038548,
            0.9931972789115646,
            0.8843537414965986,
            0.9206349206349206,
            0.9682539682539683,
            0.9977324263038548,
            0.9931972789115646,
            0.9977324263038548,
            0.9977324263038548,
            0.9977324263038548,
            0.9229024943310657,
            0.9977324263038548,
            0.9886621315192744,
            0.9863945578231292,
            0.9546485260770975,
            0.9954648526077098,
            1.0,
            0.9931972789115646
        ],
        "reward_history": [
            -4410,
            -4410,
            -3557,
            -4410,
            -1995,
            -4410,
            -3811,
            -2298,
            -4410,
            -1254,
            -566,
            -328,
            -2064,
            -4410,
            -4410,
            -4410,
            -706,
            -554,
            -356,
            -2387,
            -167,
            -1269,
            -1821,
            -2741,
            -464,
            -4410,
            -4410,
            -503,
            -1597,
            -600,
            -1865,
            -180,
            -512,
            -771,
            -307,
            -348,
            -4410,
            -4410,
            -460,
            -174,
            -271,
            -139,
            -183,
            -154,
            -1957,
            -331,
            -299,
            -605,
            -1456,
            -508,
            -400,
            -298
        ],
        "steps_history": [
            4410,
            4410,
            3658,
            4410,
            2096,
            4410,
            3912,
            2399,
            4410,
            1355,
            667,
            429,
            2165,
            4410,
            4410,
            4410,
            807,
            655,
            457,
            2488,
            268,
            1370,
            1922,
            2842,
            565,
            4410,
            4410,
            604,
            1698,
            701,
            1966,
            281,
            613,
            872,
            408,
            449,
            4410,
            4410,
            561,
            275,
            372,
            240,
            284,
            255,
            2058,
            432,
            400,
            706,
            1557,
            609,
            501,
            399
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "686/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.275881290435791,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 45,
        "policy_stability_history": [
            0.0,
            0.8798185941043084,
            0.8253968253968254,
            0.8503401360544217,
            0.8888888888888888,
            0.8435374149659864,
            0.8866213151927438,
            0.8390022675736961,
            0.927437641723356,
            0.8571428571428571,
            0.8707482993197279,
            0.9342403628117913,
            0.9387755102040817,
            0.8866213151927438,
            0.8367346938775511,
            0.9523809523809523,
            0.8956916099773242,
            0.8390022675736961,
            0.8707482993197279,
            0.9365079365079365,
            0.927437641723356,
            0.9750566893424036,
            0.9591836734693877,
            0.8843537414965986,
            0.9773242630385488,
            0.8616780045351474,
            0.9841269841269841,
            0.9863945578231292,
            0.9727891156462585,
            0.9886621315192744,
            0.891156462585034,
            0.9727891156462585,
            0.8390022675736961,
            0.9523809523809523,
            0.963718820861678,
            0.9954648526077098,
            0.9863945578231292,
            1.0,
            0.9977324263038548,
            0.9931972789115646,
            0.9863945578231292,
            0.9954648526077098,
            0.9977324263038548,
            0.9297052154195011,
            0.9229024943310657,
            0.9977324263038548
        ],
        "reward_history": [
            -3020,
            -4410,
            -4410,
            -2578,
            -4410,
            -2392,
            -1406,
            -3679,
            -1064,
            -4410,
            -4410,
            -1132,
            -1124,
            -4410,
            -4042,
            -956,
            -4410,
            -4410,
            -4410,
            -922,
            -1524,
            -323,
            -870,
            -2744,
            -545,
            -4410,
            -350,
            -507,
            -783,
            -93,
            -4410,
            -646,
            -4410,
            -752,
            -1078,
            -210,
            -736,
            -294,
            -363,
            -323,
            -839,
            -67,
            -144,
            -2542,
            -2732,
            -191
        ],
        "steps_history": [
            3121,
            4410,
            4410,
            2679,
            4410,
            2493,
            1507,
            3780,
            1165,
            4410,
            4410,
            1233,
            1225,
            4410,
            4143,
            1057,
            4410,
            4410,
            4410,
            1023,
            1625,
            424,
            971,
            2845,
            646,
            4410,
            451,
            608,
            884,
            194,
            4410,
            747,
            4410,
            853,
            1179,
            311,
            837,
            395,
            464,
            424,
            940,
            168,
            245,
            2643,
            2833,
            292
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "687/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.368489027023315,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 44,
        "policy_stability_history": [
            0.0,
            0.8117913832199547,
            0.8662131519274376,
            0.8412698412698413,
            0.8707482993197279,
            0.8820861678004536,
            0.9024943310657596,
            0.9183673469387755,
            0.9478458049886621,
            0.8571428571428571,
            0.9183673469387755,
            0.8934240362811792,
            0.8707482993197279,
            0.9659863945578231,
            0.8480725623582767,
            0.8888888888888888,
            0.8843537414965986,
            0.9523809523809523,
            0.9183673469387755,
            0.8662131519274376,
            0.963718820861678,
            0.9659863945578231,
            0.9750566893424036,
            0.9841269841269841,
            0.8662131519274376,
            0.9229024943310657,
            0.9750566893424036,
            0.909297052154195,
            0.8526077097505669,
            0.9750566893424036,
            0.8639455782312925,
            0.9024943310657596,
            0.9886621315192744,
            0.9795918367346939,
            0.9795918367346939,
            1.0,
            0.9863945578231292,
            0.9705215419501134,
            0.9523809523809523,
            0.9954648526077098,
            0.9954648526077098,
            0.9931972789115646,
            0.9977324263038548,
            0.891156462585034,
            0.9977324263038548
        ],
        "reward_history": [
            -3172,
            -4410,
            -4410,
            -4410,
            -4410,
            -1412,
            -2055,
            -1242,
            -665,
            -4410,
            -921,
            -4410,
            -4410,
            -534,
            -4410,
            -4410,
            -4410,
            -563,
            -669,
            -4410,
            -551,
            -458,
            -192,
            -274,
            -4093,
            -1779,
            -218,
            -1564,
            -3246,
            -612,
            -2953,
            -4410,
            -294,
            -629,
            -446,
            -360,
            -438,
            -920,
            -963,
            -322,
            -267,
            -567,
            -344,
            -4410,
            -467
        ],
        "steps_history": [
            3273,
            4410,
            4410,
            4410,
            4410,
            1513,
            2156,
            1343,
            766,
            4410,
            1022,
            4410,
            4410,
            635,
            4410,
            4410,
            4410,
            664,
            770,
            4410,
            652,
            559,
            293,
            375,
            4194,
            1880,
            319,
            1665,
            3347,
            713,
            3054,
            4410,
            395,
            730,
            547,
            461,
            539,
            1021,
            1064,
            423,
            368,
            668,
            445,
            4410,
            568
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "688/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.59090781211853,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8594104308390023,
            0.7959183673469388,
            0.9365079365079365,
            0.8480725623582767,
            0.873015873015873,
            0.9024943310657596,
            0.8707482993197279,
            0.8934240362811792,
            0.891156462585034,
            0.8276643990929705,
            0.9229024943310657,
            0.9002267573696145,
            0.8934240362811792,
            0.9591836734693877,
            0.9750566893424036,
            0.8526077097505669,
            0.8866213151927438,
            0.8276643990929705,
            0.9591836734693877,
            0.8752834467120182,
            0.9659863945578231,
            0.9455782312925171,
            0.8707482993197279,
            0.9591836734693877,
            0.8752834467120182,
            0.9523809523809523,
            0.8390022675736961,
            0.981859410430839,
            0.9795918367346939,
            0.8820861678004536,
            0.9297052154195011,
            0.981859410430839,
            0.9591836734693877,
            0.9931972789115646,
            0.9931972789115646,
            0.8956916099773242,
            1.0,
            1.0,
            0.891156462585034,
            0.9977324263038548
        ],
        "reward_history": [
            -4410,
            -1574,
            -4410,
            -1755,
            -4410,
            -1842,
            -2383,
            -4410,
            -4410,
            -4410,
            -4410,
            -741,
            -2445,
            -2179,
            -857,
            -270,
            -3825,
            -3249,
            -4410,
            -624,
            -3723,
            -459,
            -462,
            -4410,
            -786,
            -3081,
            -1134,
            -4410,
            -624,
            -640,
            -4410,
            -2226,
            -347,
            -772,
            -104,
            -582,
            -3570,
            -369,
            -447,
            -4410,
            -576
        ],
        "steps_history": [
            4410,
            1675,
            4410,
            1856,
            4410,
            1943,
            2484,
            4410,
            4410,
            4410,
            4410,
            842,
            2546,
            2280,
            958,
            371,
            3926,
            3350,
            4410,
            725,
            3824,
            560,
            563,
            4410,
            887,
            3182,
            1235,
            4410,
            725,
            741,
            4410,
            2327,
            448,
            873,
            205,
            683,
            3671,
            470,
            548,
            4410,
            677
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "689/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.01819896697998,
        "final_policy_stability": 0.9954648526077098,
        "episodes_to_convergence": 48,
        "policy_stability_history": [
            0.0,
            0.854875283446712,
            0.8820861678004536,
            0.8956916099773242,
            0.8866213151927438,
            0.8367346938775511,
            0.8979591836734694,
            0.9115646258503401,
            0.8616780045351474,
            0.9410430839002267,
            0.8684807256235828,
            0.9183673469387755,
            0.909297052154195,
            0.8367346938775511,
            0.8480725623582767,
            0.9727891156462585,
            0.9002267573696145,
            0.8798185941043084,
            0.9115646258503401,
            0.9659863945578231,
            0.8843537414965986,
            0.9410430839002267,
            0.9478458049886621,
            0.9455782312925171,
            0.9773242630385488,
            0.9501133786848073,
            0.9682539682539683,
            0.873015873015873,
            0.8775510204081632,
            0.9682539682539683,
            0.9841269841269841,
            0.8752834467120182,
            0.9727891156462585,
            0.9909297052154195,
            0.963718820861678,
            0.8820861678004536,
            0.9954648526077098,
            0.9705215419501134,
            0.9773242630385488,
            0.9773242630385488,
            1.0,
            0.9977324263038548,
            1.0,
            0.9977324263038548,
            0.9977324263038548,
            0.9977324263038548,
            0.9909297052154195,
            0.9795918367346939,
            0.9954648526077098
        ],
        "reward_history": [
            -573,
            -4410,
            -4410,
            -4410,
            -991,
            -4410,
            -4410,
            -1436,
            -3556,
            -1329,
            -2304,
            -1779,
            -1130,
            -4410,
            -4410,
            -348,
            -4410,
            -4410,
            -2668,
            -267,
            -4410,
            -921,
            -486,
            -1459,
            -318,
            -569,
            -462,
            -4410,
            -4410,
            -890,
            -490,
            -4410,
            -510,
            -232,
            -545,
            -3714,
            -181,
            -719,
            -601,
            -559,
            -235,
            -294,
            -71,
            -449,
            -521,
            -523,
            -513,
            -845,
            -150
        ],
        "steps_history": [
            674,
            4410,
            4410,
            4410,
            1092,
            4410,
            4410,
            1537,
            3657,
            1430,
            2405,
            1880,
            1231,
            4410,
            4410,
            449,
            4410,
            4410,
            2769,
            368,
            4410,
            1022,
            587,
            1560,
            419,
            670,
            563,
            4410,
            4410,
            991,
            591,
            4410,
            611,
            333,
            646,
            3815,
            282,
            820,
            702,
            660,
            336,
            395,
            172,
            550,
            622,
            624,
            614,
            946,
            251
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "690/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.3_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.80691123008728,
        "final_policy_stability": 0.9705215419501134,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.891156462585034,
            0.8503401360544217,
            0.9319727891156463,
            0.9138321995464853,
            0.9206349206349206,
            0.9047619047619048,
            0.8707482993197279,
            0.8707482993197279,
            0.8594104308390023,
            0.9342403628117913,
            0.9070294784580499,
            0.9115646258503401,
            0.9614512471655329,
            0.8276643990929705,
            0.927437641723356,
            0.8866213151927438,
            0.8843537414965986,
            0.981859410430839,
            0.9070294784580499,
            0.9614512471655329,
            0.9886621315192744,
            0.9909297052154195,
            0.9727891156462585,
            0.963718820861678,
            0.8956916099773242,
            0.9886621315192744,
            0.8979591836734694,
            1.0,
            0.9909297052154195,
            0.9387755102040817,
            0.9705215419501134
        ],
        "reward_history": [
            -4410,
            -1480,
            -4410,
            -1912,
            -4410,
            -1653,
            -3336,
            -4410,
            -4410,
            -4410,
            -877,
            -2677,
            -1708,
            -1113,
            -4410,
            -4410,
            -4410,
            -3522,
            -518,
            -3060,
            -1173,
            -656,
            -703,
            -664,
            -1709,
            -3036,
            -470,
            -4410,
            -624,
            -705,
            -1783,
            -1239
        ],
        "steps_history": [
            4410,
            1581,
            4410,
            2013,
            4410,
            1754,
            3437,
            4410,
            4410,
            4410,
            978,
            2778,
            1809,
            1214,
            4410,
            4410,
            4410,
            3623,
            619,
            3161,
            1274,
            757,
            804,
            765,
            1810,
            3137,
            571,
            4410,
            725,
            806,
            1884,
            1340
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "691/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.0580973625183105,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.7505668934240363,
            0.8866213151927438,
            0.8684807256235828,
            0.8843537414965986,
            0.9206349206349206,
            0.8095238095238095,
            0.81859410430839,
            0.9365079365079365,
            0.8775510204081632,
            0.8616780045351474,
            0.9070294784580499,
            0.8639455782312925,
            0.9342403628117913,
            0.9478458049886621,
            0.9705215419501134,
            0.9206349206349206,
            0.9160997732426304,
            0.8798185941043084,
            0.8866213151927438,
            0.8820861678004536,
            0.9773242630385488,
            0.9750566893424036,
            0.8843537414965986,
            0.9863945578231292,
            0.9705215419501134,
            0.9705215419501134,
            0.9750566893424036,
            1.0,
            0.9977324263038548
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -1281,
            -742,
            -4410,
            -4410,
            -1621,
            -4410,
            -4410,
            -2329,
            -4410,
            -1756,
            -1985,
            -1100,
            -4410,
            -2498,
            -4410,
            -3730,
            -4410,
            -525,
            -1602,
            -4410,
            -700,
            -794,
            -1545,
            -2221,
            -1283,
            -733
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            1382,
            843,
            4410,
            4410,
            1722,
            4410,
            4410,
            2430,
            4410,
            1857,
            2086,
            1201,
            4410,
            2599,
            4410,
            3831,
            4410,
            626,
            1703,
            4410,
            801,
            895,
            1646,
            2322,
            1384,
            834
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "692/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.52897834777832,
        "final_policy_stability": 0.9909297052154195,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.854875283446712,
            0.909297052154195,
            0.9160997732426304,
            0.8798185941043084,
            0.8775510204081632,
            0.9024943310657596,
            0.9478458049886621,
            0.854875283446712,
            0.8707482993197279,
            0.9070294784580499,
            0.9433106575963719,
            0.9160997732426304,
            0.9251700680272109,
            0.8820861678004536,
            0.9002267573696145,
            0.9773242630385488,
            0.8820861678004536,
            0.9705215419501134,
            0.9659863945578231,
            0.9047619047619048,
            0.854875283446712,
            0.927437641723356,
            0.9795918367346939,
            0.9909297052154195,
            0.9886621315192744,
            0.9614512471655329,
            0.9977324263038548,
            0.9501133786848073,
            0.9954648526077098,
            1.0,
            0.9909297052154195
        ],
        "reward_history": [
            -1031,
            -4410,
            -1040,
            -1233,
            -4410,
            -4410,
            -4410,
            -839,
            -4410,
            -4410,
            -4410,
            -1006,
            -4410,
            -2140,
            -4410,
            -2167,
            -530,
            -4410,
            -1014,
            -638,
            -4410,
            -4410,
            -3918,
            -1128,
            -191,
            -447,
            -1437,
            -326,
            -1097,
            -1308,
            -1307,
            -517
        ],
        "steps_history": [
            1132,
            4410,
            1141,
            1334,
            4410,
            4410,
            4410,
            940,
            4410,
            4410,
            4410,
            1107,
            4410,
            2241,
            4410,
            2268,
            631,
            4410,
            1115,
            739,
            4410,
            4410,
            4019,
            1229,
            292,
            548,
            1538,
            427,
            1198,
            1409,
            1408,
            618
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "693/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.83674430847168,
        "final_policy_stability": 0.9954648526077098,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8752834467120182,
            0.8866213151927438,
            0.8979591836734694,
            0.8752834467120182,
            0.9455782312925171,
            0.9387755102040817,
            0.8707482993197279,
            0.9727891156462585,
            0.8684807256235828,
            0.8956916099773242,
            0.9070294784580499,
            0.8571428571428571,
            0.8934240362811792,
            0.8956916099773242,
            0.8684807256235828,
            0.9478458049886621,
            0.9659863945578231,
            0.8662131519274376,
            0.8594104308390023,
            0.9115646258503401,
            0.9705215419501134,
            0.9954648526077098,
            0.9365079365079365,
            0.9795918367346939,
            0.8888888888888888,
            0.981859410430839,
            0.9931972789115646,
            0.9705215419501134,
            0.9727891156462585,
            0.9909297052154195,
            0.9795918367346939,
            0.9863945578231292,
            0.9886621315192744,
            0.9954648526077098
        ],
        "reward_history": [
            -4410,
            -1873,
            -4410,
            -4410,
            -4297,
            -563,
            -689,
            -4410,
            -340,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -3353,
            -912,
            -774,
            -2695,
            -4410,
            -2808,
            -1341,
            -271,
            -1591,
            -614,
            -4410,
            -725,
            -597,
            -1917,
            -933,
            -809,
            -1754,
            -978,
            -498,
            -1192
        ],
        "steps_history": [
            4410,
            1974,
            4410,
            4410,
            4398,
            664,
            790,
            4410,
            441,
            4410,
            4410,
            4410,
            4410,
            4410,
            4410,
            3454,
            1013,
            875,
            2796,
            4410,
            2909,
            1442,
            372,
            1692,
            715,
            4410,
            826,
            698,
            2018,
            1034,
            910,
            1855,
            1079,
            599,
            1293
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "694/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.503358602523804,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.891156462585034,
            0.8684807256235828,
            0.8344671201814059,
            0.9206349206349206,
            0.8571428571428571,
            0.8480725623582767,
            0.891156462585034,
            0.9251700680272109,
            0.9206349206349206,
            0.8775510204081632,
            0.8390022675736961,
            0.8684807256235828,
            0.9546485260770975,
            0.9387755102040817,
            0.8684807256235828,
            0.9047619047619048,
            0.909297052154195,
            0.981859410430839,
            0.9546485260770975,
            0.9002267573696145,
            0.9954648526077098,
            0.9909297052154195,
            0.9841269841269841,
            0.9727891156462585,
            1.0,
            0.9954648526077098,
            0.9773242630385488,
            1.0,
            0.9727891156462585,
            1.0,
            1.0
        ],
        "reward_history": [
            -2052,
            -4410,
            -4410,
            -4410,
            -4410,
            -1706,
            -3611,
            -4410,
            -2302,
            -829,
            -4410,
            -3633,
            -4410,
            -3133,
            -714,
            -769,
            -4115,
            -4410,
            -2884,
            -183,
            -938,
            -3226,
            -887,
            -378,
            -739,
            -974,
            -253,
            -420,
            -1327,
            -246,
            -1523,
            -641,
            -482
        ],
        "steps_history": [
            2153,
            4410,
            4410,
            4410,
            4410,
            1807,
            3712,
            4410,
            2403,
            930,
            4410,
            3734,
            4410,
            3234,
            815,
            870,
            4216,
            4410,
            2985,
            284,
            1039,
            3327,
            988,
            479,
            840,
            1075,
            354,
            521,
            1428,
            347,
            1624,
            742,
            583
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "695/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.529914379119873,
        "final_policy_stability": 0.9070294784580499,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.9229024943310657,
            0.8707482993197279,
            0.8662131519274376,
            0.8934240362811792,
            0.8412698412698413,
            0.9433106575963719,
            0.909297052154195,
            0.8707482993197279,
            0.8820861678004536,
            0.8639455782312925,
            0.9206349206349206,
            0.8979591836734694,
            0.9750566893424036,
            0.9047619047619048,
            0.9251700680272109,
            0.9750566893424036,
            0.8843537414965986,
            0.9795918367346939,
            0.9501133786848073,
            0.8752834467120182,
            0.9841269841269841,
            0.9705215419501134,
            0.963718820861678,
            0.9886621315192744,
            0.9795918367346939,
            0.9569160997732427,
            0.9909297052154195,
            0.9954648526077098,
            0.9183673469387755,
            0.9795918367346939,
            0.9954648526077098,
            0.9478458049886621,
            0.9977324263038548,
            0.9070294784580499
        ],
        "reward_history": [
            -4410,
            -1241,
            -4410,
            -4410,
            -2049,
            -4410,
            -557,
            -4410,
            -4410,
            -4410,
            -4410,
            -2296,
            -4410,
            -131,
            -4410,
            -1416,
            -216,
            -3246,
            -329,
            -1486,
            -4410,
            -397,
            -401,
            -625,
            -715,
            -549,
            -977,
            -469,
            -942,
            -4410,
            -482,
            -419,
            -1555,
            -217,
            -4410
        ],
        "steps_history": [
            4410,
            1342,
            4410,
            4410,
            2150,
            4410,
            658,
            4410,
            4410,
            4410,
            4410,
            2397,
            4410,
            232,
            4410,
            1517,
            317,
            3347,
            430,
            1587,
            4410,
            498,
            502,
            726,
            816,
            650,
            1078,
            570,
            1043,
            4410,
            583,
            520,
            1656,
            318,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "696/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.77166223526001,
        "final_policy_stability": 0.8843537414965986,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8503401360544217,
            0.8503401360544217,
            0.8775510204081632,
            0.9024943310657596,
            0.8526077097505669,
            0.8775510204081632,
            0.8979591836734694,
            0.9070294784580499,
            0.873015873015873,
            0.8798185941043084,
            0.9591836734693877,
            0.9682539682539683,
            0.9682539682539683,
            0.9705215419501134,
            0.8979591836734694,
            0.9705215419501134,
            0.9183673469387755,
            0.8775510204081632,
            0.9410430839002267,
            0.9705215419501134,
            0.9659863945578231,
            0.981859410430839,
            0.981859410430839,
            0.9931972789115646,
            0.981859410430839,
            0.9909297052154195,
            0.8888888888888888,
            0.8866213151927438,
            0.9863945578231292,
            0.9705215419501134,
            1.0,
            0.8843537414965986
        ],
        "reward_history": [
            -1899,
            -4410,
            -4410,
            -4410,
            -2122,
            -4410,
            -4410,
            -1749,
            -2035,
            -4410,
            -4410,
            -610,
            -268,
            -453,
            -464,
            -4410,
            -347,
            -1689,
            -3961,
            -1468,
            -451,
            -721,
            -194,
            -374,
            -236,
            -298,
            -373,
            -4410,
            -3007,
            -425,
            -796,
            -307,
            -4410
        ],
        "steps_history": [
            2000,
            4410,
            4410,
            4410,
            2223,
            4410,
            4410,
            1850,
            2136,
            4410,
            4410,
            711,
            369,
            554,
            565,
            4410,
            448,
            1790,
            4062,
            1569,
            552,
            822,
            295,
            475,
            337,
            399,
            474,
            4410,
            3108,
            526,
            897,
            408,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "697/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.545549392700195,
        "final_policy_stability": 0.9841269841269841,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.7868480725623582,
            0.909297052154195,
            0.8435374149659864,
            0.8888888888888888,
            0.9387755102040817,
            0.9501133786848073,
            0.9297052154195011,
            0.8979591836734694,
            0.9546485260770975,
            0.8616780045351474,
            0.9297052154195011,
            0.8843537414965986,
            0.8503401360544217,
            0.9455782312925171,
            0.9138321995464853,
            0.9229024943310657,
            0.8775510204081632,
            0.9455782312925171,
            0.9546485260770975,
            0.9659863945578231,
            0.9002267573696145,
            0.9886621315192744,
            0.981859410430839,
            0.9614512471655329,
            0.9659863945578231,
            0.8367346938775511,
            0.9773242630385488,
            0.981859410430839,
            0.9795918367346939,
            0.8866213151927438,
            0.981859410430839,
            0.891156462585034,
            0.9024943310657596,
            0.9886621315192744,
            0.9659863945578231,
            0.9954648526077098,
            0.9841269841269841
        ],
        "reward_history": [
            -3494,
            -4410,
            -4410,
            -4410,
            -4410,
            -1017,
            -673,
            -1001,
            -4410,
            -668,
            -4410,
            -971,
            -4410,
            -4410,
            -1161,
            -2199,
            -961,
            -4410,
            -804,
            -478,
            -679,
            -4410,
            -358,
            -503,
            -439,
            -626,
            -4410,
            -519,
            -929,
            -406,
            -4410,
            -1088,
            -4010,
            -2140,
            -353,
            -1029,
            -737,
            -638
        ],
        "steps_history": [
            3595,
            4410,
            4410,
            4410,
            4410,
            1118,
            774,
            1102,
            4410,
            769,
            4410,
            1072,
            4410,
            4410,
            1262,
            2300,
            1062,
            4410,
            905,
            579,
            780,
            4410,
            459,
            604,
            540,
            727,
            4410,
            620,
            1030,
            507,
            4410,
            1189,
            4111,
            2241,
            454,
            1130,
            838,
            739
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "698/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.7161946296691895,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.8843537414965986,
            0.9251700680272109,
            0.9501133786848073,
            0.8458049886621315,
            0.8435374149659864,
            0.8458049886621315,
            0.9138321995464853,
            0.9433106575963719,
            0.8752834467120182,
            0.8798185941043084,
            0.9024943310657596,
            0.9183673469387755,
            0.9750566893424036,
            0.9070294784580499,
            0.9047619047619048,
            0.9659863945578231,
            0.9659863945578231,
            0.9841269841269841,
            0.9795918367346939,
            0.8843537414965986,
            0.9455782312925171,
            0.9886621315192744,
            0.9138321995464853,
            0.9455782312925171,
            0.9931972789115646,
            0.9795918367346939,
            0.8888888888888888,
            0.9909297052154195,
            0.9954648526077098,
            0.9886621315192744,
            0.9886621315192744,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -4170,
            -4410,
            -4410,
            -666,
            -478,
            -4410,
            -4008,
            -4410,
            -1685,
            -925,
            -4410,
            -4410,
            -4410,
            -2545,
            -308,
            -2571,
            -4410,
            -362,
            -452,
            -890,
            -197,
            -4410,
            -1819,
            -418,
            -3457,
            -1105,
            -374,
            -500,
            -4410,
            -736,
            -201,
            -921,
            -736,
            -1650,
            -95,
            -656
        ],
        "steps_history": [
            4271,
            4410,
            4410,
            767,
            579,
            4410,
            4109,
            4410,
            1786,
            1026,
            4410,
            4410,
            4410,
            2646,
            409,
            2672,
            4410,
            463,
            553,
            991,
            298,
            4410,
            1920,
            519,
            3558,
            1206,
            475,
            601,
            4410,
            837,
            302,
            1022,
            837,
            1751,
            196,
            757
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "699/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.301913738250732,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8775510204081632,
            0.9115646258503401,
            0.9251700680272109,
            0.8458049886621315,
            0.9319727891156463,
            0.8684807256235828,
            0.8662131519274376,
            0.8752834467120182,
            0.8956916099773242,
            0.9342403628117913,
            0.8775510204081632,
            0.8594104308390023,
            0.9523809523809523,
            0.7959183673469388,
            0.9841269841269841,
            0.8594104308390023,
            0.8979591836734694,
            0.927437641723356,
            0.9795918367346939,
            0.963718820861678,
            0.9229024943310657,
            0.9682539682539683,
            0.8956916099773242,
            0.9727891156462585,
            0.9546485260770975,
            0.9863945578231292,
            0.9795918367346939,
            0.9705215419501134,
            0.891156462585034,
            0.9863945578231292,
            0.9863945578231292,
            0.9954648526077098,
            0.9886621315192744,
            0.9977324263038548,
            0.9954648526077098,
            0.9931972789115646,
            1.0
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -1181,
            -3292,
            -709,
            -2769,
            -4410,
            -4410,
            -1867,
            -727,
            -2381,
            -4410,
            -759,
            -3588,
            -138,
            -4410,
            -4410,
            -1958,
            -201,
            -516,
            -1161,
            -1023,
            -4410,
            -1019,
            -1245,
            -575,
            -808,
            -490,
            -4410,
            -336,
            -748,
            -421,
            -550,
            -606,
            -482,
            -394,
            -306
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            1282,
            3393,
            810,
            2870,
            4410,
            4410,
            1968,
            828,
            2482,
            4410,
            860,
            3689,
            239,
            4410,
            4410,
            2059,
            302,
            617,
            1262,
            1124,
            4410,
            1120,
            1346,
            676,
            909,
            591,
            4410,
            437,
            849,
            522,
            651,
            707,
            583,
            495,
            407
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "700/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.55029821395874,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.7845804988662132,
            0.8435374149659864,
            0.8571428571428571,
            0.8616780045351474,
            0.963718820861678,
            0.9183673469387755,
            0.8888888888888888,
            0.854875283446712,
            0.9614512471655329,
            0.8956916099773242,
            0.8843537414965986,
            0.8775510204081632,
            0.9546485260770975,
            0.8616780045351474,
            0.854875283446712,
            0.8163265306122449,
            0.9682539682539683,
            0.8752834467120182,
            0.9591836734693877,
            0.9750566893424036,
            0.9773242630385488,
            0.9501133786848073,
            0.9954648526077098,
            0.9002267573696145,
            0.9478458049886621,
            0.9886621315192744,
            0.9433106575963719,
            0.9863945578231292,
            0.9954648526077098,
            0.9750566893424036,
            0.9727891156462585,
            0.9954648526077098,
            1.0,
            1.0
        ],
        "reward_history": [
            -4410,
            -3861,
            -4410,
            -4410,
            -4410,
            -332,
            -1553,
            -4410,
            -3136,
            -416,
            -4410,
            -2436,
            -4410,
            -724,
            -4410,
            -4410,
            -4410,
            -640,
            -4410,
            -710,
            -231,
            -285,
            -699,
            -218,
            -4410,
            -1052,
            -433,
            -2067,
            -389,
            -388,
            -636,
            -673,
            -271,
            -230,
            -292
        ],
        "steps_history": [
            4410,
            3962,
            4410,
            4410,
            4410,
            433,
            1654,
            4410,
            3237,
            517,
            4410,
            2537,
            4410,
            825,
            4410,
            4410,
            4410,
            741,
            4410,
            811,
            332,
            386,
            800,
            319,
            4410,
            1153,
            534,
            2168,
            490,
            489,
            737,
            774,
            372,
            331,
            393
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "701/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.437738656997681,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 38,
        "policy_stability_history": [
            0.0,
            0.9229024943310657,
            0.8798185941043084,
            0.8276643990929705,
            0.8594104308390023,
            0.9433106575963719,
            0.782312925170068,
            0.8775510204081632,
            0.8820861678004536,
            0.9251700680272109,
            0.8843537414965986,
            0.9342403628117913,
            0.8412698412698413,
            0.8888888888888888,
            0.9342403628117913,
            0.854875283446712,
            0.9115646258503401,
            0.9478458049886621,
            0.8662131519274376,
            0.9251700680272109,
            0.8866213151927438,
            0.9297052154195011,
            0.9591836734693877,
            0.9841269841269841,
            0.8866213151927438,
            0.9795918367346939,
            0.963718820861678,
            0.9659863945578231,
            0.9909297052154195,
            0.9909297052154195,
            0.909297052154195,
            0.9863945578231292,
            0.9863945578231292,
            1.0,
            0.9977324263038548,
            0.9070294784580499,
            0.9841269841269841,
            0.9841269841269841,
            1.0
        ],
        "reward_history": [
            -4410,
            -840,
            -4410,
            -4410,
            -2251,
            -463,
            -4410,
            -4410,
            -4410,
            -812,
            -1608,
            -910,
            -3977,
            -4410,
            -914,
            -4410,
            -4410,
            -1032,
            -4410,
            -1075,
            -4410,
            -1662,
            -807,
            -200,
            -3152,
            -305,
            -609,
            -411,
            -230,
            -363,
            -2302,
            -654,
            -448,
            -171,
            -44,
            -4410,
            -646,
            -1056,
            -158
        ],
        "steps_history": [
            4410,
            941,
            4410,
            4410,
            2352,
            564,
            4410,
            4410,
            4410,
            913,
            1709,
            1011,
            4078,
            4410,
            1015,
            4410,
            4410,
            1133,
            4410,
            1176,
            4410,
            1763,
            908,
            301,
            3253,
            406,
            710,
            512,
            331,
            464,
            2403,
            755,
            549,
            272,
            145,
            4410,
            747,
            1157,
            259
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "702/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.89996862411499,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 44,
        "policy_stability_history": [
            0.0,
            0.8231292517006803,
            0.7981859410430839,
            0.8412698412698413,
            0.9160997732426304,
            0.9410430839002267,
            0.9024943310657596,
            0.8117913832199547,
            0.891156462585034,
            0.8752834467120182,
            0.9455782312925171,
            0.9591836734693877,
            0.9433106575963719,
            0.8639455782312925,
            0.9750566893424036,
            0.9614512471655329,
            0.8616780045351474,
            0.891156462585034,
            0.8616780045351474,
            0.9614512471655329,
            0.9546485260770975,
            0.9750566893424036,
            0.9795918367346939,
            0.963718820861678,
            0.8639455782312925,
            0.9795918367346939,
            0.981859410430839,
            0.9546485260770975,
            0.9750566893424036,
            0.9863945578231292,
            0.9047619047619048,
            0.9886621315192744,
            0.9546485260770975,
            0.9863945578231292,
            0.9863945578231292,
            1.0,
            0.9909297052154195,
            0.9954648526077098,
            0.9977324263038548,
            0.9795918367346939,
            0.9954648526077098,
            0.9977324263038548,
            1.0,
            0.9977324263038548,
            1.0
        ],
        "reward_history": [
            -4410,
            -4410,
            -3574,
            -4005,
            -1082,
            -560,
            -1401,
            -4410,
            -4410,
            -4410,
            -484,
            -339,
            -718,
            -1919,
            -302,
            -422,
            -3924,
            -4410,
            -4060,
            -840,
            -421,
            -437,
            -316,
            -435,
            -2952,
            -170,
            -111,
            -941,
            -429,
            -308,
            -1974,
            -152,
            -799,
            -314,
            -635,
            -309,
            -315,
            -436,
            -89,
            -1658,
            -389,
            -855,
            -301,
            -158,
            -555
        ],
        "steps_history": [
            4410,
            4410,
            3675,
            4106,
            1183,
            661,
            1502,
            4410,
            4410,
            4410,
            585,
            440,
            819,
            2020,
            403,
            523,
            4025,
            4410,
            4161,
            941,
            522,
            538,
            417,
            536,
            3053,
            271,
            212,
            1042,
            530,
            409,
            2075,
            253,
            900,
            415,
            736,
            410,
            416,
            537,
            190,
            1759,
            490,
            956,
            402,
            259,
            656
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "703/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.360649347305298,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 41,
        "policy_stability_history": [
            0.0,
            0.7369614512471655,
            0.8707482993197279,
            0.873015873015873,
            0.9183673469387755,
            0.9251700680272109,
            0.8639455782312925,
            0.8526077097505669,
            0.8934240362811792,
            0.8435374149659864,
            0.8526077097505669,
            0.9455782312925171,
            0.9002267573696145,
            0.9727891156462585,
            0.8526077097505669,
            0.8684807256235828,
            0.9410430839002267,
            0.9727891156462585,
            0.8866213151927438,
            0.8956916099773242,
            0.9455782312925171,
            0.963718820861678,
            0.8208616780045351,
            0.9523809523809523,
            0.9705215419501134,
            0.9886621315192744,
            0.9614512471655329,
            0.9183673469387755,
            0.9410430839002267,
            0.9863945578231292,
            0.9410430839002267,
            0.9954648526077098,
            0.9750566893424036,
            0.9863945578231292,
            0.9795918367346939,
            0.9002267573696145,
            0.9841269841269841,
            0.9977324263038548,
            1.0,
            1.0,
            1.0,
            0.9977324263038548
        ],
        "reward_history": [
            -4410,
            -4410,
            -2060,
            -4410,
            -1081,
            -832,
            -4410,
            -4410,
            -4410,
            -4410,
            -2849,
            -821,
            -1317,
            -274,
            -3197,
            -4410,
            -1287,
            -257,
            -2134,
            -4410,
            -854,
            -600,
            -4410,
            -768,
            -475,
            -125,
            -646,
            -1360,
            -1191,
            -232,
            -1566,
            -807,
            -506,
            -605,
            -428,
            -4410,
            -624,
            -316,
            -223,
            -495,
            -236,
            -666
        ],
        "steps_history": [
            4410,
            4410,
            2161,
            4410,
            1182,
            933,
            4410,
            4410,
            4410,
            4410,
            2950,
            922,
            1418,
            375,
            3298,
            4410,
            1388,
            358,
            2235,
            4410,
            955,
            701,
            4410,
            869,
            576,
            226,
            747,
            1461,
            1292,
            333,
            1667,
            908,
            607,
            706,
            529,
            4410,
            725,
            417,
            324,
            596,
            337,
            767
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "704/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.53519868850708,
        "final_policy_stability": 0.9183673469387755,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8639455782312925,
            0.8344671201814059,
            0.891156462585034,
            0.927437641723356,
            0.8775510204081632,
            0.8594104308390023,
            0.9455782312925171,
            0.8843537414965986,
            0.9342403628117913,
            0.8276643990929705,
            0.9569160997732427,
            0.9501133786848073,
            0.8412698412698413,
            0.8571428571428571,
            0.873015873015873,
            0.9455782312925171,
            0.927437641723356,
            0.9614512471655329,
            0.9863945578231292,
            0.9773242630385488,
            0.9773242630385488,
            0.8775510204081632,
            0.8934240362811792,
            0.8752834467120182,
            0.9115646258503401,
            0.9705215419501134,
            0.9909297052154195,
            0.9954648526077098,
            0.9705215419501134,
            0.9931972789115646,
            0.8843537414965986,
            0.9931972789115646,
            0.9977324263038548,
            0.9183673469387755
        ],
        "reward_history": [
            -4298,
            -4410,
            -2649,
            -4410,
            -748,
            -4410,
            -2980,
            -683,
            -4410,
            -637,
            -4410,
            -583,
            -470,
            -4410,
            -3816,
            -2212,
            -1383,
            -1429,
            -600,
            -324,
            -425,
            -273,
            -4410,
            -4410,
            -2757,
            -1603,
            -829,
            -414,
            -363,
            -593,
            -149,
            -4410,
            -266,
            -489,
            -4410
        ],
        "steps_history": [
            4399,
            4410,
            2750,
            4410,
            849,
            4410,
            3081,
            784,
            4410,
            738,
            4410,
            684,
            571,
            4410,
            3917,
            2313,
            1484,
            1530,
            701,
            425,
            526,
            374,
            4410,
            4410,
            2858,
            1704,
            930,
            515,
            464,
            694,
            250,
            4410,
            367,
            590,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "705/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.5513014793396,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.891156462585034,
            0.8526077097505669,
            0.9319727891156463,
            0.9115646258503401,
            0.9206349206349206,
            0.909297052154195,
            0.8820861678004536,
            0.9365079365079365,
            0.891156462585034,
            0.8684807256235828,
            0.9387755102040817,
            0.8843537414965986,
            0.873015873015873,
            0.909297052154195,
            0.9523809523809523,
            0.9682539682539683,
            0.9909297052154195,
            0.8866213151927438,
            0.9024943310657596,
            0.9365079365079365,
            0.9478458049886621,
            0.9070294784580499,
            0.9206349206349206,
            0.9070294784580499,
            0.9501133786848073,
            0.963718820861678,
            0.9387755102040817,
            0.8956916099773242,
            0.9750566893424036,
            0.9977324263038548,
            0.9433106575963719,
            0.9909297052154195,
            0.9795918367346939,
            1.0,
            1.0,
            1.0,
            0.9977324263038548
        ],
        "reward_history": [
            -4410,
            -1480,
            -4410,
            -1912,
            -4410,
            -1653,
            -3331,
            -4410,
            -1334,
            -4410,
            -4410,
            -842,
            -2221,
            -4410,
            -4410,
            -930,
            -839,
            -246,
            -4410,
            -4410,
            -1786,
            -771,
            -4410,
            -4410,
            -4410,
            -1222,
            -843,
            -1533,
            -4410,
            -1062,
            -272,
            -2548,
            -493,
            -1514,
            -568,
            -541,
            -899,
            -1813
        ],
        "steps_history": [
            4410,
            1581,
            4410,
            2013,
            4410,
            1754,
            3432,
            4410,
            1435,
            4410,
            4410,
            943,
            2322,
            4410,
            4410,
            1031,
            940,
            347,
            4410,
            4410,
            1887,
            872,
            4410,
            4410,
            4410,
            1323,
            944,
            1634,
            4410,
            1163,
            373,
            2649,
            594,
            1615,
            669,
            642,
            1000,
            1914
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "706/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.565262794494629,
        "final_policy_stability": 0.9954648526077098,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.7936507936507936,
            0.8321995464852607,
            0.9024943310657596,
            0.9478458049886621,
            0.9229024943310657,
            0.8616780045351474,
            0.8934240362811792,
            0.909297052154195,
            0.8979591836734694,
            0.8888888888888888,
            0.9002267573696145,
            0.9410430839002267,
            0.9070294784580499,
            0.8820861678004536,
            0.9297052154195011,
            0.8594104308390023,
            0.963718820861678,
            0.8253968253968254,
            0.9546485260770975,
            0.9773242630385488,
            0.9705215419501134,
            0.8798185941043084,
            0.8979591836734694,
            0.8775510204081632,
            0.963718820861678,
            0.9841269841269841,
            1.0,
            0.9931972789115646,
            0.9773242630385488,
            0.9682539682539683,
            0.9863945578231292,
            1.0,
            0.9954648526077098
        ],
        "reward_history": [
            -4410,
            -4410,
            -3263,
            -2212,
            -569,
            -968,
            -4410,
            -4410,
            -2447,
            -4410,
            -4410,
            -4410,
            -722,
            -2016,
            -4410,
            -1725,
            -4410,
            -812,
            -4410,
            -980,
            -421,
            -1352,
            -3561,
            -4410,
            -4410,
            -1725,
            -1066,
            -436,
            -394,
            -544,
            -1235,
            -1886,
            -1283,
            -733
        ],
        "steps_history": [
            4410,
            4410,
            3364,
            2313,
            670,
            1069,
            4410,
            4410,
            2548,
            4410,
            4410,
            4410,
            823,
            2117,
            4410,
            1826,
            4410,
            913,
            4410,
            1081,
            522,
            1453,
            3662,
            4410,
            4410,
            1826,
            1167,
            537,
            495,
            645,
            1336,
            1987,
            1384,
            834
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "707/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.535564422607422,
        "final_policy_stability": 0.9024943310657596,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.854875283446712,
            0.909297052154195,
            0.9138321995464853,
            0.8866213151927438,
            0.8843537414965986,
            0.8866213151927438,
            0.9478458049886621,
            0.9047619047619048,
            0.9614512471655329,
            0.9024943310657596,
            0.8934240362811792,
            0.8888888888888888,
            0.9002267573696145,
            0.9501133786848073,
            0.9773242630385488,
            0.9387755102040817,
            0.8775510204081632,
            0.8888888888888888,
            0.9727891156462585,
            0.9387755102040817,
            0.9047619047619048,
            0.9160997732426304,
            0.8888888888888888,
            0.963718820861678,
            0.9659863945578231,
            0.9773242630385488,
            0.9659863945578231,
            0.9931972789115646,
            0.9546485260770975,
            0.8956916099773242,
            0.9750566893424036,
            0.9909297052154195,
            0.9841269841269841,
            1.0,
            1.0,
            0.9024943310657596
        ],
        "reward_history": [
            -1031,
            -4410,
            -1040,
            -1233,
            -4410,
            -4410,
            -4410,
            -839,
            -4410,
            -740,
            -4410,
            -4410,
            -4410,
            -2862,
            -1498,
            -286,
            -1462,
            -4410,
            -4410,
            -573,
            -1448,
            -4410,
            -4410,
            -3964,
            -1985,
            -650,
            -724,
            -1357,
            -1307,
            -2116,
            -3587,
            -1843,
            -893,
            -386,
            -503,
            -740,
            -3631
        ],
        "steps_history": [
            1132,
            4410,
            1141,
            1334,
            4410,
            4410,
            4410,
            940,
            4410,
            841,
            4410,
            4410,
            4410,
            2963,
            1599,
            387,
            1563,
            4410,
            4410,
            674,
            1549,
            4410,
            4410,
            4065,
            2086,
            751,
            825,
            1458,
            1408,
            2217,
            3688,
            1944,
            994,
            487,
            604,
            841,
            3732
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "708/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.864873170852661,
        "final_policy_stability": 0.9886621315192744,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.873015873015873,
            0.8843537414965986,
            0.8979591836734694,
            0.8752834467120182,
            0.9455782312925171,
            0.9387755102040817,
            0.8707482993197279,
            0.8662131519274376,
            0.8866213151927438,
            0.873015873015873,
            0.8412698412698413,
            0.9183673469387755,
            0.8662131519274376,
            0.9319727891156463,
            0.8684807256235828,
            0.9750566893424036,
            0.9591836734693877,
            0.8707482993197279,
            0.9410430839002267,
            0.9750566893424036,
            0.9931972789115646,
            0.9727891156462585,
            0.8956916099773242,
            0.9773242630385488,
            0.9954648526077098,
            0.9206349206349206,
            0.8888888888888888,
            0.963718820861678,
            0.9863945578231292,
            0.9909297052154195,
            0.981859410430839,
            0.9229024943310657,
            0.9886621315192744
        ],
        "reward_history": [
            -4410,
            -1873,
            -4410,
            -4410,
            -4297,
            -563,
            -689,
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -1092,
            -4410,
            -1731,
            -4410,
            -411,
            -752,
            -4410,
            -1099,
            -522,
            -511,
            -1191,
            -3804,
            -807,
            -271,
            -4410,
            -4410,
            -1403,
            -820,
            -957,
            -1754,
            -2870,
            -478
        ],
        "steps_history": [
            4410,
            1974,
            4410,
            4410,
            4398,
            664,
            790,
            4410,
            4410,
            4410,
            4410,
            4410,
            1193,
            4410,
            1832,
            4410,
            512,
            853,
            4410,
            1200,
            623,
            612,
            1292,
            3905,
            908,
            372,
            4410,
            4410,
            1504,
            921,
            1058,
            1855,
            2971,
            579
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "709/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.580648899078369,
        "final_policy_stability": 0.9931972789115646,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8480725623582767,
            0.8526077097505669,
            0.8344671201814059,
            0.8639455782312925,
            0.9024943310657596,
            0.9297052154195011,
            0.854875283446712,
            0.9002267573696145,
            0.963718820861678,
            0.9342403628117913,
            0.8684807256235828,
            0.9795918367346939,
            0.9501133786848073,
            0.8684807256235828,
            0.9569160997732427,
            0.9160997732426304,
            0.9773242630385488,
            0.9047619047619048,
            0.9614512471655329,
            0.9523809523809523,
            0.9024943310657596,
            0.8707482993197279,
            0.9863945578231292,
            0.9909297052154195,
            0.8934240362811792,
            0.8979591836734694,
            0.9841269841269841,
            0.891156462585034,
            0.9977324263038548,
            1.0,
            0.9977324263038548,
            0.9931972789115646
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -4410,
            -2376,
            -1443,
            -4410,
            -2188,
            -476,
            -1107,
            -4410,
            -220,
            -931,
            -4410,
            -729,
            -4410,
            -417,
            -4410,
            -1484,
            -1571,
            -4410,
            -4125,
            -887,
            -413,
            -4082,
            -4410,
            -571,
            -4410,
            -692,
            -144,
            -421,
            -362
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            4410,
            2477,
            1544,
            4410,
            2289,
            577,
            1208,
            4410,
            321,
            1032,
            4410,
            830,
            4410,
            518,
            4410,
            1585,
            1672,
            4410,
            4226,
            988,
            514,
            4183,
            4410,
            672,
            4410,
            793,
            245,
            522,
            463
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "710/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.928129434585571,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8344671201814059,
            0.909297052154195,
            0.8752834467120182,
            0.9614512471655329,
            0.9115646258503401,
            0.8503401360544217,
            0.8888888888888888,
            0.8866213151927438,
            0.8843537414965986,
            0.9160997732426304,
            0.9024943310657596,
            0.8888888888888888,
            0.9682539682539683,
            0.81859410430839,
            0.9024943310657596,
            0.8798185941043084,
            0.963718820861678,
            0.9795918367346939,
            0.9183673469387755,
            0.9705215419501134,
            0.9342403628117913,
            0.9501133786848073,
            0.9138321995464853,
            0.9886621315192744,
            0.981859410430839,
            0.9977324263038548,
            0.9931972789115646,
            0.8956916099773242,
            0.9931972789115646,
            0.9002267573696145,
            1.0
        ],
        "reward_history": [
            -4410,
            -3807,
            -1453,
            -4410,
            -525,
            -1897,
            -4410,
            -4410,
            -4410,
            -4410,
            -1740,
            -4410,
            -3784,
            -886,
            -4410,
            -4410,
            -3047,
            -856,
            -375,
            -1935,
            -597,
            -1582,
            -699,
            -2369,
            -469,
            -962,
            -509,
            -236,
            -4410,
            -667,
            -4410,
            -578
        ],
        "steps_history": [
            4410,
            3908,
            1554,
            4410,
            626,
            1998,
            4410,
            4410,
            4410,
            4410,
            1841,
            4410,
            3885,
            987,
            4410,
            4410,
            3148,
            957,
            476,
            2036,
            698,
            1683,
            800,
            2470,
            570,
            1063,
            610,
            337,
            4410,
            768,
            4410,
            679
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "711/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.795908451080322,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.8344671201814059,
            0.8684807256235828,
            0.8390022675736961,
            0.8594104308390023,
            0.8888888888888888,
            0.8707482993197279,
            0.8458049886621315,
            0.9591836734693877,
            0.9387755102040817,
            0.8367346938775511,
            0.8820861678004536,
            0.9387755102040817,
            0.9773242630385488,
            0.9614512471655329,
            0.8866213151927438,
            0.8843537414965986,
            0.9342403628117913,
            0.873015873015873,
            0.9659863945578231,
            0.9501133786848073,
            0.9659863945578231,
            0.9569160997732427,
            0.9841269841269841,
            0.8752834467120182,
            0.9886621315192744,
            0.9501133786848073,
            0.9977324263038548,
            0.9795918367346939,
            0.9705215419501134,
            0.891156462585034,
            0.9659863945578231,
            1.0,
            0.9931972789115646,
            0.9863945578231292,
            1.0
        ],
        "reward_history": [
            -1899,
            -4410,
            -4410,
            -3653,
            -4410,
            -2300,
            -4410,
            -4410,
            -739,
            -1094,
            -4083,
            -4410,
            -1033,
            -390,
            -476,
            -4410,
            -4410,
            -1600,
            -2380,
            -745,
            -1165,
            -481,
            -660,
            -489,
            -4410,
            -361,
            -796,
            -307,
            -623,
            -772,
            -4410,
            -1339,
            -231,
            -466,
            -628,
            -301
        ],
        "steps_history": [
            2000,
            4410,
            4410,
            3754,
            4410,
            2401,
            4410,
            4410,
            840,
            1195,
            4184,
            4410,
            1134,
            491,
            577,
            4410,
            4410,
            1701,
            2481,
            846,
            1266,
            582,
            761,
            590,
            4410,
            462,
            897,
            408,
            724,
            873,
            4410,
            1440,
            332,
            567,
            729,
            402
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "712/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.687543869018555,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8707482993197279,
            0.8866213151927438,
            0.8367346938775511,
            0.8866213151927438,
            0.8775510204081632,
            0.9387755102040817,
            0.873015873015873,
            0.9115646258503401,
            0.9138321995464853,
            0.8934240362811792,
            0.9047619047619048,
            0.8820861678004536,
            0.8888888888888888,
            0.8888888888888888,
            0.9297052154195011,
            0.9160997732426304,
            0.9727891156462585,
            0.9319727891156463,
            0.9727891156462585,
            0.873015873015873,
            0.9863945578231292,
            0.981859410430839,
            0.9070294784580499,
            0.9931972789115646,
            0.9727891156462585,
            0.9954648526077098,
            0.9977324263038548,
            0.9977324263038548,
            0.9863945578231292,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -3494,
            -2725,
            -4410,
            -4410,
            -4410,
            -4410,
            -959,
            -4410,
            -2098,
            -4410,
            -4212,
            -2539,
            -4410,
            -4410,
            -4410,
            -1007,
            -2551,
            -653,
            -1426,
            -902,
            -3712,
            -527,
            -611,
            -3007,
            -423,
            -1661,
            -189,
            -337,
            -101,
            -702,
            -300,
            -446,
            -426
        ],
        "steps_history": [
            3595,
            2826,
            4410,
            4410,
            4410,
            4410,
            1060,
            4410,
            2199,
            4410,
            4313,
            2640,
            4410,
            4410,
            4410,
            1108,
            2652,
            754,
            1527,
            1003,
            3813,
            628,
            712,
            3108,
            524,
            1762,
            290,
            438,
            202,
            803,
            401,
            547,
            527
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "713/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.278258085250854,
        "final_policy_stability": 0.9841269841269841,
        "episodes_to_convergence": 41,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.8843537414965986,
            0.9251700680272109,
            0.9501133786848073,
            0.8321995464852607,
            0.927437641723356,
            0.8775510204081632,
            0.9115646258503401,
            0.9410430839002267,
            0.891156462585034,
            0.8798185941043084,
            0.9251700680272109,
            0.909297052154195,
            0.8888888888888888,
            0.8979591836734694,
            0.9569160997732427,
            0.9841269841269841,
            0.8843537414965986,
            0.9659863945578231,
            0.9569160997732427,
            0.9682539682539683,
            0.9750566893424036,
            0.9501133786848073,
            0.8979591836734694,
            0.9523809523809523,
            0.8684807256235828,
            0.8934240362811792,
            0.9863945578231292,
            0.9002267573696145,
            0.9115646258503401,
            0.9954648526077098,
            0.9909297052154195,
            0.9546485260770975,
            0.9909297052154195,
            0.9977324263038548,
            0.9773242630385488,
            0.981859410430839,
            1.0,
            0.9206349206349206,
            1.0,
            0.9841269841269841
        ],
        "reward_history": [
            -4170,
            -4410,
            -4410,
            -666,
            -478,
            -4410,
            -880,
            -4410,
            -2114,
            -856,
            -2279,
            -4410,
            -1122,
            -1565,
            -4410,
            -4410,
            -725,
            -235,
            -4410,
            -952,
            -1174,
            -596,
            -945,
            -805,
            -4410,
            -1211,
            -4410,
            -3400,
            -280,
            -4410,
            -2715,
            -95,
            -518,
            -1415,
            -562,
            -180,
            -839,
            -966,
            -870,
            -1818,
            -381,
            -724
        ],
        "steps_history": [
            4271,
            4410,
            4410,
            767,
            579,
            4410,
            981,
            4410,
            2215,
            957,
            2380,
            4410,
            1223,
            1666,
            4410,
            4410,
            826,
            336,
            4410,
            1053,
            1275,
            697,
            1046,
            906,
            4410,
            1312,
            4410,
            3501,
            381,
            4410,
            2816,
            196,
            619,
            1516,
            663,
            281,
            940,
            1067,
            971,
            1919,
            482,
            825
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "714/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.744353532791138,
        "final_policy_stability": 0.8956916099773242,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8775510204081632,
            0.9115646258503401,
            0.8662131519274376,
            0.9229024943310657,
            0.8571428571428571,
            0.8503401360544217,
            0.9251700680272109,
            0.9342403628117913,
            0.9365079365079365,
            0.8616780045351474,
            0.8707482993197279,
            0.927437641723356,
            0.9024943310657596,
            0.9750566893424036,
            0.909297052154195,
            0.8798185941043084,
            0.8843537414965986,
            0.8798185941043084,
            0.9659863945578231,
            0.9863945578231292,
            0.9841269841269841,
            0.9682539682539683,
            0.963718820861678,
            0.9183673469387755,
            0.8843537414965986,
            0.9795918367346939,
            0.9614512471655329,
            0.9909297052154195,
            0.9863945578231292,
            0.9795918367346939,
            0.9886621315192744,
            0.9750566893424036,
            0.8956916099773242
        ],
        "reward_history": [
            -4410,
            -4410,
            -4410,
            -4410,
            -997,
            -4410,
            -4410,
            -1044,
            -1116,
            -552,
            -3463,
            -4410,
            -739,
            -1745,
            -433,
            -2258,
            -4104,
            -4410,
            -4410,
            -689,
            -250,
            -476,
            -657,
            -1045,
            -1958,
            -4410,
            -440,
            -888,
            -595,
            -448,
            -497,
            -414,
            -648,
            -4410
        ],
        "steps_history": [
            4410,
            4410,
            4410,
            4410,
            1098,
            4410,
            4410,
            1145,
            1217,
            653,
            3564,
            4410,
            840,
            1846,
            534,
            2359,
            4205,
            4410,
            4410,
            790,
            351,
            577,
            758,
            1146,
            2059,
            4410,
            541,
            989,
            696,
            549,
            598,
            515,
            749,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "715/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.7616236209869385,
        "final_policy_stability": 0.854875283446712,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8049886621315193,
            0.8276643990929705,
            0.8707482993197279,
            0.9433106575963719,
            0.9365079365079365,
            0.9115646258503401,
            0.8503401360544217,
            0.909297052154195,
            0.8117913832199547,
            0.9387755102040817,
            0.9229024943310657,
            0.9410430839002267,
            0.9591836734693877,
            0.8934240362811792,
            0.8571428571428571,
            0.963718820861678,
            0.873015873015873,
            0.9206349206349206,
            0.8956916099773242,
            0.9523809523809523,
            0.9160997732426304,
            0.8843537414965986,
            0.963718820861678,
            0.9886621315192744,
            0.963718820861678,
            0.8662131519274376,
            0.9931972789115646,
            0.9523809523809523,
            0.981859410430839,
            0.9773242630385488,
            0.9977324263038548,
            1.0,
            0.9909297052154195,
            0.854875283446712
        ],
        "reward_history": [
            -3984,
            -4410,
            -3617,
            -4410,
            -785,
            -725,
            -4410,
            -4410,
            -1922,
            -4410,
            -469,
            -922,
            -855,
            -505,
            -4410,
            -4410,
            -585,
            -4410,
            -2008,
            -4410,
            -727,
            -1821,
            -4410,
            -306,
            -134,
            -760,
            -4410,
            -275,
            -911,
            -608,
            -818,
            -347,
            -86,
            -1042,
            -3703
        ],
        "steps_history": [
            4085,
            4410,
            3718,
            4410,
            886,
            826,
            4410,
            4410,
            2023,
            4410,
            570,
            1023,
            956,
            606,
            4410,
            4410,
            686,
            4410,
            2109,
            4410,
            828,
            1922,
            4410,
            407,
            235,
            861,
            4410,
            376,
            1012,
            709,
            919,
            448,
            187,
            1143,
            3804
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "716/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.558599948883057,
        "final_policy_stability": 0.9977324263038548,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.9229024943310657,
            0.8798185941043084,
            0.8253968253968254,
            0.8571428571428571,
            0.9047619047619048,
            0.9183673469387755,
            0.8390022675736961,
            0.8435374149659864,
            0.891156462585034,
            0.9024943310657596,
            0.8820861678004536,
            0.8798185941043084,
            0.9705215419501134,
            0.9138321995464853,
            0.8820861678004536,
            0.9365079365079365,
            0.8412698412698413,
            0.9682539682539683,
            0.9909297052154195,
            0.9750566893424036,
            0.9183673469387755,
            0.9795918367346939,
            0.9954648526077098,
            0.8798185941043084,
            0.873015873015873,
            0.981859410430839,
            0.9251700680272109,
            0.9682539682539683,
            0.9841269841269841,
            0.9183673469387755,
            0.9977324263038548
        ],
        "reward_history": [
            -4410,
            -840,
            -4410,
            -4410,
            -4410,
            -1707,
            -1033,
            -4410,
            -4410,
            -1335,
            -2922,
            -4410,
            -4410,
            -250,
            -4410,
            -2742,
            -1159,
            -3391,
            -564,
            -304,
            -617,
            -1257,
            -316,
            -203,
            -4410,
            -4410,
            -215,
            -1548,
            -855,
            -363,
            -2302,
            -542
        ],
        "steps_history": [
            4410,
            941,
            4410,
            4410,
            4410,
            1808,
            1134,
            4410,
            4410,
            1436,
            3023,
            4410,
            4410,
            351,
            4410,
            2843,
            1260,
            3492,
            665,
            405,
            718,
            1358,
            417,
            304,
            4410,
            4410,
            316,
            1649,
            956,
            464,
            2403,
            643
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "717/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.897118806838989,
        "final_policy_stability": 0.9705215419501134,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8231292517006803,
            0.7959183673469388,
            0.8571428571428571,
            0.9206349206349206,
            0.9047619047619048,
            0.8934240362811792,
            0.8707482993197279,
            0.8775510204081632,
            0.9455782312925171,
            0.8344671201814059,
            0.8820861678004536,
            0.9523809523809523,
            0.8979591836734694,
            0.9591836734693877,
            0.8117913832199547,
            0.8503401360544217,
            0.963718820861678,
            0.9682539682539683,
            0.9931972789115646,
            0.9773242630385488,
            0.8934240362811792,
            0.9478458049886621,
            0.891156462585034,
            0.9909297052154195,
            0.9863945578231292,
            0.9863945578231292,
            0.9841269841269841,
            0.9886621315192744,
            0.9773242630385488,
            0.9705215419501134
        ],
        "reward_history": [
            -4410,
            -4410,
            -3574,
            -4410,
            -932,
            -2014,
            -1166,
            -4410,
            -4410,
            -696,
            -3811,
            -4410,
            -761,
            -2627,
            -758,
            -4410,
            -3260,
            -481,
            -402,
            -316,
            -435,
            -4410,
            -1006,
            -2728,
            -298,
            -314,
            -539,
            -700,
            -645,
            -605,
            -1054
        ],
        "steps_history": [
            4410,
            4410,
            3675,
            4410,
            1033,
            2115,
            1267,
            4410,
            4410,
            797,
            3912,
            4410,
            862,
            2728,
            859,
            4410,
            3361,
            582,
            503,
            417,
            536,
            4410,
            1107,
            2829,
            399,
            415,
            640,
            801,
            746,
            706,
            1155
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "718/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.539860010147095,
        "final_policy_stability": 0.8956916099773242,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.8435374149659864,
            0.8321995464852607,
            0.8571428571428571,
            0.8662131519274376,
            0.8888888888888888,
            0.9501133786848073,
            0.9024943310657596,
            0.9591836734693877,
            0.8344671201814059,
            0.9614512471655329,
            0.8662131519274376,
            0.9523809523809523,
            0.8616780045351474,
            0.9750566893424036,
            0.909297052154195,
            0.9591836734693877,
            0.9002267573696145,
            0.8820861678004536,
            0.9501133786848073,
            0.9410430839002267,
            0.9501133786848073,
            0.9863945578231292,
            0.9886621315192744,
            0.9886621315192744,
            0.9478458049886621,
            0.9795918367346939,
            0.9002267573696145,
            0.9569160997732427,
            0.9886621315192744,
            0.9909297052154195,
            0.9886621315192744,
            1.0,
            0.9977324263038548,
            0.9931972789115646,
            0.8956916099773242
        ],
        "reward_history": [
            -4410,
            -4410,
            -4222,
            -4410,
            -1930,
            -2249,
            -610,
            -2008,
            -503,
            -4410,
            -217,
            -4410,
            -804,
            -4410,
            -290,
            -3152,
            -427,
            -2542,
            -4410,
            -562,
            -947,
            -1338,
            -199,
            -256,
            -133,
            -985,
            -336,
            -4410,
            -949,
            -777,
            -144,
            -388,
            -369,
            -370,
            -680,
            -2985
        ],
        "steps_history": [
            4410,
            4410,
            4323,
            4410,
            2031,
            2350,
            711,
            2109,
            604,
            4410,
            318,
            4410,
            905,
            4410,
            391,
            3253,
            528,
            2643,
            4410,
            663,
            1048,
            1439,
            300,
            357,
            234,
            1086,
            437,
            4410,
            1050,
            878,
            245,
            489,
            470,
            471,
            781,
            3086
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "719/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 10,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 36.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.211266756057739,
        "final_policy_stability": 0.873015873015873,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.8526077097505669,
            0.9591836734693877,
            0.8684807256235828,
            0.8866213151927438,
            0.9070294784580499,
            0.9319727891156463,
            0.9297052154195011,
            0.8934240362811792,
            0.8979591836734694,
            0.9705215419501134,
            0.9455782312925171,
            0.9795918367346939,
            0.8866213151927438,
            0.9455782312925171,
            0.9591836734693877,
            0.9387755102040817,
            0.909297052154195,
            0.8503401360544217,
            0.8321995464852607,
            0.8639455782312925,
            0.891156462585034,
            0.9750566893424036,
            0.9863945578231292,
            0.963718820861678,
            0.9886621315192744,
            0.9024943310657596,
            0.9909297052154195,
            0.9977324263038548,
            0.9954648526077098,
            0.9909297052154195,
            0.9909297052154195,
            0.8934240362811792,
            0.9206349206349206,
            1.0,
            0.873015873015873
        ],
        "reward_history": [
            -4410,
            -4410,
            -3039,
            -466,
            -3598,
            -4410,
            -1463,
            -687,
            -769,
            -4410,
            -4410,
            -300,
            -742,
            -304,
            -4410,
            -844,
            -483,
            -587,
            -4410,
            -4095,
            -4410,
            -4410,
            -1573,
            -1031,
            -409,
            -817,
            -486,
            -1878,
            -222,
            -294,
            -414,
            -363,
            -504,
            -3893,
            -4410,
            -391,
            -4410
        ],
        "steps_history": [
            4410,
            4410,
            3140,
            567,
            3699,
            4410,
            1564,
            788,
            870,
            4410,
            4410,
            401,
            843,
            405,
            4410,
            945,
            584,
            688,
            4410,
            4196,
            4410,
            4410,
            1674,
            1132,
            510,
            918,
            587,
            1979,
            323,
            395,
            515,
            464,
            605,
            3994,
            4410,
            492,
            4410
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "720/810",
        "save_path": "experiments/20250131_160708/training_plots/size_10/lr0.4_df0.99_eps0.1_trial4"
    }
]