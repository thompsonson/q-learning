[
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.233167886734009,
        "final_policy_stability": 0.9866666666666667,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8977777777777778,
            0.8088888888888889,
            0.8622222222222222,
            0.9022222222222223,
            0.8266666666666667,
            0.8622222222222222,
            0.8888888888888888,
            0.9111111111111111,
            0.88,
            0.8888888888888888,
            0.9466666666666667,
            0.9644444444444444,
            0.88,
            0.9555555555555556,
            0.9511111111111111,
            0.8977777777777778,
            0.9066666666666666,
            0.9466666666666667,
            0.9555555555555556,
            0.9644444444444444,
            0.9911111111111112,
            0.96,
            0.9822222222222222,
            0.9688888888888889,
            1.0,
            0.9866666666666667
        ],
        "reward_history": [
            -2250,
            -1122,
            -2250,
            -2250,
            -644,
            -2086,
            -2014,
            -1593,
            -875,
            -1416,
            -1426,
            -784,
            -226,
            -2250,
            -715,
            -375,
            -2250,
            -1594,
            -1177,
            -925,
            -2250,
            -138,
            -1210,
            -818,
            -726,
            -284,
            -585
        ],
        "steps_history": [
            2250,
            1223,
            2250,
            2250,
            745,
            2187,
            2115,
            1694,
            976,
            1517,
            1527,
            885,
            327,
            2250,
            816,
            476,
            2250,
            1695,
            1278,
            1026,
            2250,
            239,
            1311,
            919,
            827,
            385,
            686
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "361/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.279400110244751,
        "final_policy_stability": 0.9288888888888889,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.8488888888888889,
            0.9377777777777778,
            0.8488888888888889,
            0.8266666666666667,
            0.8622222222222222,
            0.8577777777777778,
            0.9333333333333333,
            0.9377777777777778,
            0.8755555555555555,
            0.9511111111111111,
            0.9511111111111111,
            0.9688888888888889,
            0.9511111111111111,
            0.9555555555555556,
            0.9066666666666666,
            0.9111111111111111,
            0.8888888888888888,
            0.8755555555555555,
            0.9822222222222222,
            0.9555555555555556,
            0.9733333333333334,
            0.9733333333333334,
            0.9733333333333334,
            0.9688888888888889,
            0.9555555555555556,
            0.9155555555555556,
            0.9955555555555555,
            0.9644444444444444,
            0.9288888888888889
        ],
        "reward_history": [
            -2250,
            -947,
            -158,
            -1603,
            -2250,
            -2250,
            -2250,
            -303,
            -645,
            -2250,
            -354,
            -582,
            -265,
            -310,
            -399,
            -855,
            -1291,
            -2250,
            -2250,
            -137,
            -1097,
            -442,
            -95,
            -353,
            -528,
            -1183,
            -2250,
            -326,
            -1410,
            -2061
        ],
        "steps_history": [
            2250,
            1048,
            259,
            1704,
            2250,
            2250,
            2250,
            404,
            746,
            2250,
            455,
            683,
            366,
            411,
            500,
            956,
            1392,
            2250,
            2250,
            238,
            1198,
            543,
            196,
            454,
            629,
            1284,
            2250,
            427,
            1511,
            2162
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "362/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.081398248672485,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.9377777777777778,
            0.8,
            0.9022222222222223,
            0.8755555555555555,
            0.8888888888888888,
            0.9422222222222222,
            0.8266666666666667,
            0.9377777777777778,
            0.8888888888888888,
            0.9555555555555556,
            0.8577777777777778,
            0.8711111111111111,
            0.9377777777777778,
            0.9022222222222223,
            0.9822222222222222,
            0.8755555555555555,
            0.8933333333333333,
            0.9466666666666667,
            0.8888888888888888,
            0.9244444444444444,
            0.96,
            0.9422222222222222,
            0.9377777777777778,
            0.9777777777777777,
            0.8977777777777778,
            0.9955555555555555,
            0.8933333333333333,
            0.9644444444444444,
            0.9955555555555555,
            0.9733333333333334,
            1.0,
            0.9688888888888889,
            0.9955555555555555
        ],
        "reward_history": [
            -543,
            -2250,
            -2250,
            -725,
            -965,
            -1934,
            -399,
            -2250,
            -398,
            -2250,
            -309,
            -2250,
            -2250,
            -1030,
            -1385,
            -100,
            -2250,
            -2250,
            -545,
            -1513,
            -1272,
            -338,
            -1473,
            -1008,
            -250,
            -1569,
            -116,
            -2250,
            -1086,
            -277,
            -522,
            -195,
            -829,
            -279
        ],
        "steps_history": [
            644,
            2250,
            2250,
            826,
            1066,
            2035,
            500,
            2250,
            499,
            2250,
            410,
            2250,
            2250,
            1131,
            1486,
            201,
            2250,
            2250,
            646,
            1614,
            1373,
            439,
            1574,
            1109,
            351,
            1670,
            217,
            2250,
            1187,
            378,
            623,
            296,
            930,
            380
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "363/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.0892486572265625,
        "final_policy_stability": 0.9866666666666667,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8888888888888888,
            0.7955555555555556,
            0.88,
            0.8577777777777778,
            0.9377777777777778,
            0.9111111111111111,
            0.9066666666666666,
            0.9466666666666667,
            0.8622222222222222,
            0.9022222222222223,
            0.9733333333333334,
            0.8755555555555555,
            0.92,
            0.9155555555555556,
            0.9155555555555556,
            0.9555555555555556,
            0.9555555555555556,
            0.8933333333333333,
            0.8977777777777778,
            0.9244444444444444,
            0.9644444444444444,
            0.9377777777777778,
            0.9555555555555556,
            0.9733333333333334,
            0.9288888888888889,
            0.9555555555555556,
            0.9688888888888889,
            0.9644444444444444,
            0.9955555555555555,
            1.0,
            0.9644444444444444,
            0.9866666666666667
        ],
        "reward_history": [
            -1378,
            -307,
            -2250,
            -881,
            -2250,
            -310,
            -1146,
            -811,
            -290,
            -2250,
            -1361,
            -131,
            -1227,
            -1546,
            -738,
            -1540,
            -378,
            -497,
            -1512,
            -1454,
            -668,
            -683,
            -1047,
            -646,
            -484,
            -933,
            -581,
            -240,
            -823,
            -637,
            -280,
            -929,
            -333
        ],
        "steps_history": [
            1479,
            408,
            2250,
            982,
            2250,
            411,
            1247,
            912,
            391,
            2250,
            1462,
            232,
            1328,
            1647,
            839,
            1641,
            479,
            598,
            1613,
            1555,
            769,
            784,
            1148,
            747,
            585,
            1034,
            682,
            341,
            924,
            738,
            381,
            1030,
            434
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "364/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.660959243774414,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.84,
            0.8977777777777778,
            0.8044444444444444,
            0.8488888888888889,
            0.8177777777777778,
            0.8266666666666667,
            0.8444444444444444,
            0.8622222222222222,
            0.9155555555555556,
            0.9644444444444444,
            0.8933333333333333,
            0.96,
            0.9066666666666666,
            0.9466666666666667,
            0.9733333333333334,
            0.9377777777777778,
            0.9555555555555556,
            0.9244444444444444,
            0.9422222222222222,
            0.9777777777777777,
            0.8711111111111111,
            0.9866666666666667,
            0.8844444444444445,
            0.9955555555555555,
            1.0,
            0.9777777777777777,
            0.9955555555555555,
            0.9555555555555556,
            0.9644444444444444,
            0.9955555555555555,
            0.9911111111111112,
            0.9644444444444444,
            0.9911111111111112,
            1.0,
            1.0,
            0.9911111111111112
        ],
        "reward_history": [
            -2250,
            -2250,
            -429,
            -1852,
            -1131,
            -2250,
            -1576,
            -2125,
            -2250,
            -814,
            -481,
            -1100,
            -135,
            -1667,
            -654,
            -338,
            -772,
            -886,
            -1613,
            -1107,
            -258,
            -2250,
            -129,
            -2250,
            -57,
            -507,
            -448,
            -237,
            -716,
            -493,
            -631,
            -645,
            -691,
            -833,
            -429,
            -492,
            -483
        ],
        "steps_history": [
            2250,
            2250,
            530,
            1953,
            1232,
            2250,
            1677,
            2226,
            2250,
            915,
            582,
            1201,
            236,
            1768,
            755,
            439,
            873,
            987,
            1714,
            1208,
            359,
            2250,
            230,
            2250,
            158,
            608,
            549,
            338,
            817,
            594,
            732,
            746,
            792,
            934,
            530,
            593,
            584
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "365/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.73235297203064,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.8711111111111111,
            0.8177777777777778,
            0.9066666666666666,
            0.8577777777777778,
            0.8355555555555556,
            0.9333333333333333,
            0.8888888888888888,
            0.8888888888888888,
            0.8311111111111111,
            0.8444444444444444,
            0.96,
            0.9777777777777777,
            0.92,
            0.9511111111111111,
            0.8266666666666667,
            0.9066666666666666,
            0.9733333333333334,
            0.9733333333333334,
            0.9244444444444444,
            0.9777777777777777,
            0.92,
            0.9955555555555555,
            0.9466666666666667,
            0.9733333333333334,
            0.9911111111111112,
            0.9733333333333334,
            0.9377777777777778,
            0.9466666666666667,
            1.0,
            0.9688888888888889,
            0.9555555555555556,
            0.9244444444444444,
            0.9911111111111112,
            0.9733333333333334,
            1.0
        ],
        "reward_history": [
            -832,
            -1162,
            -1626,
            -369,
            -2250,
            -1784,
            -2250,
            -853,
            -1041,
            -1995,
            -2250,
            -453,
            -84,
            -703,
            -329,
            -2250,
            -1354,
            -169,
            -406,
            -869,
            -123,
            -974,
            -375,
            -690,
            -526,
            -168,
            -698,
            -475,
            -779,
            -290,
            -627,
            -473,
            -745,
            -362,
            -486,
            -270
        ],
        "steps_history": [
            933,
            1263,
            1727,
            470,
            2250,
            1885,
            2250,
            954,
            1142,
            2096,
            2250,
            554,
            185,
            804,
            430,
            2250,
            1455,
            270,
            507,
            970,
            224,
            1075,
            476,
            791,
            627,
            269,
            799,
            576,
            880,
            391,
            728,
            574,
            846,
            463,
            587,
            371
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "366/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.860852241516113,
        "final_policy_stability": 0.9866666666666667,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.84,
            0.9422222222222222,
            0.8844444444444445,
            0.9111111111111111,
            0.8666666666666667,
            0.8666666666666667,
            0.9111111111111111,
            0.8977777777777778,
            0.88,
            0.8977777777777778,
            0.9555555555555556,
            0.84,
            0.9288888888888889,
            0.8755555555555555,
            0.8533333333333334,
            0.9333333333333333,
            0.92,
            0.9422222222222222,
            0.9644444444444444,
            0.9911111111111112,
            0.9688888888888889,
            0.88,
            0.9377777777777778,
            0.9955555555555555,
            0.9022222222222223,
            0.9422222222222222,
            0.9777777777777777,
            0.9644444444444444,
            0.9911111111111112,
            0.9688888888888889,
            0.9822222222222222,
            0.9866666666666667
        ],
        "reward_history": [
            -2250,
            -1905,
            -391,
            -877,
            -293,
            -1821,
            -1799,
            -455,
            -852,
            -2250,
            -1006,
            -1015,
            -2250,
            -565,
            -1202,
            -1470,
            -714,
            -2067,
            -1269,
            -697,
            -23,
            -164,
            -1498,
            -1296,
            -179,
            -2250,
            -824,
            -740,
            -622,
            -238,
            -530,
            -378,
            -445
        ],
        "steps_history": [
            2250,
            2006,
            492,
            978,
            394,
            1922,
            1900,
            556,
            953,
            2250,
            1107,
            1116,
            2250,
            666,
            1303,
            1571,
            815,
            2168,
            1370,
            798,
            124,
            265,
            1599,
            1397,
            280,
            2250,
            925,
            841,
            723,
            339,
            631,
            479,
            546
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "367/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.723809242248535,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8355555555555556,
            0.7555555555555555,
            0.8622222222222222,
            0.8355555555555556,
            0.8577777777777778,
            0.8177777777777778,
            0.84,
            0.92,
            0.9422222222222222,
            0.8222222222222222,
            0.9555555555555556,
            0.9333333333333333,
            0.9244444444444444,
            0.9466666666666667,
            0.9333333333333333,
            0.8844444444444445,
            0.8622222222222222,
            0.96,
            0.8977777777777778,
            0.9688888888888889,
            0.9955555555555555,
            0.9733333333333334,
            0.8977777777777778,
            0.8888888888888888,
            0.9822222222222222,
            0.9866666666666667,
            0.9822222222222222,
            0.9022222222222223,
            1.0,
            0.9911111111111112,
            1.0,
            0.9866666666666667,
            0.9511111111111111,
            1.0,
            0.9822222222222222,
            0.9777777777777777,
            0.9955555555555555,
            0.9911111111111112,
            0.96,
            1.0
        ],
        "reward_history": [
            -549,
            -2250,
            -2250,
            -2250,
            -2250,
            -875,
            -1688,
            -1917,
            -528,
            -186,
            -2250,
            -409,
            -366,
            -715,
            -831,
            -635,
            -913,
            -1672,
            -464,
            -1109,
            -378,
            20,
            -242,
            -1836,
            -1813,
            -240,
            -223,
            -257,
            -2250,
            -229,
            -147,
            -181,
            -186,
            -1249,
            -360,
            -234,
            -277,
            -214,
            -465,
            -1057,
            -1032
        ],
        "steps_history": [
            650,
            2250,
            2250,
            2250,
            2250,
            976,
            1789,
            2018,
            629,
            287,
            2250,
            510,
            467,
            816,
            932,
            736,
            1014,
            1773,
            565,
            1210,
            479,
            81,
            343,
            1937,
            1914,
            341,
            324,
            358,
            2250,
            330,
            248,
            282,
            287,
            1350,
            461,
            335,
            378,
            315,
            566,
            1158,
            1133
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "368/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.215240478515625,
        "final_policy_stability": 0.9866666666666667,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.88,
            0.7777777777777778,
            0.8177777777777778,
            0.8533333333333334,
            0.88,
            0.8222222222222222,
            0.9111111111111111,
            0.9244444444444444,
            0.9155555555555556,
            0.8133333333333334,
            0.8977777777777778,
            0.9155555555555556,
            0.9377777777777778,
            0.9822222222222222,
            0.9377777777777778,
            0.96,
            0.9333333333333333,
            0.9555555555555556,
            0.9733333333333334,
            0.9733333333333334,
            0.9911111111111112,
            0.9733333333333334,
            0.9777777777777777,
            0.9822222222222222,
            0.9377777777777778,
            0.9866666666666667,
            1.0,
            0.9866666666666667,
            0.9866666666666667
        ],
        "reward_history": [
            -1941,
            -1165,
            -2250,
            -2250,
            -2250,
            -1126,
            -1558,
            -490,
            -774,
            -768,
            -2250,
            -997,
            -1252,
            -584,
            -210,
            -537,
            -272,
            -509,
            -447,
            -529,
            -283,
            -170,
            -271,
            -487,
            -532,
            -1389,
            -460,
            -396,
            -489,
            -685
        ],
        "steps_history": [
            2042,
            1266,
            2250,
            2250,
            2250,
            1227,
            1659,
            591,
            875,
            869,
            2250,
            1098,
            1353,
            685,
            311,
            638,
            373,
            610,
            548,
            630,
            384,
            271,
            372,
            588,
            633,
            1490,
            561,
            497,
            590,
            786
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "369/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.775341033935547,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 38,
        "policy_stability_history": [
            0.0,
            0.8133333333333334,
            0.84,
            0.8444444444444444,
            0.8488888888888889,
            0.9466666666666667,
            0.8977777777777778,
            0.92,
            0.8844444444444445,
            0.8177777777777778,
            0.9688888888888889,
            0.8533333333333334,
            0.96,
            0.9422222222222222,
            0.9288888888888889,
            0.8977777777777778,
            0.96,
            0.9777777777777777,
            0.9333333333333333,
            0.9288888888888889,
            0.9511111111111111,
            0.9422222222222222,
            0.9822222222222222,
            1.0,
            0.9111111111111111,
            0.9555555555555556,
            1.0,
            0.9555555555555556,
            0.9555555555555556,
            0.9955555555555555,
            0.9244444444444444,
            0.9688888888888889,
            0.9422222222222222,
            0.9555555555555556,
            0.9955555555555555,
            0.9866666666666667,
            0.9777777777777777,
            1.0,
            1.0
        ],
        "reward_history": [
            -2250,
            -1435,
            -2250,
            -1524,
            -2250,
            -312,
            -584,
            -860,
            -759,
            -2250,
            -368,
            -2250,
            -366,
            -229,
            -1022,
            -1124,
            -375,
            -466,
            -653,
            -1186,
            -410,
            -1072,
            -255,
            -97,
            -739,
            -636,
            -125,
            -897,
            -290,
            -145,
            -2250,
            -321,
            -412,
            -786,
            -99,
            -308,
            -381,
            -591,
            -511
        ],
        "steps_history": [
            2250,
            1536,
            2250,
            1625,
            2250,
            413,
            685,
            961,
            860,
            2250,
            469,
            2250,
            467,
            330,
            1123,
            1225,
            476,
            567,
            754,
            1287,
            511,
            1173,
            356,
            198,
            840,
            737,
            226,
            998,
            391,
            246,
            2250,
            422,
            513,
            887,
            200,
            409,
            482,
            692,
            612
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "370/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.530960559844971,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 38,
        "policy_stability_history": [
            0.0,
            0.8355555555555556,
            0.8044444444444444,
            0.9022222222222223,
            0.8177777777777778,
            0.9377777777777778,
            0.84,
            0.8577777777777778,
            0.84,
            0.9333333333333333,
            0.96,
            0.96,
            0.9066666666666666,
            0.8888888888888888,
            0.84,
            0.9333333333333333,
            0.9511111111111111,
            0.9155555555555556,
            0.96,
            0.9377777777777778,
            0.9866666666666667,
            0.9777777777777777,
            0.9333333333333333,
            0.9777777777777777,
            0.9733333333333334,
            0.9555555555555556,
            0.9955555555555555,
            0.9866666666666667,
            0.9466666666666667,
            0.8444444444444444,
            0.8577777777777778,
            0.9866666666666667,
            0.9866666666666667,
            0.9955555555555555,
            0.96,
            0.9822222222222222,
            0.9822222222222222,
            1.0,
            0.9911111111111112
        ],
        "reward_history": [
            -865,
            -1228,
            -2250,
            -2250,
            -1630,
            -274,
            -2250,
            -2250,
            -1692,
            -318,
            -170,
            -161,
            -826,
            -930,
            -2250,
            -733,
            -347,
            -1108,
            -255,
            -574,
            -335,
            -356,
            -1268,
            -444,
            -350,
            -595,
            -118,
            -169,
            -777,
            -1582,
            -1906,
            -304,
            -387,
            -377,
            -473,
            -219,
            -1041,
            -43,
            -200
        ],
        "steps_history": [
            966,
            1329,
            2250,
            2250,
            1731,
            375,
            2250,
            2250,
            1793,
            419,
            271,
            262,
            927,
            1031,
            2250,
            834,
            448,
            1209,
            356,
            675,
            436,
            457,
            1369,
            545,
            451,
            696,
            219,
            270,
            878,
            1683,
            2007,
            405,
            488,
            478,
            574,
            320,
            1142,
            144,
            301
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "371/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.797461271286011,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.7866666666666666,
            0.8488888888888889,
            0.8444444444444444,
            0.8355555555555556,
            0.9066666666666666,
            0.9066666666666666,
            0.9155555555555556,
            0.9066666666666666,
            0.8177777777777778,
            0.9377777777777778,
            0.8711111111111111,
            0.8622222222222222,
            0.9244444444444444,
            0.8488888888888889,
            0.92,
            0.9288888888888889,
            0.9555555555555556,
            0.9644444444444444,
            0.9777777777777777,
            0.9555555555555556,
            0.9866666666666667,
            0.9555555555555556,
            0.9866666666666667,
            0.9866666666666667,
            0.9644444444444444,
            0.9733333333333334,
            0.8711111111111111,
            0.9688888888888889,
            0.8622222222222222,
            0.9422222222222222,
            0.9555555555555556,
            0.9955555555555555,
            0.9955555555555555,
            1.0,
            1.0,
            1.0,
            0.9955555555555555
        ],
        "reward_history": [
            -1271,
            -2250,
            -1039,
            -2250,
            -2250,
            -909,
            -1004,
            -673,
            -686,
            -2250,
            -592,
            -769,
            -795,
            -1348,
            -2250,
            -525,
            -1463,
            -457,
            -349,
            -207,
            -797,
            -330,
            -587,
            -186,
            -165,
            -422,
            -233,
            -2250,
            -259,
            -2132,
            -1210,
            -558,
            -88,
            -233,
            -104,
            -125,
            -272,
            -302
        ],
        "steps_history": [
            1372,
            2250,
            1140,
            2250,
            2250,
            1010,
            1105,
            774,
            787,
            2250,
            693,
            870,
            896,
            1449,
            2250,
            626,
            1564,
            558,
            450,
            308,
            898,
            431,
            688,
            287,
            266,
            523,
            334,
            2250,
            360,
            2233,
            1311,
            659,
            189,
            334,
            205,
            226,
            373,
            403
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "372/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.3655641078948975,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 43,
        "policy_stability_history": [
            0.0,
            0.7555555555555555,
            0.8266666666666667,
            0.8533333333333334,
            0.8844444444444445,
            0.8088888888888889,
            0.9377777777777778,
            0.9155555555555556,
            0.7911111111111111,
            0.9555555555555556,
            0.9644444444444444,
            0.9644444444444444,
            0.8488888888888889,
            0.9511111111111111,
            0.9466666666666667,
            0.9688888888888889,
            0.9422222222222222,
            0.9244444444444444,
            0.9155555555555556,
            0.9333333333333333,
            0.9022222222222223,
            0.8933333333333333,
            0.8577777777777778,
            0.9911111111111112,
            0.9688888888888889,
            0.9377777777777778,
            0.9733333333333334,
            0.9555555555555556,
            0.9866666666666667,
            0.9822222222222222,
            0.9111111111111111,
            0.9777777777777777,
            0.9644444444444444,
            0.9911111111111112,
            0.9333333333333333,
            0.9511111111111111,
            0.9688888888888889,
            0.9688888888888889,
            0.9955555555555555,
            0.9644444444444444,
            1.0,
            0.9733333333333334,
            0.9955555555555555,
            0.9955555555555555
        ],
        "reward_history": [
            -1795,
            -2250,
            -1598,
            -1822,
            -994,
            -2250,
            -503,
            -753,
            -1970,
            -397,
            -209,
            -168,
            -2250,
            -335,
            -480,
            -159,
            -173,
            -352,
            -778,
            -607,
            -862,
            -1386,
            -2250,
            21,
            -492,
            -623,
            -276,
            -408,
            -461,
            -89,
            -1436,
            -205,
            -311,
            -203,
            -2250,
            -556,
            -632,
            -292,
            -95,
            -199,
            -290,
            -455,
            -130,
            -214
        ],
        "steps_history": [
            1896,
            2250,
            1699,
            1923,
            1095,
            2250,
            604,
            854,
            2071,
            498,
            310,
            269,
            2250,
            436,
            581,
            260,
            274,
            453,
            879,
            708,
            963,
            1487,
            2250,
            80,
            593,
            724,
            377,
            509,
            562,
            190,
            1537,
            306,
            412,
            304,
            2250,
            657,
            733,
            393,
            196,
            300,
            391,
            556,
            231,
            315
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "373/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.515939474105835,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8444444444444444,
            0.9111111111111111,
            0.88,
            0.8755555555555555,
            0.8222222222222222,
            0.8444444444444444,
            0.8133333333333334,
            0.9288888888888889,
            0.9466666666666667,
            0.8488888888888889,
            0.9466666666666667,
            0.8222222222222222,
            0.9288888888888889,
            0.9377777777777778,
            0.92,
            0.8755555555555555,
            0.9866666666666667,
            0.96,
            0.9511111111111111,
            0.8711111111111111,
            0.9333333333333333,
            0.96,
            0.9688888888888889,
            0.9333333333333333,
            0.96,
            0.9911111111111112,
            0.9955555555555555,
            0.9911111111111112,
            0.9866666666666667,
            0.9777777777777777,
            0.9466666666666667,
            1.0,
            0.9911111111111112,
            0.9066666666666666,
            0.9911111111111112,
            0.9955555555555555,
            0.9955555555555555,
            1.0,
            0.9866666666666667,
            0.9955555555555555
        ],
        "reward_history": [
            -2250,
            -1543,
            -590,
            -1130,
            -1469,
            -2250,
            -1330,
            -2250,
            -307,
            -270,
            -1146,
            -257,
            -2063,
            -538,
            -508,
            -972,
            -1136,
            -131,
            -368,
            -515,
            -1469,
            -532,
            -143,
            -166,
            -1139,
            -408,
            -84,
            -316,
            -72,
            -180,
            -389,
            -318,
            -203,
            -211,
            -2250,
            -283,
            -503,
            -359,
            -83,
            -116,
            -119
        ],
        "steps_history": [
            2250,
            1644,
            691,
            1231,
            1570,
            2250,
            1431,
            2250,
            408,
            371,
            1247,
            358,
            2164,
            639,
            609,
            1073,
            1237,
            232,
            469,
            616,
            1570,
            633,
            244,
            267,
            1240,
            509,
            185,
            417,
            173,
            281,
            490,
            419,
            304,
            312,
            2250,
            384,
            604,
            460,
            184,
            217,
            220
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "374/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.017452716827393,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8222222222222222,
            0.8844444444444445,
            0.8177777777777778,
            0.8666666666666667,
            0.8311111111111111,
            0.9066666666666666,
            0.8444444444444444,
            0.8888888888888888,
            0.8622222222222222,
            0.9733333333333334,
            0.9422222222222222,
            0.8622222222222222,
            0.9066666666666666,
            0.9066666666666666,
            0.9422222222222222,
            0.9511111111111111,
            0.9244444444444444,
            0.9466666666666667,
            0.9644444444444444,
            0.9688888888888889,
            0.9066666666666666,
            0.8311111111111111,
            0.9733333333333334,
            0.96,
            0.9777777777777777,
            0.9733333333333334,
            1.0,
            0.9955555555555555,
            0.9733333333333334,
            0.9688888888888889,
            0.9911111111111112,
            0.9911111111111112,
            0.9955555555555555,
            0.9911111111111112
        ],
        "reward_history": [
            -2250,
            -1164,
            -808,
            -2250,
            -2250,
            -2250,
            -645,
            -2250,
            -1220,
            -2250,
            -144,
            -243,
            -2250,
            -587,
            -1146,
            -288,
            -453,
            -574,
            -478,
            -487,
            -284,
            -665,
            -2026,
            -307,
            -338,
            -222,
            -197,
            -24,
            -112,
            -301,
            -664,
            -396,
            -163,
            -171,
            -152
        ],
        "steps_history": [
            2250,
            1265,
            909,
            2250,
            2250,
            2250,
            746,
            2250,
            1321,
            2250,
            245,
            344,
            2250,
            688,
            1247,
            389,
            554,
            675,
            579,
            588,
            385,
            766,
            2127,
            408,
            439,
            323,
            298,
            125,
            213,
            402,
            765,
            497,
            264,
            272,
            253
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "375/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.9995644092559814,
        "final_policy_stability": 0.9822222222222222,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8977777777777778,
            0.8088888888888889,
            0.8622222222222222,
            0.9022222222222223,
            0.8311111111111111,
            0.8622222222222222,
            0.8844444444444445,
            0.9155555555555556,
            0.9511111111111111,
            0.9733333333333334,
            0.9022222222222223,
            0.9422222222222222,
            0.88,
            0.9377777777777778,
            0.92,
            0.9644444444444444,
            0.9422222222222222,
            0.8933333333333333,
            0.92,
            0.9866666666666667,
            0.9644444444444444,
            0.9822222222222222,
            0.9777777777777777,
            0.9555555555555556,
            0.9955555555555555,
            0.9777777777777777,
            0.9555555555555556,
            0.9866666666666667,
            0.9777777777777777,
            1.0,
            0.9822222222222222
        ],
        "reward_history": [
            -2250,
            -1122,
            -2250,
            -2250,
            -644,
            -2086,
            -2014,
            -1593,
            -875,
            -941,
            -242,
            -1558,
            -1264,
            -2250,
            -1038,
            -968,
            -180,
            -553,
            -2250,
            -1665,
            -128,
            -711,
            -1106,
            -369,
            -1054,
            -293,
            -580,
            -1111,
            -565,
            -529,
            -452,
            -357
        ],
        "steps_history": [
            2250,
            1223,
            2250,
            2250,
            745,
            2187,
            2115,
            1694,
            976,
            1042,
            343,
            1659,
            1365,
            2250,
            1139,
            1069,
            281,
            654,
            2250,
            1766,
            229,
            812,
            1207,
            470,
            1155,
            394,
            681,
            1212,
            666,
            630,
            553,
            458
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "376/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.446413993835449,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.8488888888888889,
            0.9377777777777778,
            0.8488888888888889,
            0.8222222222222222,
            0.8622222222222222,
            0.8888888888888888,
            0.9688888888888889,
            0.9777777777777777,
            0.8977777777777778,
            0.9466666666666667,
            0.9422222222222222,
            0.9022222222222223,
            0.9155555555555556,
            0.9111111111111111,
            0.8533333333333334,
            0.8355555555555556,
            0.9555555555555556,
            0.9644444444444444,
            0.9822222222222222,
            0.9688888888888889,
            0.9688888888888889,
            0.96,
            0.8755555555555555,
            0.9955555555555555,
            0.9955555555555555
        ],
        "reward_history": [
            -2250,
            -947,
            -158,
            -1603,
            -2250,
            -2250,
            -2250,
            -303,
            -222,
            -2250,
            -676,
            -683,
            -1252,
            -779,
            -1112,
            -2250,
            -2250,
            -1439,
            -517,
            -95,
            -353,
            -528,
            -1183,
            -2250,
            -326,
            -676
        ],
        "steps_history": [
            2250,
            1048,
            259,
            1704,
            2250,
            2250,
            2250,
            404,
            323,
            2250,
            777,
            784,
            1353,
            880,
            1213,
            2250,
            2250,
            1540,
            618,
            196,
            454,
            629,
            1284,
            2250,
            427,
            777
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "377/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.204329013824463,
        "final_policy_stability": 0.9733333333333334,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.9377777777777778,
            0.8,
            0.9022222222222223,
            0.8755555555555555,
            0.8888888888888888,
            0.9422222222222222,
            0.8622222222222222,
            0.9066666666666666,
            0.84,
            0.92,
            0.8488888888888889,
            0.8577777777777778,
            0.84,
            0.8488888888888889,
            0.96,
            0.9155555555555556,
            0.9422222222222222,
            0.8977777777777778,
            0.9288888888888889,
            0.92,
            0.96,
            0.9733333333333334,
            0.9066666666666666,
            0.9911111111111112,
            0.8933333333333333,
            0.9644444444444444,
            0.9955555555555555,
            0.9688888888888889,
            0.9911111111111112,
            0.9733333333333334,
            1.0,
            0.9777777777777777,
            1.0,
            0.9733333333333334
        ],
        "reward_history": [
            -543,
            -2250,
            -2250,
            -725,
            -965,
            -1934,
            -399,
            -1957,
            -590,
            -2250,
            -309,
            -2250,
            -2250,
            -2250,
            -2250,
            -525,
            -2250,
            -487,
            -1513,
            -1272,
            -2250,
            -771,
            -250,
            -1569,
            -116,
            -2250,
            -1086,
            -277,
            -522,
            -195,
            -829,
            -279,
            -684,
            -433,
            -445
        ],
        "steps_history": [
            644,
            2250,
            2250,
            826,
            1066,
            2035,
            500,
            2058,
            691,
            2250,
            410,
            2250,
            2250,
            2250,
            2250,
            626,
            2250,
            588,
            1614,
            1373,
            2250,
            872,
            351,
            1670,
            217,
            2250,
            1187,
            378,
            623,
            296,
            930,
            380,
            785,
            534,
            546
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "378/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.564985990524292,
        "final_policy_stability": 0.9022222222222223,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.8844444444444445,
            0.7733333333333333,
            0.88,
            0.8755555555555555,
            0.8666666666666667,
            0.8444444444444444,
            0.9022222222222223,
            0.8577777777777778,
            0.8844444444444445,
            0.8888888888888888,
            0.9644444444444444,
            0.9422222222222222,
            0.9511111111111111,
            0.9466666666666667,
            0.92,
            0.92,
            0.9688888888888889,
            0.9688888888888889,
            0.8488888888888889,
            0.9822222222222222,
            0.9377777777777778,
            0.9733333333333334,
            0.96,
            0.8977777777777778,
            0.9022222222222223
        ],
        "reward_history": [
            -1378,
            -307,
            -2250,
            -881,
            -2250,
            -1557,
            -2250,
            -2049,
            -2250,
            -2250,
            -2250,
            -254,
            -572,
            -388,
            -624,
            -1454,
            -668,
            -683,
            -234,
            -2250,
            -734,
            -941,
            -898,
            -1018,
            -1363,
            -2020
        ],
        "steps_history": [
            1479,
            408,
            2250,
            982,
            2250,
            1658,
            2250,
            2150,
            2250,
            2250,
            2250,
            355,
            673,
            489,
            725,
            1555,
            769,
            784,
            335,
            2250,
            835,
            1042,
            999,
            1119,
            1464,
            2121
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "379/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.956042528152466,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.84,
            0.8977777777777778,
            0.8044444444444444,
            0.8488888888888889,
            0.8177777777777778,
            0.8266666666666667,
            0.8444444444444444,
            0.8622222222222222,
            0.9155555555555556,
            0.9644444444444444,
            0.8933333333333333,
            0.96,
            0.9066666666666666,
            0.9466666666666667,
            0.9733333333333334,
            0.9377777777777778,
            0.9555555555555556,
            0.9244444444444444,
            0.9422222222222222,
            0.9777777777777777,
            0.9555555555555556,
            0.9022222222222223,
            0.8844444444444445,
            0.8933333333333333,
            0.9866666666666667,
            0.96,
            0.9644444444444444,
            0.9866666666666667,
            0.9688888888888889,
            1.0,
            1.0,
            0.9911111111111112
        ],
        "reward_history": [
            -2250,
            -2250,
            -429,
            -1852,
            -1131,
            -2250,
            -1576,
            -2125,
            -2250,
            -814,
            -481,
            -1100,
            -135,
            -1667,
            -654,
            -338,
            -772,
            -886,
            -1613,
            -1107,
            -258,
            -356,
            -2250,
            -2007,
            -2250,
            -470,
            -789,
            -645,
            -471,
            -1053,
            -429,
            -492,
            -483
        ],
        "steps_history": [
            2250,
            2250,
            530,
            1953,
            1232,
            2250,
            1677,
            2226,
            2250,
            915,
            582,
            1201,
            236,
            1768,
            755,
            439,
            873,
            987,
            1714,
            1208,
            359,
            457,
            2250,
            2108,
            2250,
            571,
            890,
            746,
            572,
            1154,
            530,
            593,
            584
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "380/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.95177435874939,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8711111111111111,
            0.8133333333333334,
            0.9066666666666666,
            0.8577777777777778,
            0.8488888888888889,
            0.8977777777777778,
            0.8888888888888888,
            0.9244444444444444,
            0.9066666666666666,
            0.96,
            0.9511111111111111,
            0.96,
            0.9377777777777778,
            0.8622222222222222,
            0.9822222222222222,
            0.8622222222222222,
            0.9777777777777777,
            0.8666666666666667,
            0.9822222222222222,
            0.9866666666666667,
            0.8977777777777778,
            0.9688888888888889,
            0.9955555555555555,
            0.8755555555555555,
            0.9777777777777777,
            0.9511111111111111,
            0.9955555555555555,
            0.9644444444444444,
            0.9822222222222222,
            0.9688888888888889,
            0.9866666666666667,
            0.9955555555555555
        ],
        "reward_history": [
            -832,
            -1162,
            -1626,
            -369,
            -2250,
            -2250,
            -2250,
            -953,
            -484,
            -1178,
            -184,
            -523,
            -377,
            -569,
            -1555,
            -84,
            -2052,
            -364,
            -2250,
            -39,
            -154,
            -1546,
            -974,
            -375,
            -2250,
            -135,
            -1746,
            -627,
            -473,
            -267,
            -377,
            -362,
            -158
        ],
        "steps_history": [
            933,
            1263,
            1727,
            470,
            2250,
            2250,
            2250,
            1054,
            585,
            1279,
            285,
            624,
            478,
            670,
            1656,
            185,
            2153,
            465,
            2250,
            140,
            255,
            1647,
            1075,
            476,
            2250,
            236,
            1847,
            728,
            574,
            368,
            478,
            463,
            259
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "381/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.44831109046936,
        "final_policy_stability": 0.9644444444444444,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.84,
            0.88,
            0.8622222222222222,
            0.8311111111111111,
            0.8311111111111111,
            0.8666666666666667,
            0.8355555555555556,
            0.9244444444444444,
            0.9466666666666667,
            0.8711111111111111,
            0.92,
            0.9022222222222223,
            0.9733333333333334,
            0.9288888888888889,
            0.84,
            0.9377777777777778,
            0.9688888888888889,
            0.9777777777777777,
            0.9777777777777777,
            0.8755555555555555,
            0.9111111111111111,
            0.8844444444444445,
            0.9688888888888889,
            0.9644444444444444,
            0.9644444444444444
        ],
        "reward_history": [
            -2250,
            -1905,
            -947,
            -1418,
            -2250,
            -2250,
            -2250,
            -2096,
            -549,
            -183,
            -1139,
            -795,
            -2250,
            -251,
            -986,
            -2250,
            -1187,
            -491,
            -229,
            -164,
            -2250,
            -1288,
            -2250,
            -581,
            -620,
            -787
        ],
        "steps_history": [
            2250,
            2006,
            1048,
            1519,
            2250,
            2250,
            2250,
            2197,
            650,
            284,
            1240,
            896,
            2250,
            352,
            1087,
            2250,
            1288,
            592,
            330,
            265,
            2250,
            1389,
            2250,
            682,
            721,
            888
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "382/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.810570955276489,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8355555555555556,
            0.7555555555555555,
            0.8622222222222222,
            0.84,
            0.8577777777777778,
            0.8177777777777778,
            0.8311111111111111,
            0.92,
            0.9377777777777778,
            0.8622222222222222,
            0.8577777777777778,
            0.8088888888888889,
            0.9066666666666666,
            0.9555555555555556,
            0.9244444444444444,
            0.9733333333333334,
            0.92,
            0.9111111111111111,
            0.9644444444444444,
            0.9333333333333333,
            0.9155555555555556,
            0.9822222222222222,
            0.9822222222222222,
            0.9422222222222222,
            0.9777777777777777,
            0.9911111111111112,
            0.9822222222222222,
            0.9866666666666667,
            0.9377777777777778,
            0.9866666666666667,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -549,
            -2250,
            -2250,
            -2250,
            -2250,
            -875,
            -1688,
            -1917,
            -528,
            -186,
            -1494,
            -2250,
            -1971,
            -836,
            -270,
            -1172,
            -209,
            -893,
            -912,
            -260,
            -610,
            -1125,
            -349,
            -89,
            -1114,
            -299,
            -223,
            -257,
            -369,
            -1802,
            -192,
            -61,
            -181,
            -228
        ],
        "steps_history": [
            650,
            2250,
            2250,
            2250,
            2250,
            976,
            1789,
            2018,
            629,
            287,
            1595,
            2250,
            2072,
            937,
            371,
            1273,
            310,
            994,
            1013,
            361,
            711,
            1226,
            450,
            190,
            1215,
            400,
            324,
            358,
            470,
            1903,
            293,
            162,
            282,
            329
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "383/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.926936149597168,
        "final_policy_stability": 0.9688888888888889,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.88,
            0.7777777777777778,
            0.8222222222222222,
            0.8533333333333334,
            0.8044444444444444,
            0.8844444444444445,
            0.8933333333333333,
            0.9244444444444444,
            0.84,
            0.9288888888888889,
            0.9422222222222222,
            0.9377777777777778,
            0.9688888888888889,
            0.9111111111111111,
            0.8977777777777778,
            0.96,
            0.9422222222222222,
            0.9688888888888889,
            0.96,
            0.9777777777777777,
            0.8888888888888888,
            0.9822222222222222,
            0.96,
            0.9555555555555556,
            0.9733333333333334,
            0.9822222222222222,
            0.9555555555555556,
            0.9555555555555556,
            0.9555555555555556,
            0.9955555555555555,
            0.9955555555555555,
            0.9733333333333334,
            0.9955555555555555,
            0.9688888888888889,
            0.9688888888888889
        ],
        "reward_history": [
            -1941,
            -1165,
            -2250,
            -2250,
            -2250,
            -2250,
            -368,
            -848,
            -583,
            -2250,
            -610,
            -684,
            -874,
            -135,
            -858,
            -1576,
            -493,
            -1077,
            -283,
            -542,
            -321,
            -1699,
            -89,
            -450,
            -705,
            -489,
            -685,
            -235,
            -836,
            -1015,
            -506,
            -328,
            -354,
            -310,
            -430,
            -676
        ],
        "steps_history": [
            2042,
            1266,
            2250,
            2250,
            2250,
            2250,
            469,
            949,
            684,
            2250,
            711,
            785,
            975,
            236,
            959,
            1677,
            594,
            1178,
            384,
            643,
            422,
            1800,
            190,
            551,
            806,
            590,
            786,
            336,
            937,
            1116,
            607,
            429,
            455,
            411,
            531,
            777
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "384/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.849457025527954,
        "final_policy_stability": 0.9555555555555556,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8088888888888889,
            0.9066666666666666,
            0.8533333333333334,
            0.8266666666666667,
            0.8711111111111111,
            0.9288888888888889,
            0.9422222222222222,
            0.9111111111111111,
            0.8844444444444445,
            0.9555555555555556,
            0.9333333333333333,
            0.8622222222222222,
            0.96,
            0.9511111111111111,
            0.8533333333333334,
            0.8488888888888889,
            0.9822222222222222,
            0.9377777777777778,
            0.9377777777777778,
            0.9511111111111111,
            0.9377777777777778,
            0.9644444444444444,
            0.9422222222222222,
            0.9555555555555556,
            0.9733333333333334,
            0.9911111111111112,
            0.9866666666666667,
            0.9644444444444444,
            1.0,
            0.9822222222222222,
            0.9955555555555555,
            0.9555555555555556
        ],
        "reward_history": [
            -2250,
            -1435,
            -2250,
            -2250,
            -1075,
            -1439,
            -496,
            -112,
            -1316,
            -998,
            -544,
            -589,
            -2250,
            -252,
            -229,
            -1219,
            -1403,
            -466,
            -561,
            -814,
            -874,
            -464,
            -739,
            -252,
            -708,
            -399,
            -362,
            -164,
            -632,
            -56,
            -261,
            -147,
            -405
        ],
        "steps_history": [
            2250,
            1536,
            2250,
            2250,
            1176,
            1540,
            597,
            213,
            1417,
            1099,
            645,
            690,
            2250,
            353,
            330,
            1320,
            1504,
            567,
            662,
            915,
            975,
            565,
            840,
            353,
            809,
            500,
            463,
            265,
            733,
            157,
            362,
            248,
            506
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "385/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.60405969619751,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8355555555555556,
            0.8044444444444444,
            0.8977777777777778,
            0.8088888888888889,
            0.92,
            0.8977777777777778,
            0.8266666666666667,
            0.9244444444444444,
            0.8177777777777778,
            0.92,
            0.92,
            0.92,
            0.9111111111111111,
            0.9511111111111111,
            0.9688888888888889,
            0.9733333333333334,
            0.8488888888888889,
            0.8977777777777778,
            0.9688888888888889,
            0.9777777777777777,
            0.92,
            0.9733333333333334,
            0.9822222222222222,
            0.9555555555555556,
            0.9911111111111112,
            0.8844444444444445,
            0.9911111111111112,
            0.9777777777777777,
            0.9333333333333333,
            0.9911111111111112,
            0.9866666666666667,
            0.9422222222222222,
            0.9955555555555555,
            0.9688888888888889,
            0.9866666666666667,
            0.9555555555555556,
            0.9911111111111112,
            0.9911111111111112,
            1.0,
            0.9955555555555555
        ],
        "reward_history": [
            -865,
            -1228,
            -2250,
            -2250,
            -1630,
            -435,
            -851,
            -2015,
            -629,
            -2095,
            -666,
            -665,
            -792,
            -521,
            -125,
            -113,
            -98,
            -2250,
            -2250,
            -128,
            -303,
            -792,
            -181,
            -323,
            -562,
            -444,
            -2250,
            -163,
            -482,
            -1562,
            -335,
            -301,
            -676,
            -133,
            -387,
            -377,
            -473,
            -219,
            -1041,
            -43,
            -200
        ],
        "steps_history": [
            966,
            1329,
            2250,
            2250,
            1731,
            536,
            952,
            2116,
            730,
            2196,
            767,
            766,
            893,
            622,
            226,
            214,
            199,
            2250,
            2250,
            229,
            404,
            893,
            282,
            424,
            663,
            545,
            2250,
            264,
            583,
            1663,
            436,
            402,
            777,
            234,
            488,
            478,
            574,
            320,
            1142,
            144,
            301
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "386/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.272754192352295,
        "final_policy_stability": 0.9733333333333334,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.7911111111111111,
            0.8488888888888889,
            0.8533333333333334,
            0.8533333333333334,
            0.8622222222222222,
            0.8533333333333334,
            0.8266666666666667,
            0.9555555555555556,
            0.8666666666666667,
            0.8977777777777778,
            0.8177777777777778,
            0.8755555555555555,
            0.8977777777777778,
            0.9333333333333333,
            0.8577777777777778,
            0.9733333333333334,
            0.9866666666666667,
            0.9822222222222222,
            0.9422222222222222,
            0.9644444444444444,
            0.8888888888888888,
            0.9644444444444444,
            0.96,
            0.9111111111111111,
            0.9288888888888889,
            0.9555555555555556,
            0.9733333333333334
        ],
        "reward_history": [
            -1271,
            -2250,
            -1039,
            -2250,
            -1004,
            -1525,
            -1806,
            -2250,
            -477,
            -2250,
            -818,
            -2250,
            -1399,
            -1055,
            -1890,
            -2250,
            -135,
            -25,
            -165,
            -422,
            -234,
            -1240,
            -634,
            -329,
            -1848,
            -997,
            -499,
            -558
        ],
        "steps_history": [
            1372,
            2250,
            1140,
            2250,
            1105,
            1626,
            1907,
            2250,
            578,
            2250,
            919,
            2250,
            1500,
            1156,
            1991,
            2250,
            236,
            126,
            266,
            523,
            335,
            1341,
            735,
            430,
            1949,
            1098,
            600,
            659
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "387/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.689307928085327,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 38,
        "policy_stability_history": [
            0.0,
            0.7555555555555555,
            0.8222222222222222,
            0.8177777777777778,
            0.8444444444444444,
            0.8044444444444444,
            0.9155555555555556,
            0.8088888888888889,
            0.9155555555555556,
            0.8177777777777778,
            0.8711111111111111,
            0.9422222222222222,
            0.96,
            0.9466666666666667,
            0.9288888888888889,
            0.9244444444444444,
            0.8311111111111111,
            0.9777777777777777,
            0.9155555555555556,
            0.84,
            0.9822222222222222,
            0.88,
            0.96,
            0.9866666666666667,
            0.9955555555555555,
            0.9688888888888889,
            0.9822222222222222,
            0.9733333333333334,
            0.9733333333333334,
            0.9288888888888889,
            0.9466666666666667,
            0.9777777777777777,
            0.9644444444444444,
            0.9955555555555555,
            0.9777777777777777,
            0.9644444444444444,
            0.9511111111111111,
            0.9688888888888889,
            1.0
        ],
        "reward_history": [
            -1795,
            -2250,
            -2250,
            -2250,
            -979,
            -2250,
            -725,
            -1373,
            -786,
            -2250,
            -1355,
            -287,
            -133,
            -602,
            -889,
            -489,
            -1900,
            -134,
            -1210,
            -1511,
            -176,
            -1984,
            -279,
            -151,
            -118,
            -328,
            -156,
            -205,
            -311,
            -726,
            -1093,
            -557,
            -370,
            -70,
            -522,
            -626,
            -786,
            -404,
            -162
        ],
        "steps_history": [
            1896,
            2250,
            2250,
            2250,
            1080,
            2250,
            826,
            1474,
            887,
            2250,
            1456,
            388,
            234,
            703,
            990,
            590,
            2001,
            235,
            1311,
            1612,
            277,
            2085,
            380,
            252,
            219,
            429,
            257,
            306,
            412,
            827,
            1194,
            658,
            471,
            171,
            623,
            727,
            887,
            505,
            263
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "388/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.485987186431885,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8444444444444444,
            0.8711111111111111,
            0.8311111111111111,
            0.8444444444444444,
            0.9511111111111111,
            0.7866666666666666,
            0.8977777777777778,
            0.8622222222222222,
            0.9111111111111111,
            0.84,
            0.9733333333333334,
            0.8622222222222222,
            0.9466666666666667,
            0.9688888888888889,
            0.92,
            0.9688888888888889,
            0.9822222222222222,
            0.9555555555555556,
            0.8622222222222222,
            0.9555555555555556,
            0.9555555555555556,
            0.96,
            0.9733333333333334,
            0.9644444444444444,
            0.8755555555555555,
            0.9466666666666667,
            0.9866666666666667,
            0.9644444444444444,
            0.9644444444444444,
            0.9911111111111112,
            0.9333333333333333,
            0.9466666666666667,
            0.9955555555555555,
            0.9866666666666667,
            0.96,
            0.9955555555555555,
            0.9866666666666667,
            0.9644444444444444,
            1.0,
            1.0
        ],
        "reward_history": [
            -2250,
            -1543,
            -1484,
            -2250,
            -1232,
            -157,
            -1883,
            -1056,
            -2250,
            -889,
            -2250,
            -156,
            -804,
            -768,
            -83,
            -908,
            -147,
            -131,
            -368,
            -1234,
            -363,
            -286,
            -295,
            -136,
            -143,
            -2250,
            -440,
            -180,
            -722,
            -601,
            -43,
            -1111,
            -913,
            -163,
            -139,
            -541,
            -188,
            -193,
            -486,
            -453,
            -9
        ],
        "steps_history": [
            2250,
            1644,
            1585,
            2250,
            1333,
            258,
            1984,
            1157,
            2250,
            990,
            2250,
            257,
            905,
            869,
            184,
            1009,
            248,
            232,
            469,
            1335,
            464,
            387,
            396,
            237,
            244,
            2250,
            541,
            281,
            823,
            702,
            144,
            1212,
            1014,
            264,
            240,
            642,
            289,
            294,
            587,
            554,
            110
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "389/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.42926287651062,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8222222222222222,
            0.8844444444444445,
            0.8177777777777778,
            0.8666666666666667,
            0.8222222222222222,
            0.8622222222222222,
            0.8844444444444445,
            0.8177777777777778,
            0.9555555555555556,
            0.9377777777777778,
            0.8222222222222222,
            0.9333333333333333,
            0.9155555555555556,
            0.8844444444444445,
            0.9422222222222222,
            0.92,
            0.9733333333333334,
            0.8177777777777778,
            0.9555555555555556,
            0.9422222222222222,
            0.9111111111111111,
            0.9466666666666667,
            0.9644444444444444,
            0.9688888888888889,
            0.9111111111111111,
            0.9866666666666667,
            0.9866666666666667,
            0.9733333333333334,
            0.9822222222222222,
            0.9644444444444444,
            1.0,
            0.9822222222222222,
            0.9911111111111112,
            1.0,
            0.9866666666666667,
            1.0,
            1.0
        ],
        "reward_history": [
            -2250,
            -1164,
            -808,
            -2250,
            -2250,
            -2250,
            -1042,
            -540,
            -2250,
            -86,
            -311,
            -1788,
            -597,
            -1029,
            -982,
            -493,
            -912,
            -472,
            -1824,
            -487,
            -850,
            -1323,
            -413,
            -244,
            -410,
            -768,
            -31,
            -24,
            -219,
            -194,
            -305,
            -213,
            -441,
            -163,
            -171,
            -244,
            -118,
            -244
        ],
        "steps_history": [
            2250,
            1265,
            909,
            2250,
            2250,
            2250,
            1143,
            641,
            2250,
            187,
            412,
            1889,
            698,
            1130,
            1083,
            594,
            1013,
            573,
            1925,
            588,
            951,
            1424,
            514,
            345,
            511,
            869,
            132,
            125,
            320,
            295,
            406,
            314,
            542,
            264,
            272,
            345,
            219,
            345
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "390/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.2_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.352837800979614,
        "final_policy_stability": 0.9822222222222222,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.9022222222222223,
            0.8222222222222222,
            0.8666666666666667,
            0.8533333333333334,
            0.8177777777777778,
            0.9155555555555556,
            0.9555555555555556,
            0.92,
            0.9511111111111111,
            0.9777777777777777,
            0.9511111111111111,
            0.88,
            0.9288888888888889,
            0.9644444444444444,
            1.0,
            0.9777777777777777,
            0.9911111111111112,
            0.9866666666666667,
            0.9911111111111112,
            0.8977777777777778,
            0.9777777777777777,
            0.96,
            0.9822222222222222
        ],
        "reward_history": [
            -861,
            -2250,
            -2250,
            -2250,
            -912,
            -1621,
            -2250,
            -1729,
            -348,
            -764,
            -790,
            -242,
            -675,
            -1994,
            -1421,
            -641,
            -223,
            -377,
            -343,
            -1000,
            -42,
            -1973,
            -875,
            -860,
            -696
        ],
        "steps_history": [
            962,
            2250,
            2250,
            2250,
            1013,
            1722,
            2250,
            1830,
            449,
            865,
            891,
            343,
            776,
            2095,
            1522,
            742,
            324,
            478,
            444,
            1101,
            143,
            2074,
            976,
            961,
            797
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "391/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.574833393096924,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8666666666666667,
            0.8177777777777778,
            0.9066666666666666,
            0.9066666666666666,
            0.8844444444444445,
            0.8977777777777778,
            0.88,
            0.8933333333333333,
            0.9911111111111112,
            0.8666666666666667,
            0.9422222222222222,
            0.9511111111111111,
            0.9733333333333334,
            0.8977777777777778,
            0.8444444444444444,
            0.9377777777777778,
            0.9022222222222223,
            0.92,
            0.9866666666666667,
            0.9777777777777777,
            1.0,
            1.0,
            1.0,
            0.9955555555555555
        ],
        "reward_history": [
            -2250,
            -947,
            -2250,
            -2250,
            -450,
            -595,
            -799,
            -1149,
            -1292,
            -263,
            -2250,
            -514,
            -631,
            -265,
            -810,
            -2250,
            -530,
            -1734,
            -2250,
            -891,
            -563,
            -104,
            -394,
            -381,
            -201
        ],
        "steps_history": [
            2250,
            1048,
            2250,
            2250,
            551,
            696,
            900,
            1250,
            1393,
            364,
            2250,
            615,
            732,
            366,
            911,
            2250,
            631,
            1835,
            2250,
            992,
            664,
            205,
            495,
            482,
            302
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "392/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.288137912750244,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8444444444444444,
            0.7911111111111111,
            0.88,
            0.8666666666666667,
            0.9511111111111111,
            0.8355555555555556,
            0.9288888888888889,
            0.9466666666666667,
            0.8977777777777778,
            0.8711111111111111,
            0.9555555555555556,
            0.88,
            0.84,
            0.9688888888888889,
            0.9111111111111111,
            0.92,
            0.9733333333333334,
            0.9955555555555555,
            0.9511111111111111,
            0.9555555555555556,
            0.9688888888888889,
            0.9644444444444444,
            1.0,
            0.9822222222222222,
            1.0,
            0.9688888888888889,
            1.0,
            0.9911111111111112
        ],
        "reward_history": [
            -543,
            -2250,
            -1635,
            -2250,
            -2250,
            -259,
            -2038,
            -590,
            -501,
            -2250,
            -1542,
            -362,
            -2250,
            -2250,
            -419,
            -1274,
            -1291,
            -741,
            -538,
            -797,
            -1513,
            -434,
            -737,
            -338,
            -972,
            -400,
            -1008,
            -134,
            -1056
        ],
        "steps_history": [
            644,
            2250,
            1736,
            2250,
            2250,
            360,
            2139,
            691,
            602,
            2250,
            1643,
            463,
            2250,
            2250,
            520,
            1375,
            1392,
            842,
            639,
            898,
            1614,
            535,
            838,
            439,
            1073,
            501,
            1109,
            235,
            1157
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "393/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.226041316986084,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.8933333333333333,
            0.8,
            0.8444444444444444,
            0.9066666666666666,
            0.8622222222222222,
            0.8444444444444444,
            0.8622222222222222,
            0.92,
            0.9466666666666667,
            0.8355555555555556,
            0.8622222222222222,
            0.8933333333333333,
            0.8977777777777778,
            0.96,
            0.9422222222222222,
            0.9244444444444444,
            0.9644444444444444,
            0.9822222222222222,
            0.9777777777777777,
            0.9911111111111112,
            0.9866666666666667,
            0.9777777777777777,
            0.9688888888888889,
            0.9688888888888889,
            0.96,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -1378,
            -307,
            -2250,
            -2250,
            -882,
            -2250,
            -2250,
            -2250,
            -777,
            -321,
            -1728,
            -1531,
            -1226,
            -1506,
            -503,
            -1454,
            -668,
            -392,
            -525,
            -712,
            -181,
            -234,
            -614,
            -839,
            -675,
            -1164,
            -637,
            -280,
            -345,
            -483
        ],
        "steps_history": [
            1479,
            408,
            2250,
            2250,
            983,
            2250,
            2250,
            2250,
            878,
            422,
            1829,
            1632,
            1327,
            1607,
            604,
            1555,
            769,
            493,
            626,
            813,
            282,
            335,
            715,
            940,
            776,
            1265,
            738,
            381,
            446,
            584
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "394/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.4929516315460205,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.8444444444444444,
            0.8755555555555555,
            0.8488888888888889,
            0.8622222222222222,
            0.8,
            0.8577777777777778,
            0.9022222222222223,
            0.8755555555555555,
            0.8888888888888888,
            0.84,
            0.9111111111111111,
            0.8977777777777778,
            0.92,
            0.8577777777777778,
            0.9422222222222222,
            0.9911111111111112,
            0.9777777777777777,
            0.9866666666666667,
            0.9822222222222222,
            1.0,
            0.9955555555555555,
            0.9866666666666667,
            0.9866666666666667,
            1.0,
            0.9911111111111112
        ],
        "reward_history": [
            -2250,
            -2250,
            -1418,
            -1264,
            -995,
            -2250,
            -1485,
            -1623,
            -2250,
            -1056,
            -2250,
            -1522,
            -985,
            -880,
            -2250,
            -1558,
            -117,
            -256,
            -140,
            -675,
            -39,
            -331,
            -574,
            -1250,
            -314,
            -468
        ],
        "steps_history": [
            2250,
            2250,
            1519,
            1365,
            1096,
            2250,
            1586,
            1724,
            2250,
            1157,
            2250,
            1623,
            1086,
            981,
            2250,
            1659,
            218,
            357,
            241,
            776,
            140,
            432,
            675,
            1351,
            415,
            569
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "395/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.095008373260498,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.8488888888888889,
            0.8977777777777778,
            0.7955555555555556,
            0.8488888888888889,
            0.8622222222222222,
            0.9022222222222223,
            0.8488888888888889,
            0.8355555555555556,
            0.8622222222222222,
            0.9555555555555556,
            0.9422222222222222,
            0.8755555555555555,
            0.9022222222222223,
            0.9466666666666667,
            0.9866666666666667,
            0.9866666666666667,
            0.9822222222222222,
            0.9688888888888889,
            0.9688888888888889,
            0.9466666666666667,
            0.9955555555555555,
            0.9333333333333333,
            0.9777777777777777,
            0.9955555555555555,
            0.9644444444444444,
            0.9688888888888889,
            1.0,
            0.9733333333333334,
            1.0
        ],
        "reward_history": [
            -2250,
            -1138,
            -504,
            -2250,
            -2250,
            -2250,
            -787,
            -1263,
            -2250,
            -2250,
            -262,
            -869,
            -1082,
            -1546,
            -667,
            -270,
            -169,
            -266,
            -102,
            -857,
            -1147,
            -375,
            -690,
            -526,
            -168,
            -698,
            -475,
            -402,
            -667,
            -627
        ],
        "steps_history": [
            2250,
            1239,
            605,
            2250,
            2250,
            2250,
            888,
            1364,
            2250,
            2250,
            363,
            970,
            1183,
            1647,
            768,
            371,
            270,
            367,
            203,
            958,
            1248,
            476,
            791,
            627,
            269,
            799,
            576,
            503,
            768,
            728
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "396/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.369077205657959,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8266666666666667,
            0.8977777777777778,
            0.8444444444444444,
            0.8977777777777778,
            0.8533333333333334,
            0.88,
            0.9066666666666666,
            0.8844444444444445,
            0.8355555555555556,
            0.9111111111111111,
            0.96,
            0.96,
            0.9288888888888889,
            0.9466666666666667,
            0.9688888888888889,
            0.92,
            0.9555555555555556,
            0.9866666666666667,
            0.9333333333333333,
            0.9155555555555556,
            0.9644444444444444,
            0.9955555555555555,
            1.0
        ],
        "reward_history": [
            -2250,
            -1434,
            -862,
            -1675,
            -787,
            -1156,
            -1436,
            -716,
            -1296,
            -2141,
            -1161,
            -566,
            -356,
            -795,
            -795,
            -208,
            -893,
            -1025,
            -127,
            -1571,
            -1310,
            -542,
            -667,
            -246
        ],
        "steps_history": [
            2250,
            1535,
            963,
            1776,
            888,
            1257,
            1537,
            817,
            1397,
            2242,
            1262,
            667,
            457,
            896,
            896,
            309,
            994,
            1126,
            228,
            1672,
            1411,
            643,
            768,
            347
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "397/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.221627235412598,
        "final_policy_stability": 0.9822222222222222,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.7644444444444445,
            0.8088888888888889,
            0.8088888888888889,
            0.8444444444444444,
            0.8533333333333334,
            0.8622222222222222,
            0.9066666666666666,
            0.9333333333333333,
            0.8711111111111111,
            0.8355555555555556,
            0.9377777777777778,
            0.9377777777777778,
            0.9555555555555556,
            0.9688888888888889,
            0.8711111111111111,
            0.9733333333333334,
            0.9288888888888889,
            0.96,
            0.9555555555555556,
            0.9955555555555555,
            0.9422222222222222,
            0.96,
            0.9377777777777778,
            0.9733333333333334,
            1.0,
            1.0,
            0.9822222222222222
        ],
        "reward_history": [
            -549,
            -2250,
            -2144,
            -2250,
            -1312,
            -1303,
            -862,
            -576,
            -366,
            -1782,
            -2250,
            -391,
            -759,
            -671,
            -229,
            -2250,
            -159,
            -557,
            -478,
            -859,
            -133,
            -464,
            -638,
            -1273,
            -610,
            -241,
            -45,
            -637
        ],
        "steps_history": [
            650,
            2250,
            2245,
            2250,
            1413,
            1404,
            963,
            677,
            467,
            1883,
            2250,
            492,
            860,
            772,
            330,
            2250,
            260,
            658,
            579,
            960,
            234,
            565,
            739,
            1374,
            711,
            342,
            146,
            738
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "398/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.094240665435791,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.84,
            0.8044444444444444,
            0.8666666666666667,
            0.9066666666666666,
            0.8177777777777778,
            0.8533333333333334,
            0.8577777777777778,
            0.8844444444444445,
            0.9244444444444444,
            0.9688888888888889,
            0.8488888888888889,
            0.9911111111111112,
            0.92,
            0.9911111111111112,
            0.9466666666666667,
            0.9555555555555556,
            0.9822222222222222,
            0.9688888888888889,
            0.9644444444444444,
            0.96,
            0.9644444444444444,
            0.9911111111111112,
            0.9955555555555555,
            0.9288888888888889,
            0.9511111111111111,
            1.0,
            0.9955555555555555,
            0.9911111111111112
        ],
        "reward_history": [
            -2250,
            -1855,
            -1675,
            -1257,
            -1177,
            -2077,
            -2250,
            -1987,
            -988,
            -891,
            -310,
            -1404,
            -125,
            -1235,
            -182,
            -537,
            -272,
            -509,
            -447,
            -529,
            -283,
            -357,
            -84,
            -321,
            -858,
            -862,
            -111,
            -615,
            -396
        ],
        "steps_history": [
            2250,
            1956,
            1776,
            1358,
            1278,
            2178,
            2250,
            2088,
            1089,
            992,
            411,
            1505,
            226,
            1336,
            283,
            638,
            373,
            610,
            548,
            630,
            384,
            458,
            185,
            422,
            959,
            963,
            212,
            716,
            497
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "399/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.10419487953186,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.76,
            0.8666666666666667,
            0.9066666666666666,
            0.7955555555555556,
            0.9511111111111111,
            0.84,
            0.92,
            0.9466666666666667,
            0.9333333333333333,
            0.8222222222222222,
            0.8977777777777778,
            0.9555555555555556,
            0.9333333333333333,
            0.96,
            0.9911111111111112,
            0.9644444444444444,
            0.8444444444444444,
            0.9422222222222222,
            0.9866666666666667,
            0.9555555555555556,
            0.9333333333333333,
            0.9822222222222222,
            0.96,
            0.9688888888888889,
            0.9777777777777777,
            1.0,
            1.0,
            0.9155555555555556,
            1.0
        ],
        "reward_history": [
            -2121,
            -2250,
            -2250,
            -838,
            -2250,
            -357,
            -1329,
            -963,
            -265,
            -565,
            -1815,
            -500,
            -102,
            -936,
            -199,
            -213,
            -285,
            -2250,
            -417,
            -67,
            -960,
            -794,
            -475,
            -318,
            -698,
            -273,
            -238,
            -11,
            -1805,
            -164
        ],
        "steps_history": [
            2222,
            2250,
            2250,
            939,
            2250,
            458,
            1430,
            1064,
            366,
            666,
            1916,
            601,
            203,
            1037,
            300,
            314,
            386,
            2250,
            518,
            168,
            1061,
            895,
            576,
            419,
            799,
            374,
            339,
            112,
            1906,
            265
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "400/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.959594249725342,
        "final_policy_stability": 0.9733333333333334,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8,
            0.84,
            0.8222222222222222,
            0.8977777777777778,
            0.8755555555555555,
            0.8044444444444444,
            0.8266666666666667,
            0.9822222222222222,
            0.9555555555555556,
            0.88,
            0.92,
            0.9422222222222222,
            0.9511111111111111,
            0.8533333333333334,
            0.9733333333333334,
            0.9777777777777777,
            0.9777777777777777,
            0.9466666666666667,
            0.8977777777777778,
            0.9022222222222223,
            0.9911111111111112,
            0.9822222222222222,
            0.9822222222222222,
            0.9733333333333334,
            0.9866666666666667,
            0.9911111111111112,
            0.9955555555555555,
            0.9733333333333334
        ],
        "reward_history": [
            -2250,
            -1581,
            -2250,
            -1348,
            -441,
            -844,
            -2250,
            -2250,
            -76,
            -145,
            -1147,
            -263,
            -90,
            -461,
            -2250,
            -649,
            -397,
            -209,
            -873,
            -854,
            -2250,
            -275,
            -181,
            -323,
            -557,
            -449,
            -130,
            -119,
            -467
        ],
        "steps_history": [
            2250,
            1682,
            2250,
            1449,
            542,
            945,
            2250,
            2250,
            177,
            246,
            1248,
            364,
            191,
            562,
            2250,
            750,
            498,
            310,
            974,
            955,
            2250,
            376,
            282,
            424,
            658,
            550,
            231,
            220,
            568
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "401/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.181147813796997,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.8177777777777778,
            0.8266666666666667,
            0.8044444444444444,
            0.8666666666666667,
            0.9022222222222223,
            0.9111111111111111,
            0.92,
            0.9333333333333333,
            0.92,
            0.8044444444444444,
            0.9288888888888889,
            0.9022222222222223,
            0.9422222222222222,
            0.9377777777777778,
            0.9288888888888889,
            0.9822222222222222,
            0.9466666666666667,
            0.84,
            0.9466666666666667,
            0.9822222222222222,
            0.9955555555555555,
            0.9511111111111111,
            0.9555555555555556,
            0.9644444444444444,
            0.9955555555555555,
            0.9777777777777777,
            0.9955555555555555
        ],
        "reward_history": [
            -2250,
            -1670,
            -2250,
            -1750,
            -1521,
            -602,
            -507,
            -369,
            -362,
            -586,
            -2250,
            -642,
            -670,
            -265,
            -429,
            -928,
            -149,
            -381,
            -2250,
            -525,
            -331,
            -149,
            -781,
            -754,
            -605,
            -182,
            -700,
            -587
        ],
        "steps_history": [
            2250,
            1771,
            2250,
            1851,
            1622,
            703,
            608,
            470,
            463,
            687,
            2250,
            743,
            771,
            366,
            530,
            1029,
            250,
            482,
            2250,
            626,
            432,
            250,
            882,
            855,
            706,
            283,
            801,
            688
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "402/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.185309886932373,
        "final_policy_stability": 0.9822222222222222,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.7155555555555555,
            0.8355555555555556,
            0.7911111111111111,
            0.9155555555555556,
            0.8177777777777778,
            0.9066666666666666,
            0.9244444444444444,
            0.8711111111111111,
            0.9555555555555556,
            0.8222222222222222,
            0.9377777777777778,
            0.9733333333333334,
            0.9555555555555556,
            0.8266666666666667,
            0.9111111111111111,
            0.8977777777777778,
            0.96,
            0.9822222222222222,
            0.9688888888888889,
            0.9555555555555556,
            0.9688888888888889,
            0.9644444444444444,
            0.9911111111111112,
            0.9955555555555555,
            0.9911111111111112,
            0.9866666666666667,
            0.9911111111111112,
            0.92,
            1.0,
            0.9822222222222222
        ],
        "reward_history": [
            -1827,
            -2250,
            -2250,
            -2250,
            -577,
            -1110,
            -522,
            -271,
            -833,
            -205,
            -1739,
            -419,
            -187,
            -168,
            -2250,
            -799,
            -1607,
            -491,
            -290,
            -476,
            -279,
            -339,
            -481,
            -135,
            -134,
            -120,
            -525,
            -472,
            -2250,
            -253,
            -519
        ],
        "steps_history": [
            1928,
            2250,
            2250,
            2250,
            678,
            1211,
            623,
            372,
            934,
            306,
            1840,
            520,
            288,
            269,
            2250,
            900,
            1708,
            592,
            391,
            577,
            380,
            440,
            582,
            236,
            235,
            221,
            626,
            573,
            2250,
            354,
            620
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "403/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.676305294036865,
        "final_policy_stability": 0.9822222222222222,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8355555555555556,
            0.9155555555555556,
            0.8088888888888889,
            0.84,
            0.9155555555555556,
            0.9288888888888889,
            0.88,
            0.9555555555555556,
            0.8444444444444444,
            0.9288888888888889,
            0.9688888888888889,
            0.9688888888888889,
            0.8577777777777778,
            0.96,
            0.9555555555555556,
            0.9244444444444444,
            0.88,
            0.96,
            0.96,
            0.9822222222222222,
            0.9911111111111112,
            0.9822222222222222,
            0.9466666666666667,
            0.9911111111111112,
            0.9777777777777777,
            0.9777777777777777,
            0.9688888888888889,
            0.9644444444444444,
            0.9911111111111112,
            1.0,
            1.0,
            0.9244444444444444,
            0.9822222222222222
        ],
        "reward_history": [
            -2250,
            -2250,
            -324,
            -2250,
            -2250,
            -1025,
            -415,
            -946,
            -293,
            -2250,
            -419,
            -96,
            -111,
            -1371,
            -492,
            -355,
            -804,
            -1078,
            -293,
            -388,
            -147,
            -131,
            -323,
            -320,
            -349,
            -342,
            -311,
            -404,
            -213,
            -218,
            -143,
            -166,
            -799,
            -379
        ],
        "steps_history": [
            2250,
            2250,
            425,
            2250,
            2250,
            1126,
            516,
            1047,
            394,
            2250,
            520,
            197,
            212,
            1472,
            593,
            456,
            905,
            1179,
            394,
            489,
            248,
            232,
            424,
            421,
            450,
            443,
            412,
            505,
            314,
            319,
            244,
            267,
            900,
            480
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "404/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.822852849960327,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.7955555555555556,
            0.8844444444444445,
            0.9022222222222223,
            0.84,
            0.8488888888888889,
            0.8711111111111111,
            0.9288888888888889,
            0.9466666666666667,
            0.88,
            0.9422222222222222,
            0.9466666666666667,
            0.9511111111111111,
            0.9511111111111111,
            0.9244444444444444,
            0.9777777777777777,
            0.9688888888888889,
            0.8488888888888889,
            0.9955555555555555,
            0.96,
            0.9511111111111111,
            0.9911111111111112,
            0.9955555555555555,
            0.9866666666666667,
            0.8888888888888888,
            0.9511111111111111,
            0.9422222222222222,
            0.96,
            0.9911111111111112,
            0.9688888888888889,
            0.9866666666666667,
            1.0
        ],
        "reward_history": [
            -2250,
            -2041,
            -2250,
            -718,
            -2250,
            -1255,
            -891,
            -903,
            -161,
            -876,
            -981,
            -404,
            -339,
            -300,
            -855,
            -135,
            -371,
            -2250,
            -156,
            -263,
            -345,
            -245,
            -46,
            -75,
            -977,
            -574,
            -478,
            -487,
            -269,
            -520,
            -264,
            -95
        ],
        "steps_history": [
            2250,
            2142,
            2250,
            819,
            2250,
            1356,
            992,
            1004,
            262,
            977,
            1082,
            505,
            440,
            401,
            956,
            236,
            472,
            2250,
            257,
            364,
            446,
            346,
            147,
            176,
            1078,
            675,
            579,
            588,
            370,
            621,
            365,
            196
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "405/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.317772626876831,
        "final_policy_stability": 0.9777777777777777,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.7822222222222223,
            0.9022222222222223,
            0.8222222222222222,
            0.8666666666666667,
            0.8533333333333334,
            0.8177777777777778,
            0.9155555555555556,
            0.9555555555555556,
            0.9244444444444444,
            0.9511111111111111,
            0.9777777777777777,
            0.9466666666666667,
            0.8666666666666667,
            0.9288888888888889,
            0.96,
            1.0,
            0.9777777777777777,
            0.9911111111111112,
            0.9866666666666667,
            0.9911111111111112,
            0.8844444444444445,
            0.9688888888888889,
            0.96,
            0.9777777777777777
        ],
        "reward_history": [
            -861,
            -2250,
            -2250,
            -2250,
            -912,
            -1621,
            -2250,
            -1729,
            -348,
            -764,
            -790,
            -242,
            -675,
            -1994,
            -1421,
            -641,
            -223,
            -377,
            -343,
            -1000,
            -42,
            -1973,
            -875,
            -860,
            -696
        ],
        "steps_history": [
            962,
            2250,
            2250,
            2250,
            1013,
            1722,
            2250,
            1830,
            449,
            865,
            891,
            343,
            776,
            2095,
            1522,
            742,
            324,
            478,
            444,
            1101,
            143,
            2074,
            976,
            961,
            797
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "406/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.187178134918213,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8666666666666667,
            0.8133333333333334,
            0.8888888888888888,
            0.9111111111111111,
            0.9022222222222223,
            0.9288888888888889,
            0.9111111111111111,
            0.9777777777777777,
            0.8844444444444445,
            0.9422222222222222,
            0.9333333333333333,
            0.8711111111111111,
            0.8755555555555555,
            0.8755555555555555,
            0.8888888888888888,
            0.9377777777777778,
            0.9822222222222222,
            0.9911111111111112,
            0.9555555555555556,
            0.96,
            0.9822222222222222,
            0.9733333333333334,
            0.9866666666666667,
            0.9555555555555556,
            0.9688888888888889,
            1.0,
            0.9822222222222222,
            0.9955555555555555
        ],
        "reward_history": [
            -2250,
            -947,
            -2250,
            -2250,
            -450,
            -786,
            -963,
            -2250,
            -87,
            -1548,
            -1277,
            -707,
            -2250,
            -1783,
            -2250,
            -1854,
            -1640,
            -95,
            -198,
            -683,
            -1183,
            -670,
            -401,
            -368,
            -2250,
            -1475,
            -681,
            -493,
            -617
        ],
        "steps_history": [
            2250,
            1048,
            2250,
            2250,
            551,
            887,
            1064,
            2250,
            188,
            1649,
            1378,
            808,
            2250,
            1884,
            2250,
            1955,
            1741,
            196,
            299,
            784,
            1284,
            771,
            502,
            469,
            2250,
            1576,
            782,
            594,
            718
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "407/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.272886514663696,
        "final_policy_stability": 0.9777777777777777,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.8444444444444444,
            0.7911111111111111,
            0.7822222222222223,
            0.8844444444444445,
            0.9688888888888889,
            0.9244444444444444,
            0.9022222222222223,
            0.9866666666666667,
            0.9422222222222222,
            0.96,
            0.8622222222222222,
            0.8711111111111111,
            0.8711111111111111,
            0.9244444444444444,
            0.9066666666666666,
            0.9244444444444444,
            0.9244444444444444,
            0.8488888888888889,
            0.9866666666666667,
            0.9733333333333334,
            0.9288888888888889,
            0.9733333333333334,
            0.9333333333333333,
            0.9466666666666667,
            0.9422222222222222,
            1.0,
            0.9777777777777777
        ],
        "reward_history": [
            -543,
            -2250,
            -1635,
            -2250,
            -2250,
            -107,
            -525,
            -1222,
            -241,
            -590,
            -341,
            -2250,
            -2250,
            -2250,
            -998,
            -1216,
            -603,
            -940,
            -2250,
            -522,
            -386,
            -1296,
            -640,
            -1159,
            -1411,
            -1509,
            -134,
            -1056
        ],
        "steps_history": [
            644,
            2250,
            1736,
            2250,
            2250,
            208,
            626,
            1323,
            342,
            691,
            442,
            2250,
            2250,
            2250,
            1099,
            1317,
            704,
            1041,
            2250,
            623,
            487,
            1397,
            741,
            1260,
            1512,
            1610,
            235,
            1157
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "408/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.222110033035278,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8933333333333333,
            0.8,
            0.8444444444444444,
            0.9066666666666666,
            0.8622222222222222,
            0.8444444444444444,
            0.8622222222222222,
            0.92,
            0.9466666666666667,
            0.8311111111111111,
            0.8577777777777778,
            0.8933333333333333,
            0.8977777777777778,
            0.96,
            0.9333333333333333,
            0.9288888888888889,
            0.9644444444444444,
            0.9822222222222222,
            0.8444444444444444,
            0.9822222222222222,
            0.9822222222222222,
            0.9511111111111111,
            0.9911111111111112,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -1378,
            -307,
            -2250,
            -2250,
            -882,
            -2250,
            -2250,
            -2250,
            -777,
            -321,
            -1728,
            -1531,
            -1226,
            -1506,
            -503,
            -1454,
            -668,
            -392,
            -525,
            -2044,
            -839,
            -675,
            -1164,
            -637,
            -280,
            -345,
            -483
        ],
        "steps_history": [
            1479,
            408,
            2250,
            2250,
            983,
            2250,
            2250,
            2250,
            878,
            422,
            1829,
            1632,
            1327,
            1607,
            604,
            1555,
            769,
            493,
            626,
            2145,
            940,
            776,
            1265,
            738,
            381,
            446,
            584
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "409/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.266467809677124,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.8444444444444444,
            0.8755555555555555,
            0.8488888888888889,
            0.8311111111111111,
            0.8222222222222222,
            0.9244444444444444,
            0.9777777777777777,
            0.8844444444444445,
            0.9022222222222223,
            0.8666666666666667,
            0.9777777777777777,
            0.9511111111111111,
            0.9377777777777778,
            0.8355555555555556,
            0.92,
            0.9644444444444444,
            0.8844444444444445,
            0.9777777777777777,
            0.9911111111111112,
            0.9955555555555555,
            0.9644444444444444,
            1.0
        ],
        "reward_history": [
            -2250,
            -2250,
            -1418,
            -1264,
            -2250,
            -2250,
            -1020,
            -39,
            -1469,
            -1310,
            -1603,
            -184,
            -274,
            -1218,
            -2250,
            -1227,
            -886,
            -2250,
            -571,
            -117,
            -256,
            -916,
            -39
        ],
        "steps_history": [
            2250,
            2250,
            1519,
            1365,
            2250,
            2250,
            1121,
            140,
            1570,
            1411,
            1704,
            285,
            375,
            1319,
            2250,
            1328,
            987,
            2250,
            672,
            218,
            357,
            1017,
            140
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "410/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.079452753067017,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.8488888888888889,
            0.8977777777777778,
            0.7955555555555556,
            0.8488888888888889,
            0.8622222222222222,
            0.9022222222222223,
            0.8488888888888889,
            0.8355555555555556,
            0.8666666666666667,
            0.9555555555555556,
            0.9422222222222222,
            0.8755555555555555,
            0.8977777777777778,
            0.9511111111111111,
            0.9866666666666667,
            0.9866666666666667,
            0.9822222222222222,
            0.9688888888888889,
            0.9688888888888889,
            0.9466666666666667,
            0.9955555555555555,
            0.9333333333333333,
            0.9777777777777777,
            0.9955555555555555,
            0.9644444444444444,
            0.9688888888888889,
            1.0,
            0.9733333333333334,
            1.0
        ],
        "reward_history": [
            -2250,
            -1138,
            -504,
            -2250,
            -2250,
            -2250,
            -787,
            -1263,
            -2250,
            -2250,
            -262,
            -869,
            -1082,
            -1546,
            -667,
            -270,
            -169,
            -266,
            -102,
            -857,
            -1147,
            -375,
            -690,
            -526,
            -168,
            -698,
            -475,
            -402,
            -667,
            -627
        ],
        "steps_history": [
            2250,
            1239,
            605,
            2250,
            2250,
            2250,
            888,
            1364,
            2250,
            2250,
            363,
            970,
            1183,
            1647,
            768,
            371,
            270,
            367,
            203,
            958,
            1248,
            476,
            791,
            627,
            269,
            799,
            576,
            503,
            768,
            728
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "411/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.357436895370483,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.8177777777777778,
            0.8977777777777778,
            0.8444444444444444,
            0.8933333333333333,
            0.8533333333333334,
            0.88,
            0.9155555555555556,
            0.88,
            0.8311111111111111,
            0.8977777777777778,
            0.9466666666666667,
            0.9466666666666667,
            0.9688888888888889,
            0.9066666666666666,
            0.9511111111111111,
            0.9422222222222222,
            0.9511111111111111,
            0.9644444444444444,
            0.9288888888888889,
            0.92,
            0.9555555555555556,
            1.0,
            0.9955555555555555,
            1.0,
            1.0
        ],
        "reward_history": [
            -2250,
            -1434,
            -862,
            -1675,
            -787,
            -1156,
            -1436,
            -716,
            -1296,
            -2250,
            -1487,
            -345,
            -243,
            -405,
            -1185,
            -286,
            -815,
            -1025,
            -344,
            -1354,
            -1427,
            -1193,
            -246,
            -220,
            -105,
            -23
        ],
        "steps_history": [
            2250,
            1535,
            963,
            1776,
            888,
            1257,
            1537,
            817,
            1397,
            2250,
            1588,
            446,
            344,
            506,
            1286,
            387,
            916,
            1126,
            445,
            1455,
            1528,
            1294,
            347,
            321,
            206,
            124
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "412/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.122616529464722,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.7644444444444445,
            0.8088888888888889,
            0.8088888888888889,
            0.8444444444444444,
            0.8533333333333334,
            0.8622222222222222,
            0.9066666666666666,
            0.9333333333333333,
            0.8711111111111111,
            0.8311111111111111,
            0.9377777777777778,
            0.9377777777777778,
            0.9511111111111111,
            0.9688888888888889,
            0.8755555555555555,
            0.9377777777777778,
            0.9733333333333334,
            0.9555555555555556,
            0.96,
            0.9822222222222222,
            0.9955555555555555,
            0.92,
            0.9866666666666667,
            0.9644444444444444,
            1.0,
            1.0
        ],
        "reward_history": [
            -549,
            -2250,
            -2144,
            -2250,
            -1312,
            -1303,
            -862,
            -576,
            -366,
            -1782,
            -2250,
            -391,
            -759,
            -671,
            -229,
            -2053,
            -913,
            -399,
            -686,
            -385,
            -209,
            -155,
            -810,
            -448,
            -1262,
            -241,
            -45
        ],
        "steps_history": [
            650,
            2250,
            2245,
            2250,
            1413,
            1404,
            963,
            677,
            467,
            1883,
            2250,
            492,
            860,
            772,
            330,
            2154,
            1014,
            500,
            787,
            486,
            310,
            256,
            911,
            549,
            1363,
            342,
            146
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "413/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.853724718093872,
        "final_policy_stability": 0.9866666666666667,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.84,
            0.8311111111111111,
            0.8711111111111111,
            0.8666666666666667,
            0.9155555555555556,
            0.8755555555555555,
            0.8844444444444445,
            0.8311111111111111,
            0.9288888888888889,
            0.9288888888888889,
            0.9466666666666667,
            0.9377777777777778,
            0.9155555555555556,
            0.9777777777777777,
            0.9377777777777778,
            0.9955555555555555,
            0.9733333333333334,
            0.96,
            0.9511111111111111,
            0.9555555555555556,
            0.9733333333333334,
            0.9688888888888889,
            0.9822222222222222,
            0.9866666666666667,
            0.9866666666666667,
            0.96,
            0.9911111111111112,
            0.9777777777777777,
            1.0,
            1.0,
            0.9866666666666667
        ],
        "reward_history": [
            -2250,
            -2250,
            -1187,
            -733,
            -1161,
            -633,
            -2250,
            -497,
            -1968,
            -901,
            -641,
            -228,
            -930,
            -807,
            -107,
            -1275,
            -125,
            -492,
            -275,
            -266,
            -820,
            -288,
            -493,
            -447,
            -529,
            -283,
            -542,
            -321,
            -467,
            -168,
            -560,
            -323
        ],
        "steps_history": [
            2250,
            2250,
            1288,
            834,
            1262,
            734,
            2250,
            598,
            2069,
            1002,
            742,
            329,
            1031,
            908,
            208,
            1376,
            226,
            593,
            376,
            367,
            921,
            389,
            594,
            548,
            630,
            384,
            643,
            422,
            568,
            269,
            661,
            424
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "414/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.235529899597168,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.76,
            0.8888888888888888,
            0.8355555555555556,
            0.8133333333333334,
            0.8844444444444445,
            0.8577777777777778,
            0.8355555555555556,
            0.9066666666666666,
            0.9244444444444444,
            0.8711111111111111,
            0.84,
            0.9555555555555556,
            0.9066666666666666,
            0.96,
            0.9733333333333334,
            0.9511111111111111,
            0.9733333333333334,
            0.9733333333333334,
            0.96,
            0.96,
            0.8933333333333333,
            0.9911111111111112,
            0.9822222222222222,
            0.9955555555555555,
            1.0,
            0.9955555555555555,
            0.9822222222222222,
            0.9911111111111112,
            1.0,
            0.9911111111111112,
            0.9911111111111112
        ],
        "reward_history": [
            -2121,
            -2250,
            -2250,
            -2250,
            -2250,
            -1136,
            -1285,
            -1880,
            -485,
            -394,
            -1258,
            -1549,
            -447,
            -1023,
            -298,
            -145,
            -645,
            -464,
            -475,
            -318,
            -1072,
            -1293,
            -399,
            -136,
            -125,
            -164,
            -349,
            -339,
            -133,
            -145,
            -535,
            -293
        ],
        "steps_history": [
            2222,
            2250,
            2250,
            2250,
            2250,
            1237,
            1386,
            1981,
            586,
            495,
            1359,
            1650,
            548,
            1124,
            399,
            246,
            746,
            565,
            576,
            419,
            1173,
            1394,
            500,
            237,
            226,
            265,
            450,
            440,
            234,
            246,
            636,
            394
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "415/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.233345985412598,
        "final_policy_stability": 0.9733333333333334,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.8,
            0.84,
            0.8222222222222222,
            0.8977777777777778,
            0.88,
            0.8088888888888889,
            0.8311111111111111,
            0.9822222222222222,
            0.9555555555555556,
            0.8755555555555555,
            0.92,
            0.9422222222222222,
            0.9511111111111111,
            0.8533333333333334,
            0.9733333333333334,
            0.9777777777777777,
            0.9777777777777777,
            0.8844444444444445,
            0.8977777777777778,
            0.9955555555555555,
            0.9822222222222222,
            0.9377777777777778,
            0.9955555555555555,
            1.0,
            0.9911111111111112,
            0.9955555555555555,
            0.9733333333333334
        ],
        "reward_history": [
            -2250,
            -1581,
            -2250,
            -1348,
            -441,
            -844,
            -2250,
            -2250,
            -76,
            -145,
            -1147,
            -263,
            -90,
            -461,
            -2250,
            -649,
            -397,
            -209,
            -1828,
            -2250,
            -275,
            -181,
            -676,
            -204,
            -449,
            -130,
            -119,
            -595
        ],
        "steps_history": [
            2250,
            1682,
            2250,
            1449,
            542,
            945,
            2250,
            2250,
            177,
            246,
            1248,
            364,
            191,
            562,
            2250,
            750,
            498,
            310,
            1929,
            2250,
            376,
            282,
            777,
            305,
            550,
            231,
            220,
            696
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "416/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.031510829925537,
        "final_policy_stability": 0.9555555555555556,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8222222222222222,
            0.8311111111111111,
            0.8088888888888889,
            0.8666666666666667,
            0.9022222222222223,
            0.9111111111111111,
            0.8977777777777778,
            0.9111111111111111,
            0.92,
            0.9377777777777778,
            0.8755555555555555,
            0.9155555555555556,
            0.9466666666666667,
            0.9066666666666666,
            0.9733333333333334,
            0.9111111111111111,
            0.8666666666666667,
            0.9066666666666666,
            0.9466666666666667,
            0.9688888888888889,
            0.9955555555555555,
            0.9555555555555556,
            0.9688888888888889,
            1.0,
            1.0,
            0.9555555555555556
        ],
        "reward_history": [
            -2250,
            -1670,
            -2250,
            -1750,
            -1521,
            -602,
            -507,
            -382,
            -349,
            -586,
            -348,
            -1253,
            -569,
            -632,
            -709,
            -81,
            -1176,
            -2250,
            -896,
            -474,
            -331,
            -149,
            -781,
            -727,
            -79,
            -207,
            -797
        ],
        "steps_history": [
            2250,
            1771,
            2250,
            1851,
            1622,
            703,
            608,
            483,
            450,
            687,
            449,
            1354,
            670,
            733,
            810,
            182,
            1277,
            2250,
            997,
            575,
            432,
            250,
            882,
            828,
            180,
            308,
            898
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "417/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.915148973464966,
        "final_policy_stability": 0.9777777777777777,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.7111111111111111,
            0.8444444444444444,
            0.8488888888888889,
            0.7822222222222223,
            0.8622222222222222,
            0.9511111111111111,
            0.9111111111111111,
            0.8266666666666667,
            0.9155555555555556,
            0.9111111111111111,
            0.9288888888888889,
            0.8488888888888889,
            0.9422222222222222,
            0.9466666666666667,
            0.9466666666666667,
            0.9733333333333334,
            0.9288888888888889,
            0.9111111111111111,
            0.9822222222222222,
            0.9555555555555556,
            0.9733333333333334,
            0.9955555555555555,
            0.9777777777777777,
            0.9911111111111112,
            0.9644444444444444,
            0.9022222222222223,
            0.9911111111111112,
            0.9466666666666667,
            1.0,
            0.9644444444444444,
            1.0,
            0.9777777777777777
        ],
        "reward_history": [
            -1827,
            -2250,
            -2250,
            -1658,
            -2250,
            -969,
            -189,
            -904,
            -2250,
            -573,
            -664,
            -292,
            -1549,
            -598,
            -186,
            -456,
            -283,
            -998,
            -565,
            -190,
            -863,
            -193,
            -134,
            -210,
            -435,
            -472,
            -745,
            -171,
            -1078,
            -206,
            -408,
            -461,
            -89
        ],
        "steps_history": [
            1928,
            2250,
            2250,
            1759,
            2250,
            1070,
            290,
            1005,
            2250,
            674,
            765,
            393,
            1650,
            699,
            287,
            557,
            384,
            1099,
            666,
            291,
            964,
            294,
            235,
            311,
            536,
            573,
            846,
            272,
            1179,
            307,
            509,
            562,
            190
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "418/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.873772144317627,
        "final_policy_stability": 0.9022222222222223,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.84,
            0.8266666666666667,
            0.7688888888888888,
            0.9333333333333333,
            0.9022222222222223,
            0.8711111111111111,
            0.8488888888888889,
            0.9333333333333333,
            0.9511111111111111,
            0.9111111111111111,
            0.9155555555555556,
            0.9511111111111111,
            0.9511111111111111,
            0.9822222222222222,
            0.9377777777777778,
            0.9377777777777778,
            0.9777777777777777,
            0.8755555555555555,
            0.9911111111111112,
            0.9777777777777777,
            0.96,
            0.9777777777777777,
            0.9422222222222222,
            0.9688888888888889,
            1.0,
            0.9911111111111112,
            0.9955555555555555,
            0.9955555555555555,
            0.9955555555555555,
            0.9688888888888889,
            1.0,
            0.9911111111111112,
            1.0,
            0.9022222222222223
        ],
        "reward_history": [
            -2250,
            -2250,
            -2250,
            -1878,
            -233,
            -458,
            -1344,
            -1137,
            -238,
            -231,
            -541,
            -825,
            -566,
            -306,
            -253,
            -619,
            -959,
            -355,
            -965,
            -607,
            -363,
            -333,
            -194,
            -803,
            -320,
            -349,
            -408,
            -245,
            -145,
            -231,
            -222,
            -136,
            -143,
            -166,
            -2250
        ],
        "steps_history": [
            2250,
            2250,
            2250,
            1979,
            334,
            559,
            1445,
            1238,
            339,
            332,
            642,
            926,
            667,
            407,
            354,
            720,
            1060,
            456,
            1066,
            708,
            464,
            434,
            295,
            904,
            421,
            450,
            509,
            346,
            246,
            332,
            323,
            237,
            244,
            267,
            2250
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "419/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.437224864959717,
        "final_policy_stability": 0.9866666666666667,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.7822222222222223,
            0.8844444444444445,
            0.8933333333333333,
            0.8666666666666667,
            0.8444444444444444,
            0.88,
            0.9022222222222223,
            0.9422222222222222,
            0.9111111111111111,
            0.92,
            0.8977777777777778,
            0.8533333333333334,
            0.9555555555555556,
            0.9422222222222222,
            0.9644444444444444,
            0.9955555555555555,
            0.9377777777777778,
            0.9822222222222222,
            0.92,
            0.9688888888888889,
            0.9955555555555555,
            0.8533333333333334,
            1.0,
            1.0,
            0.9866666666666667
        ],
        "reward_history": [
            -2250,
            -2041,
            -2250,
            -718,
            -2250,
            -2250,
            -1001,
            -1073,
            -523,
            -927,
            -926,
            -2250,
            -2250,
            -109,
            -477,
            -195,
            -103,
            -653,
            -307,
            -745,
            -487,
            -269,
            -1904,
            -121,
            -191,
            -244
        ],
        "steps_history": [
            2250,
            2142,
            2250,
            819,
            2250,
            2250,
            1102,
            1174,
            624,
            1028,
            1027,
            2250,
            2250,
            210,
            578,
            296,
            204,
            754,
            408,
            846,
            588,
            370,
            2005,
            222,
            292,
            345
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "420/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.3_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.42665958404541,
        "final_policy_stability": 0.9822222222222222,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.8088888888888889,
            0.88,
            0.9066666666666666,
            0.8222222222222222,
            0.8177777777777778,
            0.8755555555555555,
            0.8711111111111111,
            0.92,
            0.9688888888888889,
            0.9777777777777777,
            0.9733333333333334,
            0.9644444444444444,
            0.9644444444444444,
            0.9688888888888889,
            0.9777777777777777,
            1.0,
            0.9777777777777777,
            0.96,
            0.9155555555555556,
            1.0,
            0.9377777777777778,
            0.9822222222222222
        ],
        "reward_history": [
            -1246,
            -2048,
            -2250,
            -831,
            -2250,
            -2250,
            -1741,
            -1661,
            -526,
            -179,
            -942,
            -242,
            -557,
            -417,
            -382,
            -460,
            -223,
            -226,
            -480,
            -2250,
            -91,
            -626,
            -760
        ],
        "steps_history": [
            1347,
            2149,
            2250,
            932,
            2250,
            2250,
            1842,
            1762,
            627,
            280,
            1043,
            343,
            658,
            518,
            483,
            561,
            324,
            327,
            581,
            2250,
            192,
            727,
            861
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "421/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.419843912124634,
        "final_policy_stability": 0.9866666666666667,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.9244444444444444,
            0.8088888888888889,
            0.8533333333333334,
            0.8844444444444445,
            0.8177777777777778,
            0.8711111111111111,
            0.9288888888888889,
            0.8444444444444444,
            0.9333333333333333,
            0.9466666666666667,
            0.8888888888888888,
            0.9644444444444444,
            0.9377777777777778,
            0.96,
            0.9555555555555556,
            0.92,
            0.9422222222222222,
            0.8488888888888889,
            0.9822222222222222,
            0.9911111111111112,
            1.0,
            0.9866666666666667
        ],
        "reward_history": [
            -2250,
            -1047,
            -2250,
            -2250,
            -562,
            -2250,
            -1396,
            -977,
            -2250,
            -316,
            -395,
            -778,
            -297,
            -855,
            -188,
            -1002,
            -1266,
            -1340,
            -2250,
            -451,
            -563,
            -104,
            -95
        ],
        "steps_history": [
            2250,
            1148,
            2250,
            2250,
            663,
            2250,
            1497,
            1078,
            2250,
            417,
            496,
            879,
            398,
            956,
            289,
            1103,
            1367,
            1441,
            2250,
            552,
            664,
            205,
            196
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "422/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.254400253295898,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 21,
        "policy_stability_history": [
            0.0,
            0.8355555555555556,
            0.8844444444444445,
            0.8933333333333333,
            0.8311111111111111,
            0.8977777777777778,
            0.9244444444444444,
            0.8222222222222222,
            0.9422222222222222,
            0.8666666666666667,
            0.8577777777777778,
            0.9511111111111111,
            0.9688888888888889,
            1.0,
            0.92,
            0.9866666666666667,
            0.96,
            0.9955555555555555,
            1.0,
            0.9911111111111112,
            0.96,
            1.0
        ],
        "reward_history": [
            -543,
            -2250,
            -1042,
            -962,
            -1960,
            -1766,
            -442,
            -1957,
            -590,
            -1926,
            -2250,
            -1313,
            -896,
            -125,
            -2250,
            -343,
            -934,
            -374,
            -134,
            -832,
            -1200,
            -538
        ],
        "steps_history": [
            644,
            2250,
            1143,
            1063,
            2061,
            1867,
            543,
            2058,
            691,
            2027,
            2250,
            1414,
            997,
            226,
            2250,
            444,
            1035,
            475,
            235,
            933,
            1301,
            639
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "423/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.35914421081543,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.9333333333333333,
            0.8,
            0.9377777777777778,
            0.8888888888888888,
            0.9022222222222223,
            0.8266666666666667,
            0.9511111111111111,
            0.8622222222222222,
            0.8888888888888888,
            0.9644444444444444,
            0.9911111111111112,
            0.9688888888888889,
            0.9822222222222222,
            0.9822222222222222,
            0.9511111111111111,
            0.9911111111111112,
            0.9866666666666667,
            0.9911111111111112,
            0.9377777777777778,
            0.9466666666666667,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -1378,
            -214,
            -1963,
            -515,
            -928,
            -926,
            -2250,
            -437,
            -2250,
            -1947,
            -325,
            -152,
            -462,
            -198,
            -162,
            -1397,
            -230,
            -86,
            -79,
            -1164,
            -747,
            -378,
            -295,
            -101
        ],
        "steps_history": [
            1479,
            315,
            2064,
            616,
            1029,
            1027,
            2250,
            538,
            2250,
            2048,
            426,
            253,
            563,
            299,
            263,
            1498,
            331,
            187,
            180,
            1265,
            848,
            479,
            396,
            202
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "424/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.580066442489624,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8622222222222222,
            0.8222222222222222,
            0.8977777777777778,
            0.84,
            0.9111111111111111,
            0.8,
            0.9511111111111111,
            0.8888888888888888,
            0.9111111111111111,
            0.9244444444444444,
            0.9511111111111111,
            0.9644444444444444,
            0.9688888888888889,
            0.8488888888888889,
            0.9733333333333334,
            0.96,
            0.9288888888888889,
            0.8844444444444445,
            0.9422222222222222,
            0.9644444444444444,
            0.9866666666666667,
            0.9955555555555555,
            0.9911111111111112
        ],
        "reward_history": [
            -2250,
            -2250,
            -2250,
            -635,
            -2250,
            -725,
            -2250,
            -232,
            -1456,
            -1441,
            -737,
            -369,
            -462,
            -274,
            -2029,
            -257,
            -780,
            -1427,
            -2250,
            -498,
            -959,
            -117,
            -256,
            -140
        ],
        "steps_history": [
            2250,
            2250,
            2250,
            736,
            2250,
            826,
            2250,
            333,
            1557,
            1542,
            838,
            470,
            563,
            375,
            2130,
            358,
            881,
            1528,
            2250,
            599,
            1060,
            218,
            357,
            241
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "425/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.5848073959350586,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8177777777777778,
            0.8177777777777778,
            0.8311111111111111,
            0.8977777777777778,
            0.9377777777777778,
            0.9377777777777778,
            0.8044444444444444,
            0.9066666666666666,
            0.9066666666666666,
            0.9866666666666667,
            0.9688888888888889,
            0.9511111111111111,
            0.9422222222222222,
            0.9955555555555555,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -859,
            -2250,
            -2250,
            -2250,
            -569,
            -130,
            -291,
            -2250,
            -843,
            -1763,
            -91,
            -507,
            -873,
            -1081,
            -201,
            -354,
            -84,
            -125,
            -256
        ],
        "steps_history": [
            960,
            2250,
            2250,
            2250,
            670,
            231,
            392,
            2250,
            944,
            1864,
            192,
            608,
            974,
            1182,
            302,
            455,
            185,
            226,
            357
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "426/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.0730016231536865,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8222222222222222,
            0.9511111111111111,
            0.9066666666666666,
            0.9511111111111111,
            0.8133333333333334,
            0.9555555555555556,
            0.9022222222222223,
            0.9644444444444444,
            0.8622222222222222,
            0.9244444444444444,
            0.96,
            0.9688888888888889,
            0.9644444444444444,
            0.9555555555555556,
            0.9511111111111111,
            0.9688888888888889,
            0.9333333333333333,
            0.9777777777777777,
            0.9511111111111111,
            0.9822222222222222,
            0.9911111111111112,
            0.9822222222222222,
            0.9688888888888889,
            0.96,
            0.9955555555555555,
            1.0,
            0.9955555555555555,
            0.9911111111111112
        ],
        "reward_history": [
            -2012,
            -2250,
            -284,
            -877,
            -149,
            -1484,
            -320,
            -2250,
            -165,
            -2250,
            -1000,
            -256,
            -116,
            -280,
            -528,
            -390,
            -233,
            -613,
            -481,
            -385,
            -87,
            -511,
            -286,
            -717,
            -772,
            -250,
            -127,
            -1126,
            -344
        ],
        "steps_history": [
            2113,
            2250,
            385,
            978,
            250,
            1585,
            421,
            2250,
            266,
            2250,
            1101,
            357,
            217,
            381,
            629,
            491,
            334,
            714,
            582,
            486,
            188,
            612,
            387,
            818,
            873,
            351,
            228,
            1227,
            445
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "427/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.374762296676636,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 21,
        "policy_stability_history": [
            0.0,
            0.8133333333333334,
            0.8844444444444445,
            0.9333333333333333,
            0.9111111111111111,
            0.8711111111111111,
            0.8311111111111111,
            0.8133333333333334,
            0.9244444444444444,
            0.8888888888888888,
            0.9466666666666667,
            0.9377777777777778,
            0.9066666666666666,
            0.9777777777777777,
            0.8711111111111111,
            0.9644444444444444,
            0.9822222222222222,
            0.9555555555555556,
            1.0,
            0.9777777777777777,
            0.9955555555555555,
            0.9911111111111112
        ],
        "reward_history": [
            -549,
            -2250,
            -368,
            -240,
            -543,
            -720,
            -2250,
            -2250,
            -357,
            -1132,
            -364,
            -483,
            -1205,
            -362,
            -2250,
            -405,
            -319,
            -1066,
            -278,
            -831,
            -385,
            -287
        ],
        "steps_history": [
            650,
            2250,
            469,
            341,
            644,
            821,
            2250,
            2250,
            458,
            1233,
            465,
            584,
            1306,
            463,
            2250,
            506,
            420,
            1167,
            379,
            932,
            486,
            388
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "428/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.560749530792236,
        "final_policy_stability": 0.9822222222222222,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.8177777777777778,
            0.8311111111111111,
            0.8711111111111111,
            0.9422222222222222,
            0.8,
            0.8177777777777778,
            0.9111111111111111,
            0.84,
            0.8577777777777778,
            0.9644444444444444,
            0.9688888888888889,
            0.9511111111111111,
            0.9822222222222222,
            0.9511111111111111,
            0.9644444444444444,
            0.9644444444444444,
            0.9911111111111112,
            0.9911111111111112,
            0.9733333333333334,
            1.0,
            0.9911111111111112,
            0.9777777777777777,
            0.9822222222222222
        ],
        "reward_history": [
            -2250,
            -2143,
            -1643,
            -1087,
            -868,
            -176,
            -2250,
            -1892,
            -519,
            -1958,
            -1520,
            -310,
            -368,
            -935,
            -125,
            -831,
            -303,
            -495,
            -224,
            -288,
            -493,
            -447,
            -529,
            -283,
            -357
        ],
        "steps_history": [
            2250,
            2244,
            1744,
            1188,
            969,
            277,
            2250,
            1993,
            620,
            2059,
            1621,
            411,
            469,
            1036,
            226,
            932,
            404,
            596,
            325,
            389,
            594,
            548,
            630,
            384,
            458
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "429/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.75729775428772,
        "final_policy_stability": 0.9911111111111112,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8,
            0.8622222222222222,
            0.9066666666666666,
            0.8933333333333333,
            0.9377777777777778,
            0.8222222222222222,
            0.8533333333333334,
            0.9466666666666667,
            0.9288888888888889,
            0.9777777777777777,
            0.8355555555555556,
            0.9288888888888889,
            0.8933333333333333,
            0.9822222222222222,
            1.0,
            0.8888888888888888,
            0.9688888888888889,
            0.9733333333333334,
            0.9955555555555555,
            0.9911111111111112,
            0.9955555555555555,
            1.0,
            0.9911111111111112
        ],
        "reward_history": [
            -205,
            -2250,
            -1036,
            -892,
            -974,
            -101,
            -2250,
            -1822,
            -370,
            -549,
            -474,
            -2250,
            -780,
            -747,
            -287,
            -56,
            -1809,
            -229,
            -749,
            -172,
            -466,
            -666,
            -320,
            -412
        ],
        "steps_history": [
            306,
            2250,
            1137,
            993,
            1075,
            202,
            2250,
            1923,
            471,
            650,
            575,
            2250,
            881,
            848,
            388,
            157,
            1910,
            330,
            850,
            273,
            567,
            767,
            421,
            513
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "430/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.670084476470947,
        "final_policy_stability": 0.9777777777777777,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8177777777777778,
            0.8622222222222222,
            0.8266666666666667,
            0.9066666666666666,
            0.9111111111111111,
            0.9377777777777778,
            0.7866666666666666,
            0.9377777777777778,
            0.9511111111111111,
            0.9288888888888889,
            0.9422222222222222,
            0.8444444444444444,
            0.96,
            0.96,
            0.9955555555555555,
            0.8622222222222222,
            0.9866666666666667,
            0.9733333333333334,
            0.9244444444444444,
            0.96,
            0.92,
            0.9422222222222222,
            0.9555555555555556,
            0.9733333333333334,
            0.9955555555555555,
            0.9866666666666667,
            0.9911111111111112,
            0.9777777777777777
        ],
        "reward_history": [
            -865,
            -2250,
            -1118,
            -2250,
            -698,
            -1040,
            -173,
            -2250,
            -300,
            -177,
            -298,
            -532,
            -2018,
            -114,
            -318,
            -170,
            -1734,
            -284,
            -147,
            -823,
            -837,
            -803,
            -924,
            -601,
            -389,
            -215,
            -560,
            -356,
            -181
        ],
        "steps_history": [
            966,
            2250,
            1219,
            2250,
            799,
            1141,
            274,
            2250,
            401,
            278,
            399,
            633,
            2119,
            215,
            419,
            271,
            1835,
            385,
            248,
            924,
            938,
            904,
            1025,
            702,
            490,
            316,
            661,
            457,
            282
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "431/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.783920049667358,
        "final_policy_stability": 0.9866666666666667,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.8177777777777778,
            0.8266666666666667,
            0.88,
            0.8577777777777778,
            0.9244444444444444,
            0.9111111111111111,
            0.8444444444444444,
            0.9555555555555556,
            0.9511111111111111,
            0.8311111111111111,
            0.96,
            0.9822222222222222,
            0.9644444444444444,
            0.9466666666666667,
            0.9822222222222222,
            0.9688888888888889,
            0.9466666666666667,
            0.9422222222222222,
            1.0,
            0.9777777777777777,
            0.9955555555555555,
            0.9777777777777777,
            0.8844444444444445,
            0.9733333333333334,
            0.9866666666666667,
            0.9866666666666667,
            1.0,
            0.9822222222222222,
            0.9866666666666667
        ],
        "reward_history": [
            -2250,
            -1042,
            -1480,
            -734,
            -987,
            -1035,
            -591,
            -2250,
            -283,
            -443,
            -2250,
            -224,
            -28,
            -496,
            -349,
            -126,
            -312,
            -410,
            -663,
            -53,
            -230,
            -239,
            -253,
            -1428,
            -1019,
            -474,
            -331,
            -149,
            -805,
            -730
        ],
        "steps_history": [
            2250,
            1143,
            1581,
            835,
            1088,
            1136,
            692,
            2250,
            384,
            544,
            2250,
            325,
            129,
            597,
            450,
            227,
            413,
            511,
            764,
            154,
            331,
            340,
            354,
            1529,
            1120,
            575,
            432,
            250,
            906,
            831
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "432/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.552985191345215,
        "final_policy_stability": 0.9644444444444444,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.7955555555555556,
            0.8355555555555556,
            0.8533333333333334,
            0.88,
            0.8311111111111111,
            0.9466666666666667,
            0.9511111111111111,
            0.8266666666666667,
            0.9555555555555556,
            0.9377777777777778,
            0.8355555555555556,
            0.9555555555555556,
            0.9644444444444444,
            0.8755555555555555,
            0.9644444444444444,
            0.96,
            0.9777777777777777,
            0.9555555555555556,
            0.9688888888888889,
            0.9377777777777778,
            0.9644444444444444,
            0.9688888888888889,
            0.9333333333333333,
            0.9511111111111111,
            0.9955555555555555,
            0.9866666666666667,
            1.0,
            0.9644444444444444
        ],
        "reward_history": [
            -472,
            -1608,
            -2250,
            -891,
            -753,
            -2250,
            -193,
            -333,
            -2250,
            -237,
            -451,
            -2250,
            -315,
            -34,
            -1093,
            -238,
            -590,
            -300,
            -278,
            -172,
            -571,
            -173,
            -525,
            -330,
            -697,
            -84,
            -476,
            -285,
            -857
        ],
        "steps_history": [
            573,
            1709,
            2250,
            992,
            854,
            2250,
            294,
            434,
            2250,
            338,
            552,
            2250,
            416,
            135,
            1194,
            339,
            691,
            401,
            379,
            273,
            672,
            274,
            626,
            431,
            798,
            185,
            577,
            386,
            958
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "433/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.47670578956604,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8177777777777778,
            0.8133333333333334,
            0.9244444444444444,
            0.8844444444444445,
            0.8844444444444445,
            0.8888888888888888,
            0.7911111111111111,
            0.88,
            0.9688888888888889,
            0.9288888888888889,
            0.9822222222222222,
            0.9422222222222222,
            0.9555555555555556,
            0.8488888888888889,
            0.9688888888888889,
            0.96,
            0.9155555555555556,
            0.9777777777777777,
            0.9866666666666667,
            0.9422222222222222,
            0.9733333333333334,
            0.9733333333333334,
            0.9955555555555555,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2250,
            -2250,
            -1231,
            -351,
            -1093,
            -1150,
            -869,
            -2250,
            -764,
            -61,
            -759,
            -57,
            -402,
            -613,
            -1847,
            -492,
            -355,
            -965,
            -607,
            -83,
            -530,
            -277,
            -147,
            -131,
            -299,
            -77,
            -166
        ],
        "steps_history": [
            2250,
            2250,
            1332,
            452,
            1194,
            1251,
            970,
            2250,
            865,
            162,
            860,
            158,
            503,
            714,
            1948,
            593,
            456,
            1066,
            708,
            184,
            631,
            378,
            248,
            232,
            400,
            178,
            267
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "434/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.957916736602783,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.9155555555555556,
            0.8844444444444445,
            0.8666666666666667,
            0.7911111111111111,
            0.84,
            0.7955555555555556,
            0.9422222222222222,
            0.8933333333333333,
            0.9377777777777778,
            0.9422222222222222,
            0.9777777777777777,
            0.9244444444444444,
            0.9466666666666667,
            0.8888888888888888,
            0.9244444444444444,
            0.9333333333333333,
            0.9688888888888889,
            0.9644444444444444,
            0.9733333333333334,
            0.9466666666666667,
            0.9955555555555555,
            0.9955555555555555,
            0.9777777777777777,
            0.9511111111111111,
            1.0,
            0.9644444444444444,
            0.9777777777777777,
            0.9955555555555555
        ],
        "reward_history": [
            -2250,
            -313,
            -491,
            -613,
            -2250,
            -2250,
            -2115,
            -177,
            -994,
            -268,
            -656,
            -39,
            -569,
            -206,
            -648,
            -737,
            -325,
            -10,
            -203,
            -158,
            -380,
            -115,
            -135,
            -189,
            -446,
            -108,
            -449,
            -204,
            -279
        ],
        "steps_history": [
            2250,
            414,
            592,
            714,
            2250,
            2250,
            2216,
            278,
            1095,
            369,
            757,
            140,
            670,
            307,
            749,
            838,
            426,
            111,
            304,
            259,
            481,
            216,
            236,
            290,
            547,
            209,
            550,
            305,
            380
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "435/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.454700469970703,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.8088888888888889,
            0.88,
            0.9066666666666666,
            0.8266666666666667,
            0.8222222222222222,
            0.8711111111111111,
            0.8755555555555555,
            0.9244444444444444,
            0.9688888888888889,
            0.9777777777777777,
            0.9733333333333334,
            0.9644444444444444,
            0.9688888888888889,
            0.9688888888888889,
            0.9733333333333334,
            1.0,
            0.9866666666666667,
            0.9644444444444444,
            0.9066666666666666,
            1.0,
            0.9111111111111111,
            1.0
        ],
        "reward_history": [
            -1246,
            -2048,
            -2250,
            -831,
            -2250,
            -2250,
            -1741,
            -1661,
            -526,
            -179,
            -942,
            -242,
            -557,
            -417,
            -382,
            -460,
            -223,
            -226,
            -480,
            -2250,
            -91,
            -1487,
            -42
        ],
        "steps_history": [
            1347,
            2149,
            2250,
            932,
            2250,
            2250,
            1842,
            1762,
            627,
            280,
            1043,
            343,
            658,
            518,
            483,
            561,
            324,
            327,
            581,
            2250,
            192,
            1588,
            143
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "436/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.37676739692688,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.9244444444444444,
            0.8266666666666667,
            0.8622222222222222,
            0.8266666666666667,
            0.8133333333333334,
            0.8844444444444445,
            0.9644444444444444,
            0.8533333333333334,
            0.9155555555555556,
            0.9155555555555556,
            0.9466666666666667,
            0.96,
            0.8622222222222222,
            0.8666666666666667,
            0.9644444444444444,
            0.8711111111111111,
            0.96,
            0.9466666666666667,
            0.9733333333333334,
            0.9777777777777777,
            0.9955555555555555,
            0.9822222222222222,
            0.9688888888888889,
            0.9955555555555555
        ],
        "reward_history": [
            -2250,
            -1047,
            -2250,
            -1904,
            -2250,
            -1527,
            -1106,
            -147,
            -2250,
            -428,
            -1049,
            -310,
            -147,
            -2250,
            -2250,
            -367,
            -2250,
            -891,
            -768,
            -95,
            -198,
            -274,
            -308,
            -1183,
            -670
        ],
        "steps_history": [
            2250,
            1148,
            2250,
            2005,
            2250,
            1628,
            1207,
            248,
            2250,
            529,
            1150,
            411,
            248,
            2250,
            2250,
            468,
            2250,
            992,
            869,
            196,
            299,
            375,
            409,
            1284,
            771
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "437/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.6936123371124268,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 20,
        "policy_stability_history": [
            0.0,
            0.8355555555555556,
            0.8844444444444445,
            0.8933333333333333,
            0.8311111111111111,
            0.8977777777777778,
            0.9244444444444444,
            0.8444444444444444,
            0.8311111111111111,
            0.8533333333333334,
            0.9777777777777777,
            0.9155555555555556,
            0.8844444444444445,
            0.9555555555555556,
            0.9688888888888889,
            0.96,
            0.9911111111111112,
            1.0,
            0.9866666666666667,
            0.9644444444444444,
            1.0
        ],
        "reward_history": [
            -543,
            -2250,
            -1042,
            -962,
            -1960,
            -1766,
            -442,
            -1957,
            -2250,
            -2250,
            -585,
            -1095,
            -2250,
            -699,
            -766,
            -934,
            -374,
            -134,
            -832,
            -1200,
            -538
        ],
        "steps_history": [
            644,
            2250,
            1143,
            1063,
            2061,
            1867,
            543,
            2058,
            2250,
            2250,
            686,
            1196,
            2250,
            800,
            867,
            1035,
            475,
            235,
            933,
            1301,
            639
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "438/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.8270225524902344,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 20,
        "policy_stability_history": [
            0.0,
            0.9333333333333333,
            0.8,
            0.9377777777777778,
            0.8888888888888888,
            0.9022222222222223,
            0.8488888888888889,
            0.9066666666666666,
            0.8577777777777778,
            0.9111111111111111,
            0.8711111111111111,
            0.9066666666666666,
            0.9111111111111111,
            0.92,
            0.9866666666666667,
            0.9466666666666667,
            0.9911111111111112,
            0.9866666666666667,
            1.0,
            1.0,
            0.9955555555555555
        ],
        "reward_history": [
            -1378,
            -214,
            -1963,
            -515,
            -928,
            -926,
            -2250,
            -757,
            -2250,
            -1627,
            -1885,
            -1913,
            -1164,
            -747,
            -378,
            -1506,
            -503,
            -1454,
            -138,
            -429,
            -411
        ],
        "steps_history": [
            1479,
            315,
            2064,
            616,
            1029,
            1027,
            2250,
            858,
            2250,
            1728,
            1986,
            2014,
            1265,
            848,
            479,
            1607,
            604,
            1555,
            239,
            530,
            512
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "439/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.429070949554443,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8711111111111111,
            0.8266666666666667,
            0.9022222222222223,
            0.8488888888888889,
            0.9111111111111111,
            0.8888888888888888,
            0.9288888888888889,
            0.9155555555555556,
            0.9511111111111111,
            0.9644444444444444,
            0.88,
            0.96,
            0.9511111111111111,
            0.9911111111111112,
            0.9688888888888889,
            0.8355555555555556,
            0.9822222222222222,
            0.9822222222222222,
            0.9777777777777777,
            0.9777777777777777,
            0.9777777777777777,
            0.9733333333333334,
            0.9822222222222222,
            0.9955555555555555
        ],
        "reward_history": [
            -2250,
            -2250,
            -2250,
            -635,
            -2250,
            -725,
            -536,
            -1016,
            -1059,
            -544,
            -480,
            -2250,
            -499,
            -177,
            -184,
            -274,
            -2029,
            -257,
            -361,
            -534,
            -338,
            -772,
            -886,
            -708,
            -804
        ],
        "steps_history": [
            2250,
            2250,
            2250,
            736,
            2250,
            826,
            637,
            1117,
            1160,
            645,
            581,
            2250,
            600,
            278,
            285,
            375,
            2130,
            358,
            462,
            635,
            439,
            873,
            987,
            809,
            905
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "440/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.227187633514404,
        "final_policy_stability": 0.9555555555555556,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.8177777777777778,
            0.8133333333333334,
            0.8444444444444444,
            0.84,
            0.9555555555555556,
            0.9111111111111111,
            0.8311111111111111,
            0.9288888888888889,
            0.8577777777777778,
            0.9288888888888889,
            0.9733333333333334,
            0.9733333333333334,
            0.9511111111111111,
            0.9911111111111112,
            0.9644444444444444,
            0.9733333333333334,
            0.9822222222222222,
            0.9822222222222222,
            0.9866666666666667,
            1.0,
            0.9911111111111112,
            0.9555555555555556
        ],
        "reward_history": [
            -859,
            -2250,
            -2250,
            -753,
            -2250,
            -150,
            -1068,
            -2250,
            -539,
            -2250,
            -1206,
            -528,
            -158,
            -354,
            -84,
            -703,
            -792,
            -355,
            -364,
            -345,
            -211,
            -323,
            -829
        ],
        "steps_history": [
            960,
            2250,
            2250,
            854,
            2250,
            251,
            1169,
            2250,
            640,
            2250,
            1307,
            629,
            259,
            455,
            185,
            804,
            893,
            456,
            465,
            446,
            312,
            424,
            930
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "441/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.293806552886963,
        "final_policy_stability": 0.9866666666666667,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8222222222222222,
            0.9511111111111111,
            0.9066666666666666,
            0.9511111111111111,
            0.8133333333333334,
            0.9555555555555556,
            0.9022222222222223,
            0.9644444444444444,
            0.8622222222222222,
            0.9288888888888889,
            0.9022222222222223,
            0.9644444444444444,
            0.9822222222222222,
            0.9644444444444444,
            0.9555555555555556,
            0.9733333333333334,
            0.9777777777777777,
            0.9555555555555556,
            0.9822222222222222,
            0.9688888888888889,
            0.9955555555555555,
            1.0,
            1.0,
            0.9866666666666667
        ],
        "reward_history": [
            -2012,
            -2250,
            -284,
            -877,
            -149,
            -1484,
            -320,
            -2250,
            -165,
            -2250,
            -587,
            -1267,
            -688,
            -284,
            -179,
            -613,
            -481,
            -302,
            -782,
            -208,
            -795,
            -772,
            -250,
            -127,
            -1126
        ],
        "steps_history": [
            2113,
            2250,
            385,
            978,
            250,
            1585,
            421,
            2250,
            266,
            2250,
            688,
            1368,
            789,
            385,
            280,
            714,
            582,
            403,
            883,
            309,
            896,
            873,
            351,
            228,
            1227
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "442/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.501490592956543,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8133333333333334,
            0.8844444444444445,
            0.9333333333333333,
            0.8977777777777778,
            0.8755555555555555,
            0.8311111111111111,
            0.8133333333333334,
            0.9244444444444444,
            0.8933333333333333,
            0.9422222222222222,
            0.9377777777777778,
            0.9066666666666666,
            0.9777777777777777,
            0.8755555555555555,
            0.9644444444444444,
            0.9822222222222222,
            0.9688888888888889,
            0.9866666666666667,
            0.9866666666666667,
            0.9822222222222222,
            0.9866666666666667,
            0.9733333333333334,
            1.0
        ],
        "reward_history": [
            -549,
            -2250,
            -368,
            -240,
            -543,
            -720,
            -2250,
            -2250,
            -357,
            -1132,
            -364,
            -483,
            -1205,
            -362,
            -2250,
            -405,
            -319,
            -648,
            -317,
            -278,
            -831,
            -385,
            -615,
            -275
        ],
        "steps_history": [
            650,
            2250,
            469,
            341,
            644,
            821,
            2250,
            2250,
            458,
            1233,
            465,
            584,
            1306,
            463,
            2250,
            506,
            420,
            749,
            418,
            379,
            932,
            486,
            716,
            376
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "443/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.346283674240112,
        "final_policy_stability": 0.9777777777777777,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.7777777777777778,
            0.8177777777777778,
            0.8311111111111111,
            0.8711111111111111,
            0.9422222222222222,
            0.8,
            0.8177777777777778,
            0.9111111111111111,
            0.8355555555555556,
            0.8533333333333334,
            0.96,
            0.9733333333333334,
            0.9377777777777778,
            0.9866666666666667,
            0.9422222222222222,
            0.9866666666666667,
            0.9688888888888889,
            0.9377777777777778,
            0.9911111111111112,
            0.9955555555555555,
            1.0,
            0.9955555555555555,
            0.9866666666666667,
            0.9777777777777777
        ],
        "reward_history": [
            -2250,
            -2143,
            -1643,
            -1087,
            -868,
            -176,
            -2250,
            -1892,
            -534,
            -1943,
            -1520,
            -310,
            -368,
            -935,
            -125,
            -547,
            -125,
            -361,
            -820,
            -288,
            -493,
            -447,
            -529,
            -283,
            -357
        ],
        "steps_history": [
            2250,
            2244,
            1744,
            1188,
            969,
            277,
            2250,
            1993,
            635,
            2044,
            1621,
            411,
            469,
            1036,
            226,
            648,
            226,
            462,
            921,
            389,
            594,
            548,
            630,
            384,
            458
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "444/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.23368763923645,
        "final_policy_stability": 0.9866666666666667,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8,
            0.8622222222222222,
            0.9066666666666666,
            0.8933333333333333,
            0.9377777777777778,
            0.8222222222222222,
            0.8533333333333334,
            0.9466666666666667,
            0.9288888888888889,
            0.9777777777777777,
            0.84,
            0.9422222222222222,
            0.9288888888888889,
            0.9911111111111112,
            0.9733333333333334,
            0.9822222222222222,
            0.9066666666666666,
            0.9688888888888889,
            0.9733333333333334,
            0.9911111111111112,
            0.9866666666666667,
            0.9911111111111112,
            1.0,
            0.9866666666666667
        ],
        "reward_history": [
            -205,
            -2250,
            -1036,
            -892,
            -974,
            -101,
            -2250,
            -1822,
            -370,
            -549,
            -474,
            -2250,
            -780,
            -747,
            -207,
            -136,
            -158,
            -1550,
            -229,
            -749,
            -172,
            -466,
            -666,
            -320,
            -412
        ],
        "steps_history": [
            306,
            2250,
            1137,
            993,
            1075,
            202,
            2250,
            1923,
            471,
            650,
            575,
            2250,
            881,
            848,
            308,
            237,
            259,
            1651,
            330,
            850,
            273,
            567,
            767,
            421,
            513
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "445/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.2962541580200195,
        "final_policy_stability": 0.9955555555555555,
        "episodes_to_convergence": 21,
        "policy_stability_history": [
            0.0,
            0.8666666666666667,
            0.8088888888888889,
            0.7511111111111111,
            0.8222222222222222,
            0.9466666666666667,
            0.9333333333333333,
            0.9422222222222222,
            0.88,
            0.8711111111111111,
            0.9688888888888889,
            0.8711111111111111,
            0.9288888888888889,
            0.8311111111111111,
            0.9822222222222222,
            0.9777777777777777,
            0.9555555555555556,
            0.9911111111111112,
            0.9822222222222222,
            0.9777777777777777,
            1.0,
            0.9955555555555555
        ],
        "reward_history": [
            -865,
            -757,
            -2250,
            -2250,
            -1059,
            -243,
            -292,
            -489,
            -2250,
            -2250,
            -473,
            -1208,
            -589,
            -2250,
            -95,
            -196,
            -548,
            -435,
            -301,
            -326,
            -376,
            -325
        ],
        "steps_history": [
            966,
            858,
            2250,
            2250,
            1160,
            344,
            393,
            590,
            2250,
            2250,
            574,
            1309,
            690,
            2250,
            196,
            297,
            649,
            536,
            402,
            427,
            477,
            426
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "446/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.128911018371582,
        "final_policy_stability": 0.9688888888888889,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8177777777777778,
            0.8266666666666667,
            0.8844444444444445,
            0.8533333333333334,
            0.9022222222222223,
            0.9688888888888889,
            0.9466666666666667,
            0.8222222222222222,
            0.9066666666666666,
            0.8666666666666667,
            0.9466666666666667,
            0.8844444444444445,
            0.9555555555555556,
            0.9822222222222222,
            0.9911111111111112,
            0.9822222222222222,
            0.9244444444444444,
            0.9777777777777777,
            0.9866666666666667,
            0.8622222222222222,
            0.9911111111111112,
            0.9911111111111112,
            0.9866666666666667,
            0.9822222222222222,
            1.0,
            0.9955555555555555,
            1.0,
            0.9688888888888889
        ],
        "reward_history": [
            -2250,
            -1042,
            -1480,
            -734,
            -987,
            -1035,
            -142,
            -348,
            -2250,
            -491,
            -1203,
            -333,
            -2250,
            -378,
            -159,
            -296,
            -81,
            -749,
            -223,
            -192,
            -1836,
            -609,
            -309,
            -474,
            -331,
            -149,
            -253,
            -451,
            -730
        ],
        "steps_history": [
            2250,
            1143,
            1581,
            835,
            1088,
            1136,
            243,
            449,
            2250,
            592,
            1304,
            434,
            2250,
            479,
            260,
            397,
            182,
            850,
            324,
            293,
            1937,
            710,
            410,
            575,
            432,
            250,
            354,
            552,
            831
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "447/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.357941627502441,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.7955555555555556,
            0.8355555555555556,
            0.8533333333333334,
            0.88,
            0.8355555555555556,
            0.9466666666666667,
            0.9511111111111111,
            0.8222222222222222,
            0.9644444444444444,
            0.8266666666666667,
            0.9511111111111111,
            0.9288888888888889,
            0.9155555555555556,
            0.8622222222222222,
            0.9288888888888889,
            0.9777777777777777,
            0.9822222222222222,
            0.9911111111111112,
            0.9822222222222222,
            0.9155555555555556,
            0.9777777777777777,
            0.9555555555555556,
            0.9911111111111112,
            0.9866666666666667,
            1.0
        ],
        "reward_history": [
            -472,
            -1608,
            -2250,
            -891,
            -753,
            -2250,
            -193,
            -333,
            -1723,
            -226,
            -1887,
            -215,
            -730,
            -707,
            -940,
            -590,
            -238,
            -300,
            -278,
            -172,
            -746,
            -672,
            -1080,
            -84,
            -476,
            -285
        ],
        "steps_history": [
            573,
            1709,
            2250,
            992,
            854,
            2250,
            294,
            434,
            1824,
            327,
            1988,
            316,
            831,
            808,
            1041,
            691,
            339,
            401,
            379,
            273,
            847,
            773,
            1181,
            185,
            577,
            386
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "448/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.118858098983765,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8311111111111111,
            0.8622222222222222,
            0.8355555555555556,
            0.8488888888888889,
            0.8933333333333333,
            0.9422222222222222,
            0.9022222222222223,
            0.9111111111111111,
            0.9466666666666667,
            0.9377777777777778,
            0.9822222222222222,
            0.9022222222222223,
            0.9733333333333334,
            0.9688888888888889,
            0.8444444444444444,
            0.9688888888888889,
            0.9955555555555555,
            0.9777777777777777,
            0.9733333333333334,
            1.0,
            0.9911111111111112,
            0.9955555555555555,
            0.9644444444444444,
            0.96,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -2250,
            -2250,
            -716,
            -1968,
            -1666,
            -1040,
            -425,
            -890,
            -732,
            -231,
            -759,
            -14,
            -1423,
            -111,
            -184,
            -2135,
            -281,
            -51,
            -270,
            -768,
            -83,
            -301,
            -17,
            -266,
            -501,
            -299,
            -77,
            -166,
            -349
        ],
        "steps_history": [
            2250,
            2250,
            817,
            2069,
            1767,
            1141,
            526,
            991,
            833,
            332,
            860,
            115,
            1524,
            212,
            285,
            2236,
            382,
            152,
            371,
            869,
            184,
            402,
            118,
            367,
            602,
            400,
            178,
            267,
            450
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "449/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 7,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 28.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.175506353378296,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.7377777777777778,
            0.9333333333333333,
            0.8755555555555555,
            0.8133333333333334,
            0.7822222222222223,
            0.9333333333333333,
            0.8977777777777778,
            0.9822222222222222,
            0.8933333333333333,
            0.9377777777777778,
            0.9422222222222222,
            0.9688888888888889,
            0.9466666666666667,
            0.9555555555555556,
            0.96,
            0.9866666666666667,
            0.9644444444444444,
            0.9777777777777777,
            0.9422222222222222,
            0.9866666666666667,
            0.9733333333333334,
            0.9688888888888889,
            0.9866666666666667,
            0.9866666666666667,
            0.9733333333333334,
            0.9511111111111111,
            1.0,
            0.9688888888888889,
            0.9688888888888889,
            1.0
        ],
        "reward_history": [
            -2146,
            -2044,
            -261,
            -504,
            -2250,
            -2250,
            -338,
            -520,
            -47,
            -891,
            -317,
            -607,
            -112,
            -496,
            -206,
            -458,
            -89,
            -339,
            -297,
            -476,
            -163,
            -290,
            -248,
            -115,
            -135,
            -371,
            -264,
            -108,
            -449,
            -204,
            -279
        ],
        "steps_history": [
            2247,
            2145,
            362,
            605,
            2250,
            2250,
            439,
            621,
            148,
            992,
            418,
            708,
            213,
            597,
            307,
            559,
            190,
            440,
            398,
            577,
            264,
            391,
            349,
            216,
            236,
            472,
            365,
            209,
            550,
            305,
            380
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "450/810",
        "save_path": "experiments/20250131_160708/training_plots/size_7/lr0.4_df0.99_eps0.1_trial4"
    }
]