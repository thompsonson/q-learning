[
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.864239931106567,
        "final_policy_stability": 0.9704142011834319,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8047337278106509,
            0.8461538461538461,
            0.8994082840236687,
            0.9349112426035503,
            0.8402366863905325,
            0.8165680473372781,
            0.893491124260355,
            0.9289940828402367,
            0.9467455621301775,
            0.9644970414201184,
            0.893491124260355,
            0.9112426035502958,
            0.8579881656804734,
            0.9526627218934911,
            0.9171597633136095,
            0.8461538461538461,
            0.9644970414201184,
            0.8757396449704142,
            0.9289940828402367,
            0.9526627218934911,
            0.9881656804733728,
            0.9881656804733728,
            0.9822485207100592,
            0.9940828402366864,
            0.8994082840236687,
            0.8875739644970414,
            0.9526627218934911,
            0.9704142011834319,
            0.8875739644970414,
            0.9822485207100592,
            0.9704142011834319,
            0.9526627218934911,
            1.0,
            0.9704142011834319
        ],
        "reward_history": [
            -1235,
            -1690,
            -1335,
            -1690,
            -309,
            -1690,
            -1690,
            -1263,
            -737,
            -384,
            -414,
            -1690,
            -1690,
            -1690,
            -568,
            -624,
            -1690,
            -1021,
            -1690,
            -484,
            -436,
            -4,
            -154,
            -73,
            -79,
            -1036,
            -572,
            -399,
            -678,
            -1690,
            -391,
            -428,
            -931,
            4,
            -280
        ],
        "steps_history": [
            1336,
            1690,
            1436,
            1690,
            410,
            1690,
            1690,
            1364,
            838,
            485,
            515,
            1690,
            1690,
            1690,
            669,
            725,
            1690,
            1122,
            1690,
            585,
            537,
            105,
            255,
            174,
            180,
            1137,
            673,
            500,
            779,
            1690,
            492,
            529,
            1032,
            97,
            381
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "271/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.889142990112305,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.863905325443787,
            0.9112426035502958,
            0.8284023668639053,
            0.8461538461538461,
            0.9644970414201184,
            0.9822485207100592,
            0.9171597633136095,
            0.8698224852071006,
            0.8402366863905325,
            0.9230769230769231,
            0.8994082840236687,
            0.9704142011834319,
            0.893491124260355,
            0.9112426035502958,
            0.9940828402366864,
            0.8579881656804734,
            0.9112426035502958,
            0.9585798816568047,
            0.8875739644970414,
            0.9763313609467456,
            0.8994082840236687,
            0.9230769230769231,
            0.8994082840236687,
            0.9822485207100592,
            0.9940828402366864,
            1.0,
            0.8757396449704142,
            0.9763313609467456,
            0.863905325443787,
            0.9822485207100592,
            1.0
        ],
        "reward_history": [
            -770,
            -1060,
            -840,
            -1690,
            -1690,
            -255,
            14,
            -230,
            -1690,
            -1690,
            -654,
            -1690,
            -49,
            -1690,
            -1690,
            6,
            -1474,
            -457,
            -421,
            -833,
            -339,
            -1099,
            -621,
            -1690,
            -325,
            -27,
            -87,
            -1690,
            -396,
            -1522,
            -386,
            -312
        ],
        "steps_history": [
            871,
            1161,
            941,
            1690,
            1690,
            356,
            87,
            331,
            1690,
            1690,
            755,
            1690,
            150,
            1690,
            1690,
            95,
            1575,
            558,
            522,
            934,
            440,
            1200,
            722,
            1690,
            426,
            128,
            188,
            1690,
            497,
            1623,
            487,
            413
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "272/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.850562334060669,
        "final_policy_stability": 0.9822485207100592,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8047337278106509,
            0.8402366863905325,
            0.8757396449704142,
            0.9112426035502958,
            0.863905325443787,
            0.9349112426035503,
            0.8816568047337278,
            0.9881656804733728,
            0.9289940828402367,
            0.8579881656804734,
            0.8698224852071006,
            0.9171597633136095,
            0.8816568047337278,
            0.9230769230769231,
            0.9763313609467456,
            0.9822485207100592,
            0.9526627218934911,
            0.9704142011834319,
            0.9526627218934911,
            0.8520710059171598,
            0.863905325443787,
            0.9822485207100592,
            0.8698224852071006,
            0.9763313609467456,
            1.0,
            0.9940828402366864,
            0.8875739644970414,
            1.0,
            0.9822485207100592,
            0.9053254437869822,
            0.9881656804733728,
            1.0,
            0.9822485207100592
        ],
        "reward_history": [
            -1690,
            -1690,
            -1690,
            -1690,
            -463,
            -1690,
            -313,
            -698,
            -107,
            -282,
            -1690,
            -1690,
            -617,
            -1690,
            -1164,
            -277,
            -7,
            -304,
            -140,
            -788,
            -1690,
            -1690,
            -70,
            -1430,
            -459,
            -161,
            -47,
            -1334,
            -11,
            -145,
            -1514,
            -138,
            -210,
            -740
        ],
        "steps_history": [
            1690,
            1690,
            1690,
            1690,
            564,
            1690,
            414,
            799,
            208,
            383,
            1690,
            1690,
            718,
            1690,
            1265,
            378,
            108,
            405,
            241,
            889,
            1690,
            1690,
            171,
            1531,
            560,
            262,
            148,
            1435,
            112,
            246,
            1615,
            239,
            311,
            841
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "273/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.085975885391235,
        "final_policy_stability": 0.8816568047337278,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8461538461538461,
            0.8402366863905325,
            0.9408284023668639,
            0.9053254437869822,
            0.893491124260355,
            0.8757396449704142,
            0.893491124260355,
            0.9112426035502958,
            0.9526627218934911,
            0.9526627218934911,
            0.9230769230769231,
            0.9585798816568047,
            0.9526627218934911,
            0.8816568047337278,
            0.9763313609467456,
            0.8579881656804734,
            0.9112426035502958,
            0.8698224852071006,
            0.8579881656804734,
            0.9822485207100592,
            0.9822485207100592,
            0.9822485207100592,
            0.9704142011834319,
            0.9644970414201184,
            0.9408284023668639,
            0.9467455621301775,
            0.9940828402366864,
            0.8698224852071006,
            0.8875739644970414,
            0.8816568047337278
        ],
        "reward_history": [
            -742,
            -1690,
            -1690,
            -412,
            -338,
            -835,
            -1166,
            -1433,
            -1690,
            -221,
            -154,
            -333,
            -320,
            -495,
            -1690,
            -434,
            -1690,
            -826,
            -1096,
            -1690,
            -110,
            11,
            -49,
            -224,
            -402,
            -267,
            -735,
            -177,
            -1690,
            -1690,
            -1690
        ],
        "steps_history": [
            843,
            1690,
            1690,
            513,
            439,
            936,
            1267,
            1534,
            1690,
            322,
            255,
            434,
            421,
            596,
            1690,
            535,
            1690,
            927,
            1197,
            1690,
            211,
            90,
            150,
            325,
            503,
            368,
            836,
            278,
            1690,
            1690,
            1690
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "274/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.084238290786743,
        "final_policy_stability": 0.9822485207100592,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.757396449704142,
            0.8224852071005917,
            0.834319526627219,
            0.8402366863905325,
            0.8579881656804734,
            0.9112426035502958,
            0.9763313609467456,
            0.8994082840236687,
            0.8224852071005917,
            0.8994082840236687,
            0.9467455621301775,
            0.9289940828402367,
            0.9704142011834319,
            0.8816568047337278,
            0.8579881656804734,
            0.834319526627219,
            0.9822485207100592,
            0.9112426035502958,
            0.9881656804733728,
            0.9467455621301775,
            0.9408284023668639,
            0.9467455621301775,
            0.8579881656804734,
            0.8994082840236687,
            0.8698224852071006,
            0.9881656804733728,
            0.9940828402366864,
            0.9349112426035503,
            0.9822485207100592,
            0.863905325443787,
            0.9763313609467456,
            0.9763313609467456,
            0.8816568047337278,
            1.0,
            0.9822485207100592
        ],
        "reward_history": [
            -1690,
            -1690,
            -1690,
            -1690,
            -1345,
            -776,
            -377,
            -137,
            -892,
            -1690,
            -635,
            -731,
            -483,
            -142,
            -1690,
            -1690,
            -1690,
            -37,
            -612,
            -21,
            -293,
            -485,
            -498,
            -1690,
            -1690,
            -1690,
            -200,
            -75,
            -565,
            -335,
            -1690,
            -760,
            -243,
            -1581,
            -167,
            -448
        ],
        "steps_history": [
            1690,
            1690,
            1690,
            1690,
            1446,
            877,
            478,
            238,
            993,
            1690,
            736,
            832,
            584,
            243,
            1690,
            1690,
            1690,
            138,
            713,
            122,
            394,
            586,
            599,
            1690,
            1690,
            1690,
            301,
            176,
            666,
            436,
            1690,
            861,
            344,
            1682,
            268,
            549
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "275/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.848663806915283,
        "final_policy_stability": 0.9171597633136095,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.7633136094674556,
            0.8757396449704142,
            0.8461538461538461,
            0.8047337278106509,
            0.8461538461538461,
            0.8402366863905325,
            0.8816568047337278,
            0.8698224852071006,
            0.9171597633136095,
            0.8875739644970414,
            0.8520710059171598,
            0.9349112426035503,
            0.8579881656804734,
            0.893491124260355,
            0.9704142011834319,
            0.8698224852071006,
            0.9822485207100592,
            0.8520710059171598,
            0.9763313609467456,
            0.9822485207100592,
            0.9940828402366864,
            0.9644970414201184,
            0.9763313609467456,
            0.9704142011834319,
            0.9526627218934911,
            0.9585798816568047,
            0.9526627218934911,
            0.9822485207100592,
            0.9881656804733728,
            0.8698224852071006,
            0.9526627218934911,
            0.9763313609467456,
            0.9822485207100592,
            0.9940828402366864,
            0.9171597633136095
        ],
        "reward_history": [
            -1256,
            -1690,
            -1690,
            -925,
            -1690,
            -1690,
            -1048,
            -628,
            -969,
            -362,
            -578,
            -1690,
            -495,
            -1690,
            -840,
            -181,
            -1690,
            -328,
            -1690,
            -461,
            -61,
            -64,
            -253,
            -49,
            -109,
            -741,
            -170,
            -457,
            -155,
            -62,
            -1690,
            -206,
            -310,
            -428,
            -255,
            -850
        ],
        "steps_history": [
            1357,
            1690,
            1690,
            1026,
            1690,
            1690,
            1149,
            729,
            1070,
            463,
            679,
            1690,
            596,
            1690,
            941,
            282,
            1690,
            429,
            1690,
            562,
            162,
            165,
            354,
            150,
            210,
            842,
            271,
            558,
            256,
            163,
            1690,
            307,
            411,
            529,
            356,
            951
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "276/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.138695955276489,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.7692307692307693,
            0.9053254437869822,
            0.8757396449704142,
            0.8757396449704142,
            0.8047337278106509,
            0.9289940828402367,
            0.9053254437869822,
            0.8284023668639053,
            0.8284023668639053,
            0.8520710059171598,
            0.9349112426035503,
            0.9053254437869822,
            0.9230769230769231,
            0.9112426035502958,
            0.8698224852071006,
            0.9289940828402367,
            0.8757396449704142,
            0.9112426035502958,
            0.9112426035502958,
            0.893491124260355,
            0.8875739644970414,
            0.8224852071005917,
            0.9644970414201184,
            0.8698224852071006,
            0.9940828402366864,
            0.9704142011834319,
            1.0,
            0.9408284023668639,
            1.0,
            1.0
        ],
        "reward_history": [
            -1690,
            -1517,
            -524,
            -1690,
            -415,
            -1690,
            -211,
            -421,
            -1690,
            -1690,
            -1583,
            -560,
            -311,
            -312,
            -457,
            -1690,
            -357,
            -849,
            -882,
            -781,
            -1144,
            -796,
            -1690,
            -171,
            -1690,
            -91,
            -161,
            47,
            -1010,
            -91,
            -182
        ],
        "steps_history": [
            1690,
            1618,
            625,
            1690,
            516,
            1690,
            312,
            522,
            1690,
            1690,
            1684,
            661,
            412,
            413,
            558,
            1690,
            458,
            950,
            983,
            882,
            1245,
            897,
            1690,
            272,
            1690,
            192,
            262,
            54,
            1111,
            192,
            283
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "277/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.463974952697754,
        "final_policy_stability": 0.9940828402366864,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.7041420118343196,
            0.7988165680473372,
            0.9289940828402367,
            0.8165680473372781,
            0.8579881656804734,
            0.9408284023668639,
            0.9349112426035503,
            0.863905325443787,
            0.9230769230769231,
            0.9822485207100592,
            0.8402366863905325,
            0.9881656804733728,
            0.8402366863905325,
            0.8698224852071006,
            0.8106508875739645,
            0.9704142011834319,
            0.834319526627219,
            0.9644970414201184,
            0.9704142011834319,
            0.9112426035502958,
            1.0,
            0.8579881656804734,
            0.9940828402366864
        ],
        "reward_history": [
            -1690,
            -1557,
            -1690,
            -100,
            -830,
            -1020,
            -176,
            -369,
            -1690,
            -317,
            -58,
            -1690,
            -8,
            -1690,
            -645,
            -1491,
            -244,
            -1430,
            -159,
            -535,
            -1315,
            -65,
            -1690,
            -20
        ],
        "steps_history": [
            1690,
            1658,
            1690,
            201,
            931,
            1121,
            277,
            470,
            1690,
            418,
            159,
            1690,
            109,
            1690,
            746,
            1592,
            345,
            1531,
            260,
            636,
            1416,
            166,
            1690,
            121
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "278/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.1189866065979,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.8994082840236687,
            0.7928994082840237,
            0.8165680473372781,
            0.9171597633136095,
            0.8047337278106509,
            0.8165680473372781,
            0.9408284023668639,
            0.9112426035502958,
            0.9349112426035503,
            0.9763313609467456,
            0.8816568047337278,
            0.9704142011834319,
            0.9408284023668639,
            0.9112426035502958,
            0.834319526627219,
            0.9704142011834319,
            0.8284023668639053,
            0.8994082840236687,
            0.8224852071005917,
            0.8461538461538461,
            0.8816568047337278,
            0.9704142011834319,
            0.9053254437869822,
            0.9467455621301775,
            0.893491124260355,
            0.9112426035502958,
            0.9704142011834319,
            0.9053254437869822,
            1.0
        ],
        "reward_history": [
            -1112,
            -289,
            -1690,
            -1690,
            -96,
            -1690,
            -1690,
            -312,
            -283,
            -365,
            -62,
            -1690,
            -352,
            -504,
            -606,
            -1500,
            -242,
            -1224,
            -817,
            -1261,
            -1690,
            -1690,
            -403,
            -1329,
            -202,
            -1087,
            -881,
            -279,
            -835,
            -513
        ],
        "steps_history": [
            1213,
            390,
            1690,
            1690,
            197,
            1690,
            1690,
            413,
            384,
            466,
            163,
            1690,
            453,
            605,
            707,
            1601,
            343,
            1325,
            918,
            1362,
            1690,
            1690,
            504,
            1430,
            303,
            1188,
            982,
            380,
            936,
            614
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "279/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.68515419960022,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.8461538461538461,
            0.8520710059171598,
            0.9230769230769231,
            0.863905325443787,
            0.7869822485207101,
            0.8757396449704142,
            0.8224852071005917,
            0.9112426035502958,
            0.8579881656804734,
            0.9349112426035503,
            0.8402366863905325,
            0.834319526627219,
            0.8579881656804734,
            0.9408284023668639,
            0.863905325443787,
            0.9467455621301775,
            0.8402366863905325,
            0.8875739644970414,
            0.9763313609467456,
            0.9349112426035503,
            0.9881656804733728,
            0.9289940828402367,
            0.8698224852071006,
            0.9763313609467456,
            1.0,
            1.0,
            0.9881656804733728,
            0.863905325443787,
            1.0,
            1.0,
            0.9822485207100592,
            0.9940828402366864,
            1.0,
            0.863905325443787,
            1.0,
            1.0
        ],
        "reward_history": [
            -1690,
            -701,
            -816,
            -354,
            -1690,
            -1690,
            -732,
            -1690,
            -1178,
            -1690,
            -153,
            -1690,
            -1422,
            -1217,
            -198,
            -1166,
            -396,
            -1363,
            -674,
            -131,
            -586,
            -71,
            -635,
            -1690,
            -263,
            -139,
            -216,
            -103,
            -1690,
            -46,
            -236,
            -410,
            -96,
            -64,
            -1690,
            -121,
            -396
        ],
        "steps_history": [
            1690,
            802,
            917,
            455,
            1690,
            1690,
            833,
            1690,
            1279,
            1690,
            254,
            1690,
            1523,
            1318,
            299,
            1267,
            497,
            1464,
            775,
            232,
            687,
            172,
            736,
            1690,
            364,
            240,
            317,
            204,
            1690,
            147,
            337,
            511,
            197,
            165,
            1690,
            222,
            497
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "280/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.075897216796875,
        "final_policy_stability": 0.9822485207100592,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.7396449704142012,
            0.8757396449704142,
            0.8698224852071006,
            0.8579881656804734,
            0.834319526627219,
            0.9112426035502958,
            0.8106508875739645,
            0.8165680473372781,
            0.8816568047337278,
            0.8461538461538461,
            0.9349112426035503,
            0.834319526627219,
            0.9408284023668639,
            0.8816568047337278,
            0.9526627218934911,
            0.9881656804733728,
            0.9585798816568047,
            0.9408284023668639,
            0.863905325443787,
            0.9940828402366864,
            0.9763313609467456,
            0.9822485207100592,
            0.8402366863905325,
            0.9644970414201184,
            0.8698224852071006,
            0.9822485207100592
        ],
        "reward_history": [
            -1690,
            -1690,
            -448,
            -947,
            -734,
            -1690,
            -1690,
            -1690,
            -1690,
            -660,
            -1542,
            -290,
            -1690,
            -287,
            -1034,
            -254,
            -21,
            -118,
            -490,
            -1690,
            -110,
            -282,
            -145,
            -1690,
            -463,
            -952,
            -317
        ],
        "steps_history": [
            1690,
            1690,
            549,
            1048,
            835,
            1690,
            1690,
            1690,
            1690,
            761,
            1643,
            391,
            1690,
            388,
            1135,
            355,
            122,
            219,
            591,
            1690,
            211,
            383,
            246,
            1690,
            564,
            1053,
            418
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "281/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.59045672416687,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8461538461538461,
            0.7751479289940828,
            0.893491124260355,
            0.7988165680473372,
            0.7869822485207101,
            0.9349112426035503,
            0.9053254437869822,
            0.9171597633136095,
            0.9349112426035503,
            0.893491124260355,
            0.9644970414201184,
            0.7692307692307693,
            0.834319526627219,
            0.9644970414201184,
            0.9289940828402367,
            0.8284023668639053,
            0.8106508875739645,
            0.8994082840236687,
            0.8757396449704142,
            0.9467455621301775,
            0.7928994082840237,
            0.8816568047337278,
            0.863905325443787,
            0.8579881656804734,
            0.9644970414201184,
            0.9526627218934911,
            0.9763313609467456,
            0.9822485207100592,
            0.834319526627219,
            0.9881656804733728,
            0.9940828402366864,
            0.8520710059171598,
            0.9704142011834319,
            0.9881656804733728,
            0.9940828402366864,
            0.9822485207100592,
            1.0,
            0.9644970414201184,
            0.9940828402366864,
            1.0
        ],
        "reward_history": [
            -1690,
            -398,
            -1690,
            -527,
            -1690,
            -1690,
            -198,
            -185,
            -380,
            -359,
            -488,
            -109,
            -1690,
            -1494,
            -83,
            -608,
            -1690,
            -1690,
            -421,
            -540,
            -145,
            -1690,
            -1128,
            -1690,
            -1690,
            -147,
            -613,
            -344,
            -64,
            -1690,
            -186,
            -20,
            -863,
            -86,
            -104,
            -234,
            -346,
            -101,
            -430,
            -160,
            -144
        ],
        "steps_history": [
            1690,
            499,
            1690,
            628,
            1690,
            1690,
            299,
            286,
            481,
            460,
            589,
            210,
            1690,
            1595,
            184,
            709,
            1690,
            1690,
            522,
            641,
            246,
            1690,
            1229,
            1690,
            1690,
            248,
            714,
            445,
            165,
            1690,
            287,
            121,
            964,
            187,
            205,
            335,
            447,
            202,
            531,
            261,
            245
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "282/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.458227634429932,
        "final_policy_stability": 0.9822485207100592,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8047337278106509,
            0.7988165680473372,
            0.863905325443787,
            0.8816568047337278,
            0.9230769230769231,
            0.9526627218934911,
            0.8402366863905325,
            0.893491124260355,
            0.8461538461538461,
            0.9289940828402367,
            0.9467455621301775,
            0.8698224852071006,
            0.9526627218934911,
            0.9763313609467456,
            0.9763313609467456,
            0.9526627218934911,
            0.9526627218934911,
            0.8816568047337278,
            0.8106508875739645,
            0.834319526627219,
            0.9881656804733728,
            0.9467455621301775,
            0.9940828402366864,
            0.9171597633136095,
            0.8875739644970414,
            0.9585798816568047,
            0.8757396449704142,
            0.8698224852071006,
            1.0,
            0.8579881656804734,
            0.9289940828402367,
            1.0,
            0.9881656804733728,
            0.9940828402366864,
            0.9763313609467456,
            1.0,
            0.9822485207100592
        ],
        "reward_history": [
            -1216,
            -1690,
            -1690,
            -1690,
            -361,
            -291,
            -102,
            -1690,
            -317,
            -1690,
            -184,
            -116,
            -1249,
            -137,
            -67,
            -91,
            -268,
            -272,
            -1690,
            -1690,
            -1690,
            -198,
            -553,
            -18,
            -655,
            -746,
            -164,
            -698,
            -1690,
            -59,
            -1081,
            -278,
            -42,
            -295,
            -93,
            -209,
            22,
            -239
        ],
        "steps_history": [
            1317,
            1690,
            1690,
            1690,
            462,
            392,
            203,
            1690,
            418,
            1690,
            285,
            217,
            1350,
            238,
            168,
            192,
            369,
            373,
            1690,
            1690,
            1690,
            299,
            654,
            119,
            756,
            847,
            265,
            799,
            1690,
            160,
            1182,
            379,
            143,
            396,
            194,
            310,
            79,
            340
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "283/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.728309869766235,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8224852071005917,
            0.8520710059171598,
            0.7514792899408284,
            0.8816568047337278,
            0.9112426035502958,
            0.893491124260355,
            0.9112426035502958,
            0.8165680473372781,
            0.7751479289940828,
            0.9349112426035503,
            0.9822485207100592,
            0.863905325443787,
            0.9704142011834319,
            0.8520710059171598,
            0.8698224852071006,
            0.9467455621301775,
            0.893491124260355,
            0.9585798816568047,
            0.9763313609467456,
            0.8402366863905325,
            0.8698224852071006,
            0.9289940828402367,
            0.9822485207100592,
            0.9940828402366864,
            0.9704142011834319,
            0.8816568047337278,
            0.9940828402366864,
            0.9881656804733728,
            0.8875739644970414,
            0.9940828402366864,
            0.9822485207100592,
            0.9585798816568047,
            1.0
        ],
        "reward_history": [
            -1690,
            -912,
            -1042,
            -1256,
            -1690,
            -428,
            -479,
            -431,
            -1415,
            -1690,
            -299,
            -146,
            -1690,
            -127,
            -1690,
            -1118,
            -513,
            -453,
            -429,
            -270,
            -1174,
            -1287,
            -550,
            -97,
            -87,
            -358,
            -811,
            -90,
            -98,
            -1118,
            -174,
            -199,
            -427,
            -49
        ],
        "steps_history": [
            1690,
            1013,
            1143,
            1357,
            1690,
            529,
            580,
            532,
            1516,
            1690,
            400,
            247,
            1690,
            228,
            1690,
            1219,
            614,
            554,
            530,
            371,
            1275,
            1388,
            651,
            198,
            188,
            459,
            912,
            191,
            199,
            1219,
            275,
            300,
            528,
            150
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "284/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.579294919967651,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 39,
        "policy_stability_history": [
            0.0,
            0.8106508875739645,
            0.8047337278106509,
            0.7692307692307693,
            0.8165680473372781,
            0.8047337278106509,
            0.8875739644970414,
            0.8698224852071006,
            0.8402366863905325,
            0.8461538461538461,
            0.9349112426035503,
            0.8461538461538461,
            0.9763313609467456,
            0.9763313609467456,
            0.9644970414201184,
            0.9408284023668639,
            0.9763313609467456,
            0.9822485207100592,
            0.863905325443787,
            0.9881656804733728,
            0.8875739644970414,
            0.9822485207100592,
            0.8579881656804734,
            0.9881656804733728,
            0.9349112426035503,
            0.9940828402366864,
            0.9763313609467456,
            0.8461538461538461,
            0.9940828402366864,
            1.0,
            0.9940828402366864,
            0.9940828402366864,
            0.9940828402366864,
            0.9940828402366864,
            0.9822485207100592,
            0.9763313609467456,
            1.0,
            0.9763313609467456,
            0.9585798816568047,
            1.0
        ],
        "reward_history": [
            -1359,
            -1690,
            -990,
            -1690,
            -1690,
            -1690,
            -512,
            -1690,
            -1451,
            -1690,
            -623,
            -788,
            -150,
            -15,
            -51,
            -231,
            -225,
            -199,
            -1690,
            -89,
            -576,
            -49,
            -1690,
            -52,
            -859,
            -153,
            -106,
            -1690,
            -177,
            -72,
            16,
            -28,
            -73,
            -92,
            -292,
            -474,
            -32,
            -185,
            -375,
            -87
        ],
        "steps_history": [
            1460,
            1690,
            1091,
            1690,
            1690,
            1690,
            613,
            1690,
            1552,
            1690,
            724,
            889,
            251,
            116,
            152,
            332,
            326,
            300,
            1690,
            190,
            677,
            150,
            1690,
            153,
            960,
            254,
            207,
            1690,
            278,
            173,
            85,
            129,
            174,
            193,
            393,
            575,
            133,
            286,
            476,
            188
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "285/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.74636697769165,
        "final_policy_stability": 0.9289940828402367,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8047337278106509,
            0.8402366863905325,
            0.8994082840236687,
            0.9289940828402367,
            0.8402366863905325,
            0.8106508875739645,
            0.9704142011834319,
            0.9644970414201184,
            0.9230769230769231,
            0.9644970414201184,
            0.8875739644970414,
            0.8875739644970414,
            0.834319526627219,
            0.9763313609467456,
            0.9704142011834319,
            0.8698224852071006,
            0.9704142011834319,
            0.9289940828402367,
            0.9112426035502958,
            0.8165680473372781,
            0.8579881656804734,
            0.9526627218934911,
            0.9526627218934911,
            0.9822485207100592,
            0.9881656804733728,
            0.9408284023668639,
            0.8875739644970414,
            0.8461538461538461,
            1.0,
            1.0,
            0.9289940828402367
        ],
        "reward_history": [
            -1235,
            -1690,
            -1335,
            -1690,
            -309,
            -1690,
            -1259,
            -249,
            -148,
            -638,
            -255,
            -1491,
            -1690,
            -1690,
            -210,
            -41,
            -1690,
            -361,
            -624,
            -746,
            -1690,
            -1690,
            -409,
            -786,
            -433,
            -79,
            -434,
            -1174,
            -1486,
            -240,
            -74,
            -855
        ],
        "steps_history": [
            1336,
            1690,
            1436,
            1690,
            410,
            1690,
            1360,
            350,
            249,
            739,
            356,
            1592,
            1690,
            1690,
            311,
            142,
            1690,
            462,
            725,
            847,
            1690,
            1690,
            510,
            887,
            534,
            180,
            535,
            1275,
            1587,
            341,
            175,
            956
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "286/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.541372776031494,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.863905325443787,
            0.9112426035502958,
            0.8284023668639053,
            0.8461538461538461,
            0.9526627218934911,
            0.9171597633136095,
            0.9763313609467456,
            0.8402366863905325,
            0.9171597633136095,
            0.9053254437869822,
            0.8520710059171598,
            0.8402366863905325,
            0.8698224852071006,
            0.9704142011834319,
            0.8816568047337278,
            0.863905325443787,
            0.9644970414201184,
            0.9704142011834319,
            0.9526627218934911,
            0.9585798816568047,
            0.9644970414201184,
            0.9644970414201184,
            0.893491124260355,
            0.9881656804733728,
            1.0
        ],
        "reward_history": [
            -770,
            -1060,
            -840,
            -1690,
            -1690,
            -338,
            -511,
            -30,
            -916,
            -1690,
            -1690,
            -1690,
            -1690,
            -1159,
            -260,
            -1228,
            -1690,
            -113,
            -339,
            -339,
            -500,
            -248,
            -431,
            -1690,
            -325,
            -27
        ],
        "steps_history": [
            871,
            1161,
            941,
            1690,
            1690,
            439,
            612,
            131,
            1017,
            1690,
            1690,
            1690,
            1690,
            1260,
            361,
            1329,
            1690,
            214,
            440,
            440,
            601,
            349,
            532,
            1690,
            426,
            128
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "287/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.156179189682007,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8047337278106509,
            0.8461538461538461,
            0.8698224852071006,
            0.9053254437869822,
            0.893491124260355,
            0.9467455621301775,
            0.8757396449704142,
            0.9644970414201184,
            0.8579881656804734,
            0.8994082840236687,
            0.9644970414201184,
            0.9171597633136095,
            0.9230769230769231,
            0.9585798816568047,
            0.863905325443787,
            0.9644970414201184,
            0.9289940828402367,
            0.9763313609467456,
            0.834319526627219,
            0.9467455621301775,
            0.9940828402366864,
            0.9763313609467456,
            0.8816568047337278,
            0.9704142011834319,
            0.9053254437869822,
            0.9171597633136095,
            0.9585798816568047,
            0.9822485207100592,
            0.9881656804733728,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -1690,
            -1690,
            -463,
            -1690,
            -313,
            -1690,
            -201,
            -1690,
            -707,
            -409,
            -659,
            -411,
            -605,
            -1364,
            -157,
            -532,
            -140,
            -1690,
            -669,
            -64,
            -372,
            -1690,
            -240,
            -741,
            -869,
            -796,
            -216,
            -120,
            -11
        ],
        "steps_history": [
            1690,
            1690,
            1690,
            1690,
            564,
            1690,
            414,
            1690,
            302,
            1690,
            808,
            510,
            760,
            512,
            706,
            1465,
            258,
            633,
            241,
            1690,
            770,
            165,
            473,
            1690,
            341,
            842,
            970,
            897,
            317,
            221,
            112
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "288/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.0507893562316895,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.8461538461538461,
            0.8402366863905325,
            0.9408284023668639,
            0.9053254437869822,
            0.893491124260355,
            0.8757396449704142,
            0.893491124260355,
            0.9467455621301775,
            0.8875739644970414,
            0.9349112426035503,
            0.9467455621301775,
            0.9349112426035503,
            0.9289940828402367,
            0.8994082840236687,
            0.9526627218934911,
            0.9585798816568047,
            0.8698224852071006,
            0.9763313609467456,
            0.8579881656804734,
            0.8875739644970414,
            0.9112426035502958,
            0.9881656804733728,
            0.9289940828402367,
            0.9881656804733728,
            0.8757396449704142,
            0.9881656804733728,
            1.0
        ],
        "reward_history": [
            -742,
            -1690,
            -1690,
            -412,
            -338,
            -835,
            -1166,
            -1103,
            -235,
            -1690,
            -360,
            -189,
            -574,
            -495,
            -1690,
            -241,
            -517,
            -1690,
            -189,
            -1690,
            -1690,
            -897,
            -267,
            -735,
            -177,
            -1690,
            -191,
            -41
        ],
        "steps_history": [
            843,
            1690,
            1690,
            513,
            439,
            936,
            1267,
            1204,
            336,
            1690,
            461,
            290,
            675,
            596,
            1690,
            342,
            618,
            1690,
            290,
            1690,
            1690,
            998,
            368,
            836,
            278,
            1690,
            292,
            142
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "289/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.023448467254639,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.7514792899408284,
            0.8698224852071006,
            0.9467455621301775,
            0.8461538461538461,
            0.863905325443787,
            0.8284023668639053,
            0.8579881656804734,
            0.9112426035502958,
            0.9526627218934911,
            0.8461538461538461,
            0.9644970414201184,
            0.7810650887573964,
            0.8757396449704142,
            0.9763313609467456,
            0.9467455621301775,
            0.8579881656804734,
            0.9940828402366864,
            0.9585798816568047,
            0.8875739644970414,
            0.9644970414201184,
            0.9881656804733728,
            0.9644970414201184,
            0.9940828402366864,
            0.9822485207100592,
            0.9644970414201184,
            0.9881656804733728,
            0.9881656804733728,
            0.9704142011834319,
            0.8994082840236687,
            0.9763313609467456,
            0.9704142011834319,
            0.9112426035502958,
            0.9644970414201184,
            1.0,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -1690,
            -194,
            -1562,
            -1690,
            -1690,
            -1690,
            -920,
            -260,
            -1690,
            -326,
            -1690,
            -1690,
            -477,
            -429,
            -1690,
            -23,
            -57,
            -1333,
            -999,
            -81,
            -456,
            -112,
            -326,
            -931,
            -263,
            -137,
            -341,
            -1690,
            -198,
            -1563,
            -1690,
            -235,
            -167,
            -448
        ],
        "steps_history": [
            1690,
            1690,
            1690,
            295,
            1663,
            1690,
            1690,
            1690,
            1021,
            361,
            1690,
            427,
            1690,
            1690,
            578,
            530,
            1690,
            124,
            158,
            1434,
            1100,
            182,
            557,
            213,
            427,
            1032,
            364,
            238,
            442,
            1690,
            299,
            1664,
            1690,
            336,
            268,
            549
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "290/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.958378791809082,
        "final_policy_stability": 0.9822485207100592,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.7633136094674556,
            0.8757396449704142,
            0.8461538461538461,
            0.8047337278106509,
            0.8402366863905325,
            0.8402366863905325,
            0.8757396449704142,
            0.7869822485207101,
            0.9112426035502958,
            0.9408284023668639,
            0.8520710059171598,
            0.8875739644970414,
            0.9408284023668639,
            0.9053254437869822,
            0.9526627218934911,
            0.863905325443787,
            0.8520710059171598,
            0.893491124260355,
            0.9349112426035503,
            0.9822485207100592,
            0.9585798816568047,
            0.9881656804733728,
            0.9763313609467456,
            0.9940828402366864,
            0.9940828402366864,
            0.9822485207100592,
            0.9112426035502958,
            0.9526627218934911,
            1.0,
            0.9704142011834319,
            0.9822485207100592
        ],
        "reward_history": [
            -1256,
            -1690,
            -1690,
            -925,
            -1690,
            -1690,
            -1048,
            -628,
            -1530,
            -480,
            -181,
            -1242,
            -1690,
            -178,
            -689,
            -245,
            -1690,
            -1690,
            -1184,
            -401,
            -67,
            -292,
            -52,
            -569,
            -71,
            -82,
            -545,
            -1051,
            -876,
            -186,
            -310,
            -428
        ],
        "steps_history": [
            1357,
            1690,
            1690,
            1026,
            1690,
            1690,
            1149,
            729,
            1631,
            581,
            282,
            1343,
            1690,
            279,
            790,
            346,
            1690,
            1690,
            1285,
            502,
            168,
            393,
            153,
            670,
            172,
            183,
            646,
            1152,
            977,
            287,
            411,
            529
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "291/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.873305559158325,
        "final_policy_stability": 0.9585798816568047,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.7988165680473372,
            0.893491124260355,
            0.8165680473372781,
            0.8402366863905325,
            0.9408284023668639,
            0.8698224852071006,
            0.8461538461538461,
            0.893491124260355,
            0.8579881656804734,
            0.9289940828402367,
            0.9408284023668639,
            0.9763313609467456,
            0.9349112426035503,
            0.9349112426035503,
            0.834319526627219,
            0.8875739644970414,
            0.7869822485207101,
            0.8165680473372781,
            0.9644970414201184,
            0.9289940828402367,
            0.8520710059171598,
            0.8875739644970414,
            0.9289940828402367,
            0.8698224852071006,
            0.9881656804733728,
            0.9881656804733728,
            0.9763313609467456,
            0.9822485207100592,
            0.9704142011834319,
            0.9940828402366864,
            1.0,
            0.8698224852071006,
            0.9881656804733728,
            0.9585798816568047
        ],
        "reward_history": [
            -1690,
            -1516,
            -575,
            -1690,
            -844,
            -106,
            -598,
            -1690,
            -893,
            -1690,
            -316,
            -410,
            11,
            -395,
            -543,
            -1690,
            -464,
            -1690,
            -1690,
            -60,
            -289,
            -1690,
            -843,
            -642,
            -1306,
            -84,
            -440,
            -79,
            -401,
            -185,
            -161,
            47,
            -1690,
            -144,
            -410
        ],
        "steps_history": [
            1690,
            1617,
            676,
            1690,
            945,
            207,
            699,
            1690,
            994,
            1690,
            417,
            511,
            90,
            496,
            644,
            1690,
            565,
            1690,
            1690,
            161,
            390,
            1690,
            944,
            743,
            1407,
            185,
            541,
            180,
            502,
            286,
            262,
            54,
            1690,
            245,
            511
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "292/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.0215535163879395,
        "final_policy_stability": 0.9940828402366864,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.7041420118343196,
            0.8047337278106509,
            0.9349112426035503,
            0.8165680473372781,
            0.8579881656804734,
            0.9408284023668639,
            0.9349112426035503,
            0.893491124260355,
            0.9230769230769231,
            0.9822485207100592,
            0.9526627218934911,
            0.9585798816568047,
            0.7928994082840237,
            0.9763313609467456,
            0.893491124260355,
            0.9289940828402367,
            0.9171597633136095,
            0.9644970414201184,
            0.8757396449704142,
            0.9171597633136095,
            0.8698224852071006,
            0.8520710059171598,
            1.0,
            0.8284023668639053,
            0.9940828402366864,
            0.9940828402366864,
            0.8520710059171598,
            0.9881656804733728,
            1.0,
            0.9467455621301775,
            1.0,
            0.9940828402366864
        ],
        "reward_history": [
            -1690,
            -1557,
            -1690,
            -100,
            -830,
            -1020,
            -176,
            -369,
            -1690,
            -317,
            -58,
            -247,
            -136,
            -1690,
            -32,
            -1690,
            -498,
            -711,
            -217,
            -1690,
            -981,
            -1315,
            -1690,
            -100,
            -1690,
            -18,
            -72,
            -1690,
            -154,
            -75,
            -746,
            -78,
            -70
        ],
        "steps_history": [
            1690,
            1658,
            1690,
            201,
            931,
            1121,
            277,
            470,
            1690,
            418,
            159,
            348,
            237,
            1690,
            133,
            1690,
            599,
            812,
            318,
            1690,
            1082,
            1416,
            1690,
            201,
            1690,
            119,
            173,
            1690,
            255,
            176,
            847,
            179,
            171
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "293/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.7258141040802,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.9053254437869822,
            0.7692307692307693,
            0.8224852071005917,
            0.834319526627219,
            0.8047337278106509,
            0.834319526627219,
            0.834319526627219,
            0.893491124260355,
            0.9585798816568047,
            0.8698224852071006,
            0.8224852071005917,
            0.9230769230769231,
            0.9408284023668639,
            0.9644970414201184,
            0.9349112426035503,
            0.9526627218934911,
            0.8520710059171598,
            0.9822485207100592,
            0.9112426035502958,
            0.8165680473372781,
            0.9704142011834319,
            0.9644970414201184,
            0.9704142011834319,
            0.9644970414201184,
            0.9171597633136095,
            0.9704142011834319,
            0.8816568047337278,
            0.9881656804733728,
            0.9822485207100592,
            0.9585798816568047,
            0.9704142011834319,
            0.9881656804733728,
            0.8579881656804734,
            0.9644970414201184,
            0.9230769230769231,
            1.0
        ],
        "reward_history": [
            -1112,
            -289,
            -1690,
            -1690,
            -1690,
            -1690,
            -1370,
            -1690,
            -511,
            -77,
            -1026,
            -1690,
            -285,
            -185,
            -104,
            -279,
            -221,
            -1690,
            -17,
            -371,
            -1690,
            -188,
            -231,
            -481,
            -195,
            -594,
            -379,
            -849,
            -17,
            -84,
            -216,
            -268,
            -401,
            -881,
            -279,
            -1449,
            -80
        ],
        "steps_history": [
            1213,
            390,
            1690,
            1690,
            1690,
            1690,
            1471,
            1690,
            612,
            178,
            1127,
            1690,
            386,
            286,
            205,
            380,
            322,
            1690,
            118,
            472,
            1690,
            289,
            332,
            582,
            296,
            695,
            480,
            950,
            118,
            185,
            317,
            369,
            502,
            982,
            380,
            1550,
            181
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "294/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.075334787368774,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8461538461538461,
            0.8520710059171598,
            0.9171597633136095,
            0.8698224852071006,
            0.8698224852071006,
            0.8224852071005917,
            0.9289940828402367,
            0.8106508875739645,
            0.8520710059171598,
            0.8579881656804734,
            0.9467455621301775,
            0.9585798816568047,
            0.834319526627219,
            0.9526627218934911,
            0.9940828402366864,
            0.9704142011834319,
            0.9289940828402367,
            0.8402366863905325,
            0.9171597633136095,
            0.8757396449704142,
            0.9526627218934911,
            0.9230769230769231,
            0.9763313609467456,
            0.9822485207100592,
            1.0,
            0.8698224852071006,
            0.9940828402366864,
            1.0
        ],
        "reward_history": [
            -1690,
            -701,
            -816,
            -354,
            -1690,
            -1581,
            -1526,
            -232,
            -1690,
            -1690,
            -1443,
            -297,
            -113,
            -1690,
            -195,
            -109,
            -96,
            -594,
            -1690,
            -803,
            -1690,
            -537,
            -807,
            -335,
            -220,
            -255,
            -1690,
            -291,
            -72
        ],
        "steps_history": [
            1690,
            802,
            917,
            455,
            1690,
            1682,
            1627,
            333,
            1690,
            1690,
            1544,
            398,
            214,
            1690,
            296,
            210,
            197,
            695,
            1690,
            904,
            1690,
            638,
            908,
            436,
            321,
            356,
            1690,
            392,
            173
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "295/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.174144983291626,
        "final_policy_stability": 0.9881656804733728,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.7396449704142012,
            0.8757396449704142,
            0.8698224852071006,
            0.8579881656804734,
            0.8284023668639053,
            0.8402366863905325,
            0.8520710059171598,
            0.7928994082840237,
            0.8875739644970414,
            0.9526627218934911,
            0.9349112426035503,
            0.8520710059171598,
            0.8579881656804734,
            0.8165680473372781,
            0.9230769230769231,
            0.9940828402366864,
            0.9881656804733728,
            0.9289940828402367,
            0.9526627218934911,
            0.9822485207100592,
            0.9881656804733728,
            0.8461538461538461,
            0.8698224852071006,
            0.9644970414201184,
            1.0,
            0.9881656804733728
        ],
        "reward_history": [
            -1690,
            -1690,
            -448,
            -947,
            -734,
            -1690,
            -1690,
            -1062,
            -1690,
            -941,
            -149,
            -372,
            -1690,
            -1690,
            -1690,
            -494,
            -36,
            -168,
            -582,
            -196,
            -159,
            -35,
            -1690,
            -1690,
            -313,
            -62,
            -389
        ],
        "steps_history": [
            1690,
            1690,
            549,
            1048,
            835,
            1690,
            1690,
            1163,
            1690,
            1042,
            250,
            473,
            1690,
            1690,
            1690,
            595,
            137,
            269,
            683,
            297,
            260,
            136,
            1690,
            1690,
            414,
            163,
            490
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "296/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.7998809814453125,
        "final_policy_stability": 0.9881656804733728,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8461538461538461,
            0.7810650887573964,
            0.8875739644970414,
            0.7988165680473372,
            0.7869822485207101,
            0.9289940828402367,
            0.9053254437869822,
            0.9171597633136095,
            0.9349112426035503,
            0.9053254437869822,
            0.9644970414201184,
            0.7692307692307693,
            0.8461538461538461,
            0.9644970414201184,
            0.9171597633136095,
            0.8698224852071006,
            0.8284023668639053,
            0.9467455621301775,
            0.9349112426035503,
            0.9349112426035503,
            0.9704142011834319,
            0.8047337278106509,
            0.863905325443787,
            0.9349112426035503,
            0.9585798816568047,
            0.9940828402366864,
            0.9822485207100592,
            0.9467455621301775,
            0.8757396449704142,
            0.9822485207100592,
            0.9230769230769231,
            0.9881656804733728
        ],
        "reward_history": [
            -1690,
            -398,
            -1690,
            -527,
            -1690,
            -1690,
            -198,
            -185,
            -380,
            -359,
            -488,
            -109,
            -1690,
            -1494,
            -83,
            -448,
            -1690,
            -1690,
            -318,
            -366,
            -336,
            -45,
            -1690,
            -1228,
            -583,
            -166,
            -19,
            -22,
            -274,
            -1690,
            -257,
            -871,
            -97
        ],
        "steps_history": [
            1690,
            499,
            1690,
            628,
            1690,
            1690,
            299,
            286,
            481,
            460,
            589,
            210,
            1690,
            1595,
            184,
            549,
            1690,
            1690,
            419,
            467,
            437,
            146,
            1690,
            1329,
            684,
            267,
            120,
            123,
            375,
            1690,
            358,
            972,
            198
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "297/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.465056896209717,
        "final_policy_stability": 0.9940828402366864,
        "episodes_to_convergence": 41,
        "policy_stability_history": [
            0.0,
            0.8047337278106509,
            0.7988165680473372,
            0.863905325443787,
            0.8816568047337278,
            0.9230769230769231,
            0.9526627218934911,
            0.8224852071005917,
            0.8816568047337278,
            0.834319526627219,
            0.8757396449704142,
            0.8757396449704142,
            0.9053254437869822,
            0.9349112426035503,
            0.8224852071005917,
            0.9585798816568047,
            0.9408284023668639,
            0.9585798816568047,
            0.8106508875739645,
            0.9822485207100592,
            0.9585798816568047,
            0.834319526627219,
            0.9467455621301775,
            0.9763313609467456,
            0.9585798816568047,
            0.9763313609467456,
            0.9349112426035503,
            0.9053254437869822,
            0.9881656804733728,
            0.8994082840236687,
            0.9644970414201184,
            0.9881656804733728,
            0.9763313609467456,
            0.9940828402366864,
            0.893491124260355,
            0.9585798816568047,
            1.0,
            0.9467455621301775,
            1.0,
            0.8461538461538461,
            1.0,
            0.9940828402366864
        ],
        "reward_history": [
            -1216,
            -1690,
            -1690,
            -1690,
            -361,
            -291,
            -102,
            -1690,
            -317,
            -1690,
            -1039,
            -260,
            -488,
            -216,
            -1690,
            -184,
            -193,
            -217,
            -1690,
            -14,
            -124,
            -1690,
            -400,
            -18,
            -173,
            -110,
            -277,
            -639,
            -83,
            -1067,
            -445,
            -136,
            -471,
            -106,
            -1153,
            -206,
            -42,
            -295,
            -93,
            -1690,
            -73,
            25
        ],
        "steps_history": [
            1317,
            1690,
            1690,
            1690,
            462,
            392,
            203,
            1690,
            418,
            1690,
            1140,
            361,
            589,
            317,
            1690,
            285,
            294,
            318,
            1690,
            115,
            225,
            1690,
            501,
            119,
            274,
            211,
            378,
            740,
            184,
            1168,
            546,
            237,
            572,
            207,
            1254,
            307,
            143,
            396,
            194,
            1690,
            174,
            76
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "298/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.486461639404297,
        "final_policy_stability": 0.9526627218934911,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.8224852071005917,
            0.8520710059171598,
            0.7514792899408284,
            0.8816568047337278,
            0.9112426035502958,
            0.893491124260355,
            0.9112426035502958,
            0.8106508875739645,
            0.7751479289940828,
            0.9289940828402367,
            0.9704142011834319,
            0.8165680473372781,
            0.8402366863905325,
            0.9763313609467456,
            0.8106508875739645,
            0.9822485207100592,
            0.9408284023668639,
            0.9585798816568047,
            0.9763313609467456,
            0.9585798816568047,
            0.9763313609467456,
            0.9763313609467456,
            0.9704142011834319,
            0.8224852071005917,
            0.8579881656804734,
            0.9053254437869822,
            0.9822485207100592,
            0.9704142011834319,
            0.9704142011834319,
            0.9822485207100592,
            0.8520710059171598,
            0.863905325443787,
            1.0,
            0.9822485207100592,
            1.0,
            0.9526627218934911
        ],
        "reward_history": [
            -1690,
            -912,
            -1042,
            -1256,
            -1690,
            -428,
            -479,
            -431,
            -1415,
            -1690,
            -299,
            -146,
            -1036,
            -1690,
            -45,
            -1690,
            -63,
            -300,
            -386,
            -98,
            -188,
            -15,
            -105,
            -99,
            -1345,
            -1359,
            -864,
            -54,
            -203,
            -272,
            -159,
            -1690,
            -1200,
            -49,
            -274,
            -313,
            -578
        ],
        "steps_history": [
            1690,
            1013,
            1143,
            1357,
            1690,
            529,
            580,
            532,
            1516,
            1690,
            400,
            247,
            1137,
            1690,
            146,
            1690,
            164,
            401,
            487,
            199,
            289,
            116,
            206,
            200,
            1446,
            1460,
            965,
            155,
            304,
            373,
            260,
            1690,
            1301,
            150,
            375,
            414,
            679
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "299/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.511793375015259,
        "final_policy_stability": 0.9881656804733728,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8106508875739645,
            0.8047337278106509,
            0.7810650887573964,
            0.8579881656804734,
            0.8402366863905325,
            0.9230769230769231,
            0.8698224852071006,
            0.8698224852071006,
            0.9230769230769231,
            0.863905325443787,
            0.8106508875739645,
            0.834319526627219,
            0.9408284023668639,
            0.9467455621301775,
            0.9644970414201184,
            0.9526627218934911,
            0.9644970414201184,
            0.9408284023668639,
            0.9526627218934911,
            0.9289940828402367,
            0.9704142011834319,
            0.893491124260355,
            0.834319526627219,
            0.9289940828402367,
            0.9289940828402367,
            0.9763313609467456,
            0.9881656804733728,
            0.9408284023668639,
            0.9881656804733728,
            0.9763313609467456,
            0.9940828402366864,
            0.9763313609467456,
            0.9585798816568047,
            0.9881656804733728,
            0.9940828402366864,
            1.0,
            0.9881656804733728
        ],
        "reward_history": [
            -1359,
            -1690,
            -990,
            -1288,
            -1690,
            -1690,
            -813,
            -1045,
            -1690,
            -480,
            -1690,
            -1185,
            -1690,
            -599,
            -189,
            -313,
            -211,
            -91,
            -345,
            -214,
            -521,
            -206,
            -1690,
            -1690,
            -195,
            -503,
            -72,
            16,
            -395,
            -292,
            -474,
            -32,
            -185,
            -375,
            -87,
            -114,
            -52,
            -274
        ],
        "steps_history": [
            1460,
            1690,
            1091,
            1389,
            1690,
            1690,
            914,
            1146,
            1690,
            581,
            1690,
            1286,
            1690,
            700,
            290,
            414,
            312,
            192,
            446,
            315,
            622,
            307,
            1690,
            1690,
            296,
            604,
            173,
            85,
            496,
            393,
            575,
            133,
            286,
            476,
            188,
            215,
            153,
            375
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "300/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.2_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.38564920425415,
        "final_policy_stability": 0.9053254437869822,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8047337278106509,
            0.9053254437869822,
            0.8106508875739645,
            0.8224852071005917,
            0.8461538461538461,
            0.8520710059171598,
            0.834319526627219,
            0.8579881656804734,
            0.9526627218934911,
            0.8816568047337278,
            0.9585798816568047,
            0.8757396449704142,
            0.9053254437869822,
            0.8698224852071006,
            0.9822485207100592,
            0.9644970414201184,
            0.9526627218934911,
            0.9349112426035503,
            0.9526627218934911,
            0.9822485207100592,
            1.0,
            0.8994082840236687,
            0.9053254437869822
        ],
        "reward_history": [
            -987,
            -1690,
            -492,
            -1690,
            -1690,
            -1690,
            -1690,
            -718,
            -1093,
            -195,
            -603,
            -179,
            -1690,
            -1214,
            -1690,
            -92,
            -470,
            -624,
            -746,
            -639,
            -173,
            -161,
            -1690,
            -1173
        ],
        "steps_history": [
            1088,
            1690,
            593,
            1690,
            1690,
            1690,
            1690,
            819,
            1194,
            296,
            704,
            280,
            1690,
            1315,
            1690,
            193,
            571,
            725,
            847,
            740,
            274,
            262,
            1690,
            1274
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "301/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.141807556152344,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.8757396449704142,
            0.863905325443787,
            0.8698224852071006,
            0.8402366863905325,
            0.8994082840236687,
            0.9230769230769231,
            0.893491124260355,
            0.9053254437869822,
            0.9585798816568047,
            0.8579881656804734,
            0.8757396449704142,
            0.9763313609467456,
            0.9763313609467456,
            0.8579881656804734,
            0.9644970414201184,
            0.9704142011834319,
            0.9171597633136095,
            0.9526627218934911,
            0.9585798816568047,
            0.893491124260355,
            0.9763313609467456,
            0.9467455621301775,
            0.9704142011834319,
            0.9112426035502958,
            0.9230769230769231,
            1.0,
            1.0
        ],
        "reward_history": [
            -748,
            -877,
            -1690,
            -1690,
            -1690,
            -691,
            -560,
            -1690,
            -1690,
            -180,
            -990,
            -944,
            -42,
            -9,
            -1690,
            -69,
            -211,
            -1589,
            -347,
            -420,
            -1690,
            -90,
            -186,
            -346,
            -758,
            -1690,
            -148,
            -27
        ],
        "steps_history": [
            849,
            978,
            1690,
            1690,
            1690,
            792,
            661,
            1690,
            1690,
            281,
            1091,
            1045,
            143,
            110,
            1690,
            170,
            312,
            1690,
            448,
            521,
            1690,
            191,
            287,
            447,
            859,
            1690,
            249,
            128
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "302/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.373372793197632,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.863905325443787,
            0.8461538461538461,
            0.8284023668639053,
            0.9112426035502958,
            0.8816568047337278,
            0.9171597633136095,
            0.8402366863905325,
            0.9585798816568047,
            0.8224852071005917,
            0.8875739644970414,
            0.9526627218934911,
            0.9526627218934911,
            0.9230769230769231,
            0.9526627218934911,
            0.9763313609467456,
            0.9704142011834319,
            0.8520710059171598,
            0.9822485207100592,
            0.9289940828402367,
            1.0,
            0.9230769230769231,
            1.0,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -1690,
            -1690,
            -463,
            -1690,
            -314,
            -1690,
            -200,
            -1388,
            -1690,
            -250,
            -649,
            -194,
            -347,
            -161,
            -97,
            -1690,
            -68,
            -1492,
            -107,
            -1454,
            -211,
            -379
        ],
        "steps_history": [
            1690,
            1690,
            1690,
            1690,
            564,
            1690,
            415,
            1690,
            301,
            1489,
            1690,
            351,
            750,
            295,
            448,
            262,
            198,
            1690,
            169,
            1593,
            208,
            1555,
            312,
            480
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "303/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.523525714874268,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.7928994082840237,
            0.8994082840236687,
            0.8994082840236687,
            0.834319526627219,
            0.8875739644970414,
            0.863905325443787,
            0.9349112426035503,
            0.9704142011834319,
            0.8579881656804734,
            0.9526627218934911,
            0.9585798816568047,
            0.8994082840236687,
            0.9289940828402367,
            0.9526627218934911,
            0.9349112426035503,
            0.9940828402366864,
            0.9763313609467456,
            0.9053254437869822,
            0.9053254437869822,
            0.9763313609467456,
            0.8994082840236687,
            1.0,
            0.8875739644970414,
            0.9644970414201184,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -1053,
            -506,
            -1690,
            -1690,
            -1690,
            -620,
            -224,
            -676,
            -105,
            -400,
            -806,
            -376,
            -629,
            -799,
            -70,
            -147,
            -1197,
            -1690,
            -432,
            -1690,
            -152,
            -1381,
            -391,
            -282
        ],
        "steps_history": [
            1690,
            1690,
            1154,
            607,
            1690,
            1690,
            1690,
            721,
            325,
            777,
            206,
            501,
            907,
            477,
            730,
            900,
            171,
            248,
            1298,
            1690,
            533,
            1690,
            253,
            1482,
            492,
            383
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "304/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.352841138839722,
        "final_policy_stability": 0.9585798816568047,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8402366863905325,
            0.9230769230769231,
            0.757396449704142,
            0.893491124260355,
            0.8106508875739645,
            0.893491124260355,
            0.8284023668639053,
            0.9053254437869822,
            0.9467455621301775,
            0.8165680473372781,
            0.9230769230769231,
            0.8520710059171598,
            0.863905325443787,
            0.9644970414201184,
            0.9822485207100592,
            0.9349112426035503,
            0.8461538461538461,
            0.8757396449704142,
            0.9940828402366864,
            1.0,
            0.9526627218934911,
            0.9230769230769231,
            0.9585798816568047
        ],
        "reward_history": [
            -1690,
            -828,
            -274,
            -1690,
            -580,
            -1690,
            -1050,
            -1690,
            -733,
            -551,
            -1690,
            -544,
            -1690,
            -1690,
            -336,
            -168,
            -246,
            -1690,
            -1690,
            -207,
            31,
            -490,
            -891,
            -999
        ],
        "steps_history": [
            1690,
            929,
            375,
            1690,
            681,
            1690,
            1151,
            1690,
            834,
            652,
            1690,
            645,
            1690,
            1690,
            437,
            269,
            347,
            1690,
            1690,
            308,
            70,
            591,
            992,
            1100
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "305/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.058820724487305,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.834319526627219,
            0.7988165680473372,
            0.8698224852071006,
            0.834319526627219,
            0.834319526627219,
            0.9112426035502958,
            0.8757396449704142,
            0.9171597633136095,
            0.8994082840236687,
            0.8106508875739645,
            0.9349112426035503,
            0.8402366863905325,
            0.8461538461538461,
            0.9349112426035503,
            0.9940828402366864,
            0.9467455621301775,
            0.8224852071005917,
            0.9881656804733728,
            0.9940828402366864,
            0.9585798816568047,
            0.9585798816568047,
            0.9467455621301775,
            0.9940828402366864,
            0.8461538461538461,
            0.9940828402366864,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -1690,
            -441,
            -1177,
            -976,
            -1690,
            -1690,
            -697,
            -1690,
            -465,
            -756,
            -1690,
            -446,
            -1690,
            -1690,
            -271,
            5,
            -267,
            -1690,
            -291,
            -37,
            -359,
            -496,
            -291,
            -186,
            -1690,
            -436,
            -71,
            -82,
            -399
        ],
        "steps_history": [
            1690,
            542,
            1278,
            1077,
            1690,
            1690,
            798,
            1690,
            566,
            857,
            1690,
            547,
            1690,
            1690,
            372,
            96,
            368,
            1690,
            392,
            138,
            460,
            597,
            392,
            287,
            1690,
            537,
            172,
            183,
            500
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "306/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.023637056350708,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.863905325443787,
            0.9230769230769231,
            0.8165680473372781,
            0.8875739644970414,
            0.8402366863905325,
            0.9171597633136095,
            0.8757396449704142,
            0.9171597633136095,
            0.9053254437869822,
            0.9171597633136095,
            0.9349112426035503,
            0.893491124260355,
            0.8047337278106509,
            0.8816568047337278,
            0.9881656804733728,
            0.9940828402366864,
            0.9822485207100592,
            0.9881656804733728,
            0.8461538461538461,
            0.9881656804733728,
            1.0,
            0.9881656804733728,
            0.9230769230769231,
            1.0,
            1.0,
            0.9881656804733728,
            1.0
        ],
        "reward_history": [
            -1690,
            -672,
            -406,
            -857,
            -608,
            -1690,
            -182,
            -851,
            -470,
            -187,
            -231,
            -379,
            -1690,
            -1690,
            -698,
            -210,
            -232,
            -175,
            -79,
            -1158,
            -110,
            -167,
            -38,
            -939,
            -389,
            -48,
            -496,
            -103
        ],
        "steps_history": [
            1690,
            773,
            507,
            958,
            709,
            1690,
            283,
            952,
            571,
            288,
            332,
            480,
            1690,
            1690,
            799,
            311,
            333,
            276,
            180,
            1259,
            211,
            268,
            139,
            1040,
            490,
            149,
            597,
            204
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "307/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.082885026931763,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8224852071005917,
            0.9230769230769231,
            0.8461538461538461,
            0.8165680473372781,
            0.7810650887573964,
            0.8579881656804734,
            0.8816568047337278,
            0.9171597633136095,
            0.9467455621301775,
            0.8461538461538461,
            0.863905325443787,
            0.8875739644970414,
            0.8875739644970414,
            0.9763313609467456,
            0.8284023668639053,
            0.9585798816568047,
            0.9526627218934911,
            0.9704142011834319,
            0.9763313609467456,
            0.9171597633136095,
            0.9467455621301775,
            0.8698224852071006,
            1.0,
            0.9585798816568047,
            0.9822485207100592,
            1.0
        ],
        "reward_history": [
            -606,
            -1690,
            -104,
            -1690,
            -1690,
            -1301,
            -1690,
            -475,
            -554,
            -93,
            -1690,
            -784,
            -795,
            -931,
            -92,
            -1690,
            -218,
            -503,
            -14,
            -163,
            -859,
            -1072,
            -1690,
            -100,
            -838,
            -120,
            -146
        ],
        "steps_history": [
            707,
            1690,
            205,
            1690,
            1690,
            1402,
            1690,
            576,
            655,
            194,
            1690,
            885,
            896,
            1032,
            193,
            1690,
            319,
            604,
            115,
            264,
            960,
            1173,
            1690,
            201,
            939,
            221,
            247
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "308/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.341043472290039,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8757396449704142,
            0.8165680473372781,
            0.8579881656804734,
            0.8402366863905325,
            0.8284023668639053,
            0.9408284023668639,
            0.9467455621301775,
            0.8461538461538461,
            0.8698224852071006,
            0.9467455621301775,
            0.9644970414201184,
            0.9763313609467456,
            0.9704142011834319,
            0.9585798816568047,
            0.8165680473372781,
            0.9881656804733728,
            0.7988165680473372,
            0.9822485207100592,
            0.9171597633136095,
            0.9940828402366864,
            0.9881656804733728,
            0.8757396449704142,
            0.8402366863905325,
            1.0
        ],
        "reward_history": [
            -1112,
            -289,
            -1690,
            -1690,
            -1690,
            -1151,
            -304,
            -528,
            -926,
            -1690,
            -237,
            -730,
            1,
            -140,
            -154,
            -1690,
            -153,
            -1690,
            -165,
            -777,
            -153,
            -416,
            -1690,
            -1690,
            -403
        ],
        "steps_history": [
            1213,
            390,
            1690,
            1690,
            1690,
            1252,
            405,
            629,
            1027,
            1690,
            338,
            831,
            100,
            241,
            255,
            1690,
            254,
            1690,
            266,
            878,
            254,
            517,
            1690,
            1690,
            504
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "309/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.068928480148315,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8402366863905325,
            0.8402366863905325,
            0.8757396449704142,
            0.9289940828402367,
            0.8520710059171598,
            0.9230769230769231,
            0.8284023668639053,
            0.8520710059171598,
            0.8047337278106509,
            0.9112426035502958,
            0.9053254437869822,
            0.8047337278106509,
            0.834319526627219,
            0.8875739644970414,
            0.9881656804733728,
            0.9408284023668639,
            0.9349112426035503,
            1.0,
            0.9822485207100592,
            0.9940828402366864,
            0.9881656804733728,
            0.9881656804733728,
            0.8284023668639053,
            0.9704142011834319,
            1.0,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -909,
            -531,
            -539,
            -1690,
            -649,
            -1690,
            -1690,
            -879,
            -1137,
            -717,
            -1584,
            -1516,
            -877,
            -188,
            -575,
            -550,
            -421,
            -78,
            -201,
            -305,
            -49,
            -1576,
            -656,
            -255,
            -77
        ],
        "steps_history": [
            1690,
            1690,
            1010,
            632,
            640,
            1690,
            750,
            1690,
            1690,
            980,
            1238,
            818,
            1685,
            1617,
            978,
            289,
            676,
            651,
            522,
            179,
            302,
            406,
            150,
            1677,
            757,
            356,
            178
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "310/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.17444634437561,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.7455621301775148,
            0.8757396449704142,
            0.8875739644970414,
            0.8047337278106509,
            0.9289940828402367,
            0.8816568047337278,
            0.834319526627219,
            0.8816568047337278,
            0.8165680473372781,
            0.9171597633136095,
            0.9704142011834319,
            0.9230769230769231,
            0.9585798816568047,
            0.9230769230769231,
            0.9940828402366864,
            0.863905325443787,
            0.8579881656804734,
            0.9053254437869822,
            0.9467455621301775,
            0.9704142011834319,
            0.9940828402366864,
            0.9822485207100592,
            0.9704142011834319,
            0.9822485207100592,
            0.8579881656804734,
            0.9940828402366864,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -441,
            -1690,
            -1690,
            -382,
            -504,
            -995,
            -748,
            -1690,
            -614,
            10,
            -401,
            -154,
            -297,
            10,
            -1690,
            -1690,
            -550,
            -434,
            -191,
            -21,
            -118,
            -232,
            -157,
            -1690,
            -110,
            -173
        ],
        "steps_history": [
            1690,
            1690,
            542,
            1690,
            1690,
            483,
            605,
            1096,
            849,
            1690,
            715,
            91,
            502,
            255,
            398,
            91,
            1690,
            1690,
            651,
            535,
            292,
            122,
            219,
            333,
            258,
            1690,
            211,
            274
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "311/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.215965032577515,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8165680473372781,
            0.8284023668639053,
            0.7455621301775148,
            0.834319526627219,
            0.8875739644970414,
            0.9585798816568047,
            0.8579881656804734,
            0.8698224852071006,
            0.9526627218934911,
            0.9467455621301775,
            0.8284023668639053,
            0.9467455621301775,
            0.8994082840236687,
            0.9704142011834319,
            1.0,
            0.9526627218934911,
            0.834319526627219,
            0.9644970414201184,
            0.9881656804733728,
            0.9822485207100592,
            0.9704142011834319,
            0.9940828402366864,
            0.8994082840236687,
            0.9881656804733728,
            0.9704142011834319,
            0.8757396449704142,
            0.9112426035502958,
            0.8698224852071006,
            0.9881656804733728,
            1.0
        ],
        "reward_history": [
            -1690,
            -690,
            -1690,
            -1507,
            -1690,
            -616,
            -185,
            -848,
            -832,
            -100,
            -718,
            -1690,
            -415,
            -549,
            -59,
            -91,
            -513,
            -1690,
            -314,
            -118,
            -23,
            -284,
            -60,
            -692,
            -95,
            -279,
            -1327,
            -1010,
            -1690,
            -156,
            -45
        ],
        "steps_history": [
            1690,
            791,
            1690,
            1608,
            1690,
            717,
            286,
            949,
            933,
            201,
            819,
            1690,
            516,
            650,
            160,
            192,
            614,
            1690,
            415,
            219,
            124,
            385,
            161,
            793,
            196,
            380,
            1428,
            1111,
            1690,
            257,
            146
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "312/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.33847451210022,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.7988165680473372,
            0.8875739644970414,
            0.8047337278106509,
            0.8994082840236687,
            0.7988165680473372,
            0.9171597633136095,
            0.9526627218934911,
            0.8165680473372781,
            0.8284023668639053,
            0.9644970414201184,
            0.9644970414201184,
            0.8579881656804734,
            0.9289940828402367,
            0.9763313609467456,
            0.8520710059171598,
            0.9940828402366864,
            0.9763313609467456,
            0.9940828402366864,
            0.9704142011834319,
            0.8520710059171598,
            0.9644970414201184,
            1.0,
            0.9763313609467456,
            0.8520710059171598,
            1.0
        ],
        "reward_history": [
            -825,
            -1690,
            -419,
            -1690,
            -695,
            -1690,
            -200,
            -51,
            -1224,
            -1690,
            -162,
            -106,
            -1690,
            -413,
            -94,
            -1390,
            -69,
            -311,
            -95,
            -217,
            -1690,
            -239,
            -24,
            -388,
            -1690,
            -99
        ],
        "steps_history": [
            926,
            1690,
            520,
            1690,
            796,
            1690,
            301,
            152,
            1325,
            1690,
            263,
            207,
            1690,
            514,
            195,
            1491,
            170,
            412,
            196,
            318,
            1690,
            340,
            125,
            489,
            1690,
            200
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "313/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.134636878967285,
        "final_policy_stability": 0.9763313609467456,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.7633136094674556,
            0.8579881656804734,
            0.8047337278106509,
            0.8106508875739645,
            0.8047337278106509,
            0.9585798816568047,
            0.9644970414201184,
            0.7928994082840237,
            0.893491124260355,
            0.9467455621301775,
            0.8757396449704142,
            0.9349112426035503,
            0.9289940828402367,
            0.8994082840236687,
            0.9408284023668639,
            0.9349112426035503,
            0.834319526627219,
            0.9585798816568047,
            0.863905325443787,
            0.893491124260355,
            0.9881656804733728,
            0.9822485207100592,
            0.9881656804733728,
            0.8579881656804734,
            0.9881656804733728,
            0.8994082840236687,
            1.0,
            0.9763313609467456
        ],
        "reward_history": [
            -1690,
            -1317,
            -674,
            -899,
            -1690,
            -1326,
            -125,
            -107,
            -1319,
            -392,
            -455,
            -718,
            -317,
            -240,
            -341,
            -244,
            -263,
            -1690,
            -190,
            -1411,
            -512,
            -163,
            -105,
            -252,
            -1690,
            -206,
            -992,
            -86,
            -382
        ],
        "steps_history": [
            1690,
            1418,
            775,
            1000,
            1690,
            1427,
            226,
            208,
            1420,
            493,
            556,
            819,
            418,
            341,
            442,
            345,
            364,
            1690,
            291,
            1512,
            613,
            264,
            206,
            353,
            1690,
            307,
            1093,
            187,
            483
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "314/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.146626234054565,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.9053254437869822,
            0.834319526627219,
            0.7869822485207101,
            0.8698224852071006,
            0.8579881656804734,
            0.834319526627219,
            0.9349112426035503,
            0.9230769230769231,
            0.8520710059171598,
            0.9881656804733728,
            0.9171597633136095,
            0.8698224852071006,
            0.834319526627219,
            0.9526627218934911,
            0.9644970414201184,
            0.9230769230769231,
            0.9467455621301775,
            0.9704142011834319,
            0.9230769230769231,
            0.8224852071005917,
            0.8816568047337278,
            0.9940828402366864,
            0.9171597633136095,
            1.0,
            0.8402366863905325,
            1.0
        ],
        "reward_history": [
            -1370,
            -215,
            -1690,
            -1690,
            -862,
            -1061,
            -1690,
            -209,
            -425,
            -1690,
            -20,
            -1333,
            -949,
            -1690,
            -286,
            -26,
            -662,
            -139,
            -225,
            -654,
            -1324,
            -576,
            -49,
            -544,
            -125,
            -1690,
            -141
        ],
        "steps_history": [
            1471,
            316,
            1690,
            1690,
            963,
            1162,
            1690,
            310,
            526,
            1690,
            121,
            1434,
            1050,
            1690,
            387,
            127,
            763,
            240,
            326,
            755,
            1425,
            677,
            150,
            645,
            226,
            1690,
            242
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "315/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.253676652908325,
        "final_policy_stability": 0.9053254437869822,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8047337278106509,
            0.9053254437869822,
            0.8106508875739645,
            0.8224852071005917,
            0.8461538461538461,
            0.8520710059171598,
            0.834319526627219,
            0.8579881656804734,
            0.9526627218934911,
            0.8816568047337278,
            0.9585798816568047,
            0.8757396449704142,
            0.9053254437869822,
            0.8698224852071006,
            0.9822485207100592,
            0.9644970414201184,
            0.9526627218934911,
            0.9349112426035503,
            0.9526627218934911,
            0.9822485207100592,
            1.0,
            0.8994082840236687,
            0.9053254437869822
        ],
        "reward_history": [
            -987,
            -1690,
            -492,
            -1690,
            -1690,
            -1690,
            -1690,
            -718,
            -1093,
            -195,
            -603,
            -179,
            -1690,
            -1214,
            -1690,
            -92,
            -470,
            -624,
            -746,
            -639,
            -173,
            -161,
            -1690,
            -1173
        ],
        "steps_history": [
            1088,
            1690,
            593,
            1690,
            1690,
            1690,
            1690,
            819,
            1194,
            296,
            704,
            280,
            1690,
            1315,
            1690,
            193,
            571,
            725,
            847,
            740,
            274,
            262,
            1690,
            1274
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "316/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.436944484710693,
        "final_policy_stability": 0.9822485207100592,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8757396449704142,
            0.863905325443787,
            0.8698224852071006,
            0.8402366863905325,
            0.9171597633136095,
            0.8875739644970414,
            0.893491124260355,
            0.9112426035502958,
            0.9467455621301775,
            0.9704142011834319,
            0.9822485207100592,
            0.893491124260355,
            0.9171597633136095,
            0.9230769230769231,
            0.9763313609467456,
            0.8698224852071006,
            0.9467455621301775,
            0.9763313609467456,
            0.9585798816568047,
            0.9822485207100592,
            0.9644970414201184,
            0.8520710059171598,
            0.9822485207100592
        ],
        "reward_history": [
            -748,
            -877,
            -1690,
            -1690,
            -1690,
            -473,
            -1690,
            -1690,
            -867,
            -351,
            -210,
            -93,
            -1690,
            -752,
            -1690,
            -232,
            -1690,
            -1033,
            -339,
            -567,
            -431,
            -621,
            -1690,
            -325
        ],
        "steps_history": [
            849,
            978,
            1690,
            1690,
            1690,
            574,
            1690,
            1690,
            968,
            452,
            311,
            194,
            1690,
            853,
            1690,
            333,
            1690,
            1134,
            440,
            668,
            532,
            722,
            1690,
            426
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "317/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.32876992225647,
        "final_policy_stability": 0.9881656804733728,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8165680473372781,
            0.7810650887573964,
            0.9289940828402367,
            0.8047337278106509,
            0.9349112426035503,
            0.8994082840236687,
            0.8461538461538461,
            0.9585798816568047,
            0.9230769230769231,
            0.8402366863905325,
            0.9230769230769231,
            0.8520710059171598,
            0.9053254437869822,
            0.9940828402366864,
            0.8579881656804734,
            0.9940828402366864,
            0.9940828402366864,
            0.9763313609467456,
            1.0,
            0.9704142011834319,
            1.0,
            0.9940828402366864,
            0.8698224852071006,
            0.9881656804733728
        ],
        "reward_history": [
            -1690,
            -1448,
            -1690,
            -161,
            -1690,
            -341,
            -1690,
            -1113,
            -108,
            -281,
            -1690,
            -298,
            -1314,
            -1470,
            -6,
            -1690,
            -258,
            -89,
            -422,
            -140,
            -788,
            -362,
            -107,
            -1471,
            -674
        ],
        "steps_history": [
            1690,
            1549,
            1690,
            262,
            1690,
            442,
            1690,
            1214,
            209,
            382,
            1690,
            399,
            1415,
            1571,
            107,
            1690,
            359,
            190,
            523,
            241,
            889,
            463,
            208,
            1572,
            775
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "318/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.053967475891113,
        "final_policy_stability": 0.9940828402366864,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.7928994082840237,
            0.8994082840236687,
            0.8994082840236687,
            0.834319526627219,
            0.8875739644970414,
            0.863905325443787,
            0.9349112426035503,
            0.9704142011834319,
            0.8579881656804734,
            0.9526627218934911,
            0.9585798816568047,
            0.8994082840236687,
            0.9289940828402367,
            0.9526627218934911,
            0.9349112426035503,
            0.9881656804733728,
            0.9704142011834319,
            0.9053254437869822,
            0.9171597633136095,
            0.893491124260355,
            0.9822485207100592,
            0.9940828402366864,
            0.9940828402366864,
            0.9585798816568047,
            0.9822485207100592,
            0.9230769230769231,
            0.9940828402366864,
            0.9230769230769231,
            0.8757396449704142,
            0.9940828402366864
        ],
        "reward_history": [
            -1690,
            -1690,
            -1053,
            -506,
            -1690,
            -1690,
            -1690,
            -620,
            -224,
            -676,
            -105,
            -400,
            -806,
            -376,
            -629,
            -799,
            -70,
            -147,
            -1197,
            -1096,
            -1690,
            -112,
            13,
            -49,
            -727,
            -267,
            -735,
            -177,
            -1085,
            -1690,
            -100
        ],
        "steps_history": [
            1690,
            1690,
            1154,
            607,
            1690,
            1690,
            1690,
            721,
            325,
            777,
            206,
            501,
            907,
            477,
            730,
            900,
            171,
            248,
            1298,
            1197,
            1690,
            213,
            88,
            150,
            828,
            368,
            836,
            278,
            1186,
            1690,
            201
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "319/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.123871564865112,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8402366863905325,
            0.9230769230769231,
            0.7633136094674556,
            0.8402366863905325,
            0.863905325443787,
            0.9289940828402367,
            0.893491124260355,
            0.8106508875739645,
            0.9289940828402367,
            0.8698224852071006,
            0.8284023668639053,
            0.9644970414201184,
            0.9704142011834319,
            0.9940828402366864,
            0.9230769230769231,
            0.9289940828402367,
            0.8757396449704142,
            0.9940828402366864,
            0.9585798816568047,
            0.9822485207100592,
            0.9585798816568047,
            0.9053254437869822,
            1.0,
            1.0,
            0.9940828402366864,
            1.0
        ],
        "reward_history": [
            -1690,
            -828,
            -274,
            -1690,
            -1690,
            -778,
            -852,
            -905,
            -1690,
            -379,
            -1205,
            -1690,
            -155,
            -119,
            -35,
            -506,
            -414,
            -1321,
            -168,
            -584,
            -152,
            -333,
            -1522,
            -79,
            -234,
            -257,
            -128
        ],
        "steps_history": [
            1690,
            929,
            375,
            1690,
            1690,
            879,
            953,
            1006,
            1690,
            480,
            1306,
            1690,
            256,
            220,
            136,
            607,
            515,
            1422,
            269,
            685,
            253,
            434,
            1623,
            180,
            335,
            358,
            229
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "320/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.3109424114227295,
        "final_policy_stability": 0.8698224852071006,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8106508875739645,
            0.8875739644970414,
            0.8520710059171598,
            0.9112426035502958,
            0.7810650887573964,
            0.8461538461538461,
            0.8698224852071006,
            0.9644970414201184,
            0.893491124260355,
            0.9349112426035503,
            0.9349112426035503,
            0.9822485207100592,
            0.8816568047337278,
            0.9763313609467456,
            0.9467455621301775,
            0.863905325443787,
            0.8816568047337278,
            0.9763313609467456,
            1.0,
            0.9467455621301775,
            0.9289940828402367,
            0.9940828402366864,
            0.9940828402366864,
            0.8698224852071006
        ],
        "reward_history": [
            -1690,
            -1690,
            -1119,
            -1035,
            -754,
            -1690,
            -1690,
            -975,
            -219,
            -1246,
            -418,
            -542,
            -138,
            -1690,
            -182,
            -478,
            -1690,
            -1690,
            -265,
            -37,
            -359,
            -693,
            -94,
            -186,
            -1690
        ],
        "steps_history": [
            1690,
            1690,
            1220,
            1136,
            855,
            1690,
            1690,
            1076,
            320,
            1347,
            519,
            643,
            239,
            1690,
            283,
            579,
            1690,
            1690,
            366,
            138,
            460,
            794,
            195,
            287,
            1690
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "321/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.029980182647705,
        "final_policy_stability": 0.9822485207100592,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.863905325443787,
            0.9230769230769231,
            0.8224852071005917,
            0.8816568047337278,
            0.8047337278106509,
            0.9171597633136095,
            0.8165680473372781,
            0.8875739644970414,
            0.8402366863905325,
            0.8698224852071006,
            0.9230769230769231,
            0.9171597633136095,
            0.9053254437869822,
            0.9526627218934911,
            0.863905325443787,
            0.9704142011834319,
            0.8402366863905325,
            0.9408284023668639,
            0.9940828402366864,
            0.9881656804733728,
            0.9940828402366864,
            0.9822485207100592,
            0.9881656804733728,
            0.9881656804733728,
            0.9881656804733728,
            0.9822485207100592
        ],
        "reward_history": [
            -1690,
            -672,
            -406,
            -857,
            -608,
            -1690,
            -182,
            -1690,
            -832,
            -872,
            -1690,
            -254,
            -474,
            -485,
            -543,
            -1690,
            -135,
            -1690,
            -892,
            -103,
            -229,
            -756,
            -85,
            -391,
            -200,
            -275,
            -467
        ],
        "steps_history": [
            1690,
            773,
            507,
            958,
            709,
            1690,
            283,
            1690,
            933,
            973,
            1690,
            355,
            575,
            586,
            644,
            1690,
            236,
            1690,
            993,
            204,
            330,
            857,
            186,
            492,
            301,
            376,
            568
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "322/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.081161022186279,
        "final_policy_stability": 0.8402366863905325,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8224852071005917,
            0.9230769230769231,
            0.8165680473372781,
            0.8461538461538461,
            0.8106508875739645,
            0.9230769230769231,
            0.834319526627219,
            0.8698224852071006,
            0.863905325443787,
            0.9349112426035503,
            0.9230769230769231,
            0.9230769230769231,
            0.8402366863905325,
            0.9585798816568047,
            0.9230769230769231,
            0.9349112426035503,
            0.9467455621301775,
            0.9704142011834319,
            0.9822485207100592,
            0.9822485207100592,
            0.9053254437869822,
            0.9349112426035503,
            1.0,
            0.8994082840236687,
            0.9940828402366864,
            0.8402366863905325
        ],
        "reward_history": [
            -606,
            -1690,
            -104,
            -1690,
            -1690,
            -1188,
            -195,
            -818,
            -1690,
            -1690,
            -334,
            -609,
            -480,
            -1690,
            -749,
            -336,
            -367,
            -273,
            -296,
            -14,
            -143,
            -879,
            -1072,
            -65,
            -1690,
            -20,
            -1690
        ],
        "steps_history": [
            707,
            1690,
            205,
            1690,
            1690,
            1289,
            296,
            919,
            1690,
            1690,
            435,
            710,
            581,
            1690,
            850,
            437,
            468,
            374,
            397,
            115,
            244,
            980,
            1173,
            166,
            1690,
            121,
            1690
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "323/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.504964351654053,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8698224852071006,
            0.834319526627219,
            0.834319526627219,
            0.8698224852071006,
            0.9171597633136095,
            0.8047337278106509,
            0.834319526627219,
            0.9585798816568047,
            0.8165680473372781,
            0.9053254437869822,
            0.8816568047337278,
            0.9526627218934911,
            0.8520710059171598,
            0.9585798816568047,
            0.9526627218934911,
            0.9171597633136095,
            0.9644970414201184,
            0.9408284023668639,
            0.9171597633136095,
            0.9289940828402367,
            0.9644970414201184,
            0.9585798816568047,
            0.9408284023668639,
            0.9822485207100592,
            0.9704142011834319,
            0.9822485207100592,
            0.9763313609467456,
            0.9585798816568047,
            0.8816568047337278,
            1.0
        ],
        "reward_history": [
            -1112,
            -518,
            -1690,
            -1690,
            -819,
            -123,
            -1690,
            -1690,
            -149,
            -1369,
            -757,
            -771,
            -154,
            -1690,
            -153,
            -190,
            -281,
            -143,
            -78,
            -1046,
            -490,
            -153,
            -416,
            -639,
            -594,
            -170,
            -172,
            -284,
            -354,
            -1690,
            -240
        ],
        "steps_history": [
            1213,
            619,
            1690,
            1690,
            920,
            224,
            1690,
            1690,
            250,
            1470,
            858,
            872,
            255,
            1690,
            254,
            291,
            382,
            244,
            179,
            1147,
            591,
            254,
            517,
            740,
            695,
            271,
            273,
            385,
            455,
            1690,
            341
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "324/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.282979965209961,
        "final_policy_stability": 0.9822485207100592,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8402366863905325,
            0.8461538461538461,
            0.8757396449704142,
            0.9289940828402367,
            0.8520710059171598,
            0.9230769230769231,
            0.7988165680473372,
            0.7988165680473372,
            0.8698224852071006,
            0.834319526627219,
            0.9526627218934911,
            0.893491124260355,
            0.8284023668639053,
            0.8284023668639053,
            0.9349112426035503,
            0.9349112426035503,
            0.9704142011834319,
            0.9467455621301775,
            0.9940828402366864,
            0.9940828402366864,
            0.9881656804733728,
            0.9763313609467456,
            1.0,
            0.9644970414201184,
            0.9644970414201184,
            0.9881656804733728,
            0.8698224852071006,
            1.0,
            1.0,
            0.9822485207100592
        ],
        "reward_history": [
            -1690,
            -1690,
            -909,
            -531,
            -539,
            -1690,
            -649,
            -1690,
            -1690,
            -879,
            -1690,
            -189,
            -411,
            -1690,
            -1690,
            -171,
            -584,
            -371,
            -550,
            -421,
            -78,
            -201,
            -305,
            -49,
            -207,
            -360,
            -71,
            -1690,
            -58,
            -77,
            -152
        ],
        "steps_history": [
            1690,
            1690,
            1010,
            632,
            640,
            1690,
            750,
            1690,
            1690,
            980,
            1690,
            290,
            512,
            1690,
            1690,
            272,
            685,
            472,
            651,
            522,
            179,
            302,
            406,
            150,
            308,
            461,
            172,
            1690,
            159,
            178,
            253
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "325/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.164492130279541,
        "final_policy_stability": 0.9881656804733728,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.7455621301775148,
            0.8757396449704142,
            0.8875739644970414,
            0.8047337278106509,
            0.9230769230769231,
            0.8757396449704142,
            0.834319526627219,
            0.8816568047337278,
            0.8165680473372781,
            0.9171597633136095,
            0.9704142011834319,
            0.9230769230769231,
            0.9585798816568047,
            0.9230769230769231,
            0.9940828402366864,
            0.8698224852071006,
            0.8757396449704142,
            0.8875739644970414,
            0.8757396449704142,
            0.9822485207100592,
            0.9349112426035503,
            0.8698224852071006,
            0.8875739644970414,
            0.9704142011834319,
            0.9940828402366864,
            1.0,
            1.0,
            0.9822485207100592,
            0.9881656804733728
        ],
        "reward_history": [
            -1690,
            -1690,
            -441,
            -1690,
            -1690,
            -382,
            -504,
            -995,
            -748,
            -1690,
            -614,
            10,
            -401,
            -154,
            -297,
            10,
            -1690,
            -1690,
            -550,
            -1266,
            -291,
            -259,
            -1205,
            -1690,
            -145,
            -158,
            -42,
            -48,
            -319,
            -110
        ],
        "steps_history": [
            1690,
            1690,
            542,
            1690,
            1690,
            483,
            605,
            1096,
            849,
            1690,
            715,
            91,
            502,
            255,
            398,
            91,
            1690,
            1690,
            651,
            1367,
            392,
            360,
            1306,
            1690,
            246,
            259,
            143,
            149,
            420,
            211
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "326/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.827402353286743,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8165680473372781,
            0.8284023668639053,
            0.7455621301775148,
            0.8224852071005917,
            0.8875739644970414,
            0.9644970414201184,
            0.9467455621301775,
            0.8875739644970414,
            0.9171597633136095,
            0.9053254437869822,
            0.8106508875739645,
            0.8106508875739645,
            0.893491124260355,
            0.9763313609467456,
            0.9644970414201184,
            0.9349112426035503,
            0.9408284023668639,
            0.9289940828402367,
            0.9644970414201184,
            0.9644970414201184,
            0.8047337278106509,
            0.9881656804733728,
            0.9822485207100592,
            0.9940828402366864,
            0.8816568047337278,
            1.0,
            0.9940828402366864,
            1.0,
            0.8994082840236687,
            0.9289940828402367,
            1.0,
            0.9704142011834319,
            1.0
        ],
        "reward_history": [
            -1690,
            -690,
            -1690,
            -1507,
            -1690,
            -616,
            -185,
            -220,
            -527,
            -218,
            -354,
            -911,
            -1690,
            -499,
            -83,
            -345,
            -162,
            -389,
            -215,
            -149,
            -232,
            -1690,
            -10,
            -348,
            -60,
            -1690,
            -182,
            -83,
            -539,
            -1010,
            -583,
            -131,
            -552,
            -277
        ],
        "steps_history": [
            1690,
            791,
            1690,
            1608,
            1690,
            717,
            286,
            321,
            628,
            319,
            455,
            1012,
            1690,
            600,
            184,
            446,
            263,
            490,
            316,
            250,
            333,
            1690,
            111,
            449,
            161,
            1690,
            283,
            184,
            640,
            1111,
            684,
            232,
            653,
            378
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "327/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.038975715637207,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.7988165680473372,
            0.8875739644970414,
            0.8047337278106509,
            0.8994082840236687,
            0.7869822485207101,
            0.9408284023668639,
            0.9644970414201184,
            0.8816568047337278,
            0.8520710059171598,
            0.9763313609467456,
            0.8165680473372781,
            0.8579881656804734,
            0.9467455621301775,
            0.8461538461538461,
            0.9822485207100592,
            0.9349112426035503,
            0.9585798816568047,
            0.9467455621301775,
            0.9940828402366864,
            0.863905325443787,
            0.9763313609467456,
            0.9881656804733728,
            0.9704142011834319,
            0.8757396449704142,
            1.0,
            1.0
        ],
        "reward_history": [
            -825,
            -1690,
            -419,
            -1690,
            -695,
            -1690,
            -200,
            -54,
            -773,
            -1690,
            -29,
            -1181,
            -1690,
            -114,
            -1565,
            -208,
            -193,
            -217,
            -170,
            -65,
            -1267,
            -124,
            -24,
            -388,
            -1476,
            -18,
            -93
        ],
        "steps_history": [
            926,
            1690,
            520,
            1690,
            796,
            1690,
            301,
            155,
            874,
            1690,
            130,
            1282,
            1690,
            215,
            1666,
            309,
            294,
            318,
            271,
            166,
            1368,
            225,
            125,
            489,
            1577,
            119,
            194
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "328/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.0583813190460205,
        "final_policy_stability": 0.9822485207100592,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.7633136094674556,
            0.8579881656804734,
            0.8047337278106509,
            0.8165680473372781,
            0.7751479289940828,
            0.8875739644970414,
            0.8520710059171598,
            0.8816568047337278,
            0.9289940828402367,
            0.9408284023668639,
            0.8816568047337278,
            0.8461538461538461,
            0.9585798816568047,
            0.9230769230769231,
            0.7928994082840237,
            0.9822485207100592,
            0.9763313609467456,
            0.9171597633136095,
            0.9881656804733728,
            0.9763313609467456,
            0.9940828402366864,
            0.9526627218934911,
            0.9940828402366864,
            0.8165680473372781,
            0.9644970414201184,
            0.9822485207100592
        ],
        "reward_history": [
            -1690,
            -1317,
            -674,
            -899,
            -1690,
            -1326,
            -433,
            -1690,
            -518,
            -201,
            -246,
            -701,
            -1690,
            -169,
            -514,
            -1498,
            -9,
            -446,
            -512,
            -163,
            -105,
            -252,
            -250,
            -99,
            -1345,
            -187,
            -538
        ],
        "steps_history": [
            1690,
            1418,
            775,
            1000,
            1690,
            1427,
            534,
            1690,
            619,
            302,
            347,
            802,
            1690,
            270,
            615,
            1599,
            110,
            547,
            613,
            264,
            206,
            353,
            351,
            200,
            1446,
            288,
            639
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "329/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.11254620552063,
        "final_policy_stability": 0.9940828402366864,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.9053254437869822,
            0.834319526627219,
            0.7455621301775148,
            0.7869822485207101,
            0.8284023668639053,
            0.8461538461538461,
            0.7869822485207101,
            0.8875739644970414,
            0.9230769230769231,
            0.9467455621301775,
            0.9585798816568047,
            0.9230769230769231,
            0.9408284023668639,
            0.9349112426035503,
            0.7988165680473372,
            0.834319526627219,
            0.9585798816568047,
            0.9585798816568047,
            0.9822485207100592,
            0.9822485207100592,
            0.9704142011834319,
            0.9704142011834319,
            0.9763313609467456,
            0.9940828402366864,
            1.0,
            0.9940828402366864,
            0.9940828402366864,
            0.9940828402366864,
            0.8579881656804734,
            0.9940828402366864
        ],
        "reward_history": [
            -1370,
            -215,
            -1690,
            -1690,
            -1690,
            -1690,
            -1276,
            -905,
            -598,
            -319,
            -162,
            -135,
            -186,
            -404,
            -369,
            -1690,
            -754,
            -185,
            -213,
            -210,
            -279,
            -165,
            -448,
            -145,
            -112,
            -161,
            -128,
            -6,
            -207,
            -1690,
            -87
        ],
        "steps_history": [
            1471,
            316,
            1690,
            1690,
            1690,
            1690,
            1377,
            1006,
            699,
            420,
            263,
            236,
            287,
            505,
            470,
            1690,
            855,
            286,
            314,
            311,
            380,
            266,
            549,
            246,
            213,
            262,
            229,
            107,
            308,
            1690,
            188
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "330/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.3_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.374150037765503,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.7633136094674556,
            0.8994082840236687,
            0.834319526627219,
            0.893491124260355,
            0.8047337278106509,
            0.8047337278106509,
            0.8165680473372781,
            0.863905325443787,
            0.9408284023668639,
            0.8757396449704142,
            0.863905325443787,
            0.9585798816568047,
            0.863905325443787,
            0.863905325443787,
            0.9467455621301775,
            0.9526627218934911,
            0.9940828402366864,
            0.9408284023668639,
            0.9881656804733728,
            0.9881656804733728,
            0.9822485207100592,
            0.863905325443787,
            1.0,
            0.9881656804733728,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -496,
            -1690,
            -1690,
            -1690,
            -1690,
            -1306,
            -899,
            -179,
            -700,
            -1690,
            -219,
            -1690,
            -1582,
            -746,
            -477,
            -61,
            -1124,
            -296,
            -50,
            -70,
            -1690,
            -302,
            -433,
            -79
        ],
        "steps_history": [
            1690,
            1690,
            597,
            1690,
            1690,
            1690,
            1690,
            1407,
            1000,
            280,
            801,
            1690,
            320,
            1690,
            1683,
            847,
            578,
            162,
            1225,
            397,
            151,
            171,
            1690,
            403,
            534,
            180
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "331/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.416341066360474,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8875739644970414,
            0.8816568047337278,
            0.8224852071005917,
            0.8402366863905325,
            0.8461538461538461,
            0.8402366863905325,
            0.9112426035502958,
            0.8402366863905325,
            0.9585798816568047,
            0.8461538461538461,
            0.9704142011834319,
            0.9704142011834319,
            0.8875739644970414,
            0.9349112426035503,
            0.9585798816568047,
            0.8875739644970414,
            0.9585798816568047,
            0.9822485207100592,
            1.0,
            0.9112426035502958,
            1.0,
            0.9408284023668639,
            1.0,
            1.0
        ],
        "reward_history": [
            -748,
            -1082,
            -1139,
            -1690,
            -1690,
            -708,
            -990,
            -464,
            -1690,
            -97,
            -1690,
            -179,
            -240,
            -1690,
            -658,
            -706,
            -1690,
            -347,
            -255,
            -64,
            -1274,
            -339,
            -659,
            -798,
            -609
        ],
        "steps_history": [
            849,
            1183,
            1240,
            1690,
            1690,
            809,
            1091,
            565,
            1690,
            198,
            1690,
            280,
            341,
            1690,
            759,
            807,
            1690,
            448,
            356,
            165,
            1375,
            440,
            760,
            899,
            710
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "332/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.626671314239502,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.7869822485207101,
            0.7928994082840237,
            0.7988165680473372,
            0.893491124260355,
            0.9526627218934911,
            0.9289940828402367,
            0.9230769230769231,
            0.8284023668639053,
            0.9644970414201184,
            0.9940828402366864,
            0.8165680473372781,
            0.8106508875739645,
            0.8875739644970414,
            0.9704142011834319,
            0.9585798816568047,
            0.9940828402366864,
            0.9881656804733728,
            0.9881656804733728,
            1.0
        ],
        "reward_history": [
            -1690,
            -1390,
            -1690,
            -1690,
            -667,
            -77,
            -230,
            -489,
            -1690,
            -8,
            -106,
            -1690,
            -1690,
            -1012,
            -378,
            -193,
            -194,
            -310,
            -198,
            -97
        ],
        "steps_history": [
            1690,
            1491,
            1690,
            1690,
            768,
            178,
            331,
            590,
            1690,
            109,
            207,
            1690,
            1690,
            1113,
            479,
            294,
            295,
            411,
            299,
            198
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "333/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.157156467437744,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.7751479289940828,
            0.9112426035502958,
            0.9171597633136095,
            0.8520710059171598,
            0.8816568047337278,
            0.8520710059171598,
            0.8816568047337278,
            0.9171597633136095,
            0.9230769230769231,
            0.8816568047337278,
            0.8520710059171598,
            0.8402366863905325,
            0.9704142011834319,
            0.9881656804733728,
            0.9171597633136095,
            0.9526627218934911,
            0.8816568047337278,
            0.9704142011834319,
            0.9940828402366864,
            0.9822485207100592,
            0.9585798816568047,
            0.9822485207100592,
            0.9704142011834319,
            1.0,
            0.9289940828402367,
            0.9408284023668639,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -373,
            -523,
            -1690,
            -1024,
            -1047,
            -515,
            -1690,
            -409,
            -1690,
            -1690,
            -1690,
            -454,
            -277,
            -631,
            -576,
            -1690,
            -110,
            11,
            -49,
            -727,
            -267,
            -735,
            -177,
            -1085,
            -1257,
            -1043
        ],
        "steps_history": [
            1690,
            1690,
            474,
            624,
            1690,
            1125,
            1148,
            616,
            1690,
            510,
            1690,
            1690,
            1690,
            555,
            378,
            732,
            677,
            1690,
            211,
            90,
            150,
            828,
            368,
            836,
            278,
            1186,
            1358,
            1144
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "334/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.434732675552368,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.834319526627219,
            0.7692307692307693,
            0.727810650887574,
            0.8165680473372781,
            0.8461538461538461,
            0.893491124260355,
            0.9349112426035503,
            0.8461538461538461,
            0.9881656804733728,
            0.8757396449704142,
            0.9822485207100592,
            0.8757396449704142,
            0.9289940828402367,
            0.8224852071005917,
            0.9822485207100592,
            0.8461538461538461,
            0.8875739644970414,
            0.9644970414201184,
            0.9940828402366864,
            0.9763313609467456,
            0.9881656804733728,
            0.9289940828402367,
            0.9704142011834319,
            0.9940828402366864,
            1.0
        ],
        "reward_history": [
            -1690,
            -828,
            -1690,
            -1226,
            -1587,
            -781,
            -905,
            -247,
            -1690,
            -202,
            -1690,
            -271,
            -1690,
            -706,
            -1690,
            -205,
            -1690,
            -899,
            -110,
            -203,
            -257,
            -128,
            -498,
            -891,
            -999,
            -81
        ],
        "steps_history": [
            1690,
            929,
            1690,
            1327,
            1688,
            882,
            1006,
            348,
            1690,
            303,
            1690,
            372,
            1690,
            807,
            1690,
            306,
            1690,
            1000,
            211,
            304,
            358,
            229,
            599,
            992,
            1100,
            182
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "335/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.613330602645874,
        "final_policy_stability": 0.9763313609467456,
        "episodes_to_convergence": 20,
        "policy_stability_history": [
            0.0,
            0.7928994082840237,
            0.8402366863905325,
            0.8047337278106509,
            0.8698224852071006,
            0.8461538461538461,
            0.9053254437869822,
            0.9349112426035503,
            0.8816568047337278,
            0.8106508875739645,
            0.9349112426035503,
            0.834319526627219,
            0.863905325443787,
            0.9704142011834319,
            0.9408284023668639,
            0.9704142011834319,
            0.9230769230769231,
            0.8461538461538461,
            1.0,
            1.0,
            0.9763313609467456
        ],
        "reward_history": [
            -1690,
            -1690,
            -1119,
            -1690,
            -1331,
            -1690,
            -896,
            -511,
            -540,
            -1462,
            -189,
            -1690,
            -1690,
            -101,
            -419,
            -245,
            -368,
            -1690,
            -190,
            -37,
            -359
        ],
        "steps_history": [
            1690,
            1690,
            1220,
            1690,
            1432,
            1690,
            997,
            612,
            641,
            1563,
            290,
            1690,
            1690,
            202,
            520,
            346,
            469,
            1690,
            291,
            138,
            460
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "336/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.340272903442383,
        "final_policy_stability": 0.9704142011834319,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8994082840236687,
            0.8579881656804734,
            0.7988165680473372,
            0.7928994082840237,
            0.7751479289940828,
            0.9053254437869822,
            0.8994082840236687,
            0.9171597633136095,
            0.8165680473372781,
            0.8461538461538461,
            0.893491124260355,
            0.9230769230769231,
            0.9408284023668639,
            0.8520710059171598,
            0.9644970414201184,
            0.9940828402366864,
            0.9644970414201184,
            0.9881656804733728,
            0.9940828402366864,
            0.9881656804733728,
            0.9940828402366864,
            1.0,
            1.0,
            0.9704142011834319
        ],
        "reward_history": [
            -1690,
            -383,
            -1034,
            -1690,
            -1690,
            -1690,
            -768,
            -331,
            -593,
            -1102,
            -1690,
            -869,
            -798,
            -606,
            -1690,
            -285,
            -60,
            -484,
            -103,
            -229,
            -756,
            -577,
            -37,
            -62,
            -275
        ],
        "steps_history": [
            1690,
            484,
            1135,
            1690,
            1690,
            1690,
            869,
            432,
            694,
            1203,
            1690,
            970,
            899,
            707,
            1690,
            386,
            161,
            585,
            204,
            330,
            857,
            678,
            138,
            163,
            376
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "337/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.3479907512664795,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.757396449704142,
            0.7869822485207101,
            0.8284023668639053,
            0.8461538461538461,
            0.8994082840236687,
            0.8698224852071006,
            0.9704142011834319,
            0.9644970414201184,
            0.8816568047337278,
            0.8875739644970414,
            0.9644970414201184,
            0.8402366863905325,
            0.9940828402366864,
            0.9526627218934911,
            0.9881656804733728,
            0.9822485207100592,
            0.8520710059171598,
            0.9881656804733728,
            0.9763313609467456,
            0.9940828402366864,
            1.0,
            0.9289940828402367,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -1369,
            -922,
            -1410,
            -1001,
            -1690,
            -228,
            -352,
            -1690,
            -843,
            -277,
            -1690,
            -59,
            -211,
            -51,
            -108,
            -1300,
            -172,
            -123,
            -4,
            -307,
            -1315,
            -65
        ],
        "steps_history": [
            1690,
            1690,
            1470,
            1023,
            1511,
            1102,
            1690,
            329,
            453,
            1690,
            944,
            378,
            1690,
            160,
            312,
            152,
            209,
            1401,
            273,
            224,
            105,
            408,
            1416,
            166
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "338/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.5324573516845703,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 20,
        "policy_stability_history": [
            0.0,
            0.8579881656804734,
            0.7751479289940828,
            0.8284023668639053,
            0.7988165680473372,
            0.9289940828402367,
            0.8520710059171598,
            0.9822485207100592,
            0.9349112426035503,
            0.9763313609467456,
            0.9822485207100592,
            0.9704142011834319,
            0.9704142011834319,
            0.9704142011834319,
            0.9644970414201184,
            0.9585798816568047,
            0.9644970414201184,
            1.0,
            1.0,
            0.9940828402366864,
            1.0
        ],
        "reward_history": [
            -1112,
            -518,
            -1690,
            -1690,
            -780,
            -437,
            -1193,
            -172,
            -184,
            -57,
            -217,
            -162,
            -206,
            -193,
            -304,
            -506,
            -261,
            -81,
            -106,
            -266,
            -75
        ],
        "steps_history": [
            1213,
            619,
            1690,
            1690,
            881,
            538,
            1294,
            273,
            285,
            158,
            318,
            263,
            307,
            294,
            405,
            607,
            362,
            182,
            207,
            367,
            176
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "339/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.2618114948272705,
        "final_policy_stability": 0.9289940828402367,
        "episodes_to_convergence": 21,
        "policy_stability_history": [
            0.0,
            0.8106508875739645,
            0.9526627218934911,
            0.8816568047337278,
            0.8816568047337278,
            0.834319526627219,
            0.8047337278106509,
            0.8816568047337278,
            0.9585798816568047,
            0.8106508875739645,
            0.834319526627219,
            0.9704142011834319,
            0.8165680473372781,
            0.9408284023668639,
            0.8875739644970414,
            0.9940828402366864,
            0.9940828402366864,
            0.9763313609467456,
            0.9940828402366864,
            0.9940828402366864,
            0.9763313609467456,
            0.9289940828402367
        ],
        "reward_history": [
            -1690,
            -1281,
            -230,
            -546,
            -1690,
            -1394,
            -1690,
            -635,
            -90,
            -1690,
            -1190,
            -48,
            -1359,
            -411,
            -1094,
            -101,
            -62,
            -313,
            -96,
            -295,
            -198,
            -407
        ],
        "steps_history": [
            1690,
            1382,
            331,
            647,
            1690,
            1495,
            1690,
            736,
            191,
            1690,
            1291,
            149,
            1460,
            512,
            1195,
            202,
            163,
            414,
            197,
            396,
            299,
            508
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "340/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.3259687423706055,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.8224852071005917,
            0.8994082840236687,
            0.863905325443787,
            0.8224852071005917,
            0.8994082840236687,
            0.9467455621301775,
            0.8224852071005917,
            0.9526627218934911,
            0.9171597633136095,
            0.9289940828402367,
            0.8284023668639053,
            0.7988165680473372,
            0.9704142011834319,
            0.9467455621301775,
            0.9230769230769231,
            0.9585798816568047,
            0.9467455621301775,
            0.7988165680473372,
            0.8994082840236687,
            0.9881656804733728,
            0.9467455621301775,
            1.0,
            0.9881656804733728,
            0.9704142011834319,
            0.9940828402366864,
            1.0,
            1.0,
            0.9881656804733728,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -461,
            -508,
            -1690,
            -859,
            -89,
            -930,
            -85,
            -125,
            -225,
            -1074,
            -1690,
            -51,
            -513,
            -441,
            -154,
            -267,
            -1690,
            -667,
            -86,
            -438,
            -171,
            -207,
            -287,
            -86,
            -87,
            -59,
            -191,
            -21
        ],
        "steps_history": [
            1690,
            1690,
            562,
            609,
            1690,
            960,
            190,
            1031,
            186,
            226,
            326,
            1175,
            1690,
            152,
            614,
            542,
            255,
            368,
            1690,
            768,
            187,
            539,
            272,
            308,
            388,
            187,
            188,
            160,
            292,
            122
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "341/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.383497953414917,
        "final_policy_stability": 0.9940828402366864,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.834319526627219,
            0.7869822485207101,
            0.7928994082840237,
            0.8224852071005917,
            0.8520710059171598,
            0.8875739644970414,
            0.9289940828402367,
            0.8402366863905325,
            0.8698224852071006,
            0.9230769230769231,
            0.9704142011834319,
            0.893491124260355,
            0.834319526627219,
            0.9822485207100592,
            0.9349112426035503,
            0.9467455621301775,
            0.9822485207100592,
            0.9763313609467456,
            0.863905325443787,
            1.0,
            1.0,
            1.0,
            0.9940828402366864
        ],
        "reward_history": [
            -1690,
            -1476,
            -1690,
            -1011,
            -468,
            -1690,
            -830,
            -179,
            -725,
            -1690,
            -242,
            -120,
            -573,
            -836,
            -59,
            -389,
            -215,
            -149,
            -232,
            -1421,
            -118,
            -23,
            -59,
            -124
        ],
        "steps_history": [
            1690,
            1577,
            1690,
            1112,
            569,
            1690,
            931,
            280,
            826,
            1690,
            343,
            221,
            674,
            937,
            160,
            490,
            316,
            250,
            333,
            1522,
            219,
            124,
            160,
            225
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "342/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.3110105991363525,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.7633136094674556,
            0.893491124260355,
            0.8165680473372781,
            0.757396449704142,
            0.9408284023668639,
            0.9289940828402367,
            0.8994082840236687,
            0.9171597633136095,
            0.8106508875739645,
            0.7692307692307693,
            0.834319526627219,
            0.863905325443787,
            0.9467455621301775,
            0.9526627218934911,
            0.9763313609467456,
            0.9940828402366864,
            0.8047337278106509,
            0.9763313609467456,
            0.9704142011834319,
            0.9940828402366864,
            0.8875739644970414,
            1.0
        ],
        "reward_history": [
            -250,
            -1690,
            -1010,
            -1455,
            -1690,
            -124,
            -110,
            -293,
            -284,
            -1011,
            -1377,
            -1690,
            -749,
            -252,
            -135,
            -23,
            -92,
            -1690,
            -184,
            -193,
            -217,
            -1690,
            -14
        ],
        "steps_history": [
            351,
            1690,
            1111,
            1556,
            1690,
            225,
            211,
            394,
            385,
            1112,
            1478,
            1690,
            850,
            353,
            236,
            124,
            193,
            1690,
            285,
            294,
            318,
            1690,
            115
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "343/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.342098712921143,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.7988165680473372,
            0.8816568047337278,
            0.9230769230769231,
            0.8284023668639053,
            0.8047337278106509,
            0.9171597633136095,
            0.9585798816568047,
            0.9408284023668639,
            0.8106508875739645,
            0.9644970414201184,
            0.863905325443787,
            0.9526627218934911,
            0.9230769230769231,
            0.9526627218934911,
            0.9644970414201184,
            0.9704142011834319,
            0.7988165680473372,
            0.9940828402366864,
            0.8520710059171598,
            1.0,
            0.9585798816568047,
            1.0,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -395,
            -267,
            -1051,
            -1690,
            -803,
            -135,
            -116,
            -781,
            -165,
            -1690,
            -273,
            -774,
            -362,
            -174,
            -155,
            -1690,
            -45,
            -876,
            -140,
            -535,
            -299,
            -287
        ],
        "steps_history": [
            1690,
            1690,
            496,
            368,
            1152,
            1690,
            904,
            236,
            217,
            882,
            266,
            1690,
            374,
            875,
            463,
            275,
            256,
            1690,
            146,
            977,
            241,
            636,
            400,
            388
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "344/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.04198694229126,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.7751479289940828,
            0.7988165680473372,
            0.7988165680473372,
            0.8994082840236687,
            0.8284023668639053,
            0.9526627218934911,
            0.834319526627219,
            0.8994082840236687,
            0.8165680473372781,
            0.8165680473372781,
            0.863905325443787,
            0.9704142011834319,
            0.8875739644970414,
            0.8757396449704142,
            0.8284023668639053,
            0.9881656804733728,
            0.9644970414201184,
            0.9881656804733728,
            0.9704142011834319,
            0.8994082840236687,
            0.8224852071005917,
            0.9940828402366864,
            1.0,
            0.9940828402366864,
            1.0,
            1.0
        ],
        "reward_history": [
            -1356,
            -1690,
            -984,
            -1690,
            -115,
            -1026,
            -90,
            -1690,
            -571,
            -1011,
            -1286,
            -600,
            -42,
            -958,
            -512,
            -1329,
            -160,
            -330,
            -210,
            -279,
            -709,
            -1690,
            -60,
            -95,
            -49,
            -104,
            -237
        ],
        "steps_history": [
            1457,
            1690,
            1085,
            1690,
            216,
            1127,
            191,
            1690,
            672,
            1112,
            1387,
            701,
            143,
            1059,
            613,
            1430,
            261,
            431,
            311,
            380,
            810,
            1690,
            161,
            196,
            150,
            205,
            338
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "345/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.354067087173462,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.7633136094674556,
            0.8994082840236687,
            0.834319526627219,
            0.893491124260355,
            0.8106508875739645,
            0.8698224852071006,
            0.8461538461538461,
            0.8579881656804734,
            0.9349112426035503,
            0.9408284023668639,
            0.9349112426035503,
            0.8994082840236687,
            0.8757396449704142,
            0.9112426035502958,
            0.9822485207100592,
            0.9467455621301775,
            0.8994082840236687,
            0.9408284023668639,
            0.9822485207100592,
            0.9112426035502958,
            0.9704142011834319,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -496,
            -1690,
            -1690,
            -1690,
            -463,
            -1690,
            -742,
            -653,
            -425,
            -156,
            -443,
            -1690,
            -413,
            -116,
            -263,
            -1690,
            -807,
            -746,
            -1690,
            -718,
            -74
        ],
        "steps_history": [
            1690,
            1690,
            597,
            1690,
            1690,
            1690,
            564,
            1690,
            843,
            754,
            526,
            257,
            544,
            1690,
            514,
            217,
            364,
            1690,
            908,
            847,
            1690,
            819,
            175
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "346/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.213061571121216,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.8875739644970414,
            0.8816568047337278,
            0.8224852071005917,
            0.8402366863905325,
            0.8461538461538461,
            0.8402366863905325,
            0.9112426035502958,
            0.8402366863905325,
            0.9585798816568047,
            0.834319526627219,
            0.9704142011834319,
            0.9704142011834319,
            0.893491124260355,
            0.9289940828402367,
            0.9585798816568047,
            1.0,
            0.8698224852071006,
            0.9526627218934911,
            0.9763313609467456,
            1.0,
            0.9053254437869822,
            1.0
        ],
        "reward_history": [
            -748,
            -1082,
            -1139,
            -1690,
            -1690,
            -708,
            -990,
            -464,
            -1690,
            -97,
            -1690,
            -179,
            -240,
            -1690,
            -658,
            -394,
            -211,
            -1690,
            -347,
            -255,
            -64,
            -1274,
            -339
        ],
        "steps_history": [
            849,
            1183,
            1240,
            1690,
            1690,
            809,
            1091,
            565,
            1690,
            198,
            1690,
            280,
            341,
            1690,
            759,
            495,
            312,
            1690,
            448,
            356,
            165,
            1375,
            440
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "347/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.39726996421814,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 21,
        "policy_stability_history": [
            0.0,
            0.7869822485207101,
            0.7928994082840237,
            0.7988165680473372,
            0.8875739644970414,
            0.9526627218934911,
            0.9289940828402367,
            0.9230769230769231,
            0.8224852071005917,
            0.9644970414201184,
            0.9940828402366864,
            0.8165680473372781,
            0.8757396449704142,
            0.9053254437869822,
            0.9585798816568047,
            0.9644970414201184,
            0.9940828402366864,
            0.9822485207100592,
            0.9940828402366864,
            1.0,
            0.863905325443787,
            1.0
        ],
        "reward_history": [
            -1690,
            -1390,
            -1690,
            -1690,
            -667,
            -77,
            -230,
            -489,
            -1690,
            -8,
            -106,
            -1690,
            -1690,
            -1028,
            -362,
            -193,
            -194,
            -310,
            -198,
            -97,
            -1690,
            -68
        ],
        "steps_history": [
            1690,
            1491,
            1690,
            1690,
            768,
            178,
            331,
            590,
            1690,
            109,
            207,
            1690,
            1690,
            1129,
            463,
            294,
            295,
            411,
            299,
            198,
            1690,
            169
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "348/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.109930515289307,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.7751479289940828,
            0.9112426035502958,
            0.9171597633136095,
            0.8520710059171598,
            0.8816568047337278,
            0.8579881656804734,
            0.9289940828402367,
            0.863905325443787,
            0.9585798816568047,
            0.9408284023668639,
            0.9644970414201184,
            0.8994082840236687,
            0.9467455621301775,
            0.9763313609467456,
            0.8757396449704142,
            0.893491124260355,
            0.9644970414201184,
            0.9644970414201184,
            0.9940828402366864,
            0.9940828402366864,
            0.8757396449704142,
            0.863905325443787,
            0.9822485207100592,
            0.9940828402366864,
            1.0,
            0.9822485207100592,
            0.9171597633136095,
            0.9940828402366864,
            0.9763313609467456,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -373,
            -523,
            -1690,
            -1024,
            -725,
            -196,
            -587,
            -168,
            -265,
            -153,
            -1690,
            -456,
            -400,
            -1690,
            -1389,
            -231,
            -528,
            -277,
            -111,
            -1096,
            -1690,
            -112,
            13,
            -49,
            -727,
            -1103,
            -177,
            -391,
            -282
        ],
        "steps_history": [
            1690,
            1690,
            474,
            624,
            1690,
            1125,
            826,
            297,
            688,
            269,
            366,
            254,
            1690,
            557,
            501,
            1690,
            1490,
            332,
            629,
            378,
            212,
            1197,
            1690,
            213,
            88,
            150,
            828,
            1204,
            278,
            492,
            383
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "349/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.3342444896698,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.834319526627219,
            0.7692307692307693,
            0.727810650887574,
            0.8047337278106509,
            0.9171597633136095,
            0.9467455621301775,
            0.8520710059171598,
            0.8698224852071006,
            0.8284023668639053,
            0.8579881656804734,
            0.8165680473372781,
            0.8816568047337278,
            0.834319526627219,
            0.8875739644970414,
            0.9940828402366864,
            0.9940828402366864,
            0.9349112426035503,
            0.9408284023668639,
            0.9585798816568047,
            1.0,
            0.9822485207100592,
            1.0
        ],
        "reward_history": [
            -1690,
            -828,
            -1690,
            -1226,
            -1450,
            -1132,
            -289,
            -1690,
            -1690,
            -1690,
            -1690,
            -1690,
            -1690,
            -1690,
            -573,
            -256,
            -128,
            -498,
            -891,
            -999,
            -81,
            -456,
            -112
        ],
        "steps_history": [
            1690,
            929,
            1690,
            1327,
            1551,
            1233,
            390,
            1690,
            1690,
            1690,
            1690,
            1690,
            1690,
            1690,
            674,
            357,
            229,
            599,
            992,
            1100,
            182,
            557,
            213
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "350/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.505195379257202,
        "final_policy_stability": 0.9467455621301775,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.7810650887573964,
            0.8402366863905325,
            0.8047337278106509,
            0.8698224852071006,
            0.8520710059171598,
            0.9053254437869822,
            0.9349112426035503,
            0.8816568047337278,
            0.8224852071005917,
            0.9230769230769231,
            0.9289940828402367,
            0.9467455621301775,
            0.8520710059171598,
            0.9644970414201184,
            0.8284023668639053,
            0.9644970414201184,
            0.8520710059171598,
            0.9349112426035503,
            1.0,
            0.9940828402366864,
            1.0,
            0.9467455621301775
        ],
        "reward_history": [
            -1690,
            -1690,
            -1119,
            -1690,
            -1331,
            -1690,
            -896,
            -511,
            -540,
            -1462,
            -301,
            -148,
            -138,
            -1690,
            -182,
            -1690,
            -84,
            -1690,
            -659,
            -37,
            -122,
            -136,
            -229
        ],
        "steps_history": [
            1690,
            1690,
            1220,
            1690,
            1432,
            1690,
            997,
            612,
            641,
            1563,
            402,
            249,
            239,
            1690,
            283,
            1690,
            185,
            1690,
            760,
            138,
            223,
            237,
            330
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "351/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.999358415603638,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8994082840236687,
            0.8579881656804734,
            0.7988165680473372,
            0.7928994082840237,
            0.7810650887573964,
            0.8224852071005917,
            0.9408284023668639,
            0.9585798816568047,
            0.8461538461538461,
            0.9822485207100592,
            0.9585798816568047,
            0.9704142011834319,
            0.9881656804733728,
            0.9881656804733728,
            0.9940828402366864,
            0.9763313609467456,
            0.8816568047337278,
            0.8579881656804734,
            0.9822485207100592,
            0.9881656804733728,
            0.9644970414201184,
            0.9881656804733728,
            0.9644970414201184,
            0.9940828402366864,
            0.9881656804733728,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -1690,
            -383,
            -1034,
            -1690,
            -1690,
            -1690,
            -1690,
            -204,
            -22,
            -1391,
            -147,
            -199,
            -188,
            -148,
            -236,
            -210,
            -232,
            -1690,
            -1690,
            -382,
            -48,
            -131,
            -52,
            -266,
            -278,
            -756,
            -577,
            -37,
            -62
        ],
        "steps_history": [
            1690,
            484,
            1135,
            1690,
            1690,
            1690,
            1690,
            305,
            123,
            1492,
            248,
            300,
            289,
            249,
            337,
            311,
            333,
            1690,
            1690,
            483,
            149,
            232,
            153,
            367,
            379,
            857,
            678,
            138,
            163
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "352/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.552182674407959,
        "final_policy_stability": 0.9704142011834319,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.757396449704142,
            0.7869822485207101,
            0.8284023668639053,
            0.8520710059171598,
            0.8994082840236687,
            0.8757396449704142,
            0.9704142011834319,
            0.9644970414201184,
            0.8757396449704142,
            0.8698224852071006,
            0.8224852071005917,
            0.9349112426035503,
            0.9881656804733728,
            0.9881656804733728,
            0.9349112426035503,
            0.9822485207100592,
            0.9526627218934911,
            0.9112426035502958,
            0.9704142011834319,
            0.9053254437869822,
            0.9230769230769231,
            1.0,
            0.863905325443787,
            1.0,
            0.9704142011834319
        ],
        "reward_history": [
            -1690,
            -1690,
            -1369,
            -922,
            -1410,
            -1001,
            -1690,
            -228,
            -352,
            -1690,
            -843,
            -1515,
            -823,
            -51,
            -108,
            -239,
            -103,
            -273,
            -382,
            -172,
            -636,
            -1315,
            -65,
            -1690,
            -20,
            -752
        ],
        "steps_history": [
            1690,
            1690,
            1470,
            1023,
            1511,
            1102,
            1690,
            329,
            453,
            1690,
            944,
            1616,
            924,
            152,
            209,
            340,
            204,
            374,
            483,
            273,
            737,
            1416,
            166,
            1690,
            121,
            853
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "353/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.541771173477173,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 20,
        "policy_stability_history": [
            0.0,
            0.8579881656804734,
            0.7751479289940828,
            0.8284023668639053,
            0.7988165680473372,
            0.9349112426035503,
            0.8402366863905325,
            0.9644970414201184,
            0.9585798816568047,
            0.9644970414201184,
            0.9644970414201184,
            0.9644970414201184,
            0.9644970414201184,
            0.9822485207100592,
            0.8461538461538461,
            0.9526627218934911,
            0.9881656804733728,
            0.863905325443787,
            0.8994082840236687,
            1.0,
            1.0
        ],
        "reward_history": [
            -1112,
            -518,
            -1690,
            -1690,
            -780,
            -437,
            -1104,
            -111,
            -181,
            -528,
            -162,
            -206,
            -193,
            -62,
            -1690,
            -237,
            -94,
            -1131,
            -1500,
            -242,
            -190
        ],
        "steps_history": [
            1213,
            619,
            1690,
            1690,
            881,
            538,
            1205,
            212,
            282,
            629,
            263,
            307,
            294,
            163,
            1690,
            338,
            195,
            1232,
            1601,
            343,
            291
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "354/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.6623785495758057,
        "final_policy_stability": 0.9822485207100592,
        "episodes_to_convergence": 16,
        "policy_stability_history": [
            0.0,
            0.8106508875739645,
            0.9467455621301775,
            0.8757396449704142,
            0.8875739644970414,
            0.834319526627219,
            0.7928994082840237,
            0.8875739644970414,
            0.9585798816568047,
            0.8224852071005917,
            0.8224852071005917,
            0.9644970414201184,
            0.9349112426035503,
            0.9467455621301775,
            0.9822485207100592,
            0.9881656804733728,
            0.9822485207100592
        ],
        "reward_history": [
            -1690,
            -1281,
            -230,
            -546,
            -1690,
            -1394,
            -1690,
            -635,
            -90,
            -1690,
            -1493,
            -195,
            -909,
            -411,
            -644,
            -403,
            -210
        ],
        "steps_history": [
            1690,
            1382,
            331,
            647,
            1690,
            1495,
            1690,
            736,
            191,
            1690,
            1594,
            296,
            1010,
            512,
            745,
            504,
            311
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "355/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.3481879234313965,
        "final_policy_stability": 0.9881656804733728,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.8284023668639053,
            0.834319526627219,
            0.8757396449704142,
            0.8284023668639053,
            0.9171597633136095,
            0.8402366863905325,
            0.9171597633136095,
            0.8106508875739645,
            0.757396449704142,
            0.9881656804733728,
            0.9763313609467456,
            0.9467455621301775,
            0.9349112426035503,
            0.863905325443787,
            0.9940828402366864,
            0.9408284023668639,
            0.9526627218934911,
            0.9940828402366864,
            0.8994082840236687,
            0.9940828402366864,
            0.9940828402366864,
            0.8875739644970414,
            0.9940828402366864,
            0.9526627218934911,
            0.9881656804733728
        ],
        "reward_history": [
            -1690,
            -454,
            -1690,
            -962,
            -1690,
            -412,
            -1690,
            -188,
            -1054,
            -1690,
            -51,
            -28,
            -545,
            -757,
            -1690,
            -31,
            -288,
            -279,
            -99,
            -1406,
            -86,
            -87,
            -1025,
            -157,
            -259,
            -232
        ],
        "steps_history": [
            1690,
            555,
            1690,
            1063,
            1690,
            513,
            1690,
            289,
            1155,
            1690,
            152,
            129,
            646,
            858,
            1690,
            132,
            389,
            380,
            200,
            1507,
            187,
            188,
            1126,
            258,
            360,
            333
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "356/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.163450717926025,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.863905325443787,
            0.8106508875739645,
            0.8757396449704142,
            0.9467455621301775,
            0.8698224852071006,
            0.9349112426035503,
            0.9053254437869822,
            0.9289940828402367,
            0.8875739644970414,
            0.9526627218934911,
            0.9230769230769231,
            0.8520710059171598,
            0.8402366863905325,
            0.8402366863905325,
            0.8461538461538461,
            0.9704142011834319,
            0.9585798816568047,
            0.9881656804733728,
            0.9940828402366864,
            0.8698224852071006,
            0.9881656804733728,
            0.9585798816568047,
            0.9881656804733728,
            0.9467455621301775,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -1690,
            -665,
            -948,
            -401,
            -163,
            -1690,
            -198,
            -277,
            -196,
            -582,
            -146,
            -324,
            -1690,
            -916,
            -1690,
            -709,
            -83,
            -345,
            -2,
            -59,
            -1690,
            -81,
            -125,
            -160,
            -451,
            -118,
            -23,
            -59
        ],
        "steps_history": [
            1690,
            766,
            1049,
            502,
            264,
            1690,
            299,
            378,
            297,
            683,
            247,
            425,
            1690,
            1017,
            1690,
            810,
            184,
            446,
            103,
            160,
            1690,
            182,
            226,
            261,
            552,
            219,
            124,
            160
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "357/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.446990966796875,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.7633136094674556,
            0.893491124260355,
            0.8165680473372781,
            0.7928994082840237,
            0.8284023668639053,
            0.9349112426035503,
            0.834319526627219,
            0.893491124260355,
            0.8284023668639053,
            0.9644970414201184,
            0.9585798816568047,
            0.9467455621301775,
            0.9763313609467456,
            0.9467455621301775,
            0.8047337278106509,
            0.8757396449704142,
            0.9526627218934911,
            0.9822485207100592,
            1.0,
            1.0,
            0.9704142011834319,
            0.9763313609467456,
            0.9704142011834319,
            1.0,
            1.0
        ],
        "reward_history": [
            -250,
            -1690,
            -1010,
            -1455,
            -1374,
            -945,
            -100,
            -1690,
            -728,
            -1226,
            -150,
            -116,
            -120,
            -112,
            -103,
            -1690,
            -460,
            -210,
            -57,
            -107,
            -213,
            -193,
            -217,
            -170,
            -65,
            -51
        ],
        "steps_history": [
            351,
            1690,
            1111,
            1556,
            1475,
            1046,
            201,
            1690,
            829,
            1327,
            251,
            217,
            221,
            213,
            204,
            1690,
            561,
            311,
            158,
            208,
            314,
            294,
            318,
            271,
            166,
            152
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "358/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.360954999923706,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.7988165680473372,
            0.8816568047337278,
            0.9230769230769231,
            0.7810650887573964,
            0.9349112426035503,
            0.8875739644970414,
            0.9112426035502958,
            0.9408284023668639,
            0.8284023668639053,
            0.7928994082840237,
            0.8284023668639053,
            0.9704142011834319,
            0.7869822485207101,
            0.9763313609467456,
            0.9230769230769231,
            0.9881656804733728,
            0.9585798816568047,
            0.9822485207100592,
            0.9526627218934911,
            0.9289940828402367,
            1.0,
            0.8816568047337278,
            1.0
        ],
        "reward_history": [
            -1690,
            -1690,
            -395,
            -267,
            -1690,
            -179,
            -1107,
            -703,
            -206,
            -1690,
            -1331,
            -1690,
            -78,
            -1690,
            -45,
            -580,
            -2,
            -153,
            -79,
            -446,
            -982,
            -252,
            -1690,
            -206
        ],
        "steps_history": [
            1690,
            1690,
            496,
            368,
            1690,
            280,
            1208,
            804,
            307,
            1690,
            1432,
            1690,
            179,
            1690,
            146,
            681,
            103,
            254,
            180,
            547,
            1083,
            353,
            1690,
            307
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "359/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 6,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 24.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.10239315032959,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.7751479289940828,
            0.7988165680473372,
            0.7988165680473372,
            0.8994082840236687,
            0.8284023668639053,
            0.9526627218934911,
            0.834319526627219,
            0.8994082840236687,
            0.8165680473372781,
            0.8165680473372781,
            0.863905325443787,
            0.9704142011834319,
            0.8875739644970414,
            0.8757396449704142,
            0.8284023668639053,
            0.9881656804733728,
            0.9644970414201184,
            0.9881656804733728,
            0.9704142011834319,
            0.893491124260355,
            0.8224852071005917,
            0.9940828402366864,
            1.0,
            0.9940828402366864,
            1.0,
            1.0
        ],
        "reward_history": [
            -1356,
            -1690,
            -984,
            -1690,
            -115,
            -1026,
            -90,
            -1690,
            -571,
            -1011,
            -1286,
            -600,
            -42,
            -958,
            -512,
            -1329,
            -160,
            -330,
            -210,
            -279,
            -709,
            -1690,
            -60,
            -95,
            -49,
            -104,
            -237
        ],
        "steps_history": [
            1457,
            1690,
            1085,
            1690,
            216,
            1127,
            191,
            1690,
            672,
            1112,
            1387,
            701,
            143,
            1059,
            613,
            1430,
            261,
            431,
            311,
            380,
            810,
            1690,
            161,
            196,
            150,
            205,
            338
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "360/810",
        "save_path": "experiments/20250131_160708/training_plots/size_6/lr0.4_df0.99_eps0.1_trial4"
    }
]