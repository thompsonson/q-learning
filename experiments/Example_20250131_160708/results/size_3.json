[
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.0305070877075195,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.7346938775510204,
            0.8571428571428571,
            0.8979591836734694,
            0.8367346938775511,
            0.9591836734693877,
            0.9183673469387755,
            0.8979591836734694,
            0.9183673469387755,
            0.9591836734693877,
            0.9591836734693877,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -47,
            -375,
            -490,
            29,
            -302,
            13,
            -5,
            -142,
            -271,
            -69,
            -138,
            -34,
            36,
            -131,
            26
        ],
        "steps_history": [
            148,
            476,
            490,
            72,
            403,
            88,
            106,
            243,
            372,
            170,
            239,
            135,
            65,
            232,
            75
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "1/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.685262680053711,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.8571428571428571,
            0.9387755102040817,
            0.8775510204081632,
            0.8979591836734694,
            0.8775510204081632,
            0.8979591836734694,
            0.9795918367346939,
            0.9591836734693877,
            0.9795918367346939,
            0.9795918367346939
        ],
        "reward_history": [
            -204,
            -301,
            -38,
            -17,
            -192,
            -64,
            -241,
            -260,
            -72,
            -332,
            -22,
            -120
        ],
        "steps_history": [
            305,
            402,
            139,
            118,
            293,
            165,
            342,
            361,
            173,
            433,
            123,
            221
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "2/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.848290205001831,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 9,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.7959183673469388,
            0.9387755102040817,
            0.9183673469387755,
            0.9591836734693877,
            0.9591836734693877,
            0.9591836734693877,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -490,
            -109,
            -264,
            -129,
            -26,
            -81,
            -125,
            -26,
            -66,
            30
        ],
        "steps_history": [
            490,
            210,
            365,
            230,
            127,
            182,
            226,
            127,
            167,
            71
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "3/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.981595754623413,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8979591836734694,
            0.8775510204081632,
            0.8571428571428571,
            0.8979591836734694,
            0.9183673469387755,
            1.0,
            0.9591836734693877,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -209,
            -41,
            -490,
            -154,
            -216,
            -257,
            -157,
            16,
            -45,
            -16,
            23
        ],
        "steps_history": [
            310,
            142,
            490,
            255,
            317,
            358,
            258,
            85,
            146,
            117,
            78
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "4/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.45843505859375,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 16,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.7959183673469388,
            0.8979591836734694,
            0.8775510204081632,
            0.9795918367346939,
            0.9591836734693877,
            0.8775510204081632,
            0.8979591836734694,
            0.9183673469387755,
            0.9183673469387755,
            1.0,
            0.9591836734693877,
            0.9387755102040817,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -92,
            -138,
            -269,
            -490,
            -6,
            53,
            33,
            -174,
            43,
            30,
            -134,
            -146,
            -59,
            -228,
            13,
            26,
            -118
        ],
        "steps_history": [
            193,
            239,
            370,
            490,
            107,
            48,
            68,
            275,
            58,
            71,
            235,
            247,
            160,
            329,
            88,
            75,
            219
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "5/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.3265256881713867,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.8367346938775511,
            0.8571428571428571,
            0.9795918367346939,
            0.8775510204081632,
            0.9183673469387755,
            0.8979591836734694,
            0.9183673469387755,
            0.9795918367346939,
            0.8571428571428571,
            0.9387755102040817,
            0.9795918367346939,
            1.0,
            0.9795918367346939,
            1.0,
            1.0,
            1.0,
            0.9591836734693877
        ],
        "reward_history": [
            -265,
            -161,
            1,
            -100,
            71,
            -33,
            -2,
            -104,
            -99,
            11,
            2,
            -126,
            56,
            48,
            23,
            -1,
            -54,
            -38,
            -137
        ],
        "steps_history": [
            366,
            262,
            100,
            201,
            30,
            134,
            103,
            205,
            200,
            90,
            99,
            227,
            45,
            53,
            78,
            102,
            155,
            139,
            238
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "6/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.8749847412109375,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.9183673469387755,
            0.8367346938775511,
            0.8571428571428571,
            0.9183673469387755,
            0.9795918367346939,
            0.9387755102040817,
            1.0,
            0.9795918367346939,
            1.0,
            0.9795918367346939
        ],
        "reward_history": [
            -490,
            -51,
            -490,
            -36,
            -158,
            45,
            -150,
            1,
            6,
            34,
            53
        ],
        "steps_history": [
            490,
            152,
            490,
            137,
            259,
            56,
            251,
            100,
            95,
            67,
            48
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "7/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.315337896347046,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 16,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.7755102040816326,
            0.9795918367346939,
            0.8775510204081632,
            0.9387755102040817,
            0.9591836734693877,
            0.8979591836734694,
            0.9795918367346939,
            0.9795918367346939,
            0.9795918367346939,
            0.9591836734693877,
            0.9795918367346939,
            1.0,
            1.0,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -219,
            -490,
            -75,
            11,
            -81,
            -37,
            24,
            -272,
            -106,
            13,
            30,
            -75,
            -44,
            32,
            35,
            -122,
            -14
        ],
        "steps_history": [
            320,
            490,
            176,
            90,
            182,
            138,
            77,
            373,
            207,
            88,
            71,
            176,
            145,
            69,
            66,
            223,
            115
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "8/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.5889840126037598,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.8775510204081632,
            0.8163265306122449,
            0.8775510204081632,
            0.8367346938775511,
            0.9183673469387755,
            0.8571428571428571,
            0.9387755102040817,
            0.9183673469387755,
            0.8979591836734694,
            0.9795918367346939,
            0.9795918367346939,
            1.0,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            -211,
            -16,
            -77,
            -131,
            -122,
            -32,
            -227,
            35,
            -191,
            -244,
            -46,
            -87,
            29,
            -93,
            49
        ],
        "steps_history": [
            312,
            117,
            178,
            232,
            223,
            133,
            328,
            66,
            292,
            345,
            147,
            188,
            72,
            194,
            52
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "9/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7847135066986084,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.8367346938775511,
            0.9183673469387755,
            0.8775510204081632,
            0.8775510204081632,
            0.9795918367346939,
            0.9183673469387755,
            0.9795918367346939,
            0.9387755102040817,
            0.8979591836734694,
            0.9795918367346939,
            1.0,
            1.0,
            0.9591836734693877
        ],
        "reward_history": [
            -93,
            -139,
            -250,
            60,
            -24,
            -150,
            41,
            15,
            60,
            35,
            -292,
            -7,
            39,
            -45,
            -113
        ],
        "steps_history": [
            194,
            240,
            351,
            41,
            125,
            251,
            60,
            86,
            41,
            66,
            393,
            108,
            62,
            146,
            214
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "10/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.747544765472412,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.8775510204081632,
            0.9591836734693877,
            0.8775510204081632,
            0.9183673469387755,
            0.8367346938775511,
            0.8979591836734694,
            0.9387755102040817,
            0.9183673469387755,
            0.9183673469387755,
            0.8775510204081632,
            1.0,
            1.0,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            -131,
            -147,
            -56,
            -11,
            -115,
            28,
            -128,
            -135,
            67,
            5,
            -52,
            -319,
            -1,
            -1,
            3,
            4
        ],
        "steps_history": [
            232,
            248,
            157,
            112,
            216,
            73,
            229,
            236,
            34,
            96,
            153,
            420,
            102,
            102,
            98,
            97
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "11/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.383139133453369,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.8979591836734694,
            0.9387755102040817,
            0.9183673469387755,
            0.9387755102040817,
            0.8775510204081632,
            0.9591836734693877,
            0.9387755102040817,
            0.9591836734693877,
            0.9387755102040817,
            0.9183673469387755,
            0.9795918367346939,
            0.9591836734693877,
            0.9795918367346939,
            1.0,
            0.9591836734693877,
            0.9795918367346939
        ],
        "reward_history": [
            -207,
            -236,
            -31,
            43,
            -8,
            20,
            -196,
            67,
            -4,
            -193,
            8,
            -74,
            47,
            18,
            -74,
            29,
            -178,
            -335
        ],
        "steps_history": [
            308,
            337,
            132,
            58,
            109,
            81,
            297,
            34,
            105,
            294,
            93,
            175,
            54,
            83,
            175,
            72,
            279,
            436
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "12/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.298194169998169,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.8367346938775511,
            0.7959183673469388,
            0.9183673469387755,
            0.9387755102040817,
            0.9795918367346939,
            0.9591836734693877,
            0.9591836734693877,
            0.9183673469387755,
            0.9183673469387755,
            0.9795918367346939,
            1.0,
            0.9795918367346939,
            1.0,
            0.9795918367346939,
            0.9591836734693877,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            28,
            -98,
            -490,
            -171,
            -130,
            -38,
            28,
            12,
            41,
            -129,
            -29,
            22,
            13,
            30,
            48,
            76,
            -139,
            26,
            35,
            -88
        ],
        "steps_history": [
            73,
            199,
            490,
            272,
            231,
            139,
            73,
            89,
            60,
            230,
            130,
            79,
            88,
            71,
            53,
            25,
            240,
            75,
            66,
            189
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "13/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.666259765625,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.9387755102040817,
            0.7959183673469388,
            0.8571428571428571,
            0.9183673469387755,
            0.8775510204081632,
            0.9795918367346939,
            0.9387755102040817,
            0.9795918367346939,
            1.0,
            0.9795918367346939,
            0.9591836734693877
        ],
        "reward_history": [
            -214,
            -384,
            58,
            -259,
            -296,
            -9,
            -318,
            -13,
            -47,
            -3,
            20,
            26,
            -28
        ],
        "steps_history": [
            315,
            485,
            43,
            360,
            397,
            110,
            419,
            114,
            148,
            104,
            81,
            75,
            129
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "14/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.5558454990386963,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.7959183673469388,
            0.9183673469387755,
            0.8775510204081632,
            0.9183673469387755,
            0.9591836734693877,
            0.9387755102040817,
            0.8775510204081632,
            0.9591836734693877,
            0.9387755102040817,
            0.9795918367346939,
            0.9795918367346939,
            0.9591836734693877,
            0.9795918367346939
        ],
        "reward_history": [
            -89,
            38,
            -83,
            21,
            -176,
            -103,
            25,
            -35,
            -105,
            53,
            33,
            -11,
            50,
            -14,
            50
        ],
        "steps_history": [
            190,
            63,
            184,
            80,
            277,
            204,
            76,
            136,
            206,
            48,
            68,
            112,
            51,
            115,
            51
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "15/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6002557277679443,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.7551020408163265,
            0.8367346938775511,
            0.8979591836734694,
            0.8367346938775511,
            0.9591836734693877,
            0.9183673469387755,
            0.8979591836734694,
            0.9183673469387755,
            0.9591836734693877,
            0.9591836734693877,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -47,
            -375,
            -490,
            29,
            -302,
            13,
            -5,
            -142,
            -271,
            -69,
            -138,
            -34,
            36,
            -131,
            26
        ],
        "steps_history": [
            148,
            476,
            490,
            72,
            403,
            88,
            106,
            243,
            372,
            170,
            239,
            135,
            65,
            232,
            75
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "16/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.790534734725952,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.8571428571428571,
            0.9387755102040817,
            0.8775510204081632,
            0.8979591836734694,
            0.8775510204081632,
            0.8979591836734694,
            0.9795918367346939,
            0.9591836734693877,
            0.9795918367346939,
            0.9795918367346939
        ],
        "reward_history": [
            -204,
            -301,
            -38,
            -17,
            -192,
            -64,
            -241,
            -260,
            -72,
            -332,
            -22,
            -120
        ],
        "steps_history": [
            305,
            402,
            139,
            118,
            293,
            165,
            342,
            361,
            173,
            433,
            123,
            221
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "17/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9818997383117676,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 9,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.7959183673469388,
            0.9387755102040817,
            0.9183673469387755,
            0.9591836734693877,
            0.9591836734693877,
            0.9591836734693877,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -490,
            -109,
            -264,
            -129,
            -26,
            -81,
            -125,
            -26,
            -66,
            30
        ],
        "steps_history": [
            490,
            210,
            365,
            230,
            127,
            182,
            226,
            127,
            167,
            71
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "18/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9177353382110596,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8979591836734694,
            0.8775510204081632,
            0.8571428571428571,
            0.8979591836734694,
            0.9183673469387755,
            1.0,
            0.9591836734693877,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -209,
            -41,
            -490,
            -154,
            -216,
            -257,
            -157,
            16,
            -45,
            -16,
            23
        ],
        "steps_history": [
            310,
            142,
            490,
            255,
            317,
            358,
            258,
            85,
            146,
            117,
            78
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "19/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6155338287353516,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.7959183673469388,
            0.8979591836734694,
            0.8775510204081632,
            0.9795918367346939,
            0.9591836734693877,
            0.8775510204081632,
            0.8979591836734694,
            0.9183673469387755,
            0.9183673469387755,
            0.9795918367346939,
            0.9795918367346939,
            0.9387755102040817,
            1.0,
            1.0
        ],
        "reward_history": [
            -92,
            -138,
            -269,
            -490,
            -6,
            53,
            33,
            -174,
            43,
            30,
            -134,
            -146,
            -59,
            -228,
            13,
            26
        ],
        "steps_history": [
            193,
            239,
            370,
            490,
            107,
            48,
            68,
            275,
            58,
            71,
            235,
            247,
            160,
            329,
            88,
            75
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "20/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.2702531814575195,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 18,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.8367346938775511,
            0.8571428571428571,
            0.9795918367346939,
            0.8775510204081632,
            0.9183673469387755,
            0.8979591836734694,
            0.9183673469387755,
            0.9795918367346939,
            0.9183673469387755,
            0.9591836734693877,
            0.9795918367346939,
            1.0,
            0.9795918367346939,
            1.0,
            1.0,
            1.0,
            0.9591836734693877
        ],
        "reward_history": [
            -265,
            -161,
            1,
            -100,
            71,
            -33,
            -2,
            -104,
            -99,
            11,
            2,
            -126,
            56,
            48,
            23,
            -1,
            -54,
            -38,
            -137
        ],
        "steps_history": [
            366,
            262,
            100,
            201,
            30,
            134,
            103,
            205,
            200,
            90,
            99,
            227,
            45,
            53,
            78,
            102,
            155,
            139,
            238
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "21/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.67745304107666,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.9183673469387755,
            0.8367346938775511,
            0.8775510204081632,
            0.9591836734693877,
            1.0,
            0.9795918367346939,
            0.9795918367346939,
            0.9591836734693877,
            0.9591836734693877,
            0.9795918367346939,
            1.0,
            1.0
        ],
        "reward_history": [
            -490,
            -51,
            -490,
            -351,
            -150,
            1,
            6,
            34,
            51,
            -70,
            11,
            -30,
            -107
        ],
        "steps_history": [
            490,
            152,
            490,
            452,
            251,
            100,
            95,
            67,
            50,
            171,
            90,
            131,
            208
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "22/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.5970141887664795,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.7755102040816326,
            0.9387755102040817,
            0.8775510204081632,
            0.9387755102040817,
            0.9795918367346939,
            0.9387755102040817,
            0.9387755102040817,
            0.9387755102040817,
            0.9591836734693877,
            0.9795918367346939,
            0.9591836734693877,
            0.9591836734693877,
            1.0,
            1.0,
            0.9387755102040817,
            1.0
        ],
        "reward_history": [
            -219,
            -490,
            -75,
            11,
            -81,
            -37,
            24,
            -44,
            -127,
            -106,
            13,
            30,
            -75,
            -44,
            32,
            35,
            -122,
            -14
        ],
        "steps_history": [
            320,
            490,
            176,
            90,
            182,
            138,
            77,
            145,
            228,
            207,
            88,
            71,
            176,
            145,
            69,
            66,
            223,
            115
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "23/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6617538928985596,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.8775510204081632,
            0.8163265306122449,
            0.8775510204081632,
            0.8367346938775511,
            0.9183673469387755,
            0.8571428571428571,
            0.9387755102040817,
            0.9183673469387755,
            0.8979591836734694,
            0.9795918367346939,
            0.9795918367346939,
            1.0,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            -211,
            -16,
            -77,
            -131,
            -122,
            -32,
            -227,
            35,
            -191,
            -244,
            -46,
            -87,
            29,
            -93,
            49
        ],
        "steps_history": [
            312,
            117,
            178,
            232,
            223,
            133,
            328,
            66,
            292,
            345,
            147,
            188,
            72,
            194,
            52
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "24/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6254196166992188,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.8367346938775511,
            0.9183673469387755,
            0.8775510204081632,
            0.8775510204081632,
            0.9795918367346939,
            0.9183673469387755,
            1.0,
            0.9387755102040817,
            0.9183673469387755,
            0.9795918367346939,
            1.0,
            1.0
        ],
        "reward_history": [
            -93,
            -139,
            -250,
            60,
            -24,
            -150,
            41,
            15,
            60,
            35,
            -292,
            -7,
            39,
            -45
        ],
        "steps_history": [
            194,
            240,
            351,
            41,
            125,
            251,
            60,
            86,
            41,
            66,
            393,
            108,
            62,
            146
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "25/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6394710540771484,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.8775510204081632,
            0.9591836734693877,
            0.8163265306122449,
            0.8775510204081632,
            0.8979591836734694,
            0.9591836734693877,
            0.9387755102040817,
            0.9387755102040817,
            0.9795918367346939,
            0.9795918367346939,
            1.0,
            1.0,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -131,
            -147,
            -56,
            -11,
            -196,
            -322,
            -63,
            34,
            4,
            -122,
            38,
            -22,
            -1,
            -1,
            3,
            4
        ],
        "steps_history": [
            232,
            248,
            157,
            112,
            297,
            423,
            164,
            67,
            97,
            223,
            63,
            123,
            102,
            102,
            98,
            97
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "26/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.3667051792144775,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 16,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.8979591836734694,
            0.9387755102040817,
            0.9183673469387755,
            0.9387755102040817,
            0.8775510204081632,
            0.9591836734693877,
            0.9387755102040817,
            0.9591836734693877,
            0.9387755102040817,
            0.9795918367346939,
            0.9591836734693877,
            0.9795918367346939,
            1.0,
            0.9795918367346939,
            0.9591836734693877
        ],
        "reward_history": [
            -207,
            -236,
            -31,
            43,
            -8,
            20,
            -196,
            67,
            -4,
            -193,
            -167,
            47,
            18,
            -74,
            29,
            -178,
            -335
        ],
        "steps_history": [
            308,
            337,
            132,
            58,
            109,
            81,
            297,
            34,
            105,
            294,
            268,
            54,
            83,
            175,
            72,
            279,
            436
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "27/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7169787883758545,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.8367346938775511,
            0.8367346938775511,
            0.9387755102040817,
            0.8979591836734694,
            0.9183673469387755,
            0.9591836734693877,
            0.9387755102040817,
            0.9387755102040817,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            28,
            -98,
            -490,
            -220,
            -129,
            10,
            -13,
            -7,
            -129,
            -29,
            18,
            17,
            30,
            48
        ],
        "steps_history": [
            73,
            199,
            490,
            321,
            230,
            91,
            114,
            108,
            230,
            130,
            83,
            84,
            71,
            53
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "28/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.780372381210327,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.9387755102040817,
            0.7959183673469388,
            0.8571428571428571,
            0.9183673469387755,
            0.8775510204081632,
            0.9795918367346939,
            0.9387755102040817,
            0.9795918367346939,
            1.0,
            0.9795918367346939,
            0.9591836734693877
        ],
        "reward_history": [
            -214,
            -384,
            58,
            -259,
            -296,
            -9,
            -318,
            -13,
            -47,
            -3,
            20,
            26,
            -28
        ],
        "steps_history": [
            315,
            485,
            43,
            360,
            397,
            110,
            419,
            114,
            148,
            104,
            81,
            75,
            129
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "29/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.0579874515533447,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.7959183673469388,
            0.9183673469387755,
            0.8775510204081632,
            0.9183673469387755,
            0.9591836734693877,
            0.9387755102040817,
            0.8775510204081632,
            0.9591836734693877,
            0.9591836734693877,
            0.9591836734693877,
            1.0,
            0.9591836734693877,
            1.0,
            0.9795918367346939
        ],
        "reward_history": [
            -89,
            38,
            -83,
            21,
            -176,
            -103,
            25,
            -35,
            -105,
            53,
            33,
            -11,
            50,
            -14,
            50,
            -70
        ],
        "steps_history": [
            190,
            63,
            184,
            80,
            277,
            204,
            76,
            136,
            206,
            48,
            68,
            112,
            51,
            115,
            51,
            171
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "30/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.2_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6008059978485107,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8367346938775511,
            0.9591836734693877,
            0.8979591836734694,
            0.8775510204081632,
            0.9591836734693877,
            0.9795918367346939,
            1.0,
            0.9795918367346939,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            -490,
            -33,
            -275,
            15,
            1,
            -268,
            39,
            41,
            -5,
            -142,
            -271,
            -69
        ],
        "steps_history": [
            490,
            134,
            376,
            86,
            100,
            369,
            62,
            60,
            106,
            243,
            372,
            170
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "31/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6341073513031006,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7755102040816326,
            0.8163265306122449,
            0.8979591836734694,
            0.9591836734693877,
            0.9183673469387755,
            0.8775510204081632,
            0.9591836734693877,
            0.9387755102040817,
            0.9591836734693877,
            1.0,
            0.9795918367346939,
            0.9591836734693877
        ],
        "reward_history": [
            -204,
            -231,
            -87,
            -38,
            -32,
            -59,
            -64,
            10,
            -150,
            -146,
            -13,
            -72,
            -332
        ],
        "steps_history": [
            305,
            332,
            188,
            139,
            133,
            160,
            165,
            91,
            251,
            247,
            114,
            173,
            433
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "32/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.045313835144043,
        "final_policy_stability": 0.9387755102040817,
        "episodes_to_convergence": 7,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.7959183673469388,
            0.9183673469387755,
            0.9387755102040817,
            0.9795918367346939,
            0.9591836734693877,
            0.9387755102040817
        ],
        "reward_history": [
            -490,
            -109,
            -264,
            -129,
            -26,
            -81,
            -125,
            -26
        ],
        "steps_history": [
            490,
            210,
            365,
            230,
            127,
            182,
            226,
            127
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "33/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.115861415863037,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8775510204081632,
            0.8367346938775511,
            0.9183673469387755,
            0.8571428571428571,
            0.9183673469387755,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -209,
            -41,
            -48,
            -352,
            -9,
            -249,
            -490,
            -25,
            16,
            -45,
            -16
        ],
        "steps_history": [
            310,
            142,
            149,
            453,
            110,
            350,
            490,
            126,
            85,
            146,
            117
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "34/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.9909677505493164,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.7755102040816326,
            0.7959183673469388,
            0.8571428571428571,
            0.8979591836734694,
            0.9795918367346939,
            0.9387755102040817,
            0.8571428571428571,
            0.9387755102040817,
            0.9591836734693877,
            1.0,
            0.9795918367346939,
            1.0,
            1.0
        ],
        "reward_history": [
            -92,
            -138,
            -269,
            -490,
            -6,
            53,
            33,
            -174,
            43,
            -205,
            -146,
            -59,
            -228,
            13
        ],
        "steps_history": [
            193,
            239,
            370,
            490,
            107,
            48,
            68,
            275,
            58,
            306,
            247,
            160,
            329,
            88
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "35/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.62213397026062,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.8775510204081632,
            0.9183673469387755,
            0.8775510204081632,
            0.8979591836734694,
            0.7959183673469388,
            0.9183673469387755,
            0.8979591836734694,
            0.9387755102040817,
            1.0,
            0.9591836734693877,
            0.9795918367346939,
            0.9795918367346939,
            0.9591836734693877,
            1.0,
            1.0
        ],
        "reward_history": [
            -265,
            -9,
            27,
            19,
            5,
            -264,
            -2,
            -165,
            -5,
            46,
            25,
            24,
            -140,
            -75,
            -1,
            -54
        ],
        "steps_history": [
            366,
            110,
            74,
            82,
            96,
            365,
            103,
            266,
            106,
            55,
            76,
            77,
            241,
            176,
            102,
            155
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "36/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6567203998565674,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8775510204081632,
            0.8979591836734694,
            0.9795918367346939,
            0.8775510204081632,
            0.8571428571428571,
            0.9591836734693877,
            0.9591836734693877,
            0.9795918367346939,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -490,
            -51,
            -88,
            -37,
            50,
            -148,
            -158,
            45,
            -150,
            1,
            6,
            34,
            53,
            -72
        ],
        "steps_history": [
            490,
            152,
            189,
            138,
            51,
            249,
            259,
            56,
            251,
            100,
            95,
            67,
            48,
            173
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "37/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.037245035171509,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 9,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.8571428571428571,
            0.8163265306122449,
            0.9387755102040817,
            0.9387755102040817,
            0.9387755102040817,
            1.0,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -54,
            33,
            -189,
            -367,
            6,
            -81,
            -37,
            24,
            -44,
            -127
        ],
        "steps_history": [
            155,
            68,
            290,
            468,
            95,
            182,
            138,
            77,
            145,
            228
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "38/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.635004997253418,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.8367346938775511,
            0.8979591836734694,
            0.9183673469387755,
            0.8775510204081632,
            0.9795918367346939,
            0.9183673469387755,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -211,
            -16,
            -266,
            -165,
            -32,
            -227,
            35,
            -490,
            -110,
            18,
            -87,
            29
        ],
        "steps_history": [
            312,
            117,
            367,
            266,
            133,
            328,
            66,
            490,
            211,
            83,
            188,
            72
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "39/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9778625965118408,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.8163265306122449,
            0.9183673469387755,
            0.9183673469387755,
            0.8979591836734694,
            0.8979591836734694,
            1.0,
            0.9591836734693877,
            0.9795918367346939,
            0.9795918367346939
        ],
        "reward_history": [
            -93,
            -139,
            -250,
            60,
            -24,
            -150,
            -45,
            60,
            35,
            -74,
            -117
        ],
        "steps_history": [
            194,
            240,
            351,
            41,
            125,
            251,
            146,
            41,
            66,
            175,
            218
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "40/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.792501211166382,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.7142857142857143,
            0.8979591836734694,
            0.9387755102040817,
            0.8979591836734694,
            0.8979591836734694,
            0.8367346938775511,
            0.8979591836734694,
            0.9387755102040817,
            1.0,
            0.9387755102040817,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -131,
            -221,
            18,
            -11,
            -115,
            28,
            -490,
            -4,
            16,
            22,
            -122,
            38,
            -22,
            -1,
            -1
        ],
        "steps_history": [
            232,
            322,
            83,
            112,
            216,
            73,
            490,
            105,
            85,
            79,
            223,
            63,
            123,
            102,
            102
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "41/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.752471923828125,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.8367346938775511,
            0.8775510204081632,
            0.9387755102040817,
            0.8571428571428571,
            0.8979591836734694,
            0.9795918367346939,
            1.0,
            0.9591836734693877,
            0.9795918367346939,
            1.0,
            1.0,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            -206,
            -108,
            -28,
            -89,
            -8,
            -114,
            -62,
            67,
            -4,
            15,
            -107,
            55,
            -15,
            -5,
            47
        ],
        "steps_history": [
            307,
            209,
            129,
            190,
            109,
            215,
            163,
            34,
            105,
            86,
            208,
            46,
            116,
            106,
            54
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "42/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7440345287323,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.8163265306122449,
            0.8367346938775511,
            0.9387755102040817,
            0.9387755102040817,
            0.9591836734693877,
            0.9387755102040817,
            0.9387755102040817,
            0.9795918367346939,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            28,
            -98,
            -335,
            -274,
            -81,
            -38,
            28,
            -278,
            -29,
            18,
            17,
            30,
            48,
            76
        ],
        "steps_history": [
            73,
            199,
            436,
            375,
            182,
            139,
            73,
            379,
            130,
            83,
            84,
            71,
            53,
            25
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "43/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.625842809677124,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.9183673469387755,
            0.8571428571428571,
            0.9183673469387755,
            0.9387755102040817,
            0.9795918367346939,
            0.9183673469387755,
            0.9795918367346939,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -214,
            -384,
            58,
            -259,
            -296,
            29,
            63,
            -318,
            -13,
            -47,
            -3,
            20,
            26
        ],
        "steps_history": [
            315,
            485,
            43,
            360,
            397,
            72,
            38,
            419,
            114,
            148,
            104,
            81,
            75
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "44/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.669290542602539,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.7551020408163265,
            0.8775510204081632,
            0.8571428571428571,
            0.9387755102040817,
            0.8775510204081632,
            0.8775510204081632,
            0.9387755102040817,
            0.9183673469387755,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -89,
            38,
            -83,
            -256,
            -152,
            -62,
            33,
            -13,
            29,
            -245,
            50,
            -70,
            12,
            50
        ],
        "steps_history": [
            190,
            63,
            184,
            357,
            253,
            163,
            68,
            114,
            72,
            346,
            51,
            171,
            89,
            51
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "45/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.649156332015991,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8367346938775511,
            0.9591836734693877,
            0.8979591836734694,
            0.8775510204081632,
            0.9591836734693877,
            0.9795918367346939,
            1.0,
            0.9795918367346939,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            -490,
            -33,
            -275,
            15,
            1,
            -268,
            39,
            41,
            -5,
            -142,
            -271,
            -69
        ],
        "steps_history": [
            490,
            134,
            376,
            86,
            100,
            369,
            62,
            60,
            106,
            243,
            372,
            170
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "46/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.774904727935791,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7755102040816326,
            0.8163265306122449,
            0.9183673469387755,
            0.9795918367346939,
            0.9387755102040817,
            0.8775510204081632,
            0.9591836734693877,
            0.9387755102040817,
            0.9591836734693877,
            1.0,
            0.9795918367346939,
            0.9591836734693877
        ],
        "reward_history": [
            -204,
            -231,
            -87,
            -38,
            -32,
            -59,
            -64,
            10,
            -150,
            -146,
            -13,
            -72,
            -332
        ],
        "steps_history": [
            305,
            332,
            188,
            139,
            133,
            160,
            165,
            91,
            251,
            247,
            114,
            173,
            433
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "47/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.0056240558624268,
        "final_policy_stability": 0.9387755102040817,
        "episodes_to_convergence": 7,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.7959183673469388,
            0.9183673469387755,
            0.9591836734693877,
            0.9591836734693877,
            0.9591836734693877,
            0.9387755102040817
        ],
        "reward_history": [
            -490,
            -109,
            -264,
            -129,
            -26,
            -81,
            -125,
            -26
        ],
        "steps_history": [
            490,
            210,
            365,
            230,
            127,
            182,
            226,
            127
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "48/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.0874016284942627,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8775510204081632,
            0.8367346938775511,
            0.9183673469387755,
            0.8571428571428571,
            0.9183673469387755,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -209,
            -41,
            -48,
            -352,
            -9,
            -249,
            -490,
            -25,
            16,
            -45,
            -16
        ],
        "steps_history": [
            310,
            142,
            149,
            453,
            110,
            350,
            490,
            126,
            85,
            146,
            117
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "49/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.685119867324829,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.7755102040816326,
            0.7959183673469388,
            0.8571428571428571,
            0.8979591836734694,
            0.9795918367346939,
            0.9387755102040817,
            0.8571428571428571,
            0.9387755102040817,
            0.9591836734693877,
            1.0,
            0.9795918367346939,
            1.0,
            1.0
        ],
        "reward_history": [
            -92,
            -138,
            -269,
            -490,
            -6,
            53,
            33,
            -174,
            43,
            -205,
            -146,
            -59,
            -228,
            13
        ],
        "steps_history": [
            193,
            239,
            370,
            490,
            107,
            48,
            68,
            275,
            58,
            306,
            247,
            160,
            329,
            88
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "50/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.461930990219116,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 16,
        "policy_stability_history": [
            0.0,
            0.8775510204081632,
            0.9183673469387755,
            0.8775510204081632,
            0.8979591836734694,
            0.7959183673469388,
            0.9183673469387755,
            0.8979591836734694,
            0.9387755102040817,
            1.0,
            0.9591836734693877,
            1.0,
            0.9387755102040817,
            0.9591836734693877,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -265,
            -9,
            27,
            19,
            5,
            -264,
            -2,
            -165,
            -5,
            46,
            25,
            24,
            -140,
            -75,
            -1,
            -54,
            -38
        ],
        "steps_history": [
            366,
            110,
            74,
            82,
            96,
            365,
            103,
            266,
            106,
            55,
            76,
            77,
            241,
            176,
            102,
            155,
            139
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "51/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.62290358543396,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8775510204081632,
            0.8979591836734694,
            0.9795918367346939,
            0.8775510204081632,
            0.8571428571428571,
            0.9591836734693877,
            0.9795918367346939,
            0.9591836734693877,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -490,
            -51,
            -88,
            -37,
            50,
            -148,
            -158,
            45,
            -150,
            1,
            6,
            34,
            53,
            -72
        ],
        "steps_history": [
            490,
            152,
            189,
            138,
            51,
            249,
            259,
            56,
            251,
            100,
            95,
            67,
            48,
            173
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "52/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.0728766918182373,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 9,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.8571428571428571,
            0.8367346938775511,
            0.9591836734693877,
            0.9387755102040817,
            0.9387755102040817,
            1.0,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -54,
            33,
            -189,
            -367,
            6,
            -81,
            -37,
            24,
            -44,
            -127
        ],
        "steps_history": [
            155,
            68,
            290,
            468,
            95,
            182,
            138,
            77,
            145,
            228
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "53/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.506990432739258,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.8367346938775511,
            0.8979591836734694,
            0.9183673469387755,
            0.8775510204081632,
            0.9795918367346939,
            0.9183673469387755,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -211,
            -16,
            -266,
            -165,
            -32,
            -227,
            35,
            -490,
            -110,
            18,
            -87,
            29
        ],
        "steps_history": [
            312,
            117,
            367,
            266,
            133,
            328,
            66,
            490,
            211,
            83,
            188,
            72
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "54/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.996957540512085,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.8163265306122449,
            0.9183673469387755,
            0.9183673469387755,
            0.8979591836734694,
            0.8979591836734694,
            1.0,
            0.9591836734693877,
            0.9795918367346939,
            0.9795918367346939
        ],
        "reward_history": [
            -93,
            -139,
            -250,
            60,
            -24,
            -150,
            -45,
            60,
            35,
            -74,
            -117
        ],
        "steps_history": [
            194,
            240,
            351,
            41,
            125,
            251,
            146,
            41,
            66,
            175,
            218
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "55/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.695082426071167,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 14,
        "policy_stability_history": [
            0.0,
            0.7142857142857143,
            0.8979591836734694,
            0.9387755102040817,
            0.8979591836734694,
            0.8979591836734694,
            0.8367346938775511,
            0.8979591836734694,
            0.9387755102040817,
            1.0,
            0.9183673469387755,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -131,
            -221,
            18,
            -11,
            -115,
            28,
            -490,
            -4,
            16,
            22,
            -122,
            38,
            -22,
            -1,
            -1
        ],
        "steps_history": [
            232,
            322,
            83,
            112,
            216,
            73,
            490,
            105,
            85,
            79,
            223,
            63,
            123,
            102,
            102
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "56/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.587782859802246,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 15,
        "policy_stability_history": [
            0.0,
            0.7755102040816326,
            0.9591836734693877,
            0.9183673469387755,
            0.8979591836734694,
            0.8775510204081632,
            0.9591836734693877,
            0.9591836734693877,
            0.9591836734693877,
            0.9183673469387755,
            0.9387755102040817,
            0.9795918367346939,
            1.0,
            0.9387755102040817,
            1.0,
            1.0
        ],
        "reward_history": [
            -207,
            -490,
            64,
            -38,
            50,
            -33,
            35,
            4,
            67,
            -90,
            -107,
            54,
            -14,
            -15,
            57,
            18
        ],
        "steps_history": [
            308,
            490,
            37,
            139,
            51,
            134,
            66,
            97,
            34,
            191,
            208,
            47,
            115,
            116,
            44,
            83
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "57/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6434714794158936,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.8163265306122449,
            0.8367346938775511,
            0.9387755102040817,
            0.9387755102040817,
            0.9591836734693877,
            0.9387755102040817,
            0.9387755102040817,
            0.9795918367346939,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            28,
            -98,
            -335,
            -274,
            -81,
            -38,
            28,
            -278,
            -29,
            18,
            17,
            30,
            48,
            76
        ],
        "steps_history": [
            73,
            199,
            436,
            375,
            182,
            139,
            73,
            379,
            130,
            83,
            84,
            71,
            53,
            25
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "58/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.607640266418457,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.9183673469387755,
            0.8571428571428571,
            0.9183673469387755,
            0.9387755102040817,
            0.9795918367346939,
            0.9183673469387755,
            0.9795918367346939,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -214,
            -384,
            58,
            -259,
            -296,
            29,
            63,
            -318,
            -13,
            -47,
            -3,
            20,
            26
        ],
        "steps_history": [
            315,
            485,
            43,
            360,
            397,
            72,
            38,
            419,
            114,
            148,
            104,
            81,
            75
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "59/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.775465965270996,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.7551020408163265,
            0.8775510204081632,
            0.8571428571428571,
            0.9387755102040817,
            0.8775510204081632,
            0.8775510204081632,
            0.9387755102040817,
            0.9183673469387755,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -89,
            38,
            -83,
            -256,
            -152,
            -62,
            33,
            -13,
            29,
            -245,
            50,
            -70,
            12,
            50
        ],
        "steps_history": [
            190,
            63,
            184,
            357,
            253,
            163,
            68,
            114,
            72,
            346,
            51,
            171,
            89,
            51
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "60/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.3_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.620802640914917,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8775510204081632,
            0.8571428571428571,
            0.8979591836734694,
            0.8571428571428571,
            0.9591836734693877,
            0.9183673469387755,
            0.9183673469387755,
            1.0,
            1.0,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -490,
            -33,
            -54,
            60,
            -79,
            15,
            1,
            -268,
            39,
            41,
            -5,
            -142
        ],
        "steps_history": [
            490,
            134,
            155,
            41,
            180,
            86,
            100,
            369,
            62,
            60,
            106,
            243
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "61/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6794095039367676,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.7755102040816326,
            0.8163265306122449,
            0.8775510204081632,
            0.9387755102040817,
            0.9183673469387755,
            0.8979591836734694,
            0.9183673469387755,
            0.9591836734693877,
            0.9591836734693877,
            1.0,
            1.0
        ],
        "reward_history": [
            -204,
            -231,
            -87,
            -38,
            -32,
            -59,
            -64,
            10,
            -124,
            -172,
            -13,
            -72
        ],
        "steps_history": [
            305,
            332,
            188,
            139,
            133,
            160,
            165,
            91,
            225,
            273,
            114,
            173
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "62/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9420099258422852,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8571428571428571,
            0.8571428571428571,
            0.9591836734693877,
            0.9591836734693877,
            0.9387755102040817,
            0.9591836734693877,
            0.9387755102040817,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -167,
            52,
            -79,
            -490,
            23,
            -129,
            -26,
            -81,
            -125,
            -26,
            -66
        ],
        "steps_history": [
            268,
            49,
            180,
            490,
            78,
            230,
            127,
            182,
            226,
            127,
            167
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "63/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.2376251220703125,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 9,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.8775510204081632,
            0.7959183673469388,
            0.9183673469387755,
            0.8775510204081632,
            0.9387755102040817,
            0.9795918367346939,
            1.0,
            1.0
        ],
        "reward_history": [
            -209,
            -41,
            -49,
            -351,
            -9,
            -249,
            -257,
            -157,
            16,
            -45
        ],
        "steps_history": [
            310,
            142,
            150,
            452,
            110,
            350,
            358,
            258,
            85,
            146
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "64/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6678287982940674,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.7551020408163265,
            0.8163265306122449,
            0.8775510204081632,
            0.9183673469387755,
            1.0,
            0.9591836734693877,
            0.8571428571428571,
            0.9387755102040817,
            0.9387755102040817,
            1.0,
            0.9795918367346939,
            1.0,
            1.0
        ],
        "reward_history": [
            -92,
            -138,
            -269,
            -490,
            -6,
            53,
            33,
            -174,
            43,
            -205,
            -146,
            -59,
            -228,
            13
        ],
        "steps_history": [
            193,
            239,
            370,
            490,
            107,
            48,
            68,
            275,
            58,
            306,
            247,
            160,
            329,
            88
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "65/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6178627014160156,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.9183673469387755,
            0.9387755102040817,
            0.8775510204081632,
            0.8367346938775511,
            0.9591836734693877,
            0.8571428571428571,
            0.9387755102040817,
            1.0,
            1.0,
            0.9795918367346939
        ],
        "reward_history": [
            -265,
            -9,
            27,
            23,
            -47,
            -216,
            -2,
            -165,
            -5,
            46,
            33,
            2
        ],
        "steps_history": [
            366,
            110,
            74,
            78,
            148,
            317,
            103,
            266,
            106,
            55,
            68,
            99
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "66/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.042276382446289,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 7,
        "policy_stability_history": [
            0.0,
            0.9183673469387755,
            0.8163265306122449,
            0.8571428571428571,
            0.9387755102040817,
            0.9387755102040817,
            1.0,
            1.0
        ],
        "reward_history": [
            -490,
            -51,
            -88,
            -221,
            -15,
            -158,
            45,
            -150
        ],
        "steps_history": [
            490,
            152,
            189,
            322,
            116,
            259,
            56,
            251
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "67/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.044154405593872,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 8,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8163265306122449,
            0.9183673469387755,
            0.8979591836734694,
            0.9795918367346939,
            1.0,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -54,
            33,
            -189,
            -91,
            -175,
            6,
            -81,
            -37,
            24
        ],
        "steps_history": [
            155,
            68,
            290,
            192,
            276,
            95,
            182,
            138,
            77
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "68/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6662497520446777,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.8979591836734694,
            0.8367346938775511,
            0.9183673469387755,
            0.9183673469387755,
            0.8775510204081632,
            0.9591836734693877,
            0.9183673469387755,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -211,
            -16,
            -81,
            -127,
            -122,
            -32,
            -227,
            35,
            -490,
            -110,
            18,
            -87,
            29
        ],
        "steps_history": [
            312,
            117,
            182,
            228,
            223,
            133,
            328,
            66,
            490,
            211,
            83,
            188,
            72
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "69/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.553413152694702,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.7755102040816326,
            0.8571428571428571,
            0.8367346938775511,
            0.9387755102040817,
            0.8775510204081632,
            0.9387755102040817,
            0.8979591836734694,
            1.0,
            0.9591836734693877,
            0.9795918367346939,
            0.9591836734693877
        ],
        "reward_history": [
            -93,
            -139,
            21,
            -170,
            60,
            -24,
            -150,
            -45,
            60,
            35,
            -74,
            -117
        ],
        "steps_history": [
            194,
            240,
            80,
            271,
            41,
            125,
            251,
            146,
            41,
            66,
            175,
            218
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "70/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.6243646144866943,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.9183673469387755,
            0.9387755102040817,
            0.9387755102040817,
            0.8367346938775511,
            0.8979591836734694,
            0.9387755102040817,
            0.9387755102040817,
            1.0,
            1.0,
            1.0,
            0.9591836734693877
        ],
        "reward_history": [
            -131,
            -147,
            27,
            18,
            -11,
            -196,
            -120,
            -101,
            33,
            65,
            41,
            34,
            4
        ],
        "steps_history": [
            232,
            248,
            74,
            83,
            112,
            297,
            221,
            202,
            68,
            36,
            60,
            67,
            97
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "71/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.7168667316436768,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.8979591836734694,
            0.8571428571428571,
            0.8571428571428571,
            0.9795918367346939,
            0.9795918367346939,
            0.9387755102040817,
            1.0,
            1.0,
            1.0,
            0.9795918367346939
        ],
        "reward_history": [
            -206,
            -237,
            -89,
            -54,
            -231,
            67,
            -4,
            15,
            -107,
            55,
            -15,
            -15
        ],
        "steps_history": [
            307,
            338,
            190,
            155,
            332,
            34,
            105,
            86,
            208,
            46,
            116,
            116
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "72/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.0340051651000977,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 8,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.8163265306122449,
            0.8775510204081632,
            0.9795918367346939,
            0.9795918367346939,
            0.9795918367346939,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            28,
            -49,
            -376,
            -233,
            -130,
            -38,
            -121,
            -129,
            -29
        ],
        "steps_history": [
            73,
            150,
            477,
            334,
            231,
            139,
            222,
            230,
            130
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "73/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.884347677230835,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 8,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.9387755102040817,
            0.8775510204081632,
            0.9591836734693877,
            0.9795918367346939,
            0.9795918367346939,
            1.0,
            1.0
        ],
        "reward_history": [
            -214,
            -384,
            58,
            -259,
            -296,
            -9,
            -318,
            -13,
            -47
        ],
        "steps_history": [
            315,
            485,
            43,
            360,
            397,
            110,
            419,
            114,
            148
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "74/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9700367450714111,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 9,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.7755102040816326,
            0.8571428571428571,
            0.9387755102040817,
            0.9183673469387755,
            1.0,
            1.0,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            -89,
            38,
            -83,
            21,
            -24,
            -51,
            64,
            -14,
            0,
            -62
        ],
        "steps_history": [
            190,
            63,
            184,
            80,
            125,
            152,
            37,
            115,
            101,
            163
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "75/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.663663864135742,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8775510204081632,
            0.8571428571428571,
            0.8979591836734694,
            0.8571428571428571,
            0.9591836734693877,
            0.9183673469387755,
            0.9183673469387755,
            1.0,
            1.0,
            0.9591836734693877,
            0.9795918367346939
        ],
        "reward_history": [
            -490,
            -33,
            -54,
            60,
            -79,
            15,
            1,
            -268,
            39,
            41,
            -5,
            -142
        ],
        "steps_history": [
            490,
            134,
            155,
            41,
            180,
            86,
            100,
            369,
            62,
            60,
            106,
            243
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "76/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.738309383392334,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.7755102040816326,
            0.8163265306122449,
            0.8775510204081632,
            0.9387755102040817,
            0.9183673469387755,
            0.8979591836734694,
            0.9183673469387755,
            0.9591836734693877,
            0.9591836734693877,
            1.0,
            1.0
        ],
        "reward_history": [
            -204,
            -231,
            -87,
            -38,
            -32,
            -59,
            -64,
            10,
            -124,
            -172,
            -13,
            -72
        ],
        "steps_history": [
            305,
            332,
            188,
            139,
            133,
            160,
            165,
            91,
            225,
            273,
            114,
            173
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "77/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9563329219818115,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 10,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8571428571428571,
            0.8571428571428571,
            0.9591836734693877,
            0.9591836734693877,
            0.9387755102040817,
            0.9591836734693877,
            0.9387755102040817,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            -167,
            52,
            -79,
            -490,
            23,
            -129,
            -26,
            -81,
            -125,
            -26,
            -66
        ],
        "steps_history": [
            268,
            49,
            180,
            490,
            78,
            230,
            127,
            182,
            226,
            127,
            167
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "78/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.8812496662139893,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 9,
        "policy_stability_history": [
            0.0,
            0.8367346938775511,
            0.8775510204081632,
            0.7959183673469388,
            0.9183673469387755,
            0.8775510204081632,
            0.9387755102040817,
            0.9795918367346939,
            1.0,
            1.0
        ],
        "reward_history": [
            -209,
            -41,
            -49,
            -351,
            -9,
            -249,
            -257,
            -157,
            16,
            -45
        ],
        "steps_history": [
            310,
            142,
            150,
            452,
            110,
            350,
            358,
            258,
            85,
            146
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "79/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.8095414638519287,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 13,
        "policy_stability_history": [
            0.0,
            0.7551020408163265,
            0.8163265306122449,
            0.8775510204081632,
            0.9183673469387755,
            1.0,
            0.9591836734693877,
            0.8571428571428571,
            0.9387755102040817,
            0.9387755102040817,
            1.0,
            0.9795918367346939,
            1.0,
            1.0
        ],
        "reward_history": [
            -92,
            -138,
            -269,
            -490,
            -6,
            53,
            33,
            -174,
            43,
            -205,
            -146,
            -59,
            -228,
            13
        ],
        "steps_history": [
            193,
            239,
            370,
            490,
            107,
            48,
            68,
            275,
            58,
            306,
            247,
            160,
            329,
            88
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "80/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.0522682666778564,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.9183673469387755,
            0.9387755102040817,
            0.8775510204081632,
            0.8367346938775511,
            0.9591836734693877,
            0.8571428571428571,
            0.9387755102040817,
            1.0,
            1.0,
            0.9795918367346939
        ],
        "reward_history": [
            -265,
            -9,
            27,
            23,
            -47,
            -216,
            -2,
            -165,
            -5,
            46,
            33,
            2
        ],
        "steps_history": [
            366,
            110,
            74,
            78,
            148,
            317,
            103,
            266,
            106,
            55,
            68,
            99
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "81/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9425337314605713,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 7,
        "policy_stability_history": [
            0.0,
            0.9183673469387755,
            0.8163265306122449,
            0.8571428571428571,
            0.9387755102040817,
            0.9387755102040817,
            1.0,
            1.0
        ],
        "reward_history": [
            -490,
            -51,
            -88,
            -221,
            -15,
            -158,
            45,
            -150
        ],
        "steps_history": [
            490,
            152,
            189,
            322,
            116,
            259,
            56,
            251
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "82/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9127378463745117,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 8,
        "policy_stability_history": [
            0.0,
            0.8979591836734694,
            0.8163265306122449,
            0.9183673469387755,
            0.8979591836734694,
            0.9795918367346939,
            1.0,
            0.9591836734693877,
            1.0
        ],
        "reward_history": [
            -54,
            33,
            -189,
            -91,
            -175,
            6,
            -81,
            -37,
            24
        ],
        "steps_history": [
            155,
            68,
            290,
            192,
            276,
            95,
            182,
            138,
            77
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "83/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.784740924835205,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.8571428571428571,
            0.8979591836734694,
            0.8367346938775511,
            0.9183673469387755,
            0.9183673469387755,
            0.8775510204081632,
            0.9591836734693877,
            0.9183673469387755,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -211,
            -16,
            -81,
            -127,
            -122,
            -32,
            -227,
            35,
            -490,
            -110,
            18,
            -87,
            29
        ],
        "steps_history": [
            312,
            117,
            182,
            228,
            223,
            133,
            328,
            66,
            490,
            211,
            83,
            188,
            72
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "84/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.834386110305786,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.7755102040816326,
            0.8571428571428571,
            0.8367346938775511,
            0.9387755102040817,
            0.8775510204081632,
            0.9387755102040817,
            0.8979591836734694,
            1.0,
            0.9591836734693877,
            0.9795918367346939,
            0.9591836734693877
        ],
        "reward_history": [
            -93,
            -139,
            21,
            -170,
            60,
            -24,
            -150,
            -45,
            60,
            35,
            -74,
            -117
        ],
        "steps_history": [
            194,
            240,
            80,
            271,
            41,
            125,
            251,
            146,
            41,
            66,
            175,
            218
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "85/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.5498247146606445,
        "final_policy_stability": 0.9591836734693877,
        "episodes_to_convergence": 12,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.9183673469387755,
            0.9387755102040817,
            0.9387755102040817,
            0.8367346938775511,
            0.8979591836734694,
            0.9387755102040817,
            0.9387755102040817,
            1.0,
            1.0,
            1.0,
            0.9591836734693877
        ],
        "reward_history": [
            -131,
            -147,
            27,
            18,
            -11,
            -196,
            -120,
            -101,
            33,
            65,
            41,
            34,
            4
        ],
        "steps_history": [
            232,
            248,
            74,
            83,
            112,
            297,
            221,
            202,
            68,
            36,
            60,
            67,
            97
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "86/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.616765022277832,
        "final_policy_stability": 0.9795918367346939,
        "episodes_to_convergence": 11,
        "policy_stability_history": [
            0.0,
            0.8163265306122449,
            0.8979591836734694,
            0.8571428571428571,
            0.8571428571428571,
            0.9795918367346939,
            0.9795918367346939,
            0.9387755102040817,
            1.0,
            1.0,
            1.0,
            0.9795918367346939
        ],
        "reward_history": [
            -206,
            -237,
            -89,
            -54,
            -231,
            67,
            -4,
            15,
            -107,
            55,
            -15,
            -15
        ],
        "steps_history": [
            307,
            338,
            190,
            155,
            332,
            34,
            105,
            86,
            208,
            46,
            116,
            116
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "87/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9992377758026123,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 8,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.8163265306122449,
            0.8775510204081632,
            0.9795918367346939,
            0.9795918367346939,
            0.9795918367346939,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            28,
            -49,
            -376,
            -233,
            -130,
            -38,
            -121,
            -129,
            -29
        ],
        "steps_history": [
            73,
            150,
            477,
            334,
            231,
            139,
            222,
            230,
            130
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "88/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 2.0356831550598145,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 8,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.9387755102040817,
            0.8775510204081632,
            0.9387755102040817,
            0.9795918367346939,
            0.9795918367346939,
            1.0,
            1.0
        ],
        "reward_history": [
            -214,
            -384,
            58,
            -259,
            -296,
            -9,
            -318,
            -13,
            -47
        ],
        "steps_history": [
            315,
            485,
            43,
            360,
            397,
            110,
            419,
            114,
            148
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "89/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 3,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 12.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 1.9898509979248047,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 9,
        "policy_stability_history": [
            0.0,
            0.7959183673469388,
            0.7755102040816326,
            0.8571428571428571,
            0.9387755102040817,
            0.9183673469387755,
            1.0,
            1.0,
            0.9795918367346939,
            1.0
        ],
        "reward_history": [
            -89,
            38,
            -83,
            21,
            -24,
            -51,
            64,
            -14,
            0,
            -62
        ],
        "steps_history": [
            190,
            63,
            184,
            80,
            125,
            152,
            37,
            115,
            101,
            163
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "90/810",
        "save_path": "experiments/20250131_160708/training_plots/size_3/lr0.4_df0.99_eps0.1_trial4"
    }
]