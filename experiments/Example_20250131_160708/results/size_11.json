[
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 14.697649002075195,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 72,
        "policy_stability_history": [
            0.0,
            0.8431001890359168,
            0.7863894139886578,
            0.8166351606805293,
            0.8601134215500945,
            0.8506616257088847,
            0.8544423440453687,
            0.8827977315689981,
            0.8620037807183365,
            0.8563327032136105,
            0.8657844990548205,
            0.9206049149338374,
            0.9130434782608695,
            0.8638941398865785,
            0.8676748582230623,
            0.8960302457466919,
            0.9262759924385633,
            0.9281663516068053,
            0.8620037807183365,
            0.8998109640831758,
            0.9584120982986768,
            0.9224952741020794,
            0.8960302457466919,
            0.8563327032136105,
            0.9224952741020794,
            0.8582230623818525,
            0.8922495274102079,
            0.8733459357277883,
            0.8487712665406427,
            0.9338374291115312,
            0.8638941398865785,
            0.9262759924385633,
            0.9054820415879017,
            0.9017013232514177,
            0.9376181474480151,
            0.9395085066162571,
            0.9603024574669187,
            0.9054820415879017,
            0.9395085066162571,
            0.945179584120983,
            0.943289224952741,
            0.9489603024574669,
            0.9905482041587902,
            0.9243856332703214,
            0.9697542533081286,
            0.9073724007561437,
            0.9017013232514177,
            0.9168241965973535,
            0.9546313799621928,
            0.945179584120983,
            0.9716446124763705,
            0.9017013232514177,
            0.9319470699432892,
            0.9697542533081286,
            0.994328922495274,
            0.9621928166351607,
            0.9905482041587902,
            0.9905482041587902,
            0.943289224952741,
            0.9886578449905482,
            0.9243856332703214,
            0.994328922495274,
            0.994328922495274,
            0.9810964083175804,
            0.9489603024574669,
            0.9886578449905482,
            0.9716446124763705,
            0.9527410207939508,
            0.998109640831758,
            0.9508506616257089,
            0.998109640831758,
            1.0,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -4477,
            -5290,
            -3527,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2577,
            -5290,
            -5290,
            -3054,
            -5290,
            -1573,
            -3783,
            -5290,
            -783,
            -1851,
            -5290,
            -5290,
            -2942,
            -5290,
            -4225,
            -5012,
            -5290,
            -1767,
            -5290,
            -1934,
            -5290,
            -4626,
            -1737,
            -1998,
            -822,
            -5069,
            -1722,
            -1558,
            -1665,
            -3323,
            -1009,
            -4318,
            -1160,
            -5290,
            -5290,
            -5290,
            -3344,
            -3756,
            -1704,
            -5290,
            -5290,
            -1349,
            -646,
            -1877,
            -942,
            -1056,
            -4946,
            -1250,
            -4611,
            -1217,
            -501,
            -1040,
            -2342,
            -1193,
            -2064,
            -3845,
            -1023,
            -3103,
            -864,
            -530,
            -691
        ],
        "steps_history": [
            5290,
            5290,
            4578,
            5290,
            3628,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            2678,
            5290,
            5290,
            3155,
            5290,
            1674,
            3884,
            5290,
            884,
            1952,
            5290,
            5290,
            3043,
            5290,
            4326,
            5113,
            5290,
            1868,
            5290,
            2035,
            5290,
            4727,
            1838,
            2099,
            923,
            5170,
            1823,
            1659,
            1766,
            3424,
            1110,
            4419,
            1261,
            5290,
            5290,
            5290,
            3445,
            3857,
            1805,
            5290,
            5290,
            1450,
            747,
            1978,
            1043,
            1157,
            5047,
            1351,
            4712,
            1318,
            602,
            1141,
            2443,
            1294,
            2165,
            3946,
            1124,
            3204,
            965,
            631,
            792
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "721/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 15.75753378868103,
        "final_policy_stability": 0.9905482041587902,
        "episodes_to_convergence": 77,
        "policy_stability_history": [
            0.0,
            0.8960302457466919,
            0.8431001890359168,
            0.8298676748582231,
            0.9130434782608695,
            0.8525519848771267,
            0.8620037807183365,
            0.9111531190926276,
            0.9017013232514177,
            0.9017013232514177,
            0.9054820415879017,
            0.8752362948960303,
            0.9111531190926276,
            0.9243856332703214,
            0.9262759924385633,
            0.8979206049149339,
            0.8620037807183365,
            0.8903591682419659,
            0.9035916824196597,
            0.8827977315689981,
            0.9168241965973535,
            0.9035916824196597,
            0.9073724007561437,
            0.8827977315689981,
            0.8393194706994329,
            0.9035916824196597,
            0.888468809073724,
            0.9527410207939508,
            0.945179584120983,
            0.9395085066162571,
            0.8846880907372401,
            0.9754253308128544,
            0.9262759924385633,
            0.9017013232514177,
            0.8979206049149339,
            0.8941398865784499,
            0.9905482041587902,
            0.9716446124763705,
            0.9489603024574669,
            0.9527410207939508,
            0.9659735349716446,
            0.9111531190926276,
            0.9565217391304348,
            0.9130434782608695,
            0.9168241965973535,
            0.9395085066162571,
            0.9300567107750473,
            0.9603024574669187,
            0.9243856332703214,
            0.9773156899810964,
            0.9810964083175804,
            0.9829867674858223,
            0.9848771266540642,
            0.8752362948960303,
            0.9376181474480151,
            0.9130434782608695,
            0.9924385633270322,
            0.9206049149338374,
            0.9735349716446124,
            0.9281663516068053,
            0.8865784499054821,
            0.996219281663516,
            0.9867674858223062,
            0.943289224952741,
            0.9773156899810964,
            0.9338374291115312,
            0.9565217391304348,
            0.996219281663516,
            0.9905482041587902,
            0.994328922495274,
            0.9640831758034026,
            0.9621928166351607,
            0.9848771266540642,
            0.998109640831758,
            1.0,
            0.9829867674858223,
            0.994328922495274,
            0.9905482041587902
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -4513,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -3794,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4104,
            -4853,
            -3711,
            -5290,
            -5290,
            -3651,
            -3132,
            -5290,
            -3959,
            -5290,
            -2557,
            -5290,
            -1533,
            -2051,
            -2170,
            -5290,
            -1018,
            -2890,
            -5290,
            -5290,
            -5290,
            -564,
            -894,
            -2981,
            -1634,
            -791,
            -5290,
            -1615,
            -5290,
            -3251,
            -2958,
            -3297,
            -3836,
            -5290,
            -1161,
            -897,
            -1287,
            -953,
            -5290,
            -3545,
            -4104,
            -1055,
            -3641,
            -1541,
            -5290,
            -5290,
            -501,
            -1900,
            -2832,
            -2207,
            -5290,
            -2695,
            -906,
            -1237,
            -2044,
            -2586,
            -1833,
            -1349,
            -765,
            -953,
            -1660,
            -1137,
            -1768
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            4614,
            5290,
            5290,
            5290,
            5290,
            5290,
            3895,
            5290,
            5290,
            5290,
            5290,
            5290,
            4205,
            4954,
            3812,
            5290,
            5290,
            3752,
            3233,
            5290,
            4060,
            5290,
            2658,
            5290,
            1634,
            2152,
            2271,
            5290,
            1119,
            2991,
            5290,
            5290,
            5290,
            665,
            995,
            3082,
            1735,
            892,
            5290,
            1716,
            5290,
            3352,
            3059,
            3398,
            3937,
            5290,
            1262,
            998,
            1388,
            1054,
            5290,
            3646,
            4205,
            1156,
            3742,
            1642,
            5290,
            5290,
            602,
            2001,
            2933,
            2308,
            5290,
            2796,
            1007,
            1338,
            2145,
            2687,
            1934,
            1450,
            866,
            1054,
            1761,
            1238,
            1869
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "722/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 15.622122764587402,
        "final_policy_stability": 0.947069943289225,
        "episodes_to_convergence": 77,
        "policy_stability_history": [
            0.0,
            0.8487712665406427,
            0.8166351606805293,
            0.9168241965973535,
            0.9111531190926276,
            0.8846880907372401,
            0.9149338374291115,
            0.9073724007561437,
            0.8960302457466919,
            0.8733459357277883,
            0.9262759924385633,
            0.8979206049149339,
            0.9357277882797732,
            0.9357277882797732,
            0.8601134215500945,
            0.8865784499054821,
            0.9546313799621928,
            0.8846880907372401,
            0.8752362948960303,
            0.8298676748582231,
            0.8563327032136105,
            0.8752362948960303,
            0.8998109640831758,
            0.8771266540642723,
            0.8393194706994329,
            0.8582230623818525,
            0.8790170132325141,
            0.8393194706994329,
            0.9243856332703214,
            0.9716446124763705,
            0.9659735349716446,
            0.9035916824196597,
            0.9168241965973535,
            0.9527410207939508,
            0.9395085066162571,
            0.9130434782608695,
            0.9376181474480151,
            0.9206049149338374,
            0.9754253308128544,
            0.9810964083175804,
            0.9168241965973535,
            0.941398865784499,
            0.9224952741020794,
            0.9149338374291115,
            0.945179584120983,
            0.9319470699432892,
            0.9640831758034026,
            0.8941398865784499,
            0.9640831758034026,
            0.9792060491493384,
            0.8865784499054821,
            0.9224952741020794,
            0.9092627599243857,
            0.9886578449905482,
            0.9111531190926276,
            0.9754253308128544,
            0.9905482041587902,
            0.9508506616257089,
            0.9792060491493384,
            0.9810964083175804,
            0.9773156899810964,
            0.9905482041587902,
            0.9848771266540642,
            0.9924385633270322,
            0.9224952741020794,
            0.9659735349716446,
            0.998109640831758,
            0.9792060491493384,
            0.9716446124763705,
            0.9810964083175804,
            0.9924385633270322,
            0.998109640831758,
            0.9924385633270322,
            0.9187145557655955,
            0.9603024574669187,
            0.998109640831758,
            1.0,
            0.947069943289225
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4256,
            -1271,
            -2251,
            -5290,
            -1873,
            -5290,
            -2475,
            -525,
            -5290,
            -5290,
            -4648,
            -5290,
            -5290,
            -3016,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -1404,
            -595,
            -3454,
            -4373,
            -770,
            -1497,
            -4263,
            -1858,
            -5290,
            -1230,
            -1250,
            -4255,
            -2976,
            -5290,
            -5290,
            -2756,
            -3099,
            -1435,
            -5290,
            -1251,
            -1166,
            -5290,
            -5179,
            -5290,
            -318,
            -5290,
            -1083,
            -1209,
            -2811,
            -1349,
            -1117,
            -1074,
            -995,
            -1013,
            -651,
            -3190,
            -1692,
            -719,
            -1465,
            -2396,
            -1352,
            -1204,
            -1074,
            -890,
            -4586,
            -3564,
            -998,
            -675,
            -4126
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            4357,
            1372,
            2352,
            5290,
            1974,
            5290,
            2576,
            626,
            5290,
            5290,
            4749,
            5290,
            5290,
            3117,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            1505,
            696,
            3555,
            4474,
            871,
            1598,
            4364,
            1959,
            5290,
            1331,
            1351,
            4356,
            3077,
            5290,
            5290,
            2857,
            3200,
            1536,
            5290,
            1352,
            1267,
            5290,
            5280,
            5290,
            419,
            5290,
            1184,
            1310,
            2912,
            1450,
            1218,
            1175,
            1096,
            1114,
            752,
            3291,
            1793,
            820,
            1566,
            2497,
            1453,
            1305,
            1175,
            991,
            4687,
            3665,
            1099,
            776,
            4227
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "723/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 14.953488826751709,
        "final_policy_stability": 0.9716446124763705,
        "episodes_to_convergence": 71,
        "policy_stability_history": [
            0.0,
            0.8279773156899811,
            0.8865784499054821,
            0.8771266540642723,
            0.8752362948960303,
            0.8638941398865785,
            0.8827977315689981,
            0.888468809073724,
            0.8998109640831758,
            0.8733459357277883,
            0.9111531190926276,
            0.8563327032136105,
            0.8771266540642723,
            0.9206049149338374,
            0.8979206049149339,
            0.888468809073724,
            0.8941398865784499,
            0.8922495274102079,
            0.8790170132325141,
            0.8809073724007561,
            0.943289224952741,
            0.943289224952741,
            0.888468809073724,
            0.9206049149338374,
            0.8979206049149339,
            0.9243856332703214,
            0.8998109640831758,
            0.8790170132325141,
            0.9735349716446124,
            0.8601134215500945,
            0.888468809073724,
            0.9395085066162571,
            0.8809073724007561,
            0.9224952741020794,
            0.9319470699432892,
            0.9224952741020794,
            0.9035916824196597,
            0.9338374291115312,
            0.8733459357277883,
            0.9092627599243857,
            0.888468809073724,
            0.9243856332703214,
            0.9168241965973535,
            0.9697542533081286,
            0.9206049149338374,
            0.9697542533081286,
            0.9508506616257089,
            0.9035916824196597,
            0.9565217391304348,
            0.8922495274102079,
            0.9754253308128544,
            0.8903591682419659,
            0.8941398865784499,
            0.9867674858223062,
            0.9584120982986768,
            0.9640831758034026,
            0.9262759924385633,
            0.9281663516068053,
            0.9754253308128544,
            0.9754253308128544,
            0.9621928166351607,
            0.9395085066162571,
            0.9697542533081286,
            0.9489603024574669,
            0.9773156899810964,
            0.994328922495274,
            0.9546313799621928,
            0.9924385633270322,
            0.9792060491493384,
            0.9603024574669187,
            0.996219281663516,
            0.9716446124763705
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -3338,
            -5290,
            -5290,
            -5031,
            -2904,
            -5290,
            -5290,
            -5290,
            -2544,
            -5290,
            -2608,
            -5290,
            -3435,
            -4566,
            -762,
            -1291,
            -5290,
            -2839,
            -5290,
            -2777,
            -5290,
            -5290,
            -540,
            -5290,
            -5290,
            -1280,
            -4645,
            -5290,
            -2639,
            -2168,
            -3340,
            -1962,
            -5290,
            -5290,
            -4910,
            -3836,
            -3463,
            -1443,
            -5290,
            -849,
            -2290,
            -4385,
            -1990,
            -4394,
            -1166,
            -5290,
            -5290,
            -501,
            -2312,
            -2030,
            -2602,
            -5290,
            -1128,
            -1301,
            -1623,
            -3493,
            -1796,
            -2545,
            -1787,
            -745,
            -4375,
            -1317,
            -1712,
            -2221,
            -903,
            -1615
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            3439,
            5290,
            5290,
            5132,
            3005,
            5290,
            5290,
            5290,
            2645,
            5290,
            2709,
            5290,
            3536,
            4667,
            863,
            1392,
            5290,
            2940,
            5290,
            2878,
            5290,
            5290,
            641,
            5290,
            5290,
            1381,
            4746,
            5290,
            2740,
            2269,
            3441,
            2063,
            5290,
            5290,
            5011,
            3937,
            3564,
            1544,
            5290,
            950,
            2391,
            4486,
            2091,
            4495,
            1267,
            5290,
            5290,
            602,
            2413,
            2131,
            2703,
            5290,
            1229,
            1402,
            1724,
            3594,
            1897,
            2646,
            1888,
            846,
            4476,
            1418,
            1813,
            2322,
            1004,
            1716
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "724/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 16.73770046234131,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 82,
        "policy_stability_history": [
            0.0,
            0.8223062381852552,
            0.9281663516068053,
            0.8809073724007561,
            0.8601134215500945,
            0.8657844990548205,
            0.8563327032136105,
            0.8431001890359168,
            0.8809073724007561,
            0.8714555765595463,
            0.8771266540642723,
            0.8846880907372401,
            0.8752362948960303,
            0.8733459357277883,
            0.8960302457466919,
            0.8998109640831758,
            0.8752362948960303,
            0.9187145557655955,
            0.8601134215500945,
            0.9092627599243857,
            0.9357277882797732,
            0.9243856332703214,
            0.9168241965973535,
            0.9243856332703214,
            0.9395085066162571,
            0.8752362948960303,
            0.8582230623818525,
            0.9243856332703214,
            0.8998109640831758,
            0.9697542533081286,
            0.8790170132325141,
            0.9338374291115312,
            0.9357277882797732,
            0.8865784499054821,
            0.9224952741020794,
            0.9754253308128544,
            0.9300567107750473,
            0.945179584120983,
            0.9300567107750473,
            0.9281663516068053,
            0.9149338374291115,
            0.9508506616257089,
            0.9206049149338374,
            0.941398865784499,
            0.9508506616257089,
            0.9035916824196597,
            0.9111531190926276,
            0.8771266540642723,
            0.943289224952741,
            0.9546313799621928,
            0.9035916824196597,
            0.8714555765595463,
            0.9659735349716446,
            0.9017013232514177,
            0.9300567107750473,
            0.9584120982986768,
            0.9810964083175804,
            0.947069943289225,
            0.9527410207939508,
            0.9735349716446124,
            0.9224952741020794,
            0.9395085066162571,
            0.9243856332703214,
            0.9829867674858223,
            0.9508506616257089,
            0.996219281663516,
            0.9678638941398866,
            0.9810964083175804,
            0.9338374291115312,
            0.9867674858223062,
            0.9640831758034026,
            0.994328922495274,
            0.9716446124763705,
            0.9584120982986768,
            0.9754253308128544,
            0.9489603024574669,
            0.9659735349716446,
            0.994328922495274,
            0.9924385633270322,
            0.9697542533081286,
            0.9508506616257089,
            0.9773156899810964,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4908,
            -4784,
            -5290,
            -4082,
            -5290,
            -5290,
            -5290,
            -5290,
            -3378,
            -5290,
            -5290,
            -5290,
            -3508,
            -5290,
            -3881,
            -1607,
            -2465,
            -5290,
            -2791,
            -5290,
            -5290,
            -2350,
            -5290,
            -781,
            -3840,
            -2729,
            -1780,
            -5290,
            -4737,
            -1289,
            -3422,
            -2977,
            -2542,
            -2954,
            -5290,
            -2059,
            -2320,
            -2645,
            -2973,
            -3195,
            -5290,
            -5290,
            -2267,
            -1464,
            -5113,
            -5290,
            -2008,
            -3813,
            -3978,
            -2147,
            -916,
            -2412,
            -4408,
            -1455,
            -3090,
            -4098,
            -5290,
            -1265,
            -2390,
            -567,
            -1047,
            -1006,
            -3463,
            -1095,
            -3712,
            -965,
            -2050,
            -4320,
            -2573,
            -2413,
            -3530,
            -1390,
            -1420,
            -2034,
            -2085,
            -3308,
            -2672
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5009,
            4885,
            5290,
            4183,
            5290,
            5290,
            5290,
            5290,
            3479,
            5290,
            5290,
            5290,
            3609,
            5290,
            3982,
            1708,
            2566,
            5290,
            2892,
            5290,
            5290,
            2451,
            5290,
            882,
            3941,
            2830,
            1881,
            5290,
            4838,
            1390,
            3523,
            3078,
            2643,
            3055,
            5290,
            2160,
            2421,
            2746,
            3074,
            3296,
            5290,
            5290,
            2368,
            1565,
            5214,
            5290,
            2109,
            3914,
            4079,
            2248,
            1017,
            2513,
            4509,
            1556,
            3191,
            4199,
            5290,
            1366,
            2491,
            668,
            1148,
            1107,
            3564,
            1196,
            3813,
            1066,
            2151,
            4421,
            2674,
            2514,
            3631,
            1491,
            1521,
            2135,
            2186,
            3409,
            2773
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "725/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 16.329781532287598,
        "final_policy_stability": 0.9886578449905482,
        "episodes_to_convergence": 83,
        "policy_stability_history": [
            0.0,
            0.8506616257088847,
            0.8601134215500945,
            0.8714555765595463,
            0.8922495274102079,
            0.8601134215500945,
            0.8733459357277883,
            0.9319470699432892,
            0.8809073724007561,
            0.8714555765595463,
            0.8771266540642723,
            0.8827977315689981,
            0.8449905482041588,
            0.8638941398865785,
            0.8544423440453687,
            0.9111531190926276,
            0.9092627599243857,
            0.9054820415879017,
            0.9111531190926276,
            0.9111531190926276,
            0.8903591682419659,
            0.9130434782608695,
            0.9546313799621928,
            0.9357277882797732,
            0.8431001890359168,
            0.8903591682419659,
            0.8827977315689981,
            0.9017013232514177,
            0.8941398865784499,
            0.8846880907372401,
            0.9546313799621928,
            0.8979206049149339,
            0.8809073724007561,
            0.8657844990548205,
            0.941398865784499,
            0.9489603024574669,
            0.8714555765595463,
            0.9319470699432892,
            0.9546313799621928,
            0.9243856332703214,
            0.9054820415879017,
            0.9111531190926276,
            0.9130434782608695,
            0.9697542533081286,
            0.888468809073724,
            0.9376181474480151,
            0.9338374291115312,
            0.8790170132325141,
            0.9735349716446124,
            0.9395085066162571,
            0.9640831758034026,
            0.9357277882797732,
            0.9565217391304348,
            0.9773156899810964,
            0.9546313799621928,
            0.8809073724007561,
            0.9035916824196597,
            0.9111531190926276,
            0.9262759924385633,
            0.9848771266540642,
            0.9867674858223062,
            0.9810964083175804,
            0.9149338374291115,
            0.9754253308128544,
            0.9905482041587902,
            0.9867674858223062,
            0.9867674858223062,
            0.9886578449905482,
            0.9565217391304348,
            0.994328922495274,
            0.9829867674858223,
            0.9716446124763705,
            0.9905482041587902,
            0.9659735349716446,
            0.9659735349716446,
            0.947069943289225,
            0.994328922495274,
            0.996219281663516,
            0.9716446124763705,
            0.994328922495274,
            0.9754253308128544,
            0.998109640831758,
            0.9867674858223062,
            0.9886578449905482
        ],
        "reward_history": [
            -3025,
            -5290,
            -5290,
            -5290,
            -5290,
            -4922,
            -4154,
            -5290,
            -5290,
            -5290,
            -5290,
            -3090,
            -5290,
            -4256,
            -5290,
            -2173,
            -3258,
            -2346,
            -5290,
            -2236,
            -3504,
            -1795,
            -970,
            -1295,
            -5290,
            -3217,
            -5290,
            -4985,
            -3345,
            -5290,
            -526,
            -5006,
            -2576,
            -5290,
            -1822,
            -1490,
            -3934,
            -2677,
            -1013,
            -2493,
            -3143,
            -5290,
            -3955,
            -666,
            -5290,
            -1440,
            -2335,
            -5290,
            -627,
            -2588,
            -969,
            -3376,
            -1742,
            -973,
            -995,
            -5290,
            -5290,
            -5184,
            -1985,
            -941,
            -597,
            -541,
            -3076,
            -1508,
            -992,
            -1078,
            -1101,
            -1013,
            -2191,
            -669,
            -791,
            -1563,
            -1193,
            -1581,
            -2650,
            -2637,
            -1001,
            -756,
            -1832,
            -611,
            -1358,
            -627,
            -1049,
            -599
        ],
        "steps_history": [
            3126,
            5290,
            5290,
            5290,
            5290,
            5023,
            4255,
            5290,
            5290,
            5290,
            5290,
            3191,
            5290,
            4357,
            5290,
            2274,
            3359,
            2447,
            5290,
            2337,
            3605,
            1896,
            1071,
            1396,
            5290,
            3318,
            5290,
            5086,
            3446,
            5290,
            627,
            5107,
            2677,
            5290,
            1923,
            1591,
            4035,
            2778,
            1114,
            2594,
            3244,
            5290,
            4056,
            767,
            5290,
            1541,
            2436,
            5290,
            728,
            2689,
            1070,
            3477,
            1843,
            1074,
            1096,
            5290,
            5290,
            5285,
            2086,
            1042,
            698,
            642,
            3177,
            1609,
            1093,
            1179,
            1202,
            1114,
            2292,
            770,
            892,
            1664,
            1294,
            1682,
            2751,
            2738,
            1102,
            857,
            1933,
            712,
            1459,
            728,
            1150,
            700
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "726/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 17.519094467163086,
        "final_policy_stability": 0.9754253308128544,
        "episodes_to_convergence": 86,
        "policy_stability_history": [
            0.0,
            0.9054820415879017,
            0.8393194706994329,
            0.833648393194707,
            0.8563327032136105,
            0.8865784499054821,
            0.8355387523629489,
            0.8695652173913043,
            0.8544423440453687,
            0.8998109640831758,
            0.8525519848771267,
            0.8071833648393195,
            0.8431001890359168,
            0.8846880907372401,
            0.8563327032136105,
            0.8752362948960303,
            0.8846880907372401,
            0.9092627599243857,
            0.943289224952741,
            0.8771266540642723,
            0.9187145557655955,
            0.8960302457466919,
            0.8809073724007561,
            0.9262759924385633,
            0.8998109640831758,
            0.8657844990548205,
            0.9149338374291115,
            0.945179584120983,
            0.9281663516068053,
            0.8941398865784499,
            0.947069943289225,
            0.9054820415879017,
            0.941398865784499,
            0.9111531190926276,
            0.9508506616257089,
            0.8960302457466919,
            0.9035916824196597,
            0.9621928166351607,
            0.9735349716446124,
            0.9300567107750473,
            0.831758034026465,
            0.9338374291115312,
            0.9754253308128544,
            0.9508506616257089,
            0.9867674858223062,
            0.9603024574669187,
            0.8846880907372401,
            0.9584120982986768,
            0.9243856332703214,
            0.9319470699432892,
            0.9168241965973535,
            0.9262759924385633,
            0.9792060491493384,
            0.9092627599243857,
            0.9829867674858223,
            0.9546313799621928,
            0.9262759924385633,
            0.888468809073724,
            0.9735349716446124,
            0.9603024574669187,
            0.9640831758034026,
            0.9546313799621928,
            0.9584120982986768,
            0.9848771266540642,
            0.9546313799621928,
            0.9810964083175804,
            0.9527410207939508,
            0.9224952741020794,
            0.9886578449905482,
            0.9924385633270322,
            0.941398865784499,
            0.9130434782608695,
            0.996219281663516,
            0.996219281663516,
            0.9773156899810964,
            0.9773156899810964,
            0.9792060491493384,
            0.994328922495274,
            0.9659735349716446,
            0.9678638941398866,
            0.9773156899810964,
            0.994328922495274,
            0.996219281663516,
            0.9735349716446124,
            0.996219281663516,
            0.9867674858223062,
            0.9754253308128544
        ],
        "reward_history": [
            -5290,
            -5290,
            -3468,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4274,
            -5290,
            -5290,
            -5290,
            -4532,
            -5290,
            -5078,
            -3702,
            -5290,
            -5290,
            -1025,
            -5290,
            -2215,
            -5290,
            -3921,
            -2243,
            -5290,
            -3688,
            -2042,
            -1846,
            -1809,
            -5290,
            -1064,
            -2793,
            -1348,
            -3269,
            -1531,
            -5290,
            -5290,
            -1164,
            -345,
            -1962,
            -5290,
            -2088,
            -506,
            -1513,
            -534,
            -1265,
            -5290,
            -1317,
            -1959,
            -2477,
            -1830,
            -2151,
            -719,
            -2382,
            -499,
            -1691,
            -2411,
            -5290,
            -943,
            -1589,
            -1787,
            -2513,
            -2345,
            -911,
            -2353,
            -496,
            -1956,
            -5290,
            -1273,
            -787,
            -2512,
            -3592,
            -807,
            -929,
            -1365,
            -1909,
            -1189,
            -613,
            -1890,
            -1552,
            -1472,
            -671,
            -680,
            -1708,
            -926,
            -816,
            -2009
        ],
        "steps_history": [
            5290,
            5290,
            3569,
            5290,
            5290,
            5290,
            5290,
            5290,
            4375,
            5290,
            5290,
            5290,
            4633,
            5290,
            5179,
            3803,
            5290,
            5290,
            1126,
            5290,
            2316,
            5290,
            4022,
            2344,
            5290,
            3789,
            2143,
            1947,
            1910,
            5290,
            1165,
            2894,
            1449,
            3370,
            1632,
            5290,
            5290,
            1265,
            446,
            2063,
            5290,
            2189,
            607,
            1614,
            635,
            1366,
            5290,
            1418,
            2060,
            2578,
            1931,
            2252,
            820,
            2483,
            600,
            1792,
            2512,
            5290,
            1044,
            1690,
            1888,
            2614,
            2446,
            1012,
            2454,
            597,
            2057,
            5290,
            1374,
            888,
            2613,
            3693,
            908,
            1030,
            1466,
            2010,
            1290,
            714,
            1991,
            1653,
            1573,
            772,
            781,
            1809,
            1027,
            917,
            2110
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "727/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 17.63010287284851,
        "final_policy_stability": 0.998109640831758,
        "episodes_to_convergence": 91,
        "policy_stability_history": [
            0.0,
            0.8695652173913043,
            0.8582230623818525,
            0.9319470699432892,
            0.8506616257088847,
            0.8374291115311909,
            0.8979206049149339,
            0.8355387523629489,
            0.8468809073724007,
            0.8979206049149339,
            0.8487712665406427,
            0.9054820415879017,
            0.8487712665406427,
            0.9073724007561437,
            0.8657844990548205,
            0.8506616257088847,
            0.9092627599243857,
            0.9319470699432892,
            0.8809073724007561,
            0.9319470699432892,
            0.8752362948960303,
            0.8903591682419659,
            0.9187145557655955,
            0.9035916824196597,
            0.8752362948960303,
            0.8582230623818525,
            0.9035916824196597,
            0.9300567107750473,
            0.8544423440453687,
            0.9319470699432892,
            0.9300567107750473,
            0.8733459357277883,
            0.947069943289225,
            0.9565217391304348,
            0.9111531190926276,
            0.9357277882797732,
            0.8638941398865785,
            0.9603024574669187,
            0.9716446124763705,
            0.9338374291115312,
            0.9603024574669187,
            0.9546313799621928,
            0.9338374291115312,
            0.945179584120983,
            0.9924385633270322,
            0.9111531190926276,
            0.945179584120983,
            0.9508506616257089,
            0.9130434782608695,
            0.9792060491493384,
            0.9527410207939508,
            0.9621928166351607,
            0.9603024574669187,
            0.9810964083175804,
            0.8809073724007561,
            0.9565217391304348,
            0.9848771266540642,
            0.9546313799621928,
            0.8903591682419659,
            0.9376181474480151,
            0.9262759924385633,
            0.9678638941398866,
            0.9603024574669187,
            0.9300567107750473,
            0.9754253308128544,
            0.996219281663516,
            0.9829867674858223,
            0.9697542533081286,
            0.9716446124763705,
            0.9527410207939508,
            0.9716446124763705,
            0.996219281663516,
            0.9792060491493384,
            0.947069943289225,
            0.998109640831758,
            0.9659735349716446,
            0.994328922495274,
            0.9508506616257089,
            0.9678638941398866,
            0.9810964083175804,
            0.996219281663516,
            0.998109640831758,
            0.998109640831758,
            0.998109640831758,
            1.0,
            0.998109640831758,
            0.9924385633270322,
            0.9678638941398866,
            0.996219281663516,
            0.994328922495274,
            0.9735349716446124,
            0.998109640831758
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -4868,
            -5290,
            -5290,
            -5290,
            -4137,
            -5290,
            -5290,
            -5290,
            -3312,
            -2508,
            -4219,
            -5165,
            -5290,
            -1690,
            -5290,
            -1956,
            -5290,
            -5290,
            -5290,
            -2263,
            -5290,
            -4897,
            -2118,
            -1995,
            -5105,
            -1148,
            -1512,
            -5290,
            -1620,
            -1436,
            -2952,
            -3303,
            -5290,
            -1086,
            -862,
            -1951,
            -1569,
            -2120,
            -1558,
            -1541,
            -539,
            -5290,
            -2908,
            -1527,
            -4916,
            -897,
            -2049,
            -811,
            -977,
            -821,
            -5290,
            -1831,
            -576,
            -1687,
            -4682,
            -2075,
            -4361,
            -1689,
            -2251,
            -3506,
            -785,
            -685,
            -686,
            -1520,
            -1498,
            -2484,
            -1390,
            -1608,
            -1607,
            -3079,
            -847,
            -2575,
            -906,
            -3650,
            -1864,
            -1594,
            -1571,
            -812,
            -546,
            -630,
            -953,
            -692,
            -604,
            -2013,
            -940,
            -785,
            -1598,
            -473
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            4969,
            5290,
            5290,
            5290,
            4238,
            5290,
            5290,
            5290,
            3413,
            2609,
            4320,
            5266,
            5290,
            1791,
            5290,
            2057,
            5290,
            5290,
            5290,
            2364,
            5290,
            4998,
            2219,
            2096,
            5206,
            1249,
            1613,
            5290,
            1721,
            1537,
            3053,
            3404,
            5290,
            1187,
            963,
            2052,
            1670,
            2221,
            1659,
            1642,
            640,
            5290,
            3009,
            1628,
            5017,
            998,
            2150,
            912,
            1078,
            922,
            5290,
            1932,
            677,
            1788,
            4783,
            2176,
            4462,
            1790,
            2352,
            3607,
            886,
            786,
            787,
            1621,
            1599,
            2585,
            1491,
            1709,
            1708,
            3180,
            948,
            2676,
            1007,
            3751,
            1965,
            1695,
            1672,
            913,
            647,
            731,
            1054,
            793,
            705,
            2114,
            1041,
            886,
            1699,
            574
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "728/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 17.238247394561768,
        "final_policy_stability": 0.9905482041587902,
        "episodes_to_convergence": 90,
        "policy_stability_history": [
            0.0,
            0.8449905482041588,
            0.8960302457466919,
            0.8015122873345936,
            0.8827977315689981,
            0.8790170132325141,
            0.8771266540642723,
            0.8638941398865785,
            0.8960302457466919,
            0.9111531190926276,
            0.8620037807183365,
            0.8241965973534972,
            0.8752362948960303,
            0.8601134215500945,
            0.8563327032136105,
            0.8733459357277883,
            0.8449905482041588,
            0.8979206049149339,
            0.8771266540642723,
            0.8638941398865785,
            0.8922495274102079,
            0.9035916824196597,
            0.9527410207939508,
            0.945179584120983,
            0.8620037807183365,
            0.8865784499054821,
            0.9697542533081286,
            0.8979206049149339,
            0.8676748582230623,
            0.9376181474480151,
            0.9092627599243857,
            0.943289224952741,
            0.8393194706994329,
            0.8827977315689981,
            0.9187145557655955,
            0.9659735349716446,
            0.9376181474480151,
            0.9395085066162571,
            0.8922495274102079,
            0.8865784499054821,
            0.9111531190926276,
            0.8941398865784499,
            0.9130434782608695,
            0.9395085066162571,
            0.9527410207939508,
            0.9640831758034026,
            0.9338374291115312,
            0.9395085066162571,
            0.9546313799621928,
            0.9603024574669187,
            0.9848771266540642,
            0.9848771266540642,
            0.9678638941398866,
            0.9792060491493384,
            0.9678638941398866,
            0.9338374291115312,
            0.8752362948960303,
            0.9754253308128544,
            0.9867674858223062,
            0.8998109640831758,
            0.9546313799621928,
            0.9584120982986768,
            0.9792060491493384,
            0.9527410207939508,
            0.9867674858223062,
            0.9886578449905482,
            0.9848771266540642,
            0.943289224952741,
            0.9754253308128544,
            0.9886578449905482,
            0.9886578449905482,
            0.9584120982986768,
            0.9300567107750473,
            0.9773156899810964,
            0.9886578449905482,
            0.994328922495274,
            0.9735349716446124,
            0.998109640831758,
            0.9905482041587902,
            0.996219281663516,
            0.9319470699432892,
            0.9829867674858223,
            0.996219281663516,
            0.998109640831758,
            0.998109640831758,
            0.9716446124763705,
            0.9924385633270322,
            1.0,
            1.0,
            0.998109640831758,
            0.9905482041587902
        ],
        "reward_history": [
            -2950,
            -5290,
            -5290,
            -5290,
            -5290,
            -2810,
            -5290,
            -5290,
            -5290,
            -2328,
            -5290,
            -5290,
            -5290,
            -5290,
            -3958,
            -5290,
            -4677,
            -2591,
            -5290,
            -3706,
            -3610,
            -2792,
            -778,
            -1866,
            -5290,
            -4643,
            -734,
            -5290,
            -5181,
            -1787,
            -2735,
            -1223,
            -5290,
            -3309,
            -2174,
            -642,
            -1846,
            -1792,
            -3545,
            -5290,
            -1782,
            -5290,
            -2572,
            -2397,
            -1474,
            -3249,
            -1793,
            -2858,
            -1156,
            -1404,
            -758,
            -759,
            -1560,
            -1280,
            -1555,
            -5290,
            -5290,
            -1452,
            -538,
            -4302,
            -1571,
            -1318,
            -580,
            -1400,
            -625,
            -797,
            -748,
            -5036,
            -1197,
            -1095,
            -1160,
            -2861,
            -4749,
            -835,
            -1608,
            -1038,
            -1840,
            -888,
            -1543,
            -846,
            -3683,
            -1312,
            -671,
            -882,
            -876,
            -1341,
            -765,
            -1022,
            -1528,
            -839,
            -1141
        ],
        "steps_history": [
            3051,
            5290,
            5290,
            5290,
            5290,
            2911,
            5290,
            5290,
            5290,
            2429,
            5290,
            5290,
            5290,
            5290,
            4059,
            5290,
            4778,
            2692,
            5290,
            3807,
            3711,
            2893,
            879,
            1967,
            5290,
            4744,
            835,
            5290,
            5282,
            1888,
            2836,
            1324,
            5290,
            3410,
            2275,
            743,
            1947,
            1893,
            3646,
            5290,
            1883,
            5290,
            2673,
            2498,
            1575,
            3350,
            1894,
            2959,
            1257,
            1505,
            859,
            860,
            1661,
            1381,
            1656,
            5290,
            5290,
            1553,
            639,
            4403,
            1672,
            1419,
            681,
            1501,
            726,
            898,
            849,
            5137,
            1298,
            1196,
            1261,
            2962,
            4850,
            936,
            1709,
            1139,
            1941,
            989,
            1644,
            947,
            3784,
            1413,
            772,
            983,
            977,
            1442,
            866,
            1123,
            1629,
            940,
            1242
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "729/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 19.716187477111816,
        "final_policy_stability": 0.996219281663516,
        "episodes_to_convergence": 96,
        "policy_stability_history": [
            0.0,
            0.8298676748582231,
            0.8601134215500945,
            0.8941398865784499,
            0.8714555765595463,
            0.831758034026465,
            0.8487712665406427,
            0.8431001890359168,
            0.9073724007561437,
            0.8846880907372401,
            0.9130434782608695,
            0.8676748582230623,
            0.8487712665406427,
            0.8355387523629489,
            0.9224952741020794,
            0.9168241965973535,
            0.8657844990548205,
            0.8374291115311909,
            0.8676748582230623,
            0.8903591682419659,
            0.9243856332703214,
            0.888468809073724,
            0.8752362948960303,
            0.9187145557655955,
            0.9357277882797732,
            0.8771266540642723,
            0.8506616257088847,
            0.9224952741020794,
            0.8979206049149339,
            0.8582230623818525,
            0.8846880907372401,
            0.9168241965973535,
            0.9867674858223062,
            0.9168241965973535,
            0.9565217391304348,
            0.9584120982986768,
            0.9338374291115312,
            0.8582230623818525,
            0.9565217391304348,
            0.9810964083175804,
            0.9659735349716446,
            0.9659735349716446,
            0.9073724007561437,
            0.9130434782608695,
            0.9395085066162571,
            0.9716446124763705,
            0.9716446124763705,
            0.8676748582230623,
            0.9792060491493384,
            0.9073724007561437,
            0.9848771266540642,
            0.941398865784499,
            0.9735349716446124,
            0.9546313799621928,
            0.9111531190926276,
            0.9848771266540642,
            0.9678638941398866,
            0.9376181474480151,
            0.9754253308128544,
            0.9395085066162571,
            0.9792060491493384,
            0.9867674858223062,
            0.9603024574669187,
            0.9035916824196597,
            0.8998109640831758,
            0.9187145557655955,
            0.9678638941398866,
            0.9848771266540642,
            0.9905482041587902,
            0.943289224952741,
            0.9886578449905482,
            0.9754253308128544,
            0.9867674858223062,
            0.998109640831758,
            0.9716446124763705,
            0.9187145557655955,
            0.994328922495274,
            0.9792060491493384,
            0.9697542533081286,
            0.9621928166351607,
            0.9867674858223062,
            0.9867674858223062,
            0.9848771266540642,
            0.9905482041587902,
            0.9792060491493384,
            0.9697542533081286,
            0.9697542533081286,
            0.998109640831758,
            0.9924385633270322,
            0.9924385633270322,
            0.9659735349716446,
            0.998109640831758,
            0.998109640831758,
            0.998109640831758,
            0.9565217391304348,
            1.0,
            0.996219281663516
        ],
        "reward_history": [
            -4165,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4940,
            -1781,
            -5290,
            -5290,
            -4428,
            -5290,
            -5290,
            -5290,
            -2655,
            -5290,
            -4235,
            -5290,
            -3026,
            -1568,
            -3906,
            -3884,
            -1676,
            -5290,
            -4764,
            -5290,
            -2023,
            -3205,
            -5290,
            -4386,
            -4205,
            -652,
            -3163,
            -1410,
            -1462,
            -2306,
            -5290,
            -1241,
            -735,
            -1336,
            -1381,
            -4212,
            -2477,
            -1675,
            -1139,
            -1019,
            -5290,
            -607,
            -5290,
            -705,
            -2762,
            -1167,
            -2119,
            -3434,
            -718,
            -870,
            -1678,
            -717,
            -2289,
            -1028,
            -596,
            -1890,
            -3506,
            -5290,
            -3268,
            -1262,
            -849,
            -885,
            -1959,
            -1119,
            -989,
            -998,
            -770,
            -1950,
            -3954,
            -1292,
            -976,
            -1299,
            -2326,
            -1036,
            -1377,
            -1118,
            -1360,
            -1602,
            -2021,
            -1188,
            -472,
            -1025,
            -878,
            -2010,
            -886,
            -540,
            -609,
            -5290,
            -562,
            -1318
        ],
        "steps_history": [
            4266,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5041,
            1882,
            5290,
            5290,
            4529,
            5290,
            5290,
            5290,
            2756,
            5290,
            4336,
            5290,
            3127,
            1669,
            4007,
            3985,
            1777,
            5290,
            4865,
            5290,
            2124,
            3306,
            5290,
            4487,
            4306,
            753,
            3264,
            1511,
            1563,
            2407,
            5290,
            1342,
            836,
            1437,
            1482,
            4313,
            2578,
            1776,
            1240,
            1120,
            5290,
            708,
            5290,
            806,
            2863,
            1268,
            2220,
            3535,
            819,
            971,
            1779,
            818,
            2390,
            1129,
            697,
            1991,
            3607,
            5290,
            3369,
            1363,
            950,
            986,
            2060,
            1220,
            1090,
            1099,
            871,
            2051,
            4055,
            1393,
            1077,
            1400,
            2427,
            1137,
            1478,
            1219,
            1461,
            1703,
            2122,
            1289,
            573,
            1126,
            979,
            2111,
            987,
            641,
            710,
            5290,
            663,
            1419
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "730/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 18.69483494758606,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 99,
        "policy_stability_history": [
            0.0,
            0.8034026465028355,
            0.8147448015122873,
            0.8241965973534972,
            0.8903591682419659,
            0.8468809073724007,
            0.8563327032136105,
            0.9054820415879017,
            0.8809073724007561,
            0.8979206049149339,
            0.9149338374291115,
            0.9224952741020794,
            0.831758034026465,
            0.8676748582230623,
            0.8922495274102079,
            0.8620037807183365,
            0.9338374291115312,
            0.8846880907372401,
            0.8827977315689981,
            0.9262759924385633,
            0.8827977315689981,
            0.8865784499054821,
            0.9035916824196597,
            0.8865784499054821,
            0.8809073724007561,
            0.9017013232514177,
            0.9300567107750473,
            0.9111531190926276,
            0.9565217391304348,
            0.8846880907372401,
            0.9338374291115312,
            0.8393194706994329,
            0.9584120982986768,
            0.8827977315689981,
            0.9130434782608695,
            0.9111531190926276,
            0.9243856332703214,
            0.8449905482041588,
            0.8941398865784499,
            0.941398865784499,
            0.9792060491493384,
            0.9319470699432892,
            0.8846880907372401,
            0.9243856332703214,
            0.8166351606805293,
            0.947069943289225,
            0.8449905482041588,
            0.8865784499054821,
            0.9017013232514177,
            0.9659735349716446,
            0.8809073724007561,
            0.945179584120983,
            0.947069943289225,
            0.9338374291115312,
            0.9338374291115312,
            0.9054820415879017,
            0.9735349716446124,
            0.9792060491493384,
            0.9300567107750473,
            0.9754253308128544,
            0.8790170132325141,
            0.9224952741020794,
            0.9735349716446124,
            0.9054820415879017,
            0.9867674858223062,
            0.9262759924385633,
            0.9489603024574669,
            0.9621928166351607,
            0.996219281663516,
            0.9678638941398866,
            0.9792060491493384,
            0.9546313799621928,
            0.9754253308128544,
            0.9810964083175804,
            0.9754253308128544,
            0.9848771266540642,
            0.9640831758034026,
            0.9735349716446124,
            0.9829867674858223,
            0.9659735349716446,
            0.947069943289225,
            0.9924385633270322,
            0.941398865784499,
            0.9603024574669187,
            0.996219281663516,
            0.996219281663516,
            0.998109640831758,
            0.9792060491493384,
            1.0,
            0.9905482041587902,
            0.996219281663516,
            1.0,
            0.994328922495274,
            0.9659735349716446,
            0.9735349716446124,
            0.996219281663516,
            0.9829867674858223,
            0.994328922495274,
            0.998109640831758,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -4020,
            -5290,
            -5290,
            -5290,
            -2110,
            -2200,
            -5290,
            -5290,
            -3914,
            -3658,
            -2476,
            -5290,
            -2982,
            -979,
            -5290,
            -2823,
            -1377,
            -4112,
            -5290,
            -2569,
            -3738,
            -2396,
            -2224,
            -1725,
            -2088,
            -484,
            -4783,
            -1812,
            -5290,
            -933,
            -2003,
            -5290,
            -2024,
            -1654,
            -4237,
            -5290,
            -1495,
            -597,
            -2160,
            -1946,
            -2382,
            -5290,
            -1065,
            -5290,
            -3950,
            -3063,
            -816,
            -5290,
            -1240,
            -1284,
            -1664,
            -2158,
            -3380,
            -931,
            -712,
            -4926,
            -640,
            -3608,
            -3362,
            -693,
            -5290,
            -539,
            -1669,
            -1496,
            -1062,
            -550,
            -1333,
            -912,
            -1180,
            -988,
            -736,
            -801,
            -519,
            -1240,
            -1096,
            -726,
            -1059,
            -3977,
            -683,
            -2571,
            -2001,
            -363,
            -386,
            -247,
            -1326,
            -623,
            -689,
            -986,
            -614,
            -580,
            -1138,
            -1158,
            -503,
            -940,
            -820,
            -560,
            -332
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            4121,
            5290,
            5290,
            5290,
            2211,
            2301,
            5290,
            5290,
            4015,
            3759,
            2577,
            5290,
            3083,
            1080,
            5290,
            2924,
            1478,
            4213,
            5290,
            2670,
            3839,
            2497,
            2325,
            1826,
            2189,
            585,
            4884,
            1913,
            5290,
            1034,
            2104,
            5290,
            2125,
            1755,
            4338,
            5290,
            1596,
            698,
            2261,
            2047,
            2483,
            5290,
            1166,
            5290,
            4051,
            3164,
            917,
            5290,
            1341,
            1385,
            1765,
            2259,
            3481,
            1032,
            813,
            5027,
            741,
            3709,
            3463,
            794,
            5290,
            640,
            1770,
            1597,
            1163,
            651,
            1434,
            1013,
            1281,
            1089,
            837,
            902,
            620,
            1341,
            1197,
            827,
            1160,
            4078,
            784,
            2672,
            2102,
            464,
            487,
            348,
            1427,
            724,
            790,
            1087,
            715,
            681,
            1239,
            1259,
            604,
            1041,
            921,
            661,
            433
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "731/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 17.747105836868286,
        "final_policy_stability": 0.9867674858223062,
        "episodes_to_convergence": 92,
        "policy_stability_history": [
            0.0,
            0.8166351606805293,
            0.8714555765595463,
            0.9017013232514177,
            0.8733459357277883,
            0.8827977315689981,
            0.8601134215500945,
            0.8431001890359168,
            0.8582230623818525,
            0.9243856332703214,
            0.8752362948960303,
            0.9130434782608695,
            0.8695652173913043,
            0.8601134215500945,
            0.8846880907372401,
            0.9224952741020794,
            0.8204158790170132,
            0.8582230623818525,
            0.9017013232514177,
            0.8657844990548205,
            0.8676748582230623,
            0.9111531190926276,
            0.8695652173913043,
            0.8809073724007561,
            0.9281663516068053,
            0.945179584120983,
            0.8412098298676749,
            0.8922495274102079,
            0.943289224952741,
            0.945179584120983,
            0.9678638941398866,
            0.8695652173913043,
            0.9130434782608695,
            0.9508506616257089,
            0.8960302457466919,
            0.8714555765595463,
            0.941398865784499,
            0.9489603024574669,
            0.947069943289225,
            0.945179584120983,
            0.9073724007561437,
            0.9206049149338374,
            0.888468809073724,
            0.9584120982986768,
            0.8979206049149339,
            0.8676748582230623,
            0.9338374291115312,
            0.8922495274102079,
            0.9792060491493384,
            0.9565217391304348,
            0.9603024574669187,
            0.943289224952741,
            0.9376181474480151,
            0.9224952741020794,
            0.8809073724007561,
            0.947069943289225,
            0.9810964083175804,
            0.9848771266540642,
            0.9187145557655955,
            0.9224952741020794,
            0.9678638941398866,
            0.9035916824196597,
            0.9867674858223062,
            0.9829867674858223,
            0.9621928166351607,
            0.9792060491493384,
            0.9678638941398866,
            0.9565217391304348,
            0.9792060491493384,
            0.9716446124763705,
            0.8979206049149339,
            0.9092627599243857,
            0.9319470699432892,
            0.9716446124763705,
            0.9489603024574669,
            0.9697542533081286,
            0.9886578449905482,
            0.9867674858223062,
            0.9886578449905482,
            0.941398865784499,
            0.9546313799621928,
            0.9603024574669187,
            0.996219281663516,
            0.9735349716446124,
            0.9886578449905482,
            0.8922495274102079,
            0.9829867674858223,
            0.998109640831758,
            0.9697542533081286,
            1.0,
            0.9867674858223062,
            0.998109640831758,
            0.9867674858223062
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -780,
            -5290,
            -2277,
            -5290,
            -3814,
            -5290,
            -1252,
            -5290,
            -5290,
            -5290,
            -4394,
            -5290,
            -1471,
            -5290,
            -5290,
            -1774,
            -4181,
            -5089,
            -2178,
            -4634,
            -3664,
            -1811,
            -1149,
            -5290,
            -2612,
            -1538,
            -989,
            -595,
            -3744,
            -5290,
            -665,
            -5290,
            -5290,
            -714,
            -721,
            -1203,
            -912,
            -2141,
            -2467,
            -2888,
            -788,
            -4856,
            -5290,
            -1301,
            -3194,
            -618,
            -1050,
            -721,
            -1423,
            -1559,
            -2607,
            -3949,
            -1112,
            -496,
            -584,
            -4597,
            -2769,
            -1232,
            -2946,
            -644,
            -486,
            -1113,
            -470,
            -773,
            -879,
            -612,
            -759,
            -3968,
            -3162,
            -2960,
            -1442,
            -3445,
            -1002,
            -692,
            -458,
            -1077,
            -2058,
            -1799,
            -1509,
            -518,
            -863,
            -1220,
            -5290,
            -1130,
            -626,
            -1013,
            -619,
            -1270,
            -447,
            -794
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            881,
            5290,
            2378,
            5290,
            3915,
            5290,
            1353,
            5290,
            5290,
            5290,
            4495,
            5290,
            1572,
            5290,
            5290,
            1875,
            4282,
            5190,
            2279,
            4735,
            3765,
            1912,
            1250,
            5290,
            2713,
            1639,
            1090,
            696,
            3845,
            5290,
            766,
            5290,
            5290,
            815,
            822,
            1304,
            1013,
            2242,
            2568,
            2989,
            889,
            4957,
            5290,
            1402,
            3295,
            719,
            1151,
            822,
            1524,
            1660,
            2708,
            4050,
            1213,
            597,
            685,
            4698,
            2870,
            1333,
            3047,
            745,
            587,
            1214,
            571,
            874,
            980,
            713,
            860,
            4069,
            3263,
            3061,
            1543,
            3546,
            1103,
            793,
            559,
            1178,
            2159,
            1900,
            1610,
            619,
            964,
            1321,
            5290,
            1231,
            727,
            1114,
            720,
            1371,
            548,
            895
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "732/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 20.248215436935425,
        "final_policy_stability": 0.9678638941398866,
        "episodes_to_convergence": 104,
        "policy_stability_history": [
            0.0,
            0.8374291115311909,
            0.8298676748582231,
            0.8998109640831758,
            0.8393194706994329,
            0.8563327032136105,
            0.9206049149338374,
            0.8846880907372401,
            0.8809073724007561,
            0.8771266540642723,
            0.8960302457466919,
            0.8544423440453687,
            0.8790170132325141,
            0.8846880907372401,
            0.8714555765595463,
            0.8260869565217391,
            0.8412098298676749,
            0.9111531190926276,
            0.8393194706994329,
            0.8922495274102079,
            0.831758034026465,
            0.8638941398865785,
            0.9395085066162571,
            0.9300567107750473,
            0.9073724007561437,
            0.8809073724007561,
            0.8582230623818525,
            0.8241965973534972,
            0.9546313799621928,
            0.8941398865784499,
            0.9508506616257089,
            0.8638941398865785,
            0.8922495274102079,
            0.8903591682419659,
            0.8790170132325141,
            0.9206049149338374,
            0.9527410207939508,
            0.9659735349716446,
            0.9149338374291115,
            0.9489603024574669,
            0.9565217391304348,
            0.941398865784499,
            0.9168241965973535,
            0.9092627599243857,
            0.9716446124763705,
            0.9224952741020794,
            0.9281663516068053,
            0.9281663516068053,
            0.8922495274102079,
            0.9584120982986768,
            0.9130434782608695,
            0.945179584120983,
            0.9792060491493384,
            0.9357277882797732,
            0.9867674858223062,
            0.9697542533081286,
            0.945179584120983,
            0.9527410207939508,
            0.9206049149338374,
            0.9810964083175804,
            0.9187145557655955,
            0.941398865784499,
            0.9697542533081286,
            0.9773156899810964,
            0.941398865784499,
            0.9603024574669187,
            0.9792060491493384,
            0.9716446124763705,
            0.9886578449905482,
            0.8960302457466919,
            0.945179584120983,
            0.994328922495274,
            0.9867674858223062,
            0.9603024574669187,
            0.9810964083175804,
            0.998109640831758,
            0.9565217391304348,
            0.9206049149338374,
            0.9338374291115312,
            0.9659735349716446,
            0.9905482041587902,
            0.9716446124763705,
            0.9924385633270322,
            0.9810964083175804,
            0.9716446124763705,
            0.9867674858223062,
            0.9111531190926276,
            0.9678638941398866,
            0.9924385633270322,
            0.9924385633270322,
            1.0,
            0.998109640831758,
            0.9924385633270322,
            0.9867674858223062,
            0.9848771266540642,
            0.996219281663516,
            0.9848771266540642,
            0.998109640831758,
            0.9924385633270322,
            0.998109640831758,
            0.996219281663516,
            1.0,
            1.0,
            1.0,
            0.9678638941398866
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -3849,
            -5290,
            -5290,
            -899,
            -1523,
            -2369,
            -5290,
            -5290,
            -5290,
            -5290,
            -2688,
            -1945,
            -5290,
            -5290,
            -2205,
            -5290,
            -2007,
            -5027,
            -4504,
            -739,
            -1470,
            -2000,
            -3247,
            -3811,
            -5290,
            -776,
            -5290,
            -829,
            -5290,
            -4887,
            -2391,
            -5290,
            -1728,
            -1373,
            -913,
            -2856,
            -1192,
            -1049,
            -1113,
            -1735,
            -5290,
            -463,
            -4065,
            -1313,
            -2690,
            -3282,
            -1003,
            -3103,
            -2648,
            -1103,
            -1401,
            -383,
            -641,
            -1173,
            -1430,
            -3574,
            -829,
            -2728,
            -1967,
            -1208,
            -825,
            -1659,
            -2097,
            -983,
            -843,
            -949,
            -4654,
            -1810,
            -668,
            -490,
            -1306,
            -1110,
            -557,
            -1500,
            -3716,
            -3209,
            -1766,
            -708,
            -531,
            -562,
            -1113,
            -1832,
            -666,
            -4824,
            -1221,
            -611,
            -840,
            -405,
            -358,
            -597,
            -1217,
            -1073,
            -321,
            -1137,
            -468,
            -1513,
            -266,
            -620,
            -650,
            -1108,
            -396,
            -1466
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            3950,
            5290,
            5290,
            1000,
            1624,
            2470,
            5290,
            5290,
            5290,
            5290,
            2789,
            2046,
            5290,
            5290,
            2306,
            5290,
            2108,
            5128,
            4605,
            840,
            1571,
            2101,
            3348,
            3912,
            5290,
            877,
            5290,
            930,
            5290,
            4988,
            2492,
            5290,
            1829,
            1474,
            1014,
            2957,
            1293,
            1150,
            1214,
            1836,
            5290,
            564,
            4166,
            1414,
            2791,
            3383,
            1104,
            3204,
            2749,
            1204,
            1502,
            484,
            742,
            1274,
            1531,
            3675,
            930,
            2829,
            2068,
            1309,
            926,
            1760,
            2198,
            1084,
            944,
            1050,
            4755,
            1911,
            769,
            591,
            1407,
            1211,
            658,
            1601,
            3817,
            3310,
            1867,
            809,
            632,
            663,
            1214,
            1933,
            767,
            4925,
            1322,
            712,
            941,
            506,
            459,
            698,
            1318,
            1174,
            422,
            1238,
            569,
            1614,
            367,
            721,
            751,
            1209,
            497,
            1567
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "733/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 17.730626344680786,
        "final_policy_stability": 0.9924385633270322,
        "episodes_to_convergence": 95,
        "policy_stability_history": [
            0.0,
            0.8034026465028355,
            0.9035916824196597,
            0.8657844990548205,
            0.8827977315689981,
            0.8827977315689981,
            0.8468809073724007,
            0.8733459357277883,
            0.8412098298676749,
            0.8544423440453687,
            0.888468809073724,
            0.8241965973534972,
            0.9168241965973535,
            0.8827977315689981,
            0.9489603024574669,
            0.8865784499054821,
            0.8620037807183365,
            0.8582230623818525,
            0.9187145557655955,
            0.8714555765595463,
            0.8431001890359168,
            0.9262759924385633,
            0.8865784499054821,
            0.8582230623818525,
            0.9017013232514177,
            0.9168241965973535,
            0.9073724007561437,
            0.943289224952741,
            0.9319470699432892,
            0.8525519848771267,
            0.9338374291115312,
            0.9678638941398866,
            0.8903591682419659,
            0.9357277882797732,
            0.8620037807183365,
            0.9527410207939508,
            0.947069943289225,
            0.9017013232514177,
            0.945179584120983,
            0.9829867674858223,
            0.888468809073724,
            0.9376181474480151,
            0.9489603024574669,
            0.941398865784499,
            0.9187145557655955,
            0.8790170132325141,
            0.9565217391304348,
            0.8960302457466919,
            0.9319470699432892,
            0.9621928166351607,
            0.9546313799621928,
            0.9281663516068053,
            0.9262759924385633,
            0.9792060491493384,
            0.9262759924385633,
            0.888468809073724,
            0.9659735349716446,
            0.8827977315689981,
            0.9073724007561437,
            0.9527410207939508,
            0.9319470699432892,
            0.9546313799621928,
            0.9565217391304348,
            0.9754253308128544,
            0.8790170132325141,
            0.9754253308128544,
            0.9697542533081286,
            0.9546313799621928,
            0.9395085066162571,
            0.9678638941398866,
            0.9773156899810964,
            0.9527410207939508,
            0.9867674858223062,
            0.9735349716446124,
            0.9735349716446124,
            0.9603024574669187,
            0.9829867674858223,
            0.9735349716446124,
            0.9735349716446124,
            0.9867674858223062,
            0.9905482041587902,
            0.9829867674858223,
            0.9810964083175804,
            0.9319470699432892,
            0.9527410207939508,
            0.9697542533081286,
            0.9527410207939508,
            0.9867674858223062,
            0.9905482041587902,
            0.9867674858223062,
            0.9867674858223062,
            0.9867674858223062,
            1.0,
            1.0,
            0.998109640831758,
            0.9924385633270322
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -2967,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -1230,
            -5290,
            -973,
            -2348,
            -2610,
            -5290,
            -1517,
            -3411,
            -4960,
            -1403,
            -3455,
            -4498,
            -1853,
            -3151,
            -3016,
            -804,
            -1355,
            -5290,
            -1105,
            -607,
            -5290,
            -1177,
            -4249,
            -598,
            -729,
            -1936,
            -1242,
            -327,
            -4146,
            -1001,
            -953,
            -1267,
            -5290,
            -4155,
            -956,
            -2559,
            -1734,
            -1161,
            -904,
            -4319,
            -1553,
            -416,
            -1724,
            -5290,
            -1040,
            -3373,
            -3072,
            -798,
            -1885,
            -1172,
            -1301,
            -838,
            -5290,
            -675,
            -580,
            -1746,
            -2274,
            -876,
            -517,
            -2157,
            -907,
            -734,
            -905,
            -1629,
            -905,
            -938,
            -1166,
            -512,
            -935,
            -598,
            -923,
            -2383,
            -1785,
            -1201,
            -1821,
            -1069,
            -1540,
            -851,
            -1453,
            -868,
            -407,
            -483,
            -683,
            -966
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            3068,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            1331,
            5290,
            1074,
            2449,
            2711,
            5290,
            1618,
            3512,
            5061,
            1504,
            3556,
            4599,
            1954,
            3252,
            3117,
            905,
            1456,
            5290,
            1206,
            708,
            5290,
            1278,
            4350,
            699,
            830,
            2037,
            1343,
            428,
            4247,
            1102,
            1054,
            1368,
            5290,
            4256,
            1057,
            2660,
            1835,
            1262,
            1005,
            4420,
            1654,
            517,
            1825,
            5290,
            1141,
            3474,
            3173,
            899,
            1986,
            1273,
            1402,
            939,
            5290,
            776,
            681,
            1847,
            2375,
            977,
            618,
            2258,
            1008,
            835,
            1006,
            1730,
            1006,
            1039,
            1267,
            613,
            1036,
            699,
            1024,
            2484,
            1886,
            1302,
            1922,
            1170,
            1641,
            952,
            1554,
            969,
            508,
            584,
            784,
            1067
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "734/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 17.998805046081543,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 94,
        "policy_stability_history": [
            0.0,
            0.7693761814744802,
            0.888468809073724,
            0.8468809073724007,
            0.8412098298676749,
            0.8563327032136105,
            0.9054820415879017,
            0.8771266540642723,
            0.8714555765595463,
            0.8355387523629489,
            0.8790170132325141,
            0.8298676748582231,
            0.8147448015122873,
            0.8960302457466919,
            0.8601134215500945,
            0.8903591682419659,
            0.8922495274102079,
            0.8809073724007561,
            0.9357277882797732,
            0.8638941398865785,
            0.9300567107750473,
            0.8676748582230623,
            0.9224952741020794,
            0.8714555765595463,
            0.9187145557655955,
            0.9300567107750473,
            0.8449905482041588,
            0.9073724007561437,
            0.9035916824196597,
            0.888468809073724,
            0.9092627599243857,
            0.8412098298676749,
            0.8922495274102079,
            0.8790170132325141,
            0.8714555765595463,
            0.9565217391304348,
            0.9187145557655955,
            0.945179584120983,
            0.8903591682419659,
            0.9659735349716446,
            0.943289224952741,
            0.8790170132325141,
            0.8865784499054821,
            0.8638941398865785,
            0.9130434782608695,
            0.9300567107750473,
            0.947069943289225,
            0.9716446124763705,
            0.941398865784499,
            0.9508506616257089,
            0.9621928166351607,
            0.9527410207939508,
            0.9092627599243857,
            0.9792060491493384,
            0.9697542533081286,
            0.9017013232514177,
            0.8752362948960303,
            0.9584120982986768,
            0.9810964083175804,
            0.9810964083175804,
            0.9527410207939508,
            0.941398865784499,
            0.9810964083175804,
            0.9527410207939508,
            0.9886578449905482,
            0.947069943289225,
            0.8922495274102079,
            0.8525519848771267,
            0.9678638941398866,
            0.9678638941398866,
            0.943289224952741,
            0.9281663516068053,
            0.994328922495274,
            0.994328922495274,
            0.9716446124763705,
            0.9924385633270322,
            0.998109640831758,
            0.9659735349716446,
            0.9810964083175804,
            0.9565217391304348,
            0.9338374291115312,
            0.994328922495274,
            0.9716446124763705,
            0.996219281663516,
            0.994328922495274,
            0.9792060491493384,
            0.9319470699432892,
            1.0,
            0.998109640831758,
            0.996219281663516,
            0.9867674858223062,
            0.9905482041587902,
            0.9810964083175804,
            1.0,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -4997,
            -5290,
            -5290,
            -3641,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4448,
            -2470,
            -2625,
            -5290,
            -942,
            -2885,
            -792,
            -2885,
            -1904,
            -5290,
            -1274,
            -1132,
            -5290,
            -5290,
            -2647,
            -2432,
            -2505,
            -5290,
            -4087,
            -5009,
            -3643,
            -702,
            -1978,
            -1916,
            -3425,
            -627,
            -1309,
            -5290,
            -3874,
            -4480,
            -2494,
            -1377,
            -1070,
            -290,
            -3181,
            -984,
            -768,
            -990,
            -2419,
            -551,
            -755,
            -2842,
            -5290,
            -711,
            -466,
            -719,
            -1361,
            -1324,
            -362,
            -1847,
            -587,
            -2127,
            -4549,
            -4954,
            -1156,
            -1494,
            -2508,
            -2840,
            -538,
            -777,
            -1055,
            -1143,
            -536,
            -1098,
            -770,
            -2132,
            -4459,
            -500,
            -971,
            -456,
            -421,
            -1336,
            -3327,
            -859,
            -418,
            -825,
            -1099,
            -835,
            -1283,
            -649,
            -558
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5098,
            5290,
            5290,
            3742,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            4549,
            2571,
            2726,
            5290,
            1043,
            2986,
            893,
            2986,
            2005,
            5290,
            1375,
            1233,
            5290,
            5290,
            2748,
            2533,
            2606,
            5290,
            4188,
            5110,
            3744,
            803,
            2079,
            2017,
            3526,
            728,
            1410,
            5290,
            3975,
            4581,
            2595,
            1478,
            1171,
            391,
            3282,
            1085,
            869,
            1091,
            2520,
            652,
            856,
            2943,
            5290,
            812,
            567,
            820,
            1462,
            1425,
            463,
            1948,
            688,
            2228,
            4650,
            5055,
            1257,
            1595,
            2609,
            2941,
            639,
            878,
            1156,
            1244,
            637,
            1199,
            871,
            2233,
            4560,
            601,
            1072,
            557,
            522,
            1437,
            3428,
            960,
            519,
            926,
            1200,
            936,
            1384,
            750,
            659
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "735/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 16.556816339492798,
        "final_policy_stability": 0.9905482041587902,
        "episodes_to_convergence": 76,
        "policy_stability_history": [
            0.0,
            0.77882797731569,
            0.9262759924385633,
            0.9130434782608695,
            0.8941398865784499,
            0.8431001890359168,
            0.9017013232514177,
            0.8960302457466919,
            0.8865784499054821,
            0.8809073724007561,
            0.8638941398865785,
            0.8638941398865785,
            0.8733459357277883,
            0.8979206049149339,
            0.9111531190926276,
            0.9017013232514177,
            0.8771266540642723,
            0.8922495274102079,
            0.9395085066162571,
            0.8544423440453687,
            0.9035916824196597,
            0.9092627599243857,
            0.8809073724007561,
            0.9092627599243857,
            0.8714555765595463,
            0.8809073724007561,
            0.8998109640831758,
            0.9338374291115312,
            0.9092627599243857,
            0.8714555765595463,
            0.9092627599243857,
            0.9338374291115312,
            0.8620037807183365,
            0.9243856332703214,
            0.9508506616257089,
            0.8979206049149339,
            0.9111531190926276,
            0.9130434782608695,
            0.9281663516068053,
            0.9659735349716446,
            0.9603024574669187,
            0.9395085066162571,
            0.9395085066162571,
            0.9035916824196597,
            0.9149338374291115,
            0.8809073724007561,
            0.9773156899810964,
            0.8922495274102079,
            0.9546313799621928,
            0.9376181474480151,
            0.9546313799621928,
            0.9584120982986768,
            0.9848771266540642,
            0.9905482041587902,
            0.9792060491493384,
            0.9867674858223062,
            0.9905482041587902,
            0.8941398865784499,
            0.9754253308128544,
            0.9546313799621928,
            0.9792060491493384,
            0.9886578449905482,
            0.9489603024574669,
            0.9792060491493384,
            0.9716446124763705,
            0.9338374291115312,
            0.9848771266540642,
            0.9489603024574669,
            0.9886578449905482,
            0.994328922495274,
            0.996219281663516,
            0.996219281663516,
            0.9338374291115312,
            0.9848771266540642,
            0.947069943289225,
            0.9810964083175804,
            0.9905482041587902
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2234,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2654,
            -5290,
            -5290,
            -3223,
            -1042,
            -5109,
            -2596,
            -5290,
            -5290,
            -2621,
            -5290,
            -5290,
            -3065,
            -1441,
            -2535,
            -5290,
            -3208,
            -3478,
            -5290,
            -5290,
            -924,
            -2880,
            -5290,
            -3404,
            -3516,
            -1553,
            -1611,
            -2229,
            -5290,
            -5290,
            -5290,
            -5290,
            -1201,
            -5290,
            -1964,
            -1966,
            -2675,
            -2897,
            -1173,
            -646,
            -1443,
            -668,
            -969,
            -5290,
            -1802,
            -2338,
            -2172,
            -1217,
            -3825,
            -1453,
            -2064,
            -3845,
            -1023,
            -3103,
            -864,
            -530,
            -691,
            -769,
            -3940,
            -2301,
            -5290,
            -1988,
            -1440
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            2335,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            2755,
            5290,
            5290,
            3324,
            1143,
            5210,
            2697,
            5290,
            5290,
            2722,
            5290,
            5290,
            3166,
            1542,
            2636,
            5290,
            3309,
            3579,
            5290,
            5290,
            1025,
            2981,
            5290,
            3505,
            3617,
            1654,
            1712,
            2330,
            5290,
            5290,
            5290,
            5290,
            1302,
            5290,
            2065,
            2067,
            2776,
            2998,
            1274,
            747,
            1544,
            769,
            1070,
            5290,
            1903,
            2439,
            2273,
            1318,
            3926,
            1554,
            2165,
            3946,
            1124,
            3204,
            965,
            631,
            792,
            870,
            4041,
            2402,
            5290,
            2089,
            1541
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "736/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 14.500087022781372,
        "final_policy_stability": 0.9130434782608695,
        "episodes_to_convergence": 66,
        "policy_stability_history": [
            0.0,
            0.8960302457466919,
            0.8431001890359168,
            0.8279773156899811,
            0.9149338374291115,
            0.8733459357277883,
            0.8865784499054821,
            0.9319470699432892,
            0.9168241965973535,
            0.8676748582230623,
            0.9073724007561437,
            0.8790170132325141,
            0.9149338374291115,
            0.8922495274102079,
            0.8941398865784499,
            0.8903591682419659,
            0.9111531190926276,
            0.941398865784499,
            0.9035916824196597,
            0.8601134215500945,
            0.888468809073724,
            0.9206049149338374,
            0.9262759924385633,
            0.8638941398865785,
            0.8941398865784499,
            0.9111531190926276,
            0.8960302457466919,
            0.8525519848771267,
            0.9130434782608695,
            0.8676748582230623,
            0.8714555765595463,
            0.9300567107750473,
            0.9395085066162571,
            0.9130434782608695,
            0.9111531190926276,
            0.8846880907372401,
            0.9357277882797732,
            0.9111531190926276,
            0.9111531190926276,
            0.9149338374291115,
            0.9319470699432892,
            0.9073724007561437,
            0.9111531190926276,
            0.9357277882797732,
            0.9565217391304348,
            0.9262759924385633,
            0.9395085066162571,
            0.9489603024574669,
            0.9508506616257089,
            0.9508506616257089,
            0.9659735349716446,
            0.9489603024574669,
            0.9735349716446124,
            0.9603024574669187,
            0.9262759924385633,
            0.9584120982986768,
            0.9546313799621928,
            0.9243856332703214,
            0.9886578449905482,
            0.9489603024574669,
            0.9735349716446124,
            0.888468809073724,
            0.9886578449905482,
            0.9168241965973535,
            1.0,
            0.9792060491493384,
            0.9130434782608695
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -4513,
            -5290,
            -5290,
            -5290,
            -5290,
            -2775,
            -5010,
            -5290,
            -4620,
            -5290,
            -5290,
            -4095,
            -5290,
            -5290,
            -1929,
            -3113,
            -5290,
            -3241,
            -3373,
            -2013,
            -5290,
            -3847,
            -3730,
            -3237,
            -5290,
            -2868,
            -5290,
            -5290,
            -3060,
            -2125,
            -5290,
            -5290,
            -5290,
            -2324,
            -3524,
            -5290,
            -4404,
            -3178,
            -4397,
            -5290,
            -2560,
            -2502,
            -3615,
            -2735,
            -2503,
            -2488,
            -3204,
            -2016,
            -4137,
            -1289,
            -3121,
            -4230,
            -1984,
            -2907,
            -5290,
            -2024,
            -3016,
            -2395,
            -5290,
            -1042,
            -4299,
            -1270,
            -2586,
            -4457
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            4614,
            5290,
            5290,
            5290,
            5290,
            2876,
            5111,
            5290,
            4721,
            5290,
            5290,
            4196,
            5290,
            5290,
            2030,
            3214,
            5290,
            3342,
            3474,
            2114,
            5290,
            3948,
            3831,
            3338,
            5290,
            2969,
            5290,
            5290,
            3161,
            2226,
            5290,
            5290,
            5290,
            2425,
            3625,
            5290,
            4505,
            3279,
            4498,
            5290,
            2661,
            2603,
            3716,
            2836,
            2604,
            2589,
            3305,
            2117,
            4238,
            1390,
            3222,
            4331,
            2085,
            3008,
            5290,
            2125,
            3117,
            2496,
            5290,
            1143,
            4400,
            1371,
            2687,
            4558
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "737/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 15.91878056526184,
        "final_policy_stability": 0.9792060491493384,
        "episodes_to_convergence": 76,
        "policy_stability_history": [
            0.0,
            0.8468809073724007,
            0.8185255198487713,
            0.9130434782608695,
            0.8487712665406427,
            0.8733459357277883,
            0.8223062381852552,
            0.8657844990548205,
            0.8620037807183365,
            0.8752362948960303,
            0.9206049149338374,
            0.8790170132325141,
            0.9035916824196597,
            0.9262759924385633,
            0.9130434782608695,
            0.8960302457466919,
            0.9206049149338374,
            0.9017013232514177,
            0.8922495274102079,
            0.9187145557655955,
            0.945179584120983,
            0.8922495274102079,
            0.9111531190926276,
            0.8903591682419659,
            0.9073724007561437,
            0.9508506616257089,
            0.8676748582230623,
            0.8960302457466919,
            0.943289224952741,
            0.8752362948960303,
            0.9640831758034026,
            0.943289224952741,
            0.9659735349716446,
            0.9111531190926276,
            0.9168241965973535,
            0.9017013232514177,
            0.9168241965973535,
            0.9243856332703214,
            0.9168241965973535,
            0.9111531190926276,
            0.9546313799621928,
            0.9243856332703214,
            0.9659735349716446,
            0.9262759924385633,
            0.9603024574669187,
            0.9357277882797732,
            0.8695652173913043,
            0.947069943289225,
            0.947069943289225,
            0.9017013232514177,
            0.9262759924385633,
            0.9773156899810964,
            0.9716446124763705,
            0.9848771266540642,
            0.9187145557655955,
            0.9697542533081286,
            0.9395085066162571,
            0.9565217391304348,
            0.9659735349716446,
            0.9111531190926276,
            0.9792060491493384,
            0.9187145557655955,
            0.9810964083175804,
            0.9584120982986768,
            0.9905482041587902,
            0.9300567107750473,
            0.9905482041587902,
            0.9848771266540642,
            0.9697542533081286,
            0.9300567107750473,
            0.9792060491493384,
            0.9848771266540642,
            0.943289224952741,
            0.9111531190926276,
            0.9924385633270322,
            0.9924385633270322,
            0.9792060491493384
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -3846,
            -5290,
            -5290,
            -4522,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -1950,
            -5290,
            -5290,
            -2067,
            -5116,
            -3811,
            -1757,
            -1844,
            -5290,
            -5290,
            -5290,
            -2534,
            -1246,
            -4628,
            -4573,
            -1739,
            -5290,
            -1485,
            -2754,
            -1066,
            -4933,
            -5290,
            -3962,
            -5290,
            -2945,
            -2764,
            -4467,
            -1264,
            -3945,
            -897,
            -3579,
            -1322,
            -1824,
            -5290,
            -2645,
            -2806,
            -5290,
            -5290,
            -1591,
            -1174,
            -784,
            -4763,
            -1610,
            -2278,
            -1676,
            -681,
            -4487,
            -972,
            -5290,
            -1199,
            -2269,
            -719,
            -4688,
            -1005,
            -1594,
            -2662,
            -4699,
            -1333,
            -2526,
            -4126,
            -5290,
            -1400,
            -2097,
            -1624
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            3947,
            5290,
            5290,
            4623,
            5290,
            5290,
            5290,
            5290,
            5290,
            2051,
            5290,
            5290,
            2168,
            5217,
            3912,
            1858,
            1945,
            5290,
            5290,
            5290,
            2635,
            1347,
            4729,
            4674,
            1840,
            5290,
            1586,
            2855,
            1167,
            5034,
            5290,
            4063,
            5290,
            3046,
            2865,
            4568,
            1365,
            4046,
            998,
            3680,
            1423,
            1925,
            5290,
            2746,
            2907,
            5290,
            5290,
            1692,
            1275,
            885,
            4864,
            1711,
            2379,
            1777,
            782,
            4588,
            1073,
            5290,
            1300,
            2370,
            820,
            4789,
            1106,
            1695,
            2763,
            4800,
            1434,
            2627,
            4227,
            5290,
            1501,
            2198,
            1725
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "738/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 13.985179901123047,
        "final_policy_stability": 0.998109640831758,
        "episodes_to_convergence": 66,
        "policy_stability_history": [
            0.0,
            0.8865784499054821,
            0.8374291115311909,
            0.8733459357277883,
            0.8827977315689981,
            0.9206049149338374,
            0.8676748582230623,
            0.8525519848771267,
            0.8790170132325141,
            0.8998109640831758,
            0.8638941398865785,
            0.9224952741020794,
            0.8544423440453687,
            0.8412098298676749,
            0.9035916824196597,
            0.8846880907372401,
            0.8752362948960303,
            0.8638941398865785,
            0.9565217391304348,
            0.9565217391304348,
            0.9054820415879017,
            0.8846880907372401,
            0.8790170132325141,
            0.9376181474480151,
            0.8998109640831758,
            0.9130434782608695,
            0.8506616257088847,
            0.9111531190926276,
            0.9300567107750473,
            0.9017013232514177,
            0.9092627599243857,
            0.9111531190926276,
            0.8998109640831758,
            0.9187145557655955,
            0.9243856332703214,
            0.9243856332703214,
            0.9319470699432892,
            0.9678638941398866,
            0.9017013232514177,
            0.9130434782608695,
            0.9810964083175804,
            0.945179584120983,
            0.9829867674858223,
            0.9224952741020794,
            0.9357277882797732,
            0.9376181474480151,
            0.9035916824196597,
            0.9659735349716446,
            0.9527410207939508,
            0.9848771266540642,
            0.945179584120983,
            0.9810964083175804,
            0.9168241965973535,
            0.9848771266540642,
            0.8998109640831758,
            0.9678638941398866,
            0.9905482041587902,
            0.9149338374291115,
            0.9546313799621928,
            0.9508506616257089,
            0.945179584120983,
            0.994328922495274,
            0.9754253308128544,
            1.0,
            0.994328922495274,
            0.996219281663516,
            0.998109640831758
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5163,
            -4583,
            -5290,
            -5290,
            -5290,
            -1989,
            -5290,
            -5290,
            -5290,
            -5290,
            -3776,
            -5290,
            -5290,
            -977,
            -3041,
            -5290,
            -5290,
            -2390,
            -5290,
            -4182,
            -5290,
            -5290,
            -2617,
            -4019,
            -3999,
            -4983,
            -5290,
            -2978,
            -2977,
            -4217,
            -1436,
            -903,
            -5290,
            -3284,
            -679,
            -2033,
            -652,
            -5290,
            -5290,
            -2205,
            -5290,
            -720,
            -2036,
            -715,
            -2621,
            -900,
            -5290,
            -1693,
            -5095,
            -3381,
            -697,
            -5290,
            -3520,
            -2937,
            -2945,
            -1396,
            -1787,
            -745,
            -1782,
            -1149,
            -1242
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5264,
            4684,
            5290,
            5290,
            5290,
            2090,
            5290,
            5290,
            5290,
            5290,
            3877,
            5290,
            5290,
            1078,
            3142,
            5290,
            5290,
            2491,
            5290,
            4283,
            5290,
            5290,
            2718,
            4120,
            4100,
            5084,
            5290,
            3079,
            3078,
            4318,
            1537,
            1004,
            5290,
            3385,
            780,
            2134,
            753,
            5290,
            5290,
            2306,
            5290,
            821,
            2137,
            816,
            2722,
            1001,
            5290,
            1794,
            5196,
            3482,
            798,
            5290,
            3621,
            3038,
            3046,
            1497,
            1888,
            846,
            1883,
            1250,
            1343
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "739/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 15.054827213287354,
        "final_policy_stability": 0.9754253308128544,
        "episodes_to_convergence": 72,
        "policy_stability_history": [
            0.0,
            0.8960302457466919,
            0.9054820415879017,
            0.9300567107750473,
            0.8601134215500945,
            0.8563327032136105,
            0.8733459357277883,
            0.8525519848771267,
            0.8771266540642723,
            0.8960302457466919,
            0.8846880907372401,
            0.8922495274102079,
            0.8714555765595463,
            0.8903591682419659,
            0.8563327032136105,
            0.9300567107750473,
            0.8922495274102079,
            0.8846880907372401,
            0.8846880907372401,
            0.8846880907372401,
            0.8903591682419659,
            0.9017013232514177,
            0.8695652173913043,
            0.8960302457466919,
            0.8827977315689981,
            0.9565217391304348,
            0.9281663516068053,
            0.9584120982986768,
            0.9243856332703214,
            0.9527410207939508,
            0.9508506616257089,
            0.9659735349716446,
            0.9508506616257089,
            0.9300567107750473,
            0.9168241965973535,
            0.9281663516068053,
            0.9716446124763705,
            0.9243856332703214,
            0.9357277882797732,
            0.8979206049149339,
            0.8941398865784499,
            0.8582230623818525,
            0.9792060491493384,
            0.947069943289225,
            0.943289224952741,
            0.8733459357277883,
            0.9054820415879017,
            0.8714555765595463,
            0.9697542533081286,
            0.941398865784499,
            0.9640831758034026,
            0.9659735349716446,
            0.8922495274102079,
            0.9073724007561437,
            0.9319470699432892,
            0.9640831758034026,
            0.9149338374291115,
            0.9111531190926276,
            0.9546313799621928,
            0.9867674858223062,
            0.9338374291115312,
            0.9829867674858223,
            0.9754253308128544,
            0.9395085066162571,
            0.9092627599243857,
            0.9187145557655955,
            0.9281663516068053,
            0.996219281663516,
            0.9829867674858223,
            0.9565217391304348,
            0.9735349716446124,
            0.9735349716446124,
            0.9754253308128544
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4479,
            -3867,
            -5290,
            -3570,
            -5290,
            -5290,
            -5290,
            -4734,
            -1016,
            -5290,
            -5290,
            -5290,
            -5290,
            -3045,
            -5290,
            -5290,
            -5290,
            -5290,
            -1405,
            -5290,
            -1260,
            -4395,
            -2347,
            -1397,
            -1226,
            -1619,
            -3234,
            -5290,
            -4737,
            -1289,
            -3627,
            -3396,
            -5290,
            -3567,
            -5290,
            -617,
            -2624,
            -2973,
            -4770,
            -5290,
            -4491,
            -732,
            -3055,
            -1029,
            -783,
            -5290,
            -5290,
            -3209,
            -1075,
            -3364,
            -5290,
            -4024,
            -1455,
            -3090,
            -2056,
            -2164,
            -5163,
            -5290,
            -5150,
            -5290,
            -1640,
            -1285,
            -4616,
            -1529,
            -2576,
            -2430
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            4580,
            3968,
            5290,
            3671,
            5290,
            5290,
            5290,
            4835,
            1117,
            5290,
            5290,
            5290,
            5290,
            3146,
            5290,
            5290,
            5290,
            5290,
            1506,
            5290,
            1361,
            4496,
            2448,
            1498,
            1327,
            1720,
            3335,
            5290,
            4838,
            1390,
            3728,
            3497,
            5290,
            3668,
            5290,
            718,
            2725,
            3074,
            4871,
            5290,
            4592,
            833,
            3156,
            1130,
            884,
            5290,
            5290,
            3310,
            1176,
            3465,
            5290,
            4125,
            1556,
            3191,
            2157,
            2265,
            5264,
            5290,
            5251,
            5290,
            1741,
            1386,
            4717,
            1630,
            2677,
            2531
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "740/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 16.410137176513672,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 85,
        "policy_stability_history": [
            0.0,
            0.8506616257088847,
            0.8582230623818525,
            0.8714555765595463,
            0.8979206049149339,
            0.8582230623818525,
            0.8468809073724007,
            0.888468809073724,
            0.8865784499054821,
            0.8204158790170132,
            0.8563327032136105,
            0.9092627599243857,
            0.8771266540642723,
            0.8903591682419659,
            0.8676748582230623,
            0.8676748582230623,
            0.9035916824196597,
            0.8431001890359168,
            0.8676748582230623,
            0.8846880907372401,
            0.8941398865784499,
            0.9716446124763705,
            0.9206049149338374,
            0.8638941398865785,
            0.9224952741020794,
            0.8638941398865785,
            0.8657844990548205,
            0.8620037807183365,
            0.9678638941398866,
            0.8998109640831758,
            0.9527410207939508,
            0.9111531190926276,
            0.9187145557655955,
            0.8903591682419659,
            0.8733459357277883,
            0.9111531190926276,
            0.9281663516068053,
            0.9810964083175804,
            0.9073724007561437,
            0.9697542533081286,
            0.9640831758034026,
            0.9187145557655955,
            0.9584120982986768,
            0.9243856332703214,
            0.943289224952741,
            0.9905482041587902,
            0.9149338374291115,
            0.9716446124763705,
            0.9508506616257089,
            0.9603024574669187,
            0.9130434782608695,
            0.9716446124763705,
            0.9659735349716446,
            0.947069943289225,
            0.943289224952741,
            0.9224952741020794,
            0.8979206049149339,
            0.9697542533081286,
            0.9886578449905482,
            0.9206049149338374,
            0.9848771266540642,
            0.9187145557655955,
            0.8960302457466919,
            0.9640831758034026,
            0.9376181474480151,
            0.994328922495274,
            0.9773156899810964,
            0.9697542533081286,
            0.9565217391304348,
            0.9716446124763705,
            0.9829867674858223,
            0.9640831758034026,
            0.9867674858223062,
            0.9924385633270322,
            0.9924385633270322,
            0.9924385633270322,
            0.9924385633270322,
            0.9546313799621928,
            0.9924385633270322,
            0.9867674858223062,
            0.9924385633270322,
            0.9262759924385633,
            1.0,
            0.9697542533081286,
            0.9168241965973535,
            1.0
        ],
        "reward_history": [
            -3025,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2168,
            -3402,
            -5290,
            -5290,
            -2952,
            -5290,
            -5290,
            -4782,
            -4036,
            -5290,
            -5290,
            -4873,
            -2444,
            -2407,
            -552,
            -2241,
            -5290,
            -1472,
            -5290,
            -5290,
            -4406,
            -699,
            -4217,
            -1271,
            -2610,
            -1456,
            -5290,
            -5290,
            -3625,
            -2350,
            -739,
            -3134,
            -869,
            -1430,
            -5290,
            -1587,
            -2654,
            -1992,
            -761,
            -5290,
            -578,
            -1121,
            -1292,
            -4098,
            -1368,
            -2145,
            -1743,
            -2655,
            -4067,
            -4860,
            -1397,
            -579,
            -2396,
            -926,
            -4590,
            -4488,
            -1645,
            -2179,
            -1455,
            -992,
            -1278,
            -2015,
            -1592,
            -1913,
            -1675,
            -1777,
            -732,
            -947,
            -769,
            -1132,
            -2063,
            -473,
            -1001,
            -756,
            -3580,
            -643,
            -2156,
            -4482,
            -616
        ],
        "steps_history": [
            3126,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            2269,
            3503,
            5290,
            5290,
            3053,
            5290,
            5290,
            4883,
            4137,
            5290,
            5290,
            4974,
            2545,
            2508,
            653,
            2342,
            5290,
            1573,
            5290,
            5290,
            4507,
            800,
            4318,
            1372,
            2711,
            1557,
            5290,
            5290,
            3726,
            2451,
            840,
            3235,
            970,
            1531,
            5290,
            1688,
            2755,
            2093,
            862,
            5290,
            679,
            1222,
            1393,
            4199,
            1469,
            2246,
            1844,
            2756,
            4168,
            4961,
            1498,
            680,
            2497,
            1027,
            4691,
            4589,
            1746,
            2280,
            1556,
            1093,
            1379,
            2116,
            1693,
            2014,
            1776,
            1878,
            833,
            1048,
            870,
            1233,
            2164,
            574,
            1102,
            857,
            3681,
            744,
            2257,
            4583,
            717
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "741/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 16.44927716255188,
        "final_policy_stability": 0.996219281663516,
        "episodes_to_convergence": 83,
        "policy_stability_history": [
            0.0,
            0.9035916824196597,
            0.8090737240075614,
            0.8525519848771267,
            0.8809073724007561,
            0.8714555765595463,
            0.8374291115311909,
            0.8998109640831758,
            0.8733459357277883,
            0.8733459357277883,
            0.8827977315689981,
            0.9130434782608695,
            0.8525519848771267,
            0.8128544423440454,
            0.8468809073724007,
            0.8809073724007561,
            0.9149338374291115,
            0.8771266540642723,
            0.9149338374291115,
            0.9319470699432892,
            0.9357277882797732,
            0.9149338374291115,
            0.8185255198487713,
            0.8582230623818525,
            0.9206049149338374,
            0.9073724007561437,
            0.8487712665406427,
            0.9224952741020794,
            0.8676748582230623,
            0.9206049149338374,
            0.9224952741020794,
            0.8563327032136105,
            0.9319470699432892,
            0.945179584120983,
            0.947069943289225,
            0.8865784499054821,
            0.9640831758034026,
            0.943289224952741,
            0.9206049149338374,
            0.8979206049149339,
            0.9092627599243857,
            0.947069943289225,
            0.8865784499054821,
            0.8657844990548205,
            0.9621928166351607,
            0.9376181474480151,
            0.9565217391304348,
            0.943289224952741,
            0.9603024574669187,
            0.8638941398865785,
            0.9111531190926276,
            0.8846880907372401,
            0.9357277882797732,
            0.9376181474480151,
            0.9584120982986768,
            0.941398865784499,
            0.9243856332703214,
            0.9867674858223062,
            0.9716446124763705,
            0.9527410207939508,
            0.9886578449905482,
            0.9754253308128544,
            0.9603024574669187,
            0.941398865784499,
            0.8733459357277883,
            0.9716446124763705,
            0.9659735349716446,
            0.947069943289225,
            0.9338374291115312,
            0.9716446124763705,
            0.9886578449905482,
            0.9603024574669187,
            0.9300567107750473,
            0.9810964083175804,
            0.9716446124763705,
            0.9508506616257089,
            0.9603024574669187,
            0.996219281663516,
            0.9735349716446124,
            0.9735349716446124,
            0.9829867674858223,
            0.9848771266540642,
            0.9792060491493384,
            0.996219281663516
        ],
        "reward_history": [
            -5290,
            -5290,
            -4466,
            -5290,
            -2519,
            -5290,
            -5290,
            -5290,
            -4919,
            -4645,
            -1873,
            -5290,
            -5290,
            -5290,
            -4812,
            -5290,
            -2138,
            -2676,
            -2000,
            -1467,
            -882,
            -1639,
            -5290,
            -3875,
            -2065,
            -5290,
            -4406,
            -1181,
            -3410,
            -1712,
            -5290,
            -5290,
            -2120,
            -2186,
            -1079,
            -3053,
            -693,
            -1009,
            -2095,
            -5290,
            -5290,
            -1918,
            -5290,
            -5290,
            -1251,
            -1887,
            -960,
            -1241,
            -1483,
            -5290,
            -2008,
            -4572,
            -2255,
            -3031,
            -1606,
            -2259,
            -3410,
            -1012,
            -797,
            -2976,
            -711,
            -1108,
            -1554,
            -3560,
            -5290,
            -803,
            -2207,
            -2082,
            -3614,
            -1074,
            -1452,
            -2949,
            -5290,
            -717,
            -718,
            -2619,
            -1989,
            -935,
            -1404,
            -1230,
            -1437,
            -553,
            -1924,
            -996
        ],
        "steps_history": [
            5290,
            5290,
            4567,
            5290,
            2620,
            5290,
            5290,
            5290,
            5020,
            4746,
            1974,
            5290,
            5290,
            5290,
            4913,
            5290,
            2239,
            2777,
            2101,
            1568,
            983,
            1740,
            5290,
            3976,
            2166,
            5290,
            4507,
            1282,
            3511,
            1813,
            5290,
            5290,
            2221,
            2287,
            1180,
            3154,
            794,
            1110,
            2196,
            5290,
            5290,
            2019,
            5290,
            5290,
            1352,
            1988,
            1061,
            1342,
            1584,
            5290,
            2109,
            4673,
            2356,
            3132,
            1707,
            2360,
            3511,
            1113,
            898,
            3077,
            812,
            1209,
            1655,
            3661,
            5290,
            904,
            2308,
            2183,
            3715,
            1175,
            1553,
            3050,
            5290,
            818,
            819,
            2720,
            2090,
            1036,
            1505,
            1331,
            1538,
            654,
            2025,
            1097
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "742/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 17.460476636886597,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 86,
        "policy_stability_history": [
            0.0,
            0.8695652173913043,
            0.8506616257088847,
            0.8241965973534972,
            0.8676748582230623,
            0.8979206049149339,
            0.8922495274102079,
            0.8506616257088847,
            0.8771266540642723,
            0.8241965973534972,
            0.8809073724007561,
            0.8449905482041588,
            0.8506616257088847,
            0.9300567107750473,
            0.9035916824196597,
            0.8563327032136105,
            0.9281663516068053,
            0.8979206049149339,
            0.9395085066162571,
            0.8809073724007561,
            0.8733459357277883,
            0.9149338374291115,
            0.9262759924385633,
            0.8620037807183365,
            0.8941398865784499,
            0.9338374291115312,
            0.9017013232514177,
            0.8809073724007561,
            0.8771266540642723,
            0.8960302457466919,
            0.8449905482041588,
            0.9262759924385633,
            0.9262759924385633,
            0.9300567107750473,
            0.8960302457466919,
            0.9206049149338374,
            0.9565217391304348,
            0.9224952741020794,
            0.9395085066162571,
            0.9338374291115312,
            0.9054820415879017,
            0.9149338374291115,
            0.9546313799621928,
            0.9848771266540642,
            0.9206049149338374,
            0.8998109640831758,
            0.9149338374291115,
            0.941398865784499,
            0.9262759924385633,
            0.9640831758034026,
            0.9111531190926276,
            0.8941398865784499,
            0.9584120982986768,
            0.9678638941398866,
            0.9603024574669187,
            0.9829867674858223,
            0.9773156899810964,
            0.9678638941398866,
            0.9281663516068053,
            0.9773156899810964,
            0.9546313799621928,
            0.9810964083175804,
            0.9338374291115312,
            0.9678638941398866,
            0.9735349716446124,
            0.9168241965973535,
            0.9829867674858223,
            0.9735349716446124,
            0.9905482041587902,
            0.9678638941398866,
            0.9754253308128544,
            0.998109640831758,
            0.996219281663516,
            0.994328922495274,
            0.9716446124763705,
            0.9829867674858223,
            0.9773156899810964,
            0.9792060491493384,
            0.9848771266540642,
            0.9546313799621928,
            0.9621928166351607,
            0.994328922495274,
            0.998109640831758,
            0.998109640831758,
            0.996219281663516,
            0.994328922495274,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -2896,
            -5290,
            -5290,
            -5290,
            -2598,
            -5290,
            -5290,
            -4162,
            -4930,
            -1466,
            -2862,
            -5290,
            -2411,
            -4296,
            -5290,
            -3021,
            -5290,
            -1987,
            -2273,
            -5290,
            -2787,
            -1629,
            -5290,
            -4467,
            -3305,
            -5290,
            -5290,
            -5290,
            -2039,
            -1436,
            -3514,
            -5290,
            -1598,
            -1829,
            -1409,
            -3419,
            -2996,
            -2725,
            -937,
            -539,
            -2920,
            -4371,
            -5290,
            -2562,
            -3285,
            -1577,
            -4741,
            -5290,
            -950,
            -1236,
            -1558,
            -684,
            -1303,
            -1205,
            -3827,
            -1200,
            -1811,
            -1161,
            -3173,
            -785,
            -1113,
            -5013,
            -984,
            -1355,
            -1608,
            -1607,
            -1227,
            -1751,
            -475,
            -848,
            -1416,
            -899,
            -1317,
            -1718,
            -1002,
            -1864,
            -2236,
            -895,
            -846,
            -546,
            -630,
            -953,
            -692
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            2997,
            5290,
            5290,
            5290,
            2699,
            5290,
            5290,
            4263,
            5031,
            1567,
            2963,
            5290,
            2512,
            4397,
            5290,
            3122,
            5290,
            2088,
            2374,
            5290,
            2888,
            1730,
            5290,
            4568,
            3406,
            5290,
            5290,
            5290,
            2140,
            1537,
            3615,
            5290,
            1699,
            1930,
            1510,
            3520,
            3097,
            2826,
            1038,
            640,
            3021,
            4472,
            5290,
            2663,
            3386,
            1678,
            4842,
            5290,
            1051,
            1337,
            1659,
            785,
            1404,
            1306,
            3928,
            1301,
            1912,
            1262,
            3274,
            886,
            1214,
            5114,
            1085,
            1456,
            1709,
            1708,
            1328,
            1852,
            576,
            949,
            1517,
            1000,
            1418,
            1819,
            1103,
            1965,
            2337,
            996,
            947,
            647,
            731,
            1054,
            793
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "743/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 16.305948972702026,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 82,
        "policy_stability_history": [
            0.0,
            0.8449905482041588,
            0.888468809073724,
            0.8147448015122873,
            0.8714555765595463,
            0.9187145557655955,
            0.8525519848771267,
            0.8506616257088847,
            0.8733459357277883,
            0.947069943289225,
            0.8752362948960303,
            0.8998109640831758,
            0.9111531190926276,
            0.8771266540642723,
            0.8998109640831758,
            0.888468809073724,
            0.8998109640831758,
            0.8771266540642723,
            0.8374291115311909,
            0.8506616257088847,
            0.9149338374291115,
            0.8525519848771267,
            0.8525519848771267,
            0.8998109640831758,
            0.8638941398865785,
            0.888468809073724,
            0.8809073724007561,
            0.9357277882797732,
            0.8922495274102079,
            0.9054820415879017,
            0.9130434782608695,
            0.941398865784499,
            0.9584120982986768,
            0.8620037807183365,
            0.9054820415879017,
            0.943289224952741,
            0.9187145557655955,
            0.9206049149338374,
            0.8865784499054821,
            0.8771266540642723,
            0.8941398865784499,
            0.9017013232514177,
            0.8979206049149339,
            0.9735349716446124,
            0.9206049149338374,
            0.9262759924385633,
            0.9754253308128544,
            0.9300567107750473,
            0.9338374291115312,
            0.9716446124763705,
            0.9640831758034026,
            0.9735349716446124,
            0.9300567107750473,
            0.9489603024574669,
            0.9508506616257089,
            0.9130434782608695,
            0.9735349716446124,
            0.9508506616257089,
            0.9867674858223062,
            0.943289224952741,
            0.9735349716446124,
            0.9621928166351607,
            0.9243856332703214,
            0.9697542533081286,
            0.9395085066162571,
            0.9829867674858223,
            0.947069943289225,
            0.9905482041587902,
            0.9735349716446124,
            0.9886578449905482,
            0.8827977315689981,
            0.9905482041587902,
            0.9678638941398866,
            0.9735349716446124,
            0.9678638941398866,
            0.9867674858223062,
            0.9716446124763705,
            0.9565217391304348,
            0.998109640831758,
            0.998109640831758,
            0.998109640831758,
            0.996219281663516,
            1.0
        ],
        "reward_history": [
            -2950,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4585,
            -5290,
            -5290,
            -940,
            -5290,
            -2768,
            -2468,
            -3291,
            -5290,
            -2958,
            -2889,
            -5290,
            -5290,
            -5290,
            -2208,
            -5290,
            -5290,
            -1831,
            -4522,
            -3814,
            -4868,
            -1584,
            -3234,
            -5290,
            -5290,
            -889,
            -533,
            -4164,
            -3179,
            -1790,
            -2768,
            -3424,
            -4470,
            -4191,
            -5290,
            -3037,
            -4065,
            -1519,
            -5290,
            -1827,
            -758,
            -2420,
            -1752,
            -1083,
            -872,
            -789,
            -2054,
            -1653,
            -1835,
            -5290,
            -872,
            -2118,
            -884,
            -2514,
            -678,
            -1208,
            -4025,
            -1789,
            -2125,
            -1560,
            -1689,
            -529,
            -1249,
            -1123,
            -5290,
            -1055,
            -1453,
            -1369,
            -2241,
            -1295,
            -1725,
            -2448,
            -366,
            -1201,
            -671,
            -882,
            -876
        ],
        "steps_history": [
            3051,
            5290,
            5290,
            5290,
            5290,
            5290,
            4686,
            5290,
            5290,
            1041,
            5290,
            2869,
            2569,
            3392,
            5290,
            3059,
            2990,
            5290,
            5290,
            5290,
            2309,
            5290,
            5290,
            1932,
            4623,
            3915,
            4969,
            1685,
            3335,
            5290,
            5290,
            990,
            634,
            4265,
            3280,
            1891,
            2869,
            3525,
            4571,
            4292,
            5290,
            3138,
            4166,
            1620,
            5290,
            1928,
            859,
            2521,
            1853,
            1184,
            973,
            890,
            2155,
            1754,
            1936,
            5290,
            973,
            2219,
            985,
            2615,
            779,
            1309,
            4126,
            1890,
            2226,
            1661,
            1790,
            630,
            1350,
            1224,
            5290,
            1156,
            1554,
            1470,
            2342,
            1396,
            1826,
            2549,
            467,
            1302,
            772,
            983,
            977
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "744/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 18.23127818107605,
        "final_policy_stability": 0.996219281663516,
        "episodes_to_convergence": 92,
        "policy_stability_history": [
            0.0,
            0.8355387523629489,
            0.8638941398865785,
            0.888468809073724,
            0.8790170132325141,
            0.8695652173913043,
            0.8601134215500945,
            0.8412098298676749,
            0.8260869565217391,
            0.8979206049149339,
            0.9168241965973535,
            0.8582230623818525,
            0.8790170132325141,
            0.8752362948960303,
            0.8695652173913043,
            0.8809073724007561,
            0.9073724007561437,
            0.8733459357277883,
            0.8809073724007561,
            0.8657844990548205,
            0.8601134215500945,
            0.8941398865784499,
            0.8733459357277883,
            0.9281663516068053,
            0.8903591682419659,
            0.9206049149338374,
            0.8374291115311909,
            0.9206049149338374,
            0.8733459357277883,
            0.9206049149338374,
            0.943289224952741,
            0.8714555765595463,
            0.9546313799621928,
            0.8657844990548205,
            0.9017013232514177,
            0.9584120982986768,
            0.8601134215500945,
            0.9338374291115312,
            0.9281663516068053,
            0.9357277882797732,
            0.9243856332703214,
            0.9376181474480151,
            0.9035916824196597,
            0.9584120982986768,
            0.9659735349716446,
            0.8998109640831758,
            0.945179584120983,
            0.888468809073724,
            0.9621928166351607,
            0.9489603024574669,
            0.9659735349716446,
            0.9206049149338374,
            0.8979206049149339,
            0.9508506616257089,
            0.9584120982986768,
            0.9659735349716446,
            0.9678638941398866,
            0.9754253308128544,
            0.9395085066162571,
            0.8790170132325141,
            0.8941398865784499,
            0.9584120982986768,
            0.9886578449905482,
            0.9697542533081286,
            0.9035916824196597,
            0.947069943289225,
            0.9716446124763705,
            0.9810964083175804,
            0.9848771266540642,
            0.9546313799621928,
            0.8752362948960303,
            0.943289224952741,
            0.9886578449905482,
            0.9395085066162571,
            0.9886578449905482,
            0.9697542533081286,
            0.9867674858223062,
            0.9867674858223062,
            0.9924385633270322,
            0.9697542533081286,
            0.9584120982986768,
            0.9886578449905482,
            0.9716446124763705,
            1.0,
            0.996219281663516,
            0.998109640831758,
            0.998109640831758,
            0.947069943289225,
            0.9829867674858223,
            0.9810964083175804,
            0.947069943289225,
            0.9829867674858223,
            0.996219281663516
        ],
        "reward_history": [
            -4165,
            -5290,
            -5290,
            -5290,
            -2925,
            -5290,
            -5290,
            -4720,
            -5290,
            -3121,
            -1752,
            -5290,
            -4121,
            -5290,
            -5290,
            -5290,
            -2253,
            -3785,
            -5290,
            -3093,
            -3935,
            -5290,
            -5290,
            -1374,
            -5290,
            -5290,
            -5028,
            -1692,
            -5290,
            -2018,
            -1233,
            -4056,
            -1092,
            -5290,
            -5290,
            -793,
            -5290,
            -1400,
            -2160,
            -2159,
            -2319,
            -1879,
            -3872,
            -1031,
            -592,
            -4098,
            -998,
            -5290,
            -757,
            -1559,
            -1198,
            -5290,
            -3852,
            -1702,
            -1271,
            -936,
            -717,
            -1046,
            -1608,
            -5290,
            -5290,
            -2025,
            -588,
            -1010,
            -5290,
            -2299,
            -989,
            -998,
            -770,
            -1950,
            -4205,
            -1511,
            -506,
            -4166,
            -596,
            -1377,
            -1118,
            -1360,
            -895,
            -2728,
            -1188,
            -472,
            -1499,
            -1334,
            -1080,
            -886,
            -540,
            -3970,
            -1095,
            -2062,
            -2915,
            -1037,
            -949
        ],
        "steps_history": [
            4266,
            5290,
            5290,
            5290,
            3026,
            5290,
            5290,
            4821,
            5290,
            3222,
            1853,
            5290,
            4222,
            5290,
            5290,
            5290,
            2354,
            3886,
            5290,
            3194,
            4036,
            5290,
            5290,
            1475,
            5290,
            5290,
            5129,
            1793,
            5290,
            2119,
            1334,
            4157,
            1193,
            5290,
            5290,
            894,
            5290,
            1501,
            2261,
            2260,
            2420,
            1980,
            3973,
            1132,
            693,
            4199,
            1099,
            5290,
            858,
            1660,
            1299,
            5290,
            3953,
            1803,
            1372,
            1037,
            818,
            1147,
            1709,
            5290,
            5290,
            2126,
            689,
            1111,
            5290,
            2400,
            1090,
            1099,
            871,
            2051,
            4306,
            1612,
            607,
            4267,
            697,
            1478,
            1219,
            1461,
            996,
            2829,
            1289,
            573,
            1600,
            1435,
            1181,
            987,
            641,
            4071,
            1196,
            2163,
            3016,
            1138,
            1050
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "745/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 17.59516215324402,
        "final_policy_stability": 0.9792060491493384,
        "episodes_to_convergence": 89,
        "policy_stability_history": [
            0.0,
            0.8714555765595463,
            0.8109640831758034,
            0.8298676748582231,
            0.9035916824196597,
            0.8506616257088847,
            0.8695652173913043,
            0.8355387523629489,
            0.8468809073724007,
            0.8393194706994329,
            0.8374291115311909,
            0.8676748582230623,
            0.9395085066162571,
            0.8676748582230623,
            0.8223062381852552,
            0.8922495274102079,
            0.8865784499054821,
            0.8960302457466919,
            0.8865784499054821,
            0.888468809073724,
            0.8979206049149339,
            0.9054820415879017,
            0.9111531190926276,
            0.9376181474480151,
            0.8771266540642723,
            0.9319470699432892,
            0.8865784499054821,
            0.8733459357277883,
            0.945179584120983,
            0.8638941398865785,
            0.8714555765595463,
            0.8355387523629489,
            0.8657844990548205,
            0.9584120982986768,
            0.9584120982986768,
            0.8752362948960303,
            0.9035916824196597,
            0.9659735349716446,
            0.9206049149338374,
            0.9546313799621928,
            0.9319470699432892,
            0.9735349716446124,
            0.9073724007561437,
            0.9565217391304348,
            0.8846880907372401,
            0.9489603024574669,
            0.9111531190926276,
            0.945179584120983,
            0.9678638941398866,
            0.9035916824196597,
            0.9640831758034026,
            0.9017013232514177,
            0.9886578449905482,
            0.9810964083175804,
            0.9848771266540642,
            0.9621928166351607,
            0.9867674858223062,
            0.9527410207939508,
            0.9603024574669187,
            0.9716446124763705,
            0.9773156899810964,
            0.9603024574669187,
            0.9792060491493384,
            0.9886578449905482,
            0.9735349716446124,
            0.9773156899810964,
            0.9187145557655955,
            0.9319470699432892,
            0.9886578449905482,
            0.9508506616257089,
            0.9678638941398866,
            0.9924385633270322,
            0.994328922495274,
            0.9678638941398866,
            0.9338374291115312,
            0.998109640831758,
            0.998109640831758,
            0.9792060491493384,
            0.9867674858223062,
            0.9187145557655955,
            0.9905482041587902,
            0.9886578449905482,
            0.9546313799621928,
            0.9905482041587902,
            0.994328922495274,
            0.998109640831758,
            0.9924385633270322,
            0.994328922495274,
            0.9697542533081286,
            0.9792060491493384
        ],
        "reward_history": [
            -5290,
            -5290,
            -3907,
            -5290,
            -5290,
            -4298,
            -2051,
            -5290,
            -3779,
            -5290,
            -5290,
            -4131,
            -867,
            -5290,
            -5290,
            -2620,
            -5290,
            -1853,
            -2410,
            -3154,
            -2644,
            -2009,
            -1980,
            -924,
            -5290,
            -2158,
            -4241,
            -2995,
            -720,
            -5290,
            -5290,
            -5290,
            -4489,
            -668,
            -833,
            -5290,
            -4263,
            -701,
            -1407,
            -1172,
            -2211,
            -397,
            -4201,
            -946,
            -3303,
            -1615,
            -5290,
            -2140,
            -855,
            -3104,
            -731,
            -5290,
            -458,
            -785,
            -712,
            -1488,
            -865,
            -1399,
            -1253,
            -881,
            -871,
            -1426,
            -656,
            -468,
            -860,
            -428,
            -3181,
            -2225,
            -573,
            -2610,
            -1579,
            -1309,
            -579,
            -1282,
            -2101,
            -578,
            -706,
            -1115,
            -1126,
            -5290,
            -653,
            -486,
            -1528,
            -890,
            -733,
            -424,
            -503,
            -799,
            -1509,
            -1473
        ],
        "steps_history": [
            5290,
            5290,
            4008,
            5290,
            5290,
            4399,
            2152,
            5290,
            3880,
            5290,
            5290,
            4232,
            968,
            5290,
            5290,
            2721,
            5290,
            1954,
            2511,
            3255,
            2745,
            2110,
            2081,
            1025,
            5290,
            2259,
            4342,
            3096,
            821,
            5290,
            5290,
            5290,
            4590,
            769,
            934,
            5290,
            4364,
            802,
            1508,
            1273,
            2312,
            498,
            4302,
            1047,
            3404,
            1716,
            5290,
            2241,
            956,
            3205,
            832,
            5290,
            559,
            886,
            813,
            1589,
            966,
            1500,
            1354,
            982,
            972,
            1527,
            757,
            569,
            961,
            529,
            3282,
            2326,
            674,
            2711,
            1680,
            1410,
            680,
            1383,
            2202,
            679,
            807,
            1216,
            1227,
            5290,
            754,
            587,
            1629,
            991,
            834,
            525,
            604,
            900,
            1610,
            1574
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "746/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 16.972655057907104,
        "final_policy_stability": 0.998109640831758,
        "episodes_to_convergence": 88,
        "policy_stability_history": [
            0.0,
            0.8166351606805293,
            0.8752362948960303,
            0.9017013232514177,
            0.8846880907372401,
            0.8544423440453687,
            0.8374291115311909,
            0.8582230623818525,
            0.8960302457466919,
            0.8544423440453687,
            0.8071833648393195,
            0.8298676748582231,
            0.8657844990548205,
            0.8298676748582231,
            0.8563327032136105,
            0.8714555765595463,
            0.8865784499054821,
            0.8676748582230623,
            0.8979206049149339,
            0.9338374291115312,
            0.8620037807183365,
            0.9111531190926276,
            0.8865784499054821,
            0.8809073724007561,
            0.8941398865784499,
            0.9262759924385633,
            0.9489603024574669,
            0.9073724007561437,
            0.8695652173913043,
            0.8979206049149339,
            0.8771266540642723,
            0.8525519848771267,
            0.8827977315689981,
            0.9508506616257089,
            0.8941398865784499,
            0.9527410207939508,
            0.9281663516068053,
            0.8431001890359168,
            0.8998109640831758,
            0.9546313799621928,
            0.8657844990548205,
            0.8941398865784499,
            0.9829867674858223,
            0.9565217391304348,
            0.9735349716446124,
            0.8827977315689981,
            0.9773156899810964,
            0.9489603024574669,
            0.9773156899810964,
            0.941398865784499,
            0.9073724007561437,
            0.8563327032136105,
            0.9621928166351607,
            0.9224952741020794,
            0.9546313799621928,
            0.9300567107750473,
            0.9792060491493384,
            0.9678638941398866,
            0.9697542533081286,
            0.9262759924385633,
            0.9773156899810964,
            0.9659735349716446,
            0.941398865784499,
            0.947069943289225,
            0.9735349716446124,
            0.9508506616257089,
            0.9735349716446124,
            0.9640831758034026,
            0.8941398865784499,
            0.9886578449905482,
            0.9319470699432892,
            0.9584120982986768,
            0.9792060491493384,
            0.9792060491493384,
            0.9716446124763705,
            0.9867674858223062,
            0.9905482041587902,
            0.9886578449905482,
            0.9489603024574669,
            0.9697542533081286,
            0.9603024574669187,
            0.996219281663516,
            0.947069943289225,
            0.994328922495274,
            0.996219281663516,
            0.998109640831758,
            0.9792060491493384,
            1.0,
            0.998109640831758
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -780,
            -5290,
            -5290,
            -5290,
            -5290,
            -2110,
            -5290,
            -4361,
            -5290,
            -5290,
            -4810,
            -2900,
            -2821,
            -5290,
            -5290,
            -2033,
            -1368,
            -5290,
            -5290,
            -3191,
            -5290,
            -2865,
            -1211,
            -940,
            -1253,
            -5290,
            -2416,
            -4341,
            -5290,
            -3458,
            -920,
            -2239,
            -568,
            -1677,
            -5290,
            -2509,
            -811,
            -5290,
            -5290,
            -376,
            -865,
            -1220,
            -4258,
            -449,
            -1148,
            -446,
            -1174,
            -4521,
            -5290,
            -1279,
            -2246,
            -1256,
            -2166,
            -511,
            -940,
            -728,
            -1892,
            -1079,
            -925,
            -2137,
            -2416,
            -693,
            -1578,
            -926,
            -1170,
            -3163,
            -422,
            -3103,
            -1626,
            -882,
            -340,
            -1002,
            -692,
            -458,
            -1077,
            -1561,
            -536,
            -1659,
            -842,
            -1999,
            -775,
            -494,
            -444,
            -981,
            -435,
            -762
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            881,
            5290,
            5290,
            5290,
            5290,
            2211,
            5290,
            4462,
            5290,
            5290,
            4911,
            3001,
            2922,
            5290,
            5290,
            2134,
            1469,
            5290,
            5290,
            3292,
            5290,
            2966,
            1312,
            1041,
            1354,
            5290,
            2517,
            4442,
            5290,
            3559,
            1021,
            2340,
            669,
            1778,
            5290,
            2610,
            912,
            5290,
            5290,
            477,
            966,
            1321,
            4359,
            550,
            1249,
            547,
            1275,
            4622,
            5290,
            1380,
            2347,
            1357,
            2267,
            612,
            1041,
            829,
            1993,
            1180,
            1026,
            2238,
            2517,
            794,
            1679,
            1027,
            1271,
            3264,
            523,
            3204,
            1727,
            983,
            441,
            1103,
            793,
            559,
            1178,
            1662,
            637,
            1760,
            943,
            2100,
            876,
            595,
            545,
            1082,
            536,
            863
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "747/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 17.00838804244995,
        "final_policy_stability": 0.9924385633270322,
        "episodes_to_convergence": 87,
        "policy_stability_history": [
            0.0,
            0.831758034026465,
            0.9092627599243857,
            0.8468809073724007,
            0.8468809073724007,
            0.8090737240075614,
            0.8582230623818525,
            0.8374291115311909,
            0.888468809073724,
            0.8166351606805293,
            0.8393194706994329,
            0.8620037807183365,
            0.8620037807183365,
            0.8922495274102079,
            0.8449905482041588,
            0.8241965973534972,
            0.8525519848771267,
            0.8827977315689981,
            0.9224952741020794,
            0.947069943289225,
            0.8355387523629489,
            0.9357277882797732,
            0.9187145557655955,
            0.8714555765595463,
            0.8714555765595463,
            0.8676748582230623,
            0.8563327032136105,
            0.9224952741020794,
            0.8771266540642723,
            0.8638941398865785,
            0.9338374291115312,
            0.9508506616257089,
            0.9338374291115312,
            0.8487712665406427,
            0.9489603024574669,
            0.8582230623818525,
            0.8279773156899811,
            0.888468809073724,
            0.9395085066162571,
            0.9376181474480151,
            0.8714555765595463,
            0.9338374291115312,
            0.9678638941398866,
            0.9111531190926276,
            0.9735349716446124,
            0.9054820415879017,
            0.8998109640831758,
            0.9603024574669187,
            0.941398865784499,
            0.833648393194707,
            0.9338374291115312,
            0.9848771266540642,
            0.9716446124763705,
            0.9338374291115312,
            0.9243856332703214,
            0.9659735349716446,
            0.9621928166351607,
            0.9773156899810964,
            0.9527410207939508,
            0.9754253308128544,
            0.9357277882797732,
            0.9659735349716446,
            0.9792060491493384,
            0.9754253308128544,
            0.9829867674858223,
            0.9773156899810964,
            0.9678638941398866,
            0.941398865784499,
            0.9300567107750473,
            0.9924385633270322,
            0.9829867674858223,
            0.9716446124763705,
            0.9565217391304348,
            0.9754253308128544,
            0.9697542533081286,
            0.994328922495274,
            0.9054820415879017,
            0.9621928166351607,
            1.0,
            0.9886578449905482,
            0.9281663516068053,
            0.9848771266540642,
            0.9848771266540642,
            0.9716446124763705,
            0.996219281663516,
            0.996219281663516,
            0.996219281663516,
            0.9924385633270322
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -4542,
            -3743,
            -5290,
            -5290,
            -5290,
            -2566,
            -4951,
            -5290,
            -5290,
            -2618,
            -5290,
            -5290,
            -5290,
            -4511,
            -5290,
            -1286,
            -1157,
            -4777,
            -1075,
            -2321,
            -5290,
            -3717,
            -3415,
            -5290,
            -1521,
            -1818,
            -3818,
            -1404,
            -602,
            -5290,
            -4549,
            -664,
            -5290,
            -4446,
            -3120,
            -1421,
            -1449,
            -4575,
            -1659,
            -931,
            -2072,
            -405,
            -2507,
            -3344,
            -678,
            -1217,
            -5290,
            -1651,
            -323,
            -1176,
            -1994,
            -4532,
            -862,
            -785,
            -476,
            -1417,
            -707,
            -2413,
            -983,
            -843,
            -949,
            -584,
            -445,
            -1067,
            -2772,
            -2344,
            -612,
            -902,
            -1234,
            -1573,
            -1021,
            -1207,
            -644,
            -4312,
            -1791,
            -683,
            -522,
            -2167,
            -1450,
            -666,
            -1169,
            -568,
            -686,
            -580,
            -1417
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            4643,
            3844,
            5290,
            5290,
            5290,
            2667,
            5052,
            5290,
            5290,
            2719,
            5290,
            5290,
            5290,
            4612,
            5290,
            1387,
            1258,
            4878,
            1176,
            2422,
            5290,
            3818,
            3516,
            5290,
            1622,
            1919,
            3919,
            1505,
            703,
            5290,
            4650,
            765,
            5290,
            4547,
            3221,
            1522,
            1550,
            4676,
            1760,
            1032,
            2173,
            506,
            2608,
            3445,
            779,
            1318,
            5290,
            1752,
            424,
            1277,
            2095,
            4633,
            963,
            886,
            577,
            1518,
            808,
            2514,
            1084,
            944,
            1050,
            685,
            546,
            1168,
            2873,
            2445,
            713,
            1003,
            1335,
            1674,
            1122,
            1308,
            745,
            4413,
            1892,
            784,
            623,
            2268,
            1551,
            767,
            1270,
            669,
            787,
            681,
            1518
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "748/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 18.7501540184021,
        "final_policy_stability": 0.9792060491493384,
        "episodes_to_convergence": 97,
        "policy_stability_history": [
            0.0,
            0.8506616257088847,
            0.9035916824196597,
            0.8733459357277883,
            0.8865784499054821,
            0.888468809073724,
            0.8620037807183365,
            0.8393194706994329,
            0.8563327032136105,
            0.8657844990548205,
            0.8279773156899811,
            0.8582230623818525,
            0.8998109640831758,
            0.9130434782608695,
            0.8752362948960303,
            0.8431001890359168,
            0.9206049149338374,
            0.8979206049149339,
            0.9243856332703214,
            0.9357277882797732,
            0.8431001890359168,
            0.8733459357277883,
            0.9187145557655955,
            0.8790170132325141,
            0.9262759924385633,
            0.9130434782608695,
            0.947069943289225,
            0.888468809073724,
            0.8903591682419659,
            0.941398865784499,
            0.9187145557655955,
            0.8865784499054821,
            0.945179584120983,
            0.9338374291115312,
            0.9603024574669187,
            0.9508506616257089,
            0.8865784499054821,
            0.9508506616257089,
            0.9243856332703214,
            0.9659735349716446,
            0.9035916824196597,
            0.8922495274102079,
            0.9281663516068053,
            0.9508506616257089,
            0.9716446124763705,
            0.9187145557655955,
            0.9262759924385633,
            0.9546313799621928,
            0.9187145557655955,
            0.9640831758034026,
            0.9754253308128544,
            0.9376181474480151,
            0.9489603024574669,
            0.9829867674858223,
            0.9527410207939508,
            0.943289224952741,
            0.9546313799621928,
            0.8979206049149339,
            0.9621928166351607,
            0.9603024574669187,
            0.9792060491493384,
            0.8865784499054821,
            0.9697542533081286,
            0.9565217391304348,
            0.9810964083175804,
            0.947069943289225,
            0.9678638941398866,
            0.9678638941398866,
            0.9754253308128544,
            0.9546313799621928,
            0.9735349716446124,
            0.9697542533081286,
            0.9659735349716446,
            0.9848771266540642,
            0.9508506616257089,
            0.9848771266540642,
            0.9678638941398866,
            0.9829867674858223,
            0.996219281663516,
            0.9357277882797732,
            0.945179584120983,
            0.9829867674858223,
            0.9546313799621928,
            0.9754253308128544,
            0.9886578449905482,
            0.996219281663516,
            0.9584120982986768,
            0.994328922495274,
            0.9886578449905482,
            0.996219281663516,
            0.9281663516068053,
            0.9603024574669187,
            0.9829867674858223,
            0.998109640831758,
            0.9905482041587902,
            1.0,
            0.9924385633270322,
            0.9792060491493384
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -2967,
            -5290,
            -2214,
            -5290,
            -5290,
            -5290,
            -4381,
            -5290,
            -5290,
            -1768,
            -5290,
            -5290,
            -5290,
            -1644,
            -4788,
            -1329,
            -1154,
            -4185,
            -3389,
            -1540,
            -5290,
            -1203,
            -1806,
            -1189,
            -3695,
            -2850,
            -671,
            -2477,
            -5290,
            -908,
            -5290,
            -493,
            -682,
            -2027,
            -970,
            -2172,
            -883,
            -4692,
            -2552,
            -3165,
            -1054,
            -746,
            -1812,
            -2725,
            -1806,
            -5290,
            -1620,
            -593,
            -1847,
            -2232,
            -416,
            -1449,
            -1123,
            -1397,
            -5290,
            -922,
            -1488,
            -810,
            -4602,
            -1172,
            -1598,
            -541,
            -3277,
            -863,
            -1148,
            -806,
            -1032,
            -564,
            -817,
            -1257,
            -434,
            -2026,
            -664,
            -722,
            -980,
            -661,
            -2532,
            -4963,
            -470,
            -1517,
            -1388,
            -831,
            -438,
            -1380,
            -409,
            -1505,
            -590,
            -3542,
            -2422,
            -744,
            -434,
            -823,
            -538,
            -772,
            -1153
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            3068,
            5290,
            2315,
            5290,
            5290,
            5290,
            4482,
            5290,
            5290,
            1869,
            5290,
            5290,
            5290,
            1745,
            4889,
            1430,
            1255,
            4286,
            3490,
            1641,
            5290,
            1304,
            1907,
            1290,
            3796,
            2951,
            772,
            2578,
            5290,
            1009,
            5290,
            594,
            783,
            2128,
            1071,
            2273,
            984,
            4793,
            2653,
            3266,
            1155,
            847,
            1913,
            2826,
            1907,
            5290,
            1721,
            694,
            1948,
            2333,
            517,
            1550,
            1224,
            1498,
            5290,
            1023,
            1589,
            911,
            4703,
            1273,
            1699,
            642,
            3378,
            964,
            1249,
            907,
            1133,
            665,
            918,
            1358,
            535,
            2127,
            765,
            823,
            1081,
            762,
            2633,
            5064,
            571,
            1618,
            1489,
            932,
            539,
            1481,
            510,
            1606,
            691,
            3643,
            2523,
            845,
            535,
            924,
            639,
            873,
            1254
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "749/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 17.877267360687256,
        "final_policy_stability": 0.9829867674858223,
        "episodes_to_convergence": 92,
        "policy_stability_history": [
            0.0,
            0.8147448015122873,
            0.8355387523629489,
            0.8544423440453687,
            0.8355387523629489,
            0.8903591682419659,
            0.8903591682419659,
            0.888468809073724,
            0.8695652173913043,
            0.8393194706994329,
            0.8979206049149339,
            0.8620037807183365,
            0.888468809073724,
            0.8657844990548205,
            0.8695652173913043,
            0.8657844990548205,
            0.9054820415879017,
            0.8695652173913043,
            0.947069943289225,
            0.8846880907372401,
            0.8903591682419659,
            0.9565217391304348,
            0.8941398865784499,
            0.8846880907372401,
            0.8695652173913043,
            0.8582230623818525,
            0.8998109640831758,
            0.8431001890359168,
            0.8695652173913043,
            0.9584120982986768,
            0.8998109640831758,
            0.9054820415879017,
            0.8752362948960303,
            0.9130434782608695,
            0.9262759924385633,
            0.9054820415879017,
            0.9243856332703214,
            0.9243856332703214,
            0.9640831758034026,
            0.947069943289225,
            0.9357277882797732,
            0.9319470699432892,
            0.8393194706994329,
            0.9754253308128544,
            0.9640831758034026,
            0.9338374291115312,
            0.9168241965973535,
            0.9073724007561437,
            0.945179584120983,
            0.943289224952741,
            0.9754253308128544,
            0.9716446124763705,
            0.9621928166351607,
            0.9376181474480151,
            0.9168241965973535,
            0.9584120982986768,
            0.9659735349716446,
            0.9243856332703214,
            0.9659735349716446,
            0.9281663516068053,
            0.9867674858223062,
            0.9621928166351607,
            0.9130434782608695,
            0.9716446124763705,
            0.9754253308128544,
            0.8790170132325141,
            0.9848771266540642,
            0.9584120982986768,
            0.9659735349716446,
            0.9584120982986768,
            0.9810964083175804,
            0.9792060491493384,
            0.9111531190926276,
            0.9848771266540642,
            0.9584120982986768,
            0.9527410207939508,
            0.9924385633270322,
            0.9924385633270322,
            0.9773156899810964,
            0.9546313799621928,
            0.9924385633270322,
            0.998109640831758,
            0.9867674858223062,
            0.996219281663516,
            0.994328922495274,
            0.9697542533081286,
            0.9924385633270322,
            0.9640831758034026,
            0.947069943289225,
            1.0,
            0.998109640831758,
            0.9810964083175804,
            0.9829867674858223
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5003,
            -2753,
            -5290,
            -5290,
            -5290,
            -3938,
            -2379,
            -5290,
            -2719,
            -3536,
            -5290,
            -5290,
            -1739,
            -4216,
            -613,
            -4487,
            -5290,
            -708,
            -2917,
            -5290,
            -5290,
            -4730,
            -2242,
            -5290,
            -5290,
            -889,
            -2919,
            -1909,
            -5290,
            -2487,
            -1187,
            -5290,
            -2100,
            -1443,
            -966,
            -761,
            -1961,
            -997,
            -5290,
            -661,
            -595,
            -2031,
            -1934,
            -3180,
            -1679,
            -939,
            -695,
            -581,
            -541,
            -1590,
            -2677,
            -1105,
            -1142,
            -4524,
            -1043,
            -1395,
            -766,
            -1040,
            -5290,
            -1043,
            -732,
            -3446,
            -587,
            -2055,
            -2042,
            -1264,
            -1113,
            -783,
            -3192,
            -777,
            -2751,
            -1706,
            -412,
            -535,
            -886,
            -1375,
            -669,
            -777,
            -1055,
            -1143,
            -536,
            -1098,
            -770,
            -1487,
            -2866,
            -525,
            -544,
            -916,
            -1555
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5104,
            2854,
            5290,
            5290,
            5290,
            4039,
            2480,
            5290,
            2820,
            3637,
            5290,
            5290,
            1840,
            4317,
            714,
            4588,
            5290,
            809,
            3018,
            5290,
            5290,
            4831,
            2343,
            5290,
            5290,
            990,
            3020,
            2010,
            5290,
            2588,
            1288,
            5290,
            2201,
            1544,
            1067,
            862,
            2062,
            1098,
            5290,
            762,
            696,
            2132,
            2035,
            3281,
            1780,
            1040,
            796,
            682,
            642,
            1691,
            2778,
            1206,
            1243,
            4625,
            1144,
            1496,
            867,
            1141,
            5290,
            1144,
            833,
            3547,
            688,
            2156,
            2143,
            1365,
            1214,
            884,
            3293,
            878,
            2852,
            1807,
            513,
            636,
            987,
            1476,
            770,
            878,
            1156,
            1244,
            637,
            1199,
            871,
            1588,
            2967,
            626,
            645,
            1017,
            1656
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "750/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.2_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.319523096084595,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 58,
        "policy_stability_history": [
            0.0,
            0.8355387523629489,
            0.8903591682419659,
            0.8941398865784499,
            0.8506616257088847,
            0.8979206049149339,
            0.8979206049149339,
            0.8431001890359168,
            0.8733459357277883,
            0.8449905482041588,
            0.8506616257088847,
            0.8620037807183365,
            0.8752362948960303,
            0.888468809073724,
            0.8695652173913043,
            0.9243856332703214,
            0.9111531190926276,
            0.9130434782608695,
            0.8790170132325141,
            0.8827977315689981,
            0.888468809073724,
            0.8941398865784499,
            0.9262759924385633,
            0.9357277882797732,
            0.945179584120983,
            0.8657844990548205,
            0.9489603024574669,
            0.8620037807183365,
            0.9262759924385633,
            0.9565217391304348,
            0.9376181474480151,
            0.8846880907372401,
            0.943289224952741,
            0.9716446124763705,
            0.9149338374291115,
            0.8714555765595463,
            0.9886578449905482,
            0.9659735349716446,
            0.9300567107750473,
            0.9810964083175804,
            0.9603024574669187,
            0.9017013232514177,
            0.9489603024574669,
            0.943289224952741,
            0.9716446124763705,
            0.9697542533081286,
            0.9187145557655955,
            0.9603024574669187,
            0.9149338374291115,
            0.9886578449905482,
            0.996219281663516,
            0.9848771266540642,
            0.9149338374291115,
            0.9848771266540642,
            0.9924385633270322,
            0.994328922495274,
            0.9924385633270322,
            0.9773156899810964,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5183,
            -5290,
            -5290,
            -5290,
            -5011,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -1729,
            -1402,
            -5290,
            -5290,
            -5290,
            -3623,
            -5290,
            -2256,
            -2594,
            -2562,
            -5290,
            -1127,
            -3699,
            -2523,
            -772,
            -2242,
            -5290,
            -2571,
            -848,
            -5290,
            -5290,
            -785,
            -2080,
            -3890,
            -1469,
            -1722,
            -3324,
            -2398,
            -1647,
            -1135,
            -1260,
            -5290,
            -2514,
            -3177,
            -1994,
            -804,
            -1392,
            -5290,
            -2119,
            -1206,
            -1146,
            -1952,
            -1378,
            -859
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5284,
            5290,
            5290,
            5290,
            5112,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            1830,
            1503,
            5290,
            5290,
            5290,
            3724,
            5290,
            2357,
            2695,
            2663,
            5290,
            1228,
            3800,
            2624,
            873,
            2343,
            5290,
            2672,
            949,
            5290,
            5290,
            886,
            2181,
            3991,
            1570,
            1823,
            3425,
            2499,
            1748,
            1236,
            1361,
            5290,
            2615,
            3278,
            2095,
            905,
            1493,
            5290,
            2220,
            1307,
            1247,
            2053,
            1479,
            960
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "751/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.357414484024048,
        "final_policy_stability": 0.998109640831758,
        "episodes_to_convergence": 53,
        "policy_stability_history": [
            0.0,
            0.8071833648393195,
            0.9017013232514177,
            0.8960302457466919,
            0.8941398865784499,
            0.8695652173913043,
            0.8676748582230623,
            0.8714555765595463,
            0.9092627599243857,
            0.9149338374291115,
            0.8431001890359168,
            0.8827977315689981,
            0.8941398865784499,
            0.9149338374291115,
            0.8752362948960303,
            0.8449905482041588,
            0.9262759924385633,
            0.8998109640831758,
            0.8922495274102079,
            0.941398865784499,
            0.8298676748582231,
            0.9054820415879017,
            0.9111531190926276,
            0.9376181474480151,
            0.888468809073724,
            0.8790170132325141,
            0.8638941398865785,
            0.8695652173913043,
            0.8903591682419659,
            0.8922495274102079,
            0.9621928166351607,
            0.9584120982986768,
            0.9168241965973535,
            0.9243856332703214,
            0.9206049149338374,
            0.8960302457466919,
            0.9168241965973535,
            0.9905482041587902,
            0.9376181474480151,
            0.9659735349716446,
            0.9716446124763705,
            0.9130434782608695,
            0.9924385633270322,
            0.9584120982986768,
            0.996219281663516,
            0.998109640831758,
            1.0,
            0.9867674858223062,
            1.0,
            0.9773156899810964,
            0.9924385633270322,
            0.9829867674858223,
            1.0,
            0.998109640831758
        ],
        "reward_history": [
            -5290,
            -4092,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -3579,
            -5290,
            -5290,
            -4653,
            -3688,
            -2617,
            -2454,
            -5290,
            -5018,
            -1242,
            -3226,
            -5290,
            -1846,
            -5290,
            -2506,
            -2614,
            -3445,
            -5290,
            -5290,
            -5290,
            -3555,
            -4410,
            -5290,
            -1477,
            -2177,
            -3367,
            -2563,
            -3497,
            -5290,
            -4421,
            -628,
            -4148,
            -3170,
            -1241,
            -5290,
            -1803,
            -5290,
            -887,
            -1447,
            -1509,
            -1985,
            -760,
            -1304,
            -705,
            -1533,
            -1128,
            -1777
        ],
        "steps_history": [
            5290,
            4193,
            5290,
            5290,
            5290,
            5290,
            5290,
            3680,
            5290,
            5290,
            4754,
            3789,
            2718,
            2555,
            5290,
            5119,
            1343,
            3327,
            5290,
            1947,
            5290,
            2607,
            2715,
            3546,
            5290,
            5290,
            5290,
            3656,
            4511,
            5290,
            1578,
            2278,
            3468,
            2664,
            3598,
            5290,
            4522,
            729,
            4249,
            3271,
            1342,
            5290,
            1904,
            5290,
            988,
            1548,
            1610,
            2086,
            861,
            1405,
            806,
            1634,
            1229,
            1878
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "752/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.898274898529053,
        "final_policy_stability": 0.9754253308128544,
        "episodes_to_convergence": 53,
        "policy_stability_history": [
            0.0,
            0.8185255198487713,
            0.8582230623818525,
            0.8147448015122873,
            0.8449905482041588,
            0.8809073724007561,
            0.8582230623818525,
            0.8733459357277883,
            0.8468809073724007,
            0.8487712665406427,
            0.8563327032136105,
            0.8601134215500945,
            0.9206049149338374,
            0.8412098298676749,
            0.9111531190926276,
            0.8998109640831758,
            0.8752362948960303,
            0.8998109640831758,
            0.8922495274102079,
            0.8374291115311909,
            0.8714555765595463,
            0.9073724007561437,
            0.9300567107750473,
            0.9508506616257089,
            0.9206049149338374,
            0.8601134215500945,
            0.9357277882797732,
            0.8827977315689981,
            0.943289224952741,
            0.9281663516068053,
            0.941398865784499,
            0.9281663516068053,
            0.9792060491493384,
            0.9376181474480151,
            0.9017013232514177,
            0.9754253308128544,
            0.947069943289225,
            0.9716446124763705,
            0.9678638941398866,
            0.8790170132325141,
            0.947069943289225,
            0.9697542533081286,
            0.943289224952741,
            0.9810964083175804,
            0.9659735349716446,
            0.9527410207939508,
            0.9527410207939508,
            0.9697542533081286,
            0.998109640831758,
            0.9810964083175804,
            0.9659735349716446,
            0.9848771266540642,
            0.9924385633270322,
            0.9754253308128544
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5103,
            -5290,
            -5290,
            -5290,
            -5290,
            -4617,
            -5290,
            -5290,
            -5290,
            -1529,
            -4720,
            -2053,
            -1775,
            -5290,
            -5290,
            -4023,
            -5290,
            -3895,
            -3541,
            -3718,
            -1617,
            -2296,
            -5290,
            -1733,
            -4061,
            -1873,
            -3706,
            -2452,
            -5290,
            -1271,
            -2678,
            -3487,
            -1088,
            -5290,
            -1178,
            -1858,
            -5290,
            -4796,
            -1896,
            -3120,
            -945,
            -1686,
            -2477,
            -1825,
            -1279,
            -1271,
            -769,
            -2377,
            -1692,
            -1306,
            -1435
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5204,
            5290,
            5290,
            5290,
            5290,
            4718,
            5290,
            5290,
            5290,
            1630,
            4821,
            2154,
            1876,
            5290,
            5290,
            4124,
            5290,
            3996,
            3642,
            3819,
            1718,
            2397,
            5290,
            1834,
            4162,
            1974,
            3807,
            2553,
            5290,
            1372,
            2779,
            3588,
            1189,
            5290,
            1279,
            1959,
            5290,
            4897,
            1997,
            3221,
            1046,
            1787,
            2578,
            1926,
            1380,
            1372,
            870,
            2478,
            1793,
            1407,
            1536
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "753/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.878392696380615,
        "final_policy_stability": 0.9886578449905482,
        "episodes_to_convergence": 50,
        "policy_stability_history": [
            0.0,
            0.8260869565217391,
            0.8204158790170132,
            0.8638941398865785,
            0.9224952741020794,
            0.9168241965973535,
            0.8809073724007561,
            0.8506616257088847,
            0.8582230623818525,
            0.9319470699432892,
            0.8563327032136105,
            0.9338374291115312,
            0.8941398865784499,
            0.8771266540642723,
            0.8960302457466919,
            0.9527410207939508,
            0.9054820415879017,
            0.8752362948960303,
            0.9224952741020794,
            0.9017013232514177,
            0.9243856332703214,
            0.8998109640831758,
            0.8979206049149339,
            0.9149338374291115,
            0.945179584120983,
            0.9319470699432892,
            0.947069943289225,
            0.9092627599243857,
            0.8771266540642723,
            0.9035916824196597,
            0.9659735349716446,
            0.8865784499054821,
            0.9716446124763705,
            0.8865784499054821,
            0.9395085066162571,
            0.9640831758034026,
            0.9319470699432892,
            0.9243856332703214,
            0.9054820415879017,
            0.9262759924385633,
            0.9281663516068053,
            0.9206049149338374,
            0.8960302457466919,
            0.943289224952741,
            0.9867674858223062,
            0.9640831758034026,
            0.9678638941398866,
            0.9924385633270322,
            0.8979206049149339,
            0.943289224952741,
            0.9886578449905482
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -2765,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -984,
            -5290,
            -2432,
            -5290,
            -5290,
            -3737,
            -871,
            -5290,
            -3552,
            -1869,
            -2392,
            -1819,
            -3922,
            -5290,
            -3042,
            -2118,
            -1411,
            -1705,
            -2739,
            -4268,
            -2443,
            -575,
            -5290,
            -1048,
            -4005,
            -2158,
            -856,
            -2726,
            -2045,
            -4936,
            -5290,
            -2419,
            -4586,
            -5156,
            -2899,
            -2320,
            -2080,
            -2317,
            -1155,
            -5290,
            -5290,
            -1680
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            2866,
            5290,
            5290,
            5290,
            5290,
            5290,
            1085,
            5290,
            2533,
            5290,
            5290,
            3838,
            972,
            5290,
            3653,
            1970,
            2493,
            1920,
            4023,
            5290,
            3143,
            2219,
            1512,
            1806,
            2840,
            4369,
            2544,
            676,
            5290,
            1149,
            4106,
            2259,
            957,
            2827,
            2146,
            5037,
            5290,
            2520,
            4687,
            5257,
            3000,
            2421,
            2181,
            2418,
            1256,
            5290,
            5290,
            1781
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "754/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.611736536026001,
        "final_policy_stability": 0.9810964083175804,
        "episodes_to_convergence": 52,
        "policy_stability_history": [
            0.0,
            0.8241965973534972,
            0.9395085066162571,
            0.8506616257088847,
            0.9073724007561437,
            0.8563327032136105,
            0.8393194706994329,
            0.8941398865784499,
            0.8714555765595463,
            0.8827977315689981,
            0.8941398865784499,
            0.9262759924385633,
            0.9395085066162571,
            0.9092627599243857,
            0.9054820415879017,
            0.8714555765595463,
            0.8695652173913043,
            0.8922495274102079,
            0.8922495274102079,
            0.888468809073724,
            0.8979206049149339,
            0.8790170132325141,
            0.8676748582230623,
            0.9584120982986768,
            0.9319470699432892,
            0.9017013232514177,
            0.9357277882797732,
            0.8790170132325141,
            0.9262759924385633,
            0.9584120982986768,
            0.8771266540642723,
            0.9735349716446124,
            0.8979206049149339,
            0.9527410207939508,
            0.9792060491493384,
            0.9716446124763705,
            0.9224952741020794,
            0.9243856332703214,
            0.9243856332703214,
            0.9754253308128544,
            0.9584120982986768,
            0.9168241965973535,
            0.9848771266540642,
            0.8998109640831758,
            0.9659735349716446,
            0.9338374291115312,
            0.9848771266540642,
            0.9319470699432892,
            0.9716446124763705,
            0.996219281663516,
            0.9848771266540642,
            0.947069943289225,
            0.9810964083175804
        ],
        "reward_history": [
            -5290,
            -4872,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2225,
            -5290,
            -5290,
            -5290,
            -1779,
            -5290,
            -5290,
            -5290,
            -5290,
            -3621,
            -5290,
            -3821,
            -5290,
            -5290,
            -4243,
            -4081,
            -566,
            -5290,
            -5290,
            -2866,
            -5290,
            -3046,
            -1620,
            -5290,
            -881,
            -5290,
            -1513,
            -836,
            -823,
            -1895,
            -3699,
            -4090,
            -1887,
            -1471,
            -5290,
            -760,
            -5290,
            -2114,
            -5290,
            -644,
            -4379,
            -1970,
            -673,
            -2087,
            -3488,
            -1295
        ],
        "steps_history": [
            5290,
            4973,
            5290,
            5290,
            5290,
            5290,
            5290,
            2326,
            5290,
            5290,
            5290,
            1880,
            5290,
            5290,
            5290,
            5290,
            3722,
            5290,
            3922,
            5290,
            5290,
            4344,
            4182,
            667,
            5290,
            5290,
            2967,
            5290,
            3147,
            1721,
            5290,
            982,
            5290,
            1614,
            937,
            924,
            1996,
            3800,
            4191,
            1988,
            1572,
            5290,
            861,
            5290,
            2215,
            5290,
            745,
            4480,
            2071,
            774,
            2188,
            3589,
            1396
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "755/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 14.271087646484375,
        "final_policy_stability": 0.9924385633270322,
        "episodes_to_convergence": 72,
        "policy_stability_history": [
            0.0,
            0.8147448015122873,
            0.8979206049149339,
            0.8525519848771267,
            0.8241965973534972,
            0.8903591682419659,
            0.8809073724007561,
            0.9224952741020794,
            0.8544423440453687,
            0.8846880907372401,
            0.8695652173913043,
            0.8695652173913043,
            0.8525519848771267,
            0.9319470699432892,
            0.8676748582230623,
            0.9206049149338374,
            0.8865784499054821,
            0.8393194706994329,
            0.9640831758034026,
            0.9187145557655955,
            0.8393194706994329,
            0.9300567107750473,
            0.9017013232514177,
            0.8827977315689981,
            0.9206049149338374,
            0.9092627599243857,
            0.947069943289225,
            0.8695652173913043,
            0.8960302457466919,
            0.9508506616257089,
            0.9073724007561437,
            0.9168241965973535,
            0.8657844990548205,
            0.9035916824196597,
            0.9130434782608695,
            0.9206049149338374,
            0.9508506616257089,
            0.9735349716446124,
            0.9149338374291115,
            0.9697542533081286,
            0.9017013232514177,
            0.9621928166351607,
            0.941398865784499,
            0.9810964083175804,
            0.9773156899810964,
            0.9754253308128544,
            0.9187145557655955,
            0.998109640831758,
            0.9810964083175804,
            0.9584120982986768,
            0.9489603024574669,
            0.9810964083175804,
            0.9754253308128544,
            0.998109640831758,
            0.994328922495274,
            0.9886578449905482,
            0.9867674858223062,
            0.9886578449905482,
            0.9508506616257089,
            0.996219281663516,
            0.996219281663516,
            0.994328922495274,
            0.994328922495274,
            0.9678638941398866,
            0.9735349716446124,
            1.0,
            0.9829867674858223,
            0.996219281663516,
            0.998109640831758,
            0.9867674858223062,
            0.9867674858223062,
            1.0,
            0.9924385633270322
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4542,
            -5290,
            -3692,
            -2851,
            -4111,
            -1330,
            -2844,
            -1928,
            -5290,
            -5290,
            -636,
            -1255,
            -5290,
            -2088,
            -1855,
            -4596,
            -5290,
            -1952,
            -1890,
            -3401,
            -5290,
            -1085,
            -3124,
            -2100,
            -5290,
            -5290,
            -3824,
            -4729,
            -1688,
            -1053,
            -3401,
            -1236,
            -2863,
            -1139,
            -1751,
            -772,
            -1358,
            -1255,
            -4204,
            -677,
            -1370,
            -1528,
            -1987,
            -1543,
            -1148,
            -489,
            -618,
            -521,
            -867,
            -1073,
            -1857,
            -986,
            -599,
            -1352,
            -860,
            -1184,
            -1298,
            -344,
            -734,
            -442,
            -633,
            -706,
            -873,
            -686,
            -1198
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            4643,
            5290,
            3793,
            2952,
            4212,
            1431,
            2945,
            2029,
            5290,
            5290,
            737,
            1356,
            5290,
            2189,
            1956,
            4697,
            5290,
            2053,
            1991,
            3502,
            5290,
            1186,
            3225,
            2201,
            5290,
            5290,
            3925,
            4830,
            1789,
            1154,
            3502,
            1337,
            2964,
            1240,
            1852,
            873,
            1459,
            1356,
            4305,
            778,
            1471,
            1629,
            2088,
            1644,
            1249,
            590,
            719,
            622,
            968,
            1174,
            1958,
            1087,
            700,
            1453,
            961,
            1285,
            1399,
            445,
            835,
            543,
            734,
            807,
            974,
            787,
            1299
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "756/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.856233835220337,
        "final_policy_stability": 0.9678638941398866,
        "episodes_to_convergence": 63,
        "policy_stability_history": [
            0.0,
            0.9092627599243857,
            0.8223062381852552,
            0.833648393194707,
            0.8771266540642723,
            0.8960302457466919,
            0.8657844990548205,
            0.8638941398865785,
            0.9111531190926276,
            0.8846880907372401,
            0.8827977315689981,
            0.8676748582230623,
            0.8374291115311909,
            0.888468809073724,
            0.8638941398865785,
            0.8752362948960303,
            0.8487712665406427,
            0.9017013232514177,
            0.8771266540642723,
            0.945179584120983,
            0.9281663516068053,
            0.8260869565217391,
            0.9017013232514177,
            0.8922495274102079,
            0.9054820415879017,
            0.9508506616257089,
            0.8865784499054821,
            0.9149338374291115,
            0.9017013232514177,
            0.9546313799621928,
            0.9489603024574669,
            0.9262759924385633,
            0.9376181474480151,
            0.9754253308128544,
            0.9073724007561437,
            0.9603024574669187,
            0.9659735349716446,
            0.9678638941398866,
            0.9243856332703214,
            0.9187145557655955,
            0.9168241965973535,
            0.9527410207939508,
            0.9092627599243857,
            0.945179584120983,
            0.9508506616257089,
            0.9848771266540642,
            0.8979206049149339,
            0.9792060491493384,
            0.9810964083175804,
            0.9527410207939508,
            0.9886578449905482,
            0.9867674858223062,
            0.9149338374291115,
            0.9716446124763705,
            0.9886578449905482,
            0.9659735349716446,
            0.994328922495274,
            0.996219281663516,
            0.996219281663516,
            0.994328922495274,
            1.0,
            0.9810964083175804,
            0.998109640831758,
            0.9678638941398866
        ],
        "reward_history": [
            -3888,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4185,
            -1253,
            -5290,
            -2222,
            -5290,
            -5290,
            -4004,
            -5290,
            -2467,
            -4873,
            -2290,
            -5003,
            -1330,
            -1722,
            -5290,
            -1871,
            -5290,
            -3154,
            -1213,
            -3383,
            -2475,
            -2523,
            -770,
            -1645,
            -3846,
            -3518,
            -943,
            -5290,
            -1400,
            -1714,
            -844,
            -1609,
            -4322,
            -5156,
            -807,
            -3759,
            -1781,
            -1469,
            -738,
            -5290,
            -819,
            -887,
            -1790,
            -566,
            -715,
            -4601,
            -1094,
            -954,
            -2357,
            -1670,
            -1276,
            -383,
            -553,
            -973,
            -1600,
            -1756,
            -2631
        ],
        "steps_history": [
            3989,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            4286,
            1354,
            5290,
            2323,
            5290,
            5290,
            4105,
            5290,
            2568,
            4974,
            2391,
            5104,
            1431,
            1823,
            5290,
            1972,
            5290,
            3255,
            1314,
            3484,
            2576,
            2624,
            871,
            1746,
            3947,
            3619,
            1044,
            5290,
            1501,
            1815,
            945,
            1710,
            4423,
            5257,
            908,
            3860,
            1882,
            1570,
            839,
            5290,
            920,
            988,
            1891,
            667,
            816,
            4702,
            1195,
            1055,
            2458,
            1771,
            1377,
            484,
            654,
            1074,
            1701,
            1857,
            2732
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "757/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.884529113769531,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 63,
        "policy_stability_history": [
            0.0,
            0.8090737240075614,
            0.8374291115311909,
            0.8109640831758034,
            0.9073724007561437,
            0.8676748582230623,
            0.8468809073724007,
            0.8979206049149339,
            0.8166351606805293,
            0.8771266540642723,
            0.8676748582230623,
            0.9243856332703214,
            0.8865784499054821,
            0.9206049149338374,
            0.9130434782608695,
            0.9017013232514177,
            0.8752362948960303,
            0.9035916824196597,
            0.9111531190926276,
            0.8941398865784499,
            0.9281663516068053,
            0.9168241965973535,
            0.9338374291115312,
            0.8846880907372401,
            0.9300567107750473,
            0.941398865784499,
            0.9224952741020794,
            0.9149338374291115,
            0.9546313799621928,
            0.8903591682419659,
            0.8638941398865785,
            0.9376181474480151,
            0.9243856332703214,
            0.9338374291115312,
            0.8903591682419659,
            0.9678638941398866,
            0.8941398865784499,
            0.945179584120983,
            0.8714555765595463,
            0.9168241965973535,
            0.9168241965973535,
            0.9773156899810964,
            0.9584120982986768,
            0.994328922495274,
            0.9640831758034026,
            0.9262759924385633,
            0.9716446124763705,
            0.9584120982986768,
            0.9886578449905482,
            0.9073724007561437,
            0.9792060491493384,
            0.9886578449905482,
            0.9905482041587902,
            0.9149338374291115,
            0.9848771266540642,
            0.9886578449905482,
            0.998109640831758,
            0.9735349716446124,
            0.9773156899810964,
            0.9697542533081286,
            0.9111531190926276,
            0.9905482041587902,
            1.0,
            1.0
        ],
        "reward_history": [
            -5290,
            -3454,
            -3385,
            -5290,
            -5290,
            -5290,
            -5290,
            -2413,
            -5290,
            -5290,
            -5290,
            -1778,
            -2716,
            -2523,
            -5290,
            -2752,
            -5290,
            -2469,
            -2128,
            -3387,
            -1166,
            -5290,
            -2650,
            -5290,
            -1594,
            -1785,
            -1775,
            -2575,
            -963,
            -3037,
            -5290,
            -2377,
            -2238,
            -1585,
            -3237,
            -913,
            -5290,
            -1933,
            -2827,
            -4329,
            -3089,
            -1425,
            -1648,
            -436,
            -1376,
            -1651,
            -1058,
            -1769,
            -451,
            -3595,
            -822,
            -569,
            -946,
            -4394,
            -789,
            -428,
            -539,
            -1075,
            -1153,
            -1792,
            -4113,
            -840,
            -494,
            -757
        ],
        "steps_history": [
            5290,
            3555,
            3486,
            5290,
            5290,
            5290,
            5290,
            2514,
            5290,
            5290,
            5290,
            1879,
            2817,
            2624,
            5290,
            2853,
            5290,
            2570,
            2229,
            3488,
            1267,
            5290,
            2751,
            5290,
            1695,
            1886,
            1876,
            2676,
            1064,
            3138,
            5290,
            2478,
            2339,
            1686,
            3338,
            1014,
            5290,
            2034,
            2928,
            4430,
            3190,
            1526,
            1749,
            537,
            1477,
            1752,
            1159,
            1870,
            552,
            3696,
            923,
            670,
            1047,
            4495,
            890,
            529,
            640,
            1176,
            1254,
            1893,
            4214,
            941,
            595,
            858
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "758/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.035810232162476,
        "final_policy_stability": 0.9924385633270322,
        "episodes_to_convergence": 59,
        "policy_stability_history": [
            0.0,
            0.8771266540642723,
            0.9017013232514177,
            0.8601134215500945,
            0.8052930056710775,
            0.8960302457466919,
            0.8241965973534972,
            0.8544423440453687,
            0.8790170132325141,
            0.8638941398865785,
            0.8638941398865785,
            0.9073724007561437,
            0.8809073724007561,
            0.888468809073724,
            0.9206049149338374,
            0.888468809073724,
            0.8809073724007561,
            0.8790170132325141,
            0.8790170132325141,
            0.9017013232514177,
            0.9206049149338374,
            0.9111531190926276,
            0.941398865784499,
            0.8582230623818525,
            0.945179584120983,
            0.9603024574669187,
            0.9035916824196597,
            0.9319470699432892,
            0.9111531190926276,
            0.9659735349716446,
            0.9735349716446124,
            0.9357277882797732,
            0.9338374291115312,
            0.9035916824196597,
            0.9281663516068053,
            0.9603024574669187,
            0.9092627599243857,
            0.9376181474480151,
            0.9848771266540642,
            0.9017013232514177,
            0.9867674858223062,
            0.9243856332703214,
            0.9848771266540642,
            0.994328922495274,
            0.943289224952741,
            0.945179584120983,
            0.9640831758034026,
            0.9584120982986768,
            0.9792060491493384,
            0.994328922495274,
            0.9924385633270322,
            0.9905482041587902,
            0.9792060491493384,
            0.998109640831758,
            0.9754253308128544,
            0.9395085066162571,
            0.9716446124763705,
            0.9792060491493384,
            1.0,
            0.9924385633270322
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -3850,
            -5290,
            -2484,
            -5290,
            -5290,
            -5290,
            -5290,
            -4674,
            -1977,
            -5290,
            -1998,
            -1671,
            -2241,
            -2556,
            -5290,
            -4231,
            -1491,
            -1760,
            -2343,
            -1622,
            -5290,
            -2045,
            -813,
            -5290,
            -2103,
            -5290,
            -950,
            -978,
            -2483,
            -2405,
            -3351,
            -3865,
            -1321,
            -5030,
            -2518,
            -284,
            -5290,
            -686,
            -3507,
            -757,
            -1474,
            -1838,
            -1921,
            -1707,
            -2208,
            -1470,
            -475,
            -929,
            -1366,
            -2132,
            -622,
            -852,
            -1891,
            -2528,
            -1669,
            -427,
            -2577
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            3951,
            5290,
            2585,
            5290,
            5290,
            5290,
            5290,
            4775,
            2078,
            5290,
            2099,
            1772,
            2342,
            2657,
            5290,
            4332,
            1592,
            1861,
            2444,
            1723,
            5290,
            2146,
            914,
            5290,
            2204,
            5290,
            1051,
            1079,
            2584,
            2506,
            3452,
            3966,
            1422,
            5131,
            2619,
            385,
            5290,
            787,
            3608,
            858,
            1575,
            1939,
            2022,
            1808,
            2309,
            1571,
            576,
            1030,
            1467,
            2233,
            723,
            953,
            1992,
            2629,
            1770,
            528,
            2678
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "759/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 14.613085985183716,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 75,
        "policy_stability_history": [
            0.0,
            0.8298676748582231,
            0.8941398865784499,
            0.8601134215500945,
            0.8865784499054821,
            0.8638941398865785,
            0.8752362948960303,
            0.833648393194707,
            0.8544423440453687,
            0.8563327032136105,
            0.9338374291115312,
            0.8827977315689981,
            0.8449905482041588,
            0.9017013232514177,
            0.8695652173913043,
            0.8733459357277883,
            0.8960302457466919,
            0.9489603024574669,
            0.9092627599243857,
            0.9149338374291115,
            0.9130434782608695,
            0.8809073724007561,
            0.9187145557655955,
            0.9300567107750473,
            0.8771266540642723,
            0.9092627599243857,
            0.8620037807183365,
            0.9073724007561437,
            0.9035916824196597,
            0.9584120982986768,
            0.943289224952741,
            0.9678638941398866,
            0.9527410207939508,
            0.9773156899810964,
            0.9584120982986768,
            0.9565217391304348,
            0.9603024574669187,
            0.9017013232514177,
            0.9149338374291115,
            0.947069943289225,
            0.8601134215500945,
            0.9546313799621928,
            0.9224952741020794,
            0.9111531190926276,
            0.947069943289225,
            0.9735349716446124,
            0.9640831758034026,
            0.9527410207939508,
            0.941398865784499,
            0.9584120982986768,
            0.9054820415879017,
            0.8657844990548205,
            0.9716446124763705,
            0.994328922495274,
            0.9754253308128544,
            0.9829867674858223,
            0.9640831758034026,
            0.9924385633270322,
            0.9754253308128544,
            0.9905482041587902,
            0.9810964083175804,
            0.996219281663516,
            0.9924385633270322,
            0.9621928166351607,
            0.994328922495274,
            0.9848771266540642,
            0.994328922495274,
            0.9924385633270322,
            0.998109640831758,
            0.998109640831758,
            1.0,
            0.998109640831758,
            0.9640831758034026,
            1.0,
            0.9678638941398866,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -2546,
            -2529,
            -5290,
            -5290,
            -5290,
            -5290,
            -4399,
            -1431,
            -5290,
            -5290,
            -5290,
            -5290,
            -3383,
            -2666,
            -1010,
            -1400,
            -2418,
            -3717,
            -5290,
            -1506,
            -1373,
            -5290,
            -5290,
            -5290,
            -3041,
            -5290,
            -841,
            -1426,
            -892,
            -1191,
            -753,
            -958,
            -824,
            -1215,
            -5290,
            -3800,
            -1688,
            -5290,
            -2115,
            -4042,
            -5290,
            -1673,
            -923,
            -971,
            -2277,
            -2439,
            -1526,
            -3611,
            -5290,
            -956,
            -929,
            -1122,
            -986,
            -1825,
            -695,
            -1055,
            -580,
            -1349,
            -816,
            -1003,
            -2096,
            -1605,
            -1040,
            -683,
            -1380,
            -542,
            -389,
            -569,
            -529,
            -1860,
            -535,
            -3418,
            -596
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            2647,
            2630,
            5290,
            5290,
            5290,
            5290,
            4500,
            1532,
            5290,
            5290,
            5290,
            5290,
            3484,
            2767,
            1111,
            1501,
            2519,
            3818,
            5290,
            1607,
            1474,
            5290,
            5290,
            5290,
            3142,
            5290,
            942,
            1527,
            993,
            1292,
            854,
            1059,
            925,
            1316,
            5290,
            3901,
            1789,
            5290,
            2216,
            4143,
            5290,
            1774,
            1024,
            1072,
            2378,
            2540,
            1627,
            3712,
            5290,
            1057,
            1030,
            1223,
            1087,
            1926,
            796,
            1156,
            681,
            1450,
            917,
            1104,
            2197,
            1706,
            1141,
            784,
            1481,
            643,
            490,
            670,
            630,
            1961,
            636,
            3519,
            697
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "760/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 14.202528476715088,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 71,
        "policy_stability_history": [
            0.0,
            0.8922495274102079,
            0.782608695652174,
            0.9130434782608695,
            0.8601134215500945,
            0.8979206049149339,
            0.8601134215500945,
            0.9092627599243857,
            0.8771266540642723,
            0.8752362948960303,
            0.8279773156899811,
            0.8998109640831758,
            0.8525519848771267,
            0.8601134215500945,
            0.8601134215500945,
            0.888468809073724,
            0.8657844990548205,
            0.9130434782608695,
            0.9206049149338374,
            0.9300567107750473,
            0.9073724007561437,
            0.8941398865784499,
            0.8676748582230623,
            0.8809073724007561,
            0.9319470699432892,
            0.943289224952741,
            0.8733459357277883,
            0.8809073724007561,
            0.8998109640831758,
            0.9300567107750473,
            0.9357277882797732,
            0.8544423440453687,
            0.947069943289225,
            0.9357277882797732,
            0.9640831758034026,
            0.8733459357277883,
            0.9357277882797732,
            0.9527410207939508,
            0.8790170132325141,
            0.9130434782608695,
            0.8638941398865785,
            0.9243856332703214,
            0.9735349716446124,
            0.9111531190926276,
            0.9357277882797732,
            0.8638941398865785,
            0.9697542533081286,
            0.9773156899810964,
            0.9508506616257089,
            0.8922495274102079,
            0.9810964083175804,
            0.9206049149338374,
            0.9395085066162571,
            0.9792060491493384,
            0.9224952741020794,
            0.9886578449905482,
            0.8752362948960303,
            0.9527410207939508,
            0.9924385633270322,
            0.9848771266540642,
            0.9867674858223062,
            0.9376181474480151,
            0.9754253308128544,
            0.9640831758034026,
            0.8960302457466919,
            0.9659735349716446,
            0.9546313799621928,
            0.9905482041587902,
            0.9773156899810964,
            1.0,
            0.9905482041587902,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -2254,
            -1499,
            -5290,
            -1213,
            -4174,
            -1588,
            -5290,
            -1934,
            -4658,
            -5290,
            -5290,
            -3819,
            -5290,
            -2442,
            -1537,
            -1006,
            -2268,
            -2890,
            -5290,
            -5290,
            -864,
            -1271,
            -2869,
            -2289,
            -2277,
            -1109,
            -955,
            -5290,
            -1065,
            -1726,
            -698,
            -4250,
            -1528,
            -875,
            -3981,
            -2074,
            -5290,
            -2860,
            -573,
            -1601,
            -1335,
            -5290,
            -634,
            -851,
            -959,
            -3052,
            -677,
            -2130,
            -1928,
            -791,
            -2340,
            -634,
            -3769,
            -1208,
            -485,
            -1142,
            -485,
            -1533,
            -843,
            -1992,
            -5290,
            -1300,
            -2662,
            -541,
            -1073,
            -475,
            -686,
            -592
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            2355,
            1600,
            5290,
            1314,
            4275,
            1689,
            5290,
            2035,
            4759,
            5290,
            5290,
            3920,
            5290,
            2543,
            1638,
            1107,
            2369,
            2991,
            5290,
            5290,
            965,
            1372,
            2970,
            2390,
            2378,
            1210,
            1056,
            5290,
            1166,
            1827,
            799,
            4351,
            1629,
            976,
            4082,
            2175,
            5290,
            2961,
            674,
            1702,
            1436,
            5290,
            735,
            952,
            1060,
            3153,
            778,
            2231,
            2029,
            892,
            2441,
            735,
            3870,
            1309,
            586,
            1243,
            586,
            1634,
            944,
            2093,
            5290,
            1401,
            2763,
            642,
            1174,
            576,
            787,
            693
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "761/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.725787878036499,
        "final_policy_stability": 0.9848771266540642,
        "episodes_to_convergence": 65,
        "policy_stability_history": [
            0.0,
            0.833648393194707,
            0.833648393194707,
            0.8714555765595463,
            0.8147448015122873,
            0.8771266540642723,
            0.8487712665406427,
            0.8638941398865785,
            0.888468809073724,
            0.8638941398865785,
            0.8582230623818525,
            0.8525519848771267,
            0.8733459357277883,
            0.9243856332703214,
            0.9073724007561437,
            0.8544423440453687,
            0.8714555765595463,
            0.9357277882797732,
            0.8790170132325141,
            0.8827977315689981,
            0.8922495274102079,
            0.8827977315689981,
            0.9376181474480151,
            0.8827977315689981,
            0.9281663516068053,
            0.9508506616257089,
            0.9697542533081286,
            0.9546313799621928,
            0.9111531190926276,
            0.9243856332703214,
            0.8941398865784499,
            0.9546313799621928,
            0.9149338374291115,
            0.9319470699432892,
            0.9168241965973535,
            0.9754253308128544,
            0.9319470699432892,
            0.8601134215500945,
            0.9508506616257089,
            0.941398865784499,
            0.9716446124763705,
            0.9262759924385633,
            0.9640831758034026,
            0.9829867674858223,
            0.9054820415879017,
            0.9924385633270322,
            0.9678638941398866,
            0.9584120982986768,
            0.9924385633270322,
            0.9773156899810964,
            0.9338374291115312,
            0.9716446124763705,
            0.994328922495274,
            1.0,
            0.994328922495274,
            0.9867674858223062,
            0.9716446124763705,
            0.994328922495274,
            0.9735349716446124,
            0.9810964083175804,
            0.9886578449905482,
            0.996219281663516,
            0.9867674858223062,
            0.996219281663516,
            0.9187145557655955,
            0.9848771266540642
        ],
        "reward_history": [
            -5290,
            -5290,
            -4219,
            -3775,
            -5147,
            -5290,
            -5290,
            -5290,
            -2648,
            -5290,
            -2818,
            -5290,
            -5290,
            -855,
            -2787,
            -5290,
            -5290,
            -1248,
            -3869,
            -4195,
            -3115,
            -2912,
            -1025,
            -5290,
            -1470,
            -993,
            -955,
            -642,
            -2818,
            -1946,
            -3659,
            -995,
            -4205,
            -2981,
            -1862,
            -788,
            -2417,
            -5125,
            -907,
            -2220,
            -725,
            -2755,
            -1828,
            -851,
            -3216,
            -607,
            -949,
            -1404,
            -416,
            -827,
            -2202,
            -2233,
            -630,
            -593,
            -591,
            -597,
            -1653,
            -471,
            -1380,
            -1074,
            -493,
            -327,
            -997,
            -593,
            -5081,
            -709
        ],
        "steps_history": [
            5290,
            5290,
            4320,
            3876,
            5248,
            5290,
            5290,
            5290,
            2749,
            5290,
            2919,
            5290,
            5290,
            956,
            2888,
            5290,
            5290,
            1349,
            3970,
            4296,
            3216,
            3013,
            1126,
            5290,
            1571,
            1094,
            1056,
            743,
            2919,
            2047,
            3760,
            1096,
            4306,
            3082,
            1963,
            889,
            2518,
            5226,
            1008,
            2321,
            826,
            2856,
            1929,
            952,
            3317,
            708,
            1050,
            1505,
            517,
            928,
            2303,
            2334,
            731,
            694,
            692,
            698,
            1754,
            572,
            1481,
            1175,
            594,
            428,
            1098,
            694,
            5182,
            810
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "762/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 14.551099061965942,
        "final_policy_stability": 0.9678638941398866,
        "episodes_to_convergence": 74,
        "policy_stability_history": [
            0.0,
            0.8809073724007561,
            0.8449905482041588,
            0.888468809073724,
            0.8582230623818525,
            0.8695652173913043,
            0.8922495274102079,
            0.8620037807183365,
            0.8676748582230623,
            0.8241965973534972,
            0.9111531190926276,
            0.8733459357277883,
            0.8525519848771267,
            0.8223062381852552,
            0.9357277882797732,
            0.8374291115311909,
            0.8790170132325141,
            0.8582230623818525,
            0.8714555765595463,
            0.8695652173913043,
            0.9092627599243857,
            0.8355387523629489,
            0.9149338374291115,
            0.9262759924385633,
            0.8468809073724007,
            0.9565217391304348,
            0.945179584120983,
            0.945179584120983,
            0.8790170132325141,
            0.9281663516068053,
            0.9603024574669187,
            0.9092627599243857,
            0.8733459357277883,
            0.8620037807183365,
            0.9565217391304348,
            0.9319470699432892,
            0.9357277882797732,
            0.9187145557655955,
            0.8979206049149339,
            0.9319470699432892,
            0.941398865784499,
            0.888468809073724,
            0.9697542533081286,
            0.9792060491493384,
            0.9035916824196597,
            0.9810964083175804,
            0.9697542533081286,
            0.9886578449905482,
            0.945179584120983,
            0.9395085066162571,
            0.9621928166351607,
            0.9716446124763705,
            0.9546313799621928,
            0.9754253308128544,
            0.996219281663516,
            0.9659735349716446,
            0.9319470699432892,
            0.9867674858223062,
            0.9603024574669187,
            0.996219281663516,
            0.9735349716446124,
            0.9754253308128544,
            0.994328922495274,
            0.9773156899810964,
            0.9754253308128544,
            0.998109640831758,
            0.9924385633270322,
            0.998109640831758,
            0.9810964083175804,
            0.996219281663516,
            0.996219281663516,
            1.0,
            0.9565217391304348,
            0.994328922495274,
            0.9678638941398866
        ],
        "reward_history": [
            -5290,
            -5290,
            -4989,
            -5290,
            -5290,
            -2167,
            -1719,
            -5290,
            -5290,
            -4404,
            -3386,
            -3234,
            -4234,
            -5290,
            -1783,
            -5290,
            -5290,
            -3363,
            -5290,
            -1945,
            -1493,
            -5290,
            -1337,
            -2009,
            -5290,
            -741,
            -1287,
            -1490,
            -5178,
            -3931,
            -695,
            -2789,
            -3802,
            -5290,
            -739,
            -1614,
            -1069,
            -2232,
            -5290,
            -1292,
            -1290,
            -3311,
            -1023,
            -856,
            -5290,
            -463,
            -836,
            -487,
            -1801,
            -4158,
            -1615,
            -665,
            -1485,
            -715,
            -311,
            -1804,
            -1696,
            -391,
            -1900,
            -755,
            -1077,
            -1072,
            -674,
            -757,
            -1430,
            -414,
            -1085,
            -280,
            -1492,
            -829,
            -668,
            -261,
            -1597,
            -731,
            -1821
        ],
        "steps_history": [
            5290,
            5290,
            5090,
            5290,
            5290,
            2268,
            1820,
            5290,
            5290,
            4505,
            3487,
            3335,
            4335,
            5290,
            1884,
            5290,
            5290,
            3464,
            5290,
            2046,
            1594,
            5290,
            1438,
            2110,
            5290,
            842,
            1388,
            1591,
            5279,
            4032,
            796,
            2890,
            3903,
            5290,
            840,
            1715,
            1170,
            2333,
            5290,
            1393,
            1391,
            3412,
            1124,
            957,
            5290,
            564,
            937,
            588,
            1902,
            4259,
            1716,
            766,
            1586,
            816,
            412,
            1905,
            1797,
            492,
            2001,
            856,
            1178,
            1173,
            775,
            858,
            1531,
            515,
            1186,
            381,
            1593,
            930,
            769,
            362,
            1698,
            832,
            1922
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "763/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 13.54076337814331,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 70,
        "policy_stability_history": [
            0.0,
            0.8355387523629489,
            0.8809073724007561,
            0.8582230623818525,
            0.8676748582230623,
            0.8147448015122873,
            0.8223062381852552,
            0.8223062381852552,
            0.831758034026465,
            0.8695652173913043,
            0.8657844990548205,
            0.8525519848771267,
            0.8620037807183365,
            0.9168241965973535,
            0.8355387523629489,
            0.8941398865784499,
            0.8260869565217391,
            0.9187145557655955,
            0.8260869565217391,
            0.8657844990548205,
            0.8865784499054821,
            0.8449905482041588,
            0.9357277882797732,
            0.8865784499054821,
            0.8544423440453687,
            0.943289224952741,
            0.8449905482041588,
            0.9017013232514177,
            0.9281663516068053,
            0.9281663516068053,
            0.8676748582230623,
            0.9603024574669187,
            0.8771266540642723,
            0.8941398865784499,
            0.9508506616257089,
            0.9130434782608695,
            0.9130434782608695,
            0.8865784499054821,
            0.9621928166351607,
            0.9149338374291115,
            0.9810964083175804,
            0.947069943289225,
            0.9924385633270322,
            0.9773156899810964,
            0.9300567107750473,
            0.9640831758034026,
            0.9659735349716446,
            0.9867674858223062,
            0.9829867674858223,
            0.9848771266540642,
            0.9829867674858223,
            0.9810964083175804,
            0.9810964083175804,
            0.9603024574669187,
            0.9905482041587902,
            0.9546313799621928,
            0.9243856332703214,
            0.9867674858223062,
            0.9924385633270322,
            0.9848771266540642,
            0.9867674858223062,
            0.996219281663516,
            0.9773156899810964,
            0.9905482041587902,
            0.994328922495274,
            0.9905482041587902,
            0.998109640831758,
            0.998109640831758,
            0.998109640831758,
            1.0,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -4990,
            -5290,
            -5290,
            -5290,
            -5290,
            -5158,
            -5290,
            -5290,
            -3065,
            -5290,
            -1861,
            -4683,
            -2737,
            -4849,
            -1598,
            -5290,
            -5290,
            -2192,
            -5290,
            -2200,
            -3041,
            -5290,
            -957,
            -5290,
            -3289,
            -1412,
            -2145,
            -3478,
            -710,
            -5290,
            -3191,
            -902,
            -1844,
            -4499,
            -5290,
            -1055,
            -3666,
            -735,
            -2310,
            -508,
            -593,
            -1847,
            -1479,
            -1055,
            -601,
            -861,
            -826,
            -756,
            -837,
            -669,
            -2065,
            -947,
            -1712,
            -3398,
            -1035,
            -997,
            -953,
            -697,
            -700,
            -1389,
            -714,
            -541,
            -691,
            -541,
            -1068,
            -674,
            -863,
            -1148
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5091,
            5290,
            5290,
            5290,
            5290,
            5259,
            5290,
            5290,
            3166,
            5290,
            1962,
            4784,
            2838,
            4950,
            1699,
            5290,
            5290,
            2293,
            5290,
            2301,
            3142,
            5290,
            1058,
            5290,
            3390,
            1513,
            2246,
            3579,
            811,
            5290,
            3292,
            1003,
            1945,
            4600,
            5290,
            1156,
            3767,
            836,
            2411,
            609,
            694,
            1948,
            1580,
            1156,
            702,
            962,
            927,
            857,
            938,
            770,
            2166,
            1048,
            1813,
            3499,
            1136,
            1098,
            1054,
            798,
            801,
            1490,
            815,
            642,
            792,
            642,
            1169,
            775,
            964,
            1249
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "764/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 14.28847074508667,
        "final_policy_stability": 0.996219281663516,
        "episodes_to_convergence": 72,
        "policy_stability_history": [
            0.0,
            0.8374291115311909,
            0.8714555765595463,
            0.9073724007561437,
            0.8449905482041588,
            0.8412098298676749,
            0.8147448015122873,
            0.9130434782608695,
            0.8052930056710775,
            0.9017013232514177,
            0.8620037807183365,
            0.8676748582230623,
            0.8979206049149339,
            0.833648393194707,
            0.8109640831758034,
            0.8827977315689981,
            0.8544423440453687,
            0.9319470699432892,
            0.9206049149338374,
            0.8941398865784499,
            0.8506616257088847,
            0.8922495274102079,
            0.9206049149338374,
            0.945179584120983,
            0.8733459357277883,
            0.8998109640831758,
            0.9300567107750473,
            0.9527410207939508,
            0.8903591682419659,
            0.9810964083175804,
            0.8998109640831758,
            0.9565217391304348,
            0.9149338374291115,
            0.8903591682419659,
            0.9640831758034026,
            0.9621928166351607,
            0.9508506616257089,
            0.9357277882797732,
            0.8506616257088847,
            0.9187145557655955,
            0.9565217391304348,
            0.9527410207939508,
            0.9716446124763705,
            0.9508506616257089,
            0.9810964083175804,
            0.833648393194707,
            0.9565217391304348,
            0.8827977315689981,
            0.9735349716446124,
            0.9206049149338374,
            0.9640831758034026,
            0.9640831758034026,
            0.9905482041587902,
            0.9697542533081286,
            0.9697542533081286,
            0.9565217391304348,
            0.945179584120983,
            0.9867674858223062,
            0.9829867674858223,
            0.9584120982986768,
            0.8998109640831758,
            0.9867674858223062,
            0.9886578449905482,
            0.9773156899810964,
            0.994328922495274,
            0.9848771266540642,
            0.9848771266540642,
            1.0,
            0.998109640831758,
            1.0,
            0.996219281663516,
            0.9716446124763705,
            0.996219281663516
        ],
        "reward_history": [
            -5290,
            -2768,
            -5290,
            -5290,
            -4485,
            -5290,
            -5290,
            -1743,
            -5290,
            -5290,
            -4998,
            -5290,
            -2798,
            -3922,
            -4355,
            -5290,
            -5290,
            -1940,
            -1805,
            -1468,
            -4411,
            -2680,
            -2007,
            -997,
            -5290,
            -2245,
            -2558,
            -749,
            -2184,
            -409,
            -3432,
            -803,
            -2096,
            -5290,
            -601,
            -852,
            -1202,
            -1753,
            -5290,
            -2939,
            -822,
            -1275,
            -546,
            -1313,
            -536,
            -5290,
            -984,
            -5290,
            -759,
            -1797,
            -937,
            -1173,
            -466,
            -974,
            -934,
            -824,
            -1752,
            -909,
            -974,
            -1493,
            -5290,
            -594,
            -541,
            -819,
            -670,
            -590,
            -1227,
            -391,
            -381,
            -302,
            -779,
            -1091,
            -787
        ],
        "steps_history": [
            5290,
            2869,
            5290,
            5290,
            4586,
            5290,
            5290,
            1844,
            5290,
            5290,
            5099,
            5290,
            2899,
            4023,
            4456,
            5290,
            5290,
            2041,
            1906,
            1569,
            4512,
            2781,
            2108,
            1098,
            5290,
            2346,
            2659,
            850,
            2285,
            510,
            3533,
            904,
            2197,
            5290,
            702,
            953,
            1303,
            1854,
            5290,
            3040,
            923,
            1376,
            647,
            1414,
            637,
            5290,
            1085,
            5290,
            860,
            1898,
            1038,
            1274,
            567,
            1075,
            1035,
            925,
            1853,
            1010,
            1075,
            1594,
            5290,
            695,
            642,
            920,
            771,
            691,
            1328,
            492,
            482,
            403,
            880,
            1192,
            888
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "765/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 13.36709713935852,
        "final_policy_stability": 0.998109640831758,
        "episodes_to_convergence": 64,
        "policy_stability_history": [
            0.0,
            0.8355387523629489,
            0.8903591682419659,
            0.8941398865784499,
            0.8431001890359168,
            0.8695652173913043,
            0.8865784499054821,
            0.8695652173913043,
            0.8638941398865785,
            0.9149338374291115,
            0.831758034026465,
            0.9224952741020794,
            0.8733459357277883,
            0.8298676748582231,
            0.8827977315689981,
            0.8374291115311909,
            0.8922495274102079,
            0.8771266540642723,
            0.8638941398865785,
            0.9224952741020794,
            0.9111531190926276,
            0.941398865784499,
            0.9489603024574669,
            0.9281663516068053,
            0.8449905482041588,
            0.943289224952741,
            0.9224952741020794,
            0.9187145557655955,
            0.945179584120983,
            0.8827977315689981,
            0.8979206049149339,
            0.9357277882797732,
            0.8865784499054821,
            0.9187145557655955,
            0.9489603024574669,
            0.9754253308128544,
            0.8809073724007561,
            0.9716446124763705,
            0.945179584120983,
            0.9376181474480151,
            0.9886578449905482,
            0.9565217391304348,
            0.9300567107750473,
            0.9659735349716446,
            0.9565217391304348,
            0.9281663516068053,
            0.9754253308128544,
            0.9697542533081286,
            0.9716446124763705,
            0.9168241965973535,
            0.9867674858223062,
            0.9829867674858223,
            0.945179584120983,
            0.9773156899810964,
            0.9754253308128544,
            0.994328922495274,
            0.9697542533081286,
            0.9905482041587902,
            0.9603024574669187,
            0.945179584120983,
            0.9678638941398866,
            0.996219281663516,
            1.0,
            0.9678638941398866,
            0.998109640831758
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4966,
            -4542,
            -1753,
            -5290,
            -1171,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -1830,
            -5290,
            -1152,
            -942,
            -1351,
            -5290,
            -1570,
            -2218,
            -3875,
            -1764,
            -5290,
            -4751,
            -2737,
            -5290,
            -3520,
            -1461,
            -1103,
            -5290,
            -1021,
            -5290,
            -3069,
            -1469,
            -2338,
            -5290,
            -1665,
            -3288,
            -5290,
            -1722,
            -1807,
            -1785,
            -5290,
            -1578,
            -2365,
            -2355,
            -1466,
            -2049,
            -939,
            -1784,
            -1683,
            -1553,
            -4697,
            -3463,
            -969,
            -1171,
            -3208,
            -1234
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5067,
            4643,
            1854,
            5290,
            1272,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            1931,
            5290,
            1253,
            1043,
            1452,
            5290,
            1671,
            2319,
            3976,
            1865,
            5290,
            4852,
            2838,
            5290,
            3621,
            1562,
            1204,
            5290,
            1122,
            5290,
            3170,
            1570,
            2439,
            5290,
            1766,
            3389,
            5290,
            1823,
            1908,
            1886,
            5290,
            1679,
            2466,
            2456,
            1567,
            2150,
            1040,
            1885,
            1784,
            1654,
            4798,
            3564,
            1070,
            1272,
            3309,
            1335
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "766/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 13.169840812683105,
        "final_policy_stability": 0.9489603024574669,
        "episodes_to_convergence": 63,
        "policy_stability_history": [
            0.0,
            0.8052930056710775,
            0.9017013232514177,
            0.8979206049149339,
            0.8941398865784499,
            0.8695652173913043,
            0.8827977315689981,
            0.8846880907372401,
            0.9092627599243857,
            0.8979206049149339,
            0.941398865784499,
            0.8695652173913043,
            0.9206049149338374,
            0.8903591682419659,
            0.8998109640831758,
            0.9017013232514177,
            0.9092627599243857,
            0.9130434782608695,
            0.9168241965973535,
            0.9187145557655955,
            0.8601134215500945,
            0.9224952741020794,
            0.9149338374291115,
            0.8657844990548205,
            0.9073724007561437,
            0.9300567107750473,
            0.8960302457466919,
            0.9376181474480151,
            0.8393194706994329,
            0.9300567107750473,
            0.945179584120983,
            0.8846880907372401,
            0.945179584120983,
            0.9773156899810964,
            0.9621928166351607,
            0.945179584120983,
            0.996219281663516,
            0.9716446124763705,
            0.8601134215500945,
            0.9867674858223062,
            0.9262759924385633,
            0.8903591682419659,
            0.9810964083175804,
            0.9565217391304348,
            0.8998109640831758,
            0.9508506616257089,
            0.945179584120983,
            0.945179584120983,
            0.9867674858223062,
            0.9300567107750473,
            0.9905482041587902,
            0.9754253308128544,
            0.996219281663516,
            0.9489603024574669,
            0.9867674858223062,
            0.9905482041587902,
            0.9886578449905482,
            0.9867674858223062,
            0.994328922495274,
            0.9886578449905482,
            0.9905482041587902,
            0.9924385633270322,
            0.9829867674858223,
            0.9489603024574669
        ],
        "reward_history": [
            -5290,
            -4092,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -3579,
            -5290,
            -3660,
            -5290,
            -5290,
            -2234,
            -5290,
            -4420,
            -3909,
            -1771,
            -1715,
            -1989,
            -5290,
            -5290,
            -2363,
            -5290,
            -5290,
            -3048,
            -2526,
            -4178,
            -2060,
            -5290,
            -2111,
            -2097,
            -5290,
            -1557,
            -815,
            -1137,
            -2346,
            -608,
            -1042,
            -5290,
            -933,
            -5290,
            -5290,
            -1062,
            -3170,
            -5290,
            -2507,
            -2672,
            -3876,
            -1030,
            -3411,
            -1527,
            -1304,
            -705,
            -5290,
            -1984,
            -921,
            -2056,
            -1614,
            -1900,
            -1184,
            -1450,
            -2503,
            -3394,
            -4035
        ],
        "steps_history": [
            5290,
            4193,
            5290,
            5290,
            5290,
            5290,
            5290,
            3680,
            5290,
            3761,
            5290,
            5290,
            2335,
            5290,
            4521,
            4010,
            1872,
            1816,
            2090,
            5290,
            5290,
            2464,
            5290,
            5290,
            3149,
            2627,
            4279,
            2161,
            5290,
            2212,
            2198,
            5290,
            1658,
            916,
            1238,
            2447,
            709,
            1143,
            5290,
            1034,
            5290,
            5290,
            1163,
            3271,
            5290,
            2608,
            2773,
            3977,
            1131,
            3512,
            1628,
            1405,
            806,
            5290,
            2085,
            1022,
            2157,
            1715,
            2001,
            1285,
            1551,
            2604,
            3495,
            4136
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "767/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.29577922821045,
        "final_policy_stability": 0.996219281663516,
        "episodes_to_convergence": 54,
        "policy_stability_history": [
            0.0,
            0.8185255198487713,
            0.8525519848771267,
            0.8960302457466919,
            0.8393194706994329,
            0.888468809073724,
            0.8865784499054821,
            0.9395085066162571,
            0.8922495274102079,
            0.8809073724007561,
            0.8582230623818525,
            0.8695652173913043,
            0.9300567107750473,
            0.9130434782608695,
            0.9206049149338374,
            0.9395085066162571,
            0.8393194706994329,
            0.9092627599243857,
            0.9243856332703214,
            0.9111531190926276,
            0.8960302457466919,
            0.8620037807183365,
            0.8790170132325141,
            0.9092627599243857,
            0.9716446124763705,
            0.9243856332703214,
            0.9376181474480151,
            0.8809073724007561,
            0.9546313799621928,
            0.8846880907372401,
            0.8809073724007561,
            0.9357277882797732,
            0.9130434782608695,
            0.9035916824196597,
            0.9130434782608695,
            0.9546313799621928,
            0.9640831758034026,
            0.9640831758034026,
            0.9716446124763705,
            0.8620037807183365,
            0.9243856332703214,
            0.9262759924385633,
            0.8601134215500945,
            0.9489603024574669,
            0.9735349716446124,
            0.9149338374291115,
            0.941398865784499,
            0.9224952741020794,
            0.8941398865784499,
            0.9149338374291115,
            0.9678638941398866,
            0.9867674858223062,
            0.9678638941398866,
            1.0,
            0.996219281663516
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -3396,
            -765,
            -5290,
            -5290,
            -5290,
            -4620,
            -1231,
            -5290,
            -1731,
            -5290,
            -4340,
            -2077,
            -1622,
            -2594,
            -3240,
            -5290,
            -5290,
            -5290,
            -1114,
            -2146,
            -2223,
            -4599,
            -930,
            -4652,
            -4017,
            -2125,
            -2185,
            -3118,
            -3556,
            -2588,
            -1232,
            -1345,
            -1324,
            -5290,
            -5290,
            -4530,
            -5290,
            -2115,
            -978,
            -3973,
            -1805,
            -3234,
            -5290,
            -5290,
            -1666,
            -1152,
            -2556,
            -964,
            -1131
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            3497,
            866,
            5290,
            5290,
            5290,
            4721,
            1332,
            5290,
            1832,
            5290,
            4441,
            2178,
            1723,
            2695,
            3341,
            5290,
            5290,
            5290,
            1215,
            2247,
            2324,
            4700,
            1031,
            4753,
            4118,
            2226,
            2286,
            3219,
            3657,
            2689,
            1333,
            1446,
            1425,
            5290,
            5290,
            4631,
            5290,
            2216,
            1079,
            4074,
            1906,
            3335,
            5290,
            5290,
            1767,
            1253,
            2657,
            1065,
            1232
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "768/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.462795495986938,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 60,
        "policy_stability_history": [
            0.0,
            0.8260869565217391,
            0.8204158790170132,
            0.8582230623818525,
            0.9262759924385633,
            0.9206049149338374,
            0.8449905482041588,
            0.8960302457466919,
            0.8298676748582231,
            0.9092627599243857,
            0.9243856332703214,
            0.8790170132325141,
            0.8979206049149339,
            0.9508506616257089,
            0.8638941398865785,
            0.888468809073724,
            0.8657844990548205,
            0.8790170132325141,
            0.9111531190926276,
            0.8695652173913043,
            0.9073724007561437,
            0.8998109640831758,
            0.8695652173913043,
            0.888468809073724,
            0.9281663516068053,
            0.9187145557655955,
            0.945179584120983,
            0.9224952741020794,
            0.9754253308128544,
            0.9338374291115312,
            0.8979206049149339,
            0.9149338374291115,
            0.9262759924385633,
            0.9565217391304348,
            0.9395085066162571,
            0.945179584120983,
            0.9508506616257089,
            0.996219281663516,
            0.9678638941398866,
            0.9754253308128544,
            0.9017013232514177,
            0.9848771266540642,
            0.9678638941398866,
            0.945179584120983,
            0.9792060491493384,
            0.8979206049149339,
            0.9262759924385633,
            0.9810964083175804,
            0.9716446124763705,
            0.9773156899810964,
            0.9262759924385633,
            0.994328922495274,
            0.9886578449905482,
            0.9357277882797732,
            0.9754253308128544,
            0.9924385633270322,
            0.9924385633270322,
            1.0,
            0.996219281663516,
            1.0,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -2765,
            -5290,
            -1479,
            -5290,
            -3226,
            -5290,
            -2168,
            -5290,
            -4197,
            -5290,
            -1198,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2638,
            -5290,
            -4974,
            -5290,
            -2134,
            -2855,
            -2725,
            -2270,
            -1340,
            -2106,
            -4763,
            -5290,
            -2870,
            -2515,
            -4225,
            -2795,
            -3942,
            -971,
            -2113,
            -1183,
            -5290,
            -1488,
            -1674,
            -2317,
            -1155,
            -5290,
            -2596,
            -998,
            -1575,
            -1498,
            -3616,
            -1211,
            -1809,
            -5290,
            -1541,
            -1828,
            -1836,
            -676,
            -1367,
            -1292,
            -1123
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            2866,
            5290,
            1580,
            5290,
            3327,
            5290,
            2269,
            5290,
            4298,
            5290,
            1299,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            2739,
            5290,
            5075,
            5290,
            2235,
            2956,
            2826,
            2371,
            1441,
            2207,
            4864,
            5290,
            2971,
            2616,
            4326,
            2896,
            4043,
            1072,
            2214,
            1284,
            5290,
            1589,
            1775,
            2418,
            1256,
            5290,
            2697,
            1099,
            1676,
            1599,
            3717,
            1312,
            1910,
            5290,
            1642,
            1929,
            1937,
            777,
            1468,
            1393,
            1224
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "769/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.317468643188477,
        "final_policy_stability": 0.9603024574669187,
        "episodes_to_convergence": 53,
        "policy_stability_history": [
            0.0,
            0.8223062381852552,
            0.9376181474480151,
            0.8506616257088847,
            0.9111531190926276,
            0.9149338374291115,
            0.8676748582230623,
            0.8676748582230623,
            0.8960302457466919,
            0.8979206049149339,
            0.8223062381852552,
            0.9092627599243857,
            0.8922495274102079,
            0.9187145557655955,
            0.8563327032136105,
            0.8638941398865785,
            0.947069943289225,
            0.8374291115311909,
            0.8771266540642723,
            0.9224952741020794,
            0.9319470699432892,
            0.8733459357277883,
            0.8865784499054821,
            0.8903591682419659,
            0.9130434782608695,
            0.8922495274102079,
            0.9376181474480151,
            0.9792060491493384,
            0.9092627599243857,
            0.9376181474480151,
            0.9206049149338374,
            0.9716446124763705,
            0.8998109640831758,
            0.9017013232514177,
            0.943289224952741,
            0.9338374291115312,
            0.9149338374291115,
            0.9659735349716446,
            0.9867674858223062,
            0.9565217391304348,
            0.9735349716446124,
            0.9054820415879017,
            0.9243856332703214,
            0.9810964083175804,
            0.9565217391304348,
            0.9603024574669187,
            0.943289224952741,
            0.9319470699432892,
            0.9886578449905482,
            0.9773156899810964,
            0.9810964083175804,
            0.9867674858223062,
            1.0,
            0.9603024574669187
        ],
        "reward_history": [
            -5290,
            -4872,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -1740,
            -5290,
            -5290,
            -1378,
            -5290,
            -1092,
            -5290,
            -5290,
            -1040,
            -5290,
            -5290,
            -2144,
            -1240,
            -5290,
            -5290,
            -5290,
            -5290,
            -4526,
            -2792,
            -959,
            -5290,
            -2730,
            -2963,
            -1288,
            -3865,
            -4102,
            -2122,
            -3728,
            -5290,
            -1195,
            -848,
            -1862,
            -529,
            -4348,
            -3257,
            -707,
            -2647,
            -2696,
            -3312,
            -2780,
            -584,
            -1400,
            -2377,
            -2143,
            -1917,
            -2699
        ],
        "steps_history": [
            5290,
            4973,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            1841,
            5290,
            5290,
            1479,
            5290,
            1193,
            5290,
            5290,
            1141,
            5290,
            5290,
            2245,
            1341,
            5290,
            5290,
            5290,
            5290,
            4627,
            2893,
            1060,
            5290,
            2831,
            3064,
            1389,
            3966,
            4203,
            2223,
            3829,
            5290,
            1296,
            949,
            1963,
            630,
            4449,
            3358,
            808,
            2748,
            2797,
            3413,
            2881,
            685,
            1501,
            2478,
            2244,
            2018,
            2800
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "770/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 13.40092158317566,
        "final_policy_stability": 0.9905482041587902,
        "episodes_to_convergence": 64,
        "policy_stability_history": [
            0.0,
            0.8147448015122873,
            0.9054820415879017,
            0.9092627599243857,
            0.8525519848771267,
            0.8790170132325141,
            0.8657844990548205,
            0.8601134215500945,
            0.8809073724007561,
            0.8714555765595463,
            0.8903591682419659,
            0.8714555765595463,
            0.8771266540642723,
            0.8922495274102079,
            0.8752362948960303,
            0.8468809073724007,
            0.8601134215500945,
            0.9300567107750473,
            0.8657844990548205,
            0.8752362948960303,
            0.8412098298676749,
            0.9206049149338374,
            0.8941398865784499,
            0.8695652173913043,
            0.8525519848771267,
            0.8771266540642723,
            0.8846880907372401,
            0.941398865784499,
            0.941398865784499,
            0.9659735349716446,
            0.888468809073724,
            0.947069943289225,
            0.8922495274102079,
            0.9338374291115312,
            0.8544423440453687,
            0.9092627599243857,
            0.9206049149338374,
            0.9357277882797732,
            0.9395085066162571,
            0.9035916824196597,
            0.9300567107750473,
            0.9603024574669187,
            0.8998109640831758,
            0.9584120982986768,
            0.9489603024574669,
            0.945179584120983,
            0.9262759924385633,
            0.9489603024574669,
            0.9508506616257089,
            0.9716446124763705,
            0.9792060491493384,
            0.9338374291115312,
            0.9848771266540642,
            0.9886578449905482,
            0.9716446124763705,
            0.9489603024574669,
            0.9924385633270322,
            0.9659735349716446,
            0.9621928166351607,
            0.9810964083175804,
            0.9886578449905482,
            0.998109640831758,
            1.0,
            0.9867674858223062,
            0.9905482041587902
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -2529,
            -5290,
            -5290,
            -5290,
            -2285,
            -5290,
            -2066,
            -4031,
            -4236,
            -5290,
            -3127,
            -5290,
            -5290,
            -1642,
            -3889,
            -5290,
            -3529,
            -2236,
            -1608,
            -4284,
            -3445,
            -5290,
            -5290,
            -1601,
            -893,
            -743,
            -5290,
            -1663,
            -5290,
            -2529,
            -5290,
            -2308,
            -2299,
            -2503,
            -1340,
            -3974,
            -2400,
            -1321,
            -5290,
            -1473,
            -2672,
            -2534,
            -2625,
            -1738,
            -2208,
            -1463,
            -1397,
            -1756,
            -1352,
            -659,
            -1999,
            -2507,
            -633,
            -1477,
            -889,
            -1453,
            -870,
            -2097,
            -877,
            -1403,
            -1397
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            2630,
            5290,
            5290,
            5290,
            2386,
            5290,
            2167,
            4132,
            4337,
            5290,
            3228,
            5290,
            5290,
            1743,
            3990,
            5290,
            3630,
            2337,
            1709,
            4385,
            3546,
            5290,
            5290,
            1702,
            994,
            844,
            5290,
            1764,
            5290,
            2630,
            5290,
            2409,
            2400,
            2604,
            1441,
            4075,
            2501,
            1422,
            5290,
            1574,
            2773,
            2635,
            2726,
            1839,
            2309,
            1564,
            1498,
            1857,
            1453,
            760,
            2100,
            2608,
            734,
            1578,
            990,
            1554,
            971,
            2198,
            978,
            1504,
            1498
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "771/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 13.58685851097107,
        "final_policy_stability": 0.9810964083175804,
        "episodes_to_convergence": 69,
        "policy_stability_history": [
            0.0,
            0.9092627599243857,
            0.8223062381852552,
            0.833648393194707,
            0.8676748582230623,
            0.8922495274102079,
            0.8582230623818525,
            0.9035916824196597,
            0.8827977315689981,
            0.8487712665406427,
            0.8790170132325141,
            0.8657844990548205,
            0.8657844990548205,
            0.8563327032136105,
            0.8922495274102079,
            0.8506616257088847,
            0.8752362948960303,
            0.945179584120983,
            0.9017013232514177,
            0.8998109640831758,
            0.9262759924385633,
            0.8298676748582231,
            0.9281663516068053,
            0.8846880907372401,
            0.8941398865784499,
            0.9697542533081286,
            0.888468809073724,
            0.9224952741020794,
            0.943289224952741,
            0.9773156899810964,
            0.943289224952741,
            0.9111531190926276,
            0.9678638941398866,
            0.8960302457466919,
            0.9281663516068053,
            0.9621928166351607,
            0.8979206049149339,
            0.9035916824196597,
            0.9867674858223062,
            0.9754253308128544,
            0.9546313799621928,
            0.9054820415879017,
            0.8903591682419659,
            0.9829867674858223,
            0.9735349716446124,
            0.9659735349716446,
            0.9905482041587902,
            0.9546313799621928,
            0.9735349716446124,
            0.9848771266540642,
            0.9716446124763705,
            0.9848771266540642,
            0.9754253308128544,
            0.9867674858223062,
            0.9886578449905482,
            0.9924385633270322,
            0.9924385633270322,
            0.9905482041587902,
            0.998109640831758,
            0.9905482041587902,
            0.941398865784499,
            0.994328922495274,
            0.9848771266540642,
            0.9754253308128544,
            0.996219281663516,
            0.9810964083175804,
            0.998109640831758,
            0.9867674858223062,
            0.994328922495274,
            0.9810964083175804
        ],
        "reward_history": [
            -3888,
            -5290,
            -5290,
            -5290,
            -5290,
            -2489,
            -4517,
            -5290,
            -5290,
            -4534,
            -2450,
            -5290,
            -5290,
            -3811,
            -2787,
            -5290,
            -5290,
            -1116,
            -3845,
            -2087,
            -2128,
            -5290,
            -1456,
            -5290,
            -3425,
            -447,
            -4254,
            -3953,
            -1165,
            -856,
            -1606,
            -2844,
            -936,
            -5290,
            -2032,
            -1905,
            -3671,
            -4560,
            -1195,
            -712,
            -1831,
            -3418,
            -5290,
            -863,
            -1076,
            -1131,
            -630,
            -1932,
            -1158,
            -1153,
            -832,
            -1175,
            -1336,
            -629,
            -818,
            -697,
            -915,
            -884,
            -675,
            -1225,
            -3773,
            -1685,
            -905,
            -2028,
            -973,
            -1600,
            -1756,
            -1158,
            -1372,
            -1070
        ],
        "steps_history": [
            3989,
            5290,
            5290,
            5290,
            5290,
            2590,
            4618,
            5290,
            5290,
            4635,
            2551,
            5290,
            5290,
            3912,
            2888,
            5290,
            5290,
            1217,
            3946,
            2188,
            2229,
            5290,
            1557,
            5290,
            3526,
            548,
            4355,
            4054,
            1266,
            957,
            1707,
            2945,
            1037,
            5290,
            2133,
            2006,
            3772,
            4661,
            1296,
            813,
            1932,
            3519,
            5290,
            964,
            1177,
            1232,
            731,
            2033,
            1259,
            1254,
            933,
            1276,
            1437,
            730,
            919,
            798,
            1016,
            985,
            776,
            1326,
            3874,
            1786,
            1006,
            2129,
            1074,
            1701,
            1857,
            1259,
            1473,
            1171
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "772/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.724310874938965,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 61,
        "policy_stability_history": [
            0.0,
            0.8090737240075614,
            0.8374291115311909,
            0.8260869565217391,
            0.9035916824196597,
            0.8147448015122873,
            0.8941398865784499,
            0.8412098298676749,
            0.9035916824196597,
            0.9035916824196597,
            0.8525519848771267,
            0.8979206049149339,
            0.9149338374291115,
            0.8790170132325141,
            0.8809073724007561,
            0.9527410207939508,
            0.8657844990548205,
            0.8449905482041588,
            0.9300567107750473,
            0.9168241965973535,
            0.9376181474480151,
            0.8487712665406427,
            0.8676748582230623,
            0.8809073724007561,
            0.9092627599243857,
            0.943289224952741,
            0.888468809073724,
            0.9697542533081286,
            0.9300567107750473,
            0.9054820415879017,
            0.9017013232514177,
            0.9565217391304348,
            0.9149338374291115,
            0.8657844990548205,
            0.9395085066162571,
            0.9527410207939508,
            0.8922495274102079,
            0.9659735349716446,
            0.9111531190926276,
            0.9224952741020794,
            0.9848771266540642,
            0.9565217391304348,
            0.947069943289225,
            0.9886578449905482,
            0.9867674858223062,
            0.9810964083175804,
            0.9357277882797732,
            0.9867674858223062,
            0.9792060491493384,
            0.9848771266540642,
            0.9792060491493384,
            0.9489603024574669,
            0.9886578449905482,
            0.9886578449905482,
            0.9867674858223062,
            0.998109640831758,
            0.9054820415879017,
            0.9603024574669187,
            0.943289224952741,
            0.998109640831758,
            1.0,
            1.0
        ],
        "reward_history": [
            -5290,
            -3454,
            -3385,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2686,
            -2955,
            -1334,
            -5290,
            -5290,
            -809,
            -3739,
            -4912,
            -1308,
            -1171,
            -1174,
            -5290,
            -4962,
            -5290,
            -5290,
            -1674,
            -4979,
            -1319,
            -2820,
            -4091,
            -2281,
            -1915,
            -2329,
            -5290,
            -1839,
            -851,
            -5290,
            -1456,
            -4174,
            -4017,
            -443,
            -1911,
            -1769,
            -451,
            -570,
            -583,
            -2240,
            -930,
            -461,
            -946,
            -810,
            -2338,
            -721,
            -710,
            -830,
            -539,
            -4222,
            -2168,
            -2785,
            -494,
            -757,
            -878
        ],
        "steps_history": [
            5290,
            3555,
            3486,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            2787,
            3056,
            1435,
            5290,
            5290,
            910,
            3840,
            5013,
            1409,
            1272,
            1275,
            5290,
            5063,
            5290,
            5290,
            1775,
            5080,
            1420,
            2921,
            4192,
            2382,
            2016,
            2430,
            5290,
            1940,
            952,
            5290,
            1557,
            4275,
            4118,
            544,
            2012,
            1870,
            552,
            671,
            684,
            2341,
            1031,
            562,
            1047,
            911,
            2439,
            822,
            811,
            931,
            640,
            4323,
            2269,
            2886,
            595,
            858,
            979
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "773/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 13.136977672576904,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 64,
        "policy_stability_history": [
            0.0,
            0.8431001890359168,
            0.9035916824196597,
            0.8487712665406427,
            0.8657844990548205,
            0.8563327032136105,
            0.8903591682419659,
            0.8563327032136105,
            0.8941398865784499,
            0.8431001890359168,
            0.8752362948960303,
            0.9130434782608695,
            0.8752362948960303,
            0.9035916824196597,
            0.9300567107750473,
            0.9073724007561437,
            0.8468809073724007,
            0.8695652173913043,
            0.8790170132325141,
            0.8941398865784499,
            0.9168241965973535,
            0.8998109640831758,
            0.8979206049149339,
            0.8922495274102079,
            0.9243856332703214,
            0.9640831758034026,
            0.9376181474480151,
            0.9678638941398866,
            0.8714555765595463,
            0.9338374291115312,
            0.9168241965973535,
            0.9092627599243857,
            0.9168241965973535,
            0.9527410207939508,
            0.9092627599243857,
            0.8922495274102079,
            0.9697542533081286,
            0.9640831758034026,
            0.9792060491493384,
            0.8827977315689981,
            0.9886578449905482,
            0.9621928166351607,
            0.9262759924385633,
            0.9697542533081286,
            0.8960302457466919,
            0.9621928166351607,
            0.9224952741020794,
            0.9508506616257089,
            0.9735349716446124,
            0.9111531190926276,
            0.9546313799621928,
            0.9792060491493384,
            0.9810964083175804,
            0.9773156899810964,
            0.9603024574669187,
            0.9754253308128544,
            0.9735349716446124,
            0.9489603024574669,
            0.9300567107750473,
            0.9206049149338374,
            0.9792060491493384,
            0.9924385633270322,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -974,
            -5290,
            -5290,
            -5290,
            -5290,
            -2902,
            -2559,
            -5290,
            -5290,
            -1594,
            -5290,
            -5290,
            -1054,
            -2790,
            -5290,
            -4977,
            -5290,
            -4074,
            -1463,
            -5290,
            -2366,
            -5290,
            -2023,
            -907,
            -1195,
            -725,
            -5290,
            -1464,
            -1968,
            -2515,
            -2979,
            -2209,
            -2599,
            -5012,
            -974,
            -1059,
            -284,
            -5290,
            -330,
            -1506,
            -2256,
            -757,
            -5290,
            -1673,
            -4059,
            -1505,
            -1468,
            -3706,
            -1424,
            -1400,
            -1494,
            -801,
            -1920,
            -1952,
            -1351,
            -2856,
            -4377,
            -4019,
            -1337,
            -904,
            -494,
            -674,
            -637
        ],
        "steps_history": [
            5290,
            5290,
            1075,
            5290,
            5290,
            5290,
            5290,
            3003,
            2660,
            5290,
            5290,
            1695,
            5290,
            5290,
            1155,
            2891,
            5290,
            5078,
            5290,
            4175,
            1564,
            5290,
            2467,
            5290,
            2124,
            1008,
            1296,
            826,
            5290,
            1565,
            2069,
            2616,
            3080,
            2310,
            2700,
            5113,
            1075,
            1160,
            385,
            5290,
            431,
            1607,
            2357,
            858,
            5290,
            1774,
            4160,
            1606,
            1569,
            3807,
            1525,
            1501,
            1595,
            902,
            2021,
            2053,
            1452,
            2957,
            4478,
            4120,
            1438,
            1005,
            595,
            775,
            738
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "774/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 14.335484266281128,
        "final_policy_stability": 0.9792060491493384,
        "episodes_to_convergence": 72,
        "policy_stability_history": [
            0.0,
            0.831758034026465,
            0.8979206049149339,
            0.8449905482041588,
            0.8166351606805293,
            0.8128544423440454,
            0.8601134215500945,
            0.8506616257088847,
            0.8960302457466919,
            0.9281663516068053,
            0.8979206049149339,
            0.9262759924385633,
            0.8185255198487713,
            0.941398865784499,
            0.8657844990548205,
            0.9130434782608695,
            0.9357277882797732,
            0.9489603024574669,
            0.9338374291115312,
            0.9281663516068053,
            0.8752362948960303,
            0.9054820415879017,
            0.8809073724007561,
            0.9243856332703214,
            0.9111531190926276,
            0.8960302457466919,
            0.8827977315689981,
            0.9187145557655955,
            0.831758034026465,
            0.8525519848771267,
            0.9376181474480151,
            0.9621928166351607,
            0.9603024574669187,
            0.9376181474480151,
            0.9357277882797732,
            0.9697542533081286,
            0.9640831758034026,
            0.9621928166351607,
            0.9206049149338374,
            0.9546313799621928,
            0.9206049149338374,
            0.9281663516068053,
            0.9224952741020794,
            0.9810964083175804,
            0.9678638941398866,
            0.9829867674858223,
            0.9319470699432892,
            0.996219281663516,
            0.9640831758034026,
            0.9130434782608695,
            0.8922495274102079,
            0.9924385633270322,
            0.9697542533081286,
            0.9716446124763705,
            0.996219281663516,
            0.9810964083175804,
            0.9886578449905482,
            0.9640831758034026,
            0.9754253308128544,
            0.9867674858223062,
            0.9810964083175804,
            0.945179584120983,
            0.998109640831758,
            0.9810964083175804,
            0.998109640831758,
            0.9735349716446124,
            0.9886578449905482,
            0.9867674858223062,
            0.9735349716446124,
            0.994328922495274,
            1.0,
            0.996219281663516,
            0.9792060491493384
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -4828,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4193,
            -1506,
            -5128,
            -1320,
            -5290,
            -1994,
            -1341,
            -1022,
            -1720,
            -1400,
            -3666,
            -2267,
            -5290,
            -1112,
            -2142,
            -5290,
            -5290,
            -5290,
            -5290,
            -5167,
            -1376,
            -685,
            -1180,
            -2215,
            -3695,
            -608,
            -646,
            -1245,
            -3028,
            -1901,
            -4718,
            -2467,
            -3446,
            -779,
            -1010,
            -1062,
            -2977,
            -458,
            -1488,
            -3772,
            -5290,
            -1505,
            -882,
            -1277,
            -859,
            -916,
            -776,
            -1457,
            -1016,
            -369,
            -1122,
            -2912,
            -695,
            -1055,
            -580,
            -1349,
            -816,
            -1003,
            -2096,
            -1605,
            -1040,
            -683,
            -1380
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            4929,
            5290,
            5290,
            5290,
            5290,
            5290,
            4294,
            1607,
            5229,
            1421,
            5290,
            2095,
            1442,
            1123,
            1821,
            1501,
            3767,
            2368,
            5290,
            1213,
            2243,
            5290,
            5290,
            5290,
            5290,
            5268,
            1477,
            786,
            1281,
            2316,
            3796,
            709,
            747,
            1346,
            3129,
            2002,
            4819,
            2568,
            3547,
            880,
            1111,
            1163,
            3078,
            559,
            1589,
            3873,
            5290,
            1606,
            983,
            1378,
            960,
            1017,
            877,
            1558,
            1117,
            470,
            1223,
            3013,
            796,
            1156,
            681,
            1450,
            917,
            1104,
            2197,
            1706,
            1141,
            784,
            1481
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "775/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 13.607101917266846,
        "final_policy_stability": 0.9886578449905482,
        "episodes_to_convergence": 69,
        "policy_stability_history": [
            0.0,
            0.8809073724007561,
            0.8185255198487713,
            0.8657844990548205,
            0.8506616257088847,
            0.8657844990548205,
            0.8563327032136105,
            0.9035916824196597,
            0.9017013232514177,
            0.8544423440453687,
            0.8506616257088847,
            0.8544423440453687,
            0.8676748582230623,
            0.9017013232514177,
            0.8412098298676749,
            0.9187145557655955,
            0.833648393194707,
            0.8487712665406427,
            0.8733459357277883,
            0.9168241965973535,
            0.8582230623818525,
            0.9073724007561437,
            0.9073724007561437,
            0.9319470699432892,
            0.8922495274102079,
            0.9357277882797732,
            0.8506616257088847,
            0.9206049149338374,
            0.8960302457466919,
            0.9111531190926276,
            0.8733459357277883,
            0.8449905482041588,
            0.9584120982986768,
            0.9489603024574669,
            0.8941398865784499,
            0.8960302457466919,
            0.8752362948960303,
            0.941398865784499,
            0.9395085066162571,
            0.9754253308128544,
            0.9735349716446124,
            0.9565217391304348,
            0.9640831758034026,
            0.9754253308128544,
            0.9754253308128544,
            0.9054820415879017,
            0.888468809073724,
            0.9716446124763705,
            0.9886578449905482,
            0.9905482041587902,
            0.9867674858223062,
            0.994328922495274,
            0.9924385633270322,
            0.8809073724007561,
            0.994328922495274,
            0.9905482041587902,
            0.9224952741020794,
            0.9508506616257089,
            0.9886578449905482,
            0.9886578449905482,
            0.9886578449905482,
            0.9905482041587902,
            0.9659735349716446,
            0.9697542533081286,
            0.9735349716446124,
            0.9565217391304348,
            0.998109640831758,
            1.0,
            1.0,
            0.9886578449905482
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2163,
            -5290,
            -2725,
            -5290,
            -2894,
            -4740,
            -5290,
            -4494,
            -1318,
            -3701,
            -1398,
            -3352,
            -5290,
            -3425,
            -1264,
            -2550,
            -2058,
            -2729,
            -1554,
            -3026,
            -1088,
            -5290,
            -1606,
            -5290,
            -1937,
            -3539,
            -5290,
            -1026,
            -875,
            -5290,
            -3423,
            -5080,
            -1356,
            -2091,
            -575,
            -813,
            -1620,
            -970,
            -482,
            -854,
            -4121,
            -5290,
            -1371,
            -791,
            -536,
            -427,
            -464,
            -437,
            -5290,
            -696,
            -485,
            -3474,
            -1648,
            -515,
            -752,
            -946,
            -720,
            -1067,
            -1877,
            -863,
            -2685,
            -872,
            -733,
            -881,
            -493
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            2264,
            5290,
            2826,
            5290,
            2995,
            4841,
            5290,
            4595,
            1419,
            3802,
            1499,
            3453,
            5290,
            3526,
            1365,
            2651,
            2159,
            2830,
            1655,
            3127,
            1189,
            5290,
            1707,
            5290,
            2038,
            3640,
            5290,
            1127,
            976,
            5290,
            3524,
            5181,
            1457,
            2192,
            676,
            914,
            1721,
            1071,
            583,
            955,
            4222,
            5290,
            1472,
            892,
            637,
            528,
            565,
            538,
            5290,
            797,
            586,
            3575,
            1749,
            616,
            853,
            1047,
            821,
            1168,
            1978,
            964,
            2786,
            973,
            834,
            982,
            594
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "776/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 13.649219036102295,
        "final_policy_stability": 0.9886578449905482,
        "episodes_to_convergence": 70,
        "policy_stability_history": [
            0.0,
            0.833648393194707,
            0.8431001890359168,
            0.8620037807183365,
            0.8752362948960303,
            0.8412098298676749,
            0.8846880907372401,
            0.8865784499054821,
            0.8355387523629489,
            0.8279773156899811,
            0.8374291115311909,
            0.9035916824196597,
            0.8034026465028355,
            0.9489603024574669,
            0.9168241965973535,
            0.8563327032136105,
            0.8771266540642723,
            0.8941398865784499,
            0.9546313799621928,
            0.8714555765595463,
            0.8223062381852552,
            0.9149338374291115,
            0.8809073724007561,
            0.9092627599243857,
            0.8846880907372401,
            0.9376181474480151,
            0.9130434782608695,
            0.9546313799621928,
            0.8582230623818525,
            0.8809073724007561,
            0.8809073724007561,
            0.9678638941398866,
            0.9111531190926276,
            0.9697542533081286,
            0.8714555765595463,
            0.9073724007561437,
            0.9224952741020794,
            0.9621928166351607,
            0.9716446124763705,
            0.9508506616257089,
            0.9603024574669187,
            0.947069943289225,
            0.9810964083175804,
            0.9319470699432892,
            0.941398865784499,
            0.945179584120983,
            0.9697542533081286,
            0.994328922495274,
            0.8676748582230623,
            0.9489603024574669,
            0.9848771266540642,
            0.9621928166351607,
            0.9905482041587902,
            0.941398865784499,
            0.9792060491493384,
            0.8506616257088847,
            0.996219281663516,
            0.9716446124763705,
            0.9792060491493384,
            0.9905482041587902,
            0.996219281663516,
            0.9924385633270322,
            0.9792060491493384,
            0.9905482041587902,
            0.9886578449905482,
            0.998109640831758,
            0.9829867674858223,
            0.9924385633270322,
            0.9584120982986768,
            0.9640831758034026,
            0.9886578449905482
        ],
        "reward_history": [
            -5290,
            -5290,
            -4292,
            -3906,
            -5290,
            -5290,
            -2466,
            -5290,
            -5290,
            -4253,
            -5290,
            -2852,
            -5290,
            -662,
            -1249,
            -5290,
            -5290,
            -2381,
            -1033,
            -4385,
            -5290,
            -1580,
            -3018,
            -2107,
            -3522,
            -915,
            -2354,
            -955,
            -5290,
            -4078,
            -5290,
            -489,
            -2613,
            -415,
            -3717,
            -4362,
            -4305,
            -976,
            -627,
            -1077,
            -721,
            -1213,
            -643,
            -2658,
            -1769,
            -2084,
            -803,
            -239,
            -5290,
            -2642,
            -597,
            -1653,
            -471,
            -2186,
            -698,
            -5290,
            -533,
            -1541,
            -709,
            -892,
            -768,
            -555,
            -1134,
            -683,
            -1274,
            -526,
            -644,
            -794,
            -1463,
            -2600,
            -1101
        ],
        "steps_history": [
            5290,
            5290,
            4393,
            4007,
            5290,
            5290,
            2567,
            5290,
            5290,
            4354,
            5290,
            2953,
            5290,
            763,
            1350,
            5290,
            5290,
            2482,
            1134,
            4486,
            5290,
            1681,
            3119,
            2208,
            3623,
            1016,
            2455,
            1056,
            5290,
            4179,
            5290,
            590,
            2714,
            516,
            3818,
            4463,
            4406,
            1077,
            728,
            1178,
            822,
            1314,
            744,
            2759,
            1870,
            2185,
            904,
            340,
            5290,
            2743,
            698,
            1754,
            572,
            2287,
            799,
            5290,
            634,
            1642,
            810,
            993,
            869,
            656,
            1235,
            784,
            1375,
            627,
            745,
            895,
            1564,
            2701,
            1202
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "777/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 13.483485698699951,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 70,
        "policy_stability_history": [
            0.0,
            0.9054820415879017,
            0.8393194706994329,
            0.8506616257088847,
            0.8582230623818525,
            0.888468809073724,
            0.8279773156899811,
            0.8752362948960303,
            0.8733459357277883,
            0.9187145557655955,
            0.8714555765595463,
            0.8865784499054821,
            0.9149338374291115,
            0.9300567107750473,
            0.8790170132325141,
            0.8393194706994329,
            0.8998109640831758,
            0.9376181474480151,
            0.8998109640831758,
            0.8809073724007561,
            0.9017013232514177,
            0.9111531190926276,
            0.943289224952741,
            0.8563327032136105,
            0.9565217391304348,
            0.9357277882797732,
            0.9300567107750473,
            0.941398865784499,
            0.8790170132325141,
            0.9338374291115312,
            0.8941398865784499,
            0.8449905482041588,
            0.9300567107750473,
            0.8960302457466919,
            0.9886578449905482,
            0.9603024574669187,
            0.947069943289225,
            0.9697542533081286,
            0.9376181474480151,
            0.8582230623818525,
            0.8979206049149339,
            0.947069943289225,
            0.9792060491493384,
            0.9621928166351607,
            0.9357277882797732,
            0.9659735349716446,
            0.9376181474480151,
            0.9376181474480151,
            0.9829867674858223,
            0.9716446124763705,
            0.9546313799621928,
            0.8998109640831758,
            0.9716446124763705,
            0.9338374291115312,
            0.9867674858223062,
            0.994328922495274,
            0.9716446124763705,
            0.9754253308128544,
            0.943289224952741,
            0.994328922495274,
            0.994328922495274,
            0.9527410207939508,
            0.9886578449905482,
            0.9924385633270322,
            0.996219281663516,
            0.9376181474480151,
            0.9886578449905482,
            1.0,
            0.994328922495274,
            0.9886578449905482,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -3645,
            -4640,
            -5290,
            -4979,
            -5290,
            -5290,
            -5290,
            -2992,
            -5290,
            -1513,
            -794,
            -2164,
            -5290,
            -2930,
            -1286,
            -5290,
            -5290,
            -2008,
            -1390,
            -1208,
            -5290,
            -941,
            -1101,
            -1552,
            -1127,
            -4607,
            -2713,
            -2598,
            -5290,
            -1331,
            -3183,
            -467,
            -1315,
            -978,
            -755,
            -1404,
            -5290,
            -2721,
            -1901,
            -666,
            -1029,
            -1306,
            -1072,
            -1443,
            -2741,
            -508,
            -831,
            -1020,
            -3941,
            -1168,
            -3226,
            -753,
            -982,
            -999,
            -1016,
            -4551,
            -715,
            -311,
            -1804,
            -425,
            -719,
            -534,
            -3064,
            -619,
            -357,
            -255,
            -716,
            -258
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            3746,
            4741,
            5290,
            5080,
            5290,
            5290,
            5290,
            3093,
            5290,
            1614,
            895,
            2265,
            5290,
            3031,
            1387,
            5290,
            5290,
            2109,
            1491,
            1309,
            5290,
            1042,
            1202,
            1653,
            1228,
            4708,
            2814,
            2699,
            5290,
            1432,
            3284,
            568,
            1416,
            1079,
            856,
            1505,
            5290,
            2822,
            2002,
            767,
            1130,
            1407,
            1173,
            1544,
            2842,
            609,
            932,
            1121,
            4042,
            1269,
            3327,
            854,
            1083,
            1100,
            1117,
            4652,
            816,
            412,
            1905,
            526,
            820,
            635,
            3165,
            720,
            458,
            356,
            817,
            359
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "778/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 14.26203441619873,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 73,
        "policy_stability_history": [
            0.0,
            0.8034026465028355,
            0.8487712665406427,
            0.831758034026465,
            0.8790170132325141,
            0.8941398865784499,
            0.8733459357277883,
            0.9149338374291115,
            0.8638941398865785,
            0.8695652173913043,
            0.8714555765595463,
            0.9130434782608695,
            0.8544423440453687,
            0.8449905482041588,
            0.9092627599243857,
            0.8204158790170132,
            0.8714555765595463,
            0.8563327032136105,
            0.8998109640831758,
            0.8922495274102079,
            0.9092627599243857,
            0.9168241965973535,
            0.9395085066162571,
            0.8449905482041588,
            0.9243856332703214,
            0.9224952741020794,
            0.8468809073724007,
            0.947069943289225,
            0.941398865784499,
            0.9073724007561437,
            0.9262759924385633,
            0.9640831758034026,
            0.9640831758034026,
            0.8790170132325141,
            0.9149338374291115,
            0.9546313799621928,
            0.8468809073724007,
            0.9621928166351607,
            0.8903591682419659,
            0.8979206049149339,
            0.9395085066162571,
            0.9697542533081286,
            0.9527410207939508,
            0.9640831758034026,
            0.9735349716446124,
            0.9546313799621928,
            0.9546313799621928,
            0.9603024574669187,
            0.9546313799621928,
            0.8752362948960303,
            0.9867674858223062,
            0.9130434782608695,
            0.9603024574669187,
            0.9886578449905482,
            0.9603024574669187,
            0.9792060491493384,
            0.9584120982986768,
            0.9508506616257089,
            0.996219281663516,
            0.9867674858223062,
            0.994328922495274,
            0.9924385633270322,
            0.9735349716446124,
            0.9829867674858223,
            0.9697542533081286,
            0.9508506616257089,
            0.996219281663516,
            0.996219281663516,
            0.9905482041587902,
            0.9829867674858223,
            1.0,
            0.9848771266540642,
            0.998109640831758,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -3431,
            -1459,
            -2312,
            -2313,
            -3833,
            -1007,
            -5290,
            -5290,
            -1629,
            -4490,
            -3435,
            -3658,
            -5290,
            -2758,
            -5290,
            -1870,
            -695,
            -5290,
            -1456,
            -1369,
            -4503,
            -1082,
            -1621,
            -1935,
            -2262,
            -554,
            -1034,
            -3036,
            -5290,
            -1778,
            -5290,
            -742,
            -5290,
            -2312,
            -1935,
            -881,
            -566,
            -728,
            -1040,
            -1095,
            -2835,
            -824,
            -1489,
            -5290,
            -593,
            -5290,
            -1595,
            -595,
            -996,
            -911,
            -1514,
            -1553,
            -416,
            -1396,
            -866,
            -769,
            -837,
            -669,
            -2065,
            -1863,
            -796,
            -724,
            -456,
            -894,
            -538,
            -1064,
            -552,
            -798
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            3532,
            1560,
            2413,
            2414,
            3934,
            1108,
            5290,
            5290,
            1730,
            4591,
            3536,
            3759,
            5290,
            2859,
            5290,
            1971,
            796,
            5290,
            1557,
            1470,
            4604,
            1183,
            1722,
            2036,
            2363,
            655,
            1135,
            3137,
            5290,
            1879,
            5290,
            843,
            5290,
            2413,
            2036,
            982,
            667,
            829,
            1141,
            1196,
            2936,
            925,
            1590,
            5290,
            694,
            5290,
            1696,
            696,
            1097,
            1012,
            1615,
            1654,
            517,
            1497,
            967,
            870,
            938,
            770,
            2166,
            1964,
            897,
            825,
            557,
            995,
            639,
            1165,
            653,
            899
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "779/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.724333763122559,
        "final_policy_stability": 0.996219281663516,
        "episodes_to_convergence": 65,
        "policy_stability_history": [
            0.0,
            0.8487712665406427,
            0.8506616257088847,
            0.8431001890359168,
            0.888468809073724,
            0.8790170132325141,
            0.8676748582230623,
            0.8601134215500945,
            0.8620037807183365,
            0.8468809073724007,
            0.8431001890359168,
            0.8827977315689981,
            0.8657844990548205,
            0.8695652173913043,
            0.9149338374291115,
            0.8941398865784499,
            0.8922495274102079,
            0.8771266540642723,
            0.8620037807183365,
            0.9300567107750473,
            0.9187145557655955,
            0.8827977315689981,
            0.9224952741020794,
            0.8525519848771267,
            0.9508506616257089,
            0.8960302457466919,
            0.9357277882797732,
            0.8695652173913043,
            0.9054820415879017,
            0.9508506616257089,
            0.9716446124763705,
            0.8544423440453687,
            0.8865784499054821,
            0.9395085066162571,
            0.9697542533081286,
            0.9111531190926276,
            0.9376181474480151,
            0.9035916824196597,
            0.9716446124763705,
            0.9376181474480151,
            0.8809073724007561,
            0.9773156899810964,
            0.9168241965973535,
            0.9659735349716446,
            0.9338374291115312,
            0.9584120982986768,
            0.9092627599243857,
            0.9716446124763705,
            0.941398865784499,
            0.994328922495274,
            0.9792060491493384,
            0.9867674858223062,
            0.9829867674858223,
            0.8809073724007561,
            0.994328922495274,
            0.9395085066162571,
            0.9905482041587902,
            0.9546313799621928,
            0.9735349716446124,
            0.9924385633270322,
            0.9924385633270322,
            0.9867674858223062,
            0.9905482041587902,
            0.994328922495274,
            0.998109640831758,
            0.996219281663516
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4277,
            -2631,
            -5290,
            -5290,
            -4150,
            -2842,
            -5290,
            -3335,
            -1533,
            -5290,
            -5290,
            -4749,
            -2680,
            -1603,
            -1854,
            -2437,
            -885,
            -5290,
            -680,
            -5290,
            -1404,
            -5290,
            -3259,
            -1270,
            -605,
            -5290,
            -4005,
            -1458,
            -720,
            -2025,
            -1318,
            -2304,
            -822,
            -1748,
            -4364,
            -525,
            -2052,
            -1063,
            -1824,
            -1109,
            -4083,
            -1421,
            -1989,
            -461,
            -348,
            -404,
            -681,
            -3687,
            -247,
            -2752,
            -582,
            -1253,
            -797,
            -523,
            -561,
            -784,
            -629,
            -376,
            -601,
            -670
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            4378,
            2732,
            5290,
            5290,
            4251,
            2943,
            5290,
            3436,
            1634,
            5290,
            5290,
            4850,
            2781,
            1704,
            1955,
            2538,
            986,
            5290,
            781,
            5290,
            1505,
            5290,
            3360,
            1371,
            706,
            5290,
            4106,
            1559,
            821,
            2126,
            1419,
            2405,
            923,
            1849,
            4465,
            626,
            2153,
            1164,
            1925,
            1210,
            4184,
            1522,
            2090,
            562,
            449,
            505,
            782,
            3788,
            348,
            2853,
            683,
            1354,
            898,
            624,
            662,
            885,
            730,
            477,
            702,
            771
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "780/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.3_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.393615484237671,
        "final_policy_stability": 0.998109640831758,
        "episodes_to_convergence": 54,
        "policy_stability_history": [
            0.0,
            0.8922495274102079,
            0.8733459357277883,
            0.833648393194707,
            0.9073724007561437,
            0.8525519848771267,
            0.888468809073724,
            0.9130434782608695,
            0.8468809073724007,
            0.8865784499054821,
            0.8827977315689981,
            0.8714555765595463,
            0.8638941398865785,
            0.9092627599243857,
            0.9206049149338374,
            0.9168241965973535,
            0.9111531190926276,
            0.8620037807183365,
            0.8827977315689981,
            0.9565217391304348,
            0.8846880907372401,
            0.8771266540642723,
            0.9886578449905482,
            0.8941398865784499,
            0.9168241965973535,
            0.8979206049149339,
            0.943289224952741,
            0.9584120982986768,
            0.9489603024574669,
            0.9243856332703214,
            0.9508506616257089,
            0.9508506616257089,
            0.9357277882797732,
            0.9621928166351607,
            0.9187145557655955,
            0.9603024574669187,
            0.8922495274102079,
            0.8695652173913043,
            0.9584120982986768,
            0.9243856332703214,
            0.9848771266540642,
            0.9603024574669187,
            0.9111531190926276,
            0.9735349716446124,
            0.9678638941398866,
            0.9640831758034026,
            0.9924385633270322,
            0.9773156899810964,
            0.9810964083175804,
            0.9792060491493384,
            0.994328922495274,
            0.9565217391304348,
            0.996219281663516,
            0.9810964083175804,
            0.998109640831758
        ],
        "reward_history": [
            -3253,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5044,
            -5290,
            -5290,
            -5290,
            -2226,
            -5290,
            -2811,
            -5290,
            -5290,
            -5290,
            -1352,
            -3410,
            -4075,
            -826,
            -5290,
            -3833,
            -5290,
            -1726,
            -1119,
            -2637,
            -5290,
            -1924,
            -1662,
            -1686,
            -1935,
            -3711,
            -1461,
            -4205,
            -5290,
            -2388,
            -3890,
            -1469,
            -1722,
            -3752,
            -2064,
            -1553,
            -1811,
            -1376,
            -1735,
            -2203,
            -2374,
            -397,
            -1905,
            -545,
            -3525,
            -1370
        ],
        "steps_history": [
            3354,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5145,
            5290,
            5290,
            5290,
            2327,
            5290,
            2912,
            5290,
            5290,
            5290,
            1453,
            3511,
            4176,
            927,
            5290,
            3934,
            5290,
            1827,
            1220,
            2738,
            5290,
            2025,
            1763,
            1787,
            2036,
            3812,
            1562,
            4306,
            5290,
            2489,
            3991,
            1570,
            1823,
            3853,
            2165,
            1654,
            1912,
            1477,
            1836,
            2304,
            2475,
            498,
            2006,
            646,
            3626,
            1471
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "781/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.205800771713257,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 51,
        "policy_stability_history": [
            0.0,
            0.8941398865784499,
            0.8941398865784499,
            0.8506616257088847,
            0.8449905482041588,
            0.9035916824196597,
            0.8638941398865785,
            0.8544423440453687,
            0.8714555765595463,
            0.8960302457466919,
            0.8657844990548205,
            0.9187145557655955,
            0.8601134215500945,
            0.8960302457466919,
            0.8771266540642723,
            0.941398865784499,
            0.9168241965973535,
            0.947069943289225,
            0.8657844990548205,
            0.8998109640831758,
            0.9281663516068053,
            0.9149338374291115,
            0.9395085066162571,
            0.888468809073724,
            0.8638941398865785,
            0.8922495274102079,
            0.945179584120983,
            0.943289224952741,
            0.9565217391304348,
            0.888468809073724,
            0.8903591682419659,
            0.9243856332703214,
            0.9338374291115312,
            0.9206049149338374,
            0.9319470699432892,
            0.947069943289225,
            0.9224952741020794,
            0.9395085066162571,
            0.9035916824196597,
            0.9243856332703214,
            0.9792060491493384,
            0.996219281663516,
            0.9527410207939508,
            0.9584120982986768,
            0.9281663516068053,
            0.994328922495274,
            0.9546313799621928,
            0.994328922495274,
            1.0,
            0.9810964083175804,
            0.9754253308128544,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -2893,
            -5290,
            -5290,
            -5290,
            -5290,
            -3262,
            -5290,
            -1790,
            -5290,
            -5290,
            -5290,
            -937,
            -5290,
            -931,
            -4217,
            -5290,
            -1445,
            -2828,
            -2024,
            -3023,
            -5290,
            -4057,
            -2609,
            -1604,
            -1749,
            -2784,
            -4840,
            -3053,
            -2625,
            -2314,
            -1682,
            -2468,
            -5290,
            -2262,
            -5290,
            -3209,
            -1414,
            -733,
            -3982,
            -4065,
            -4191,
            -622,
            -2782,
            -1469,
            -1102,
            -3548,
            -1941,
            -1509
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            2994,
            5290,
            5290,
            5290,
            5290,
            3363,
            5290,
            1891,
            5290,
            5290,
            5290,
            1038,
            5290,
            1032,
            4318,
            5290,
            1546,
            2929,
            2125,
            3124,
            5290,
            4158,
            2710,
            1705,
            1850,
            2885,
            4941,
            3154,
            2726,
            2415,
            1783,
            2569,
            5290,
            2363,
            5290,
            3310,
            1515,
            834,
            4083,
            4166,
            4292,
            723,
            2883,
            1570,
            1203,
            3649,
            2042,
            1610
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "782/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.813384056091309,
        "final_policy_stability": 0.996219281663516,
        "episodes_to_convergence": 47,
        "policy_stability_history": [
            0.0,
            0.8676748582230623,
            0.8714555765595463,
            0.9376181474480151,
            0.9224952741020794,
            0.8506616257088847,
            0.8979206049149339,
            0.888468809073724,
            0.9054820415879017,
            0.9092627599243857,
            0.9149338374291115,
            0.8714555765595463,
            0.888468809073724,
            0.8922495274102079,
            0.9187145557655955,
            0.9035916824196597,
            0.888468809073724,
            0.9338374291115312,
            0.9621928166351607,
            0.9035916824196597,
            0.8979206049149339,
            0.888468809073724,
            0.9187145557655955,
            0.8922495274102079,
            0.945179584120983,
            0.8998109640831758,
            0.9111531190926276,
            0.947069943289225,
            0.9017013232514177,
            0.9035916824196597,
            0.8771266540642723,
            0.9924385633270322,
            0.9357277882797732,
            0.9848771266540642,
            0.947069943289225,
            0.9357277882797732,
            0.9792060491493384,
            0.9810964083175804,
            0.9243856332703214,
            0.945179584120983,
            0.9829867674858223,
            0.9546313799621928,
            0.9867674858223062,
            0.9905482041587902,
            0.9716446124763705,
            0.9508506616257089,
            0.998109640831758,
            0.996219281663516
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -1769,
            -5290,
            -5290,
            -4895,
            -5290,
            -4267,
            -1902,
            -4856,
            -2268,
            -1813,
            -1252,
            -5290,
            -3263,
            -5290,
            -5290,
            -3399,
            -1617,
            -3637,
            -2494,
            -2308,
            -5290,
            -4571,
            -5290,
            -648,
            -4431,
            -1268,
            -3818,
            -2760,
            -915,
            -1261,
            -4860,
            -3230,
            -1109,
            -3330,
            -734,
            -1279,
            -1896,
            -3120,
            -945,
            -1344
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            1870,
            5290,
            5290,
            4996,
            5290,
            4368,
            2003,
            4957,
            2369,
            1914,
            1353,
            5290,
            3364,
            5290,
            5290,
            3500,
            1718,
            3738,
            2595,
            2409,
            5290,
            4672,
            5290,
            749,
            4532,
            1369,
            3919,
            2861,
            1016,
            1362,
            4961,
            3331,
            1210,
            3431,
            835,
            1380,
            1997,
            3221,
            1046,
            1445
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "783/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.841777801513672,
        "final_policy_stability": 0.998109640831758,
        "episodes_to_convergence": 49,
        "policy_stability_history": [
            0.0,
            0.8279773156899811,
            0.9092627599243857,
            0.8676748582230623,
            0.8279773156899811,
            0.9206049149338374,
            0.8449905482041588,
            0.9035916824196597,
            0.8752362948960303,
            0.8960302457466919,
            0.9187145557655955,
            0.8771266540642723,
            0.8903591682419659,
            0.8298676748582231,
            0.9017013232514177,
            0.9092627599243857,
            0.9017013232514177,
            0.8525519848771267,
            0.9508506616257089,
            0.8657844990548205,
            0.9376181474480151,
            0.9017013232514177,
            0.8714555765595463,
            0.9017013232514177,
            0.8998109640831758,
            0.9603024574669187,
            0.9319470699432892,
            0.9376181474480151,
            0.9527410207939508,
            0.9773156899810964,
            0.945179584120983,
            0.9678638941398866,
            0.9565217391304348,
            0.943289224952741,
            0.9319470699432892,
            0.9376181474480151,
            0.9829867674858223,
            0.9754253308128544,
            0.998109640831758,
            0.9640831758034026,
            0.9659735349716446,
            0.9678638941398866,
            0.9905482041587902,
            0.996219281663516,
            0.9735349716446124,
            0.941398865784499,
            1.0,
            0.9810964083175804,
            0.9867674858223062,
            0.998109640831758
        ],
        "reward_history": [
            -5170,
            -4119,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -3269,
            -5290,
            -3928,
            -5290,
            -5290,
            -2520,
            -5290,
            -5088,
            -5290,
            -1985,
            -5290,
            -2279,
            -5290,
            -5290,
            -4667,
            -3681,
            -2755,
            -2198,
            -2267,
            -1346,
            -856,
            -3373,
            -1251,
            -2466,
            -2891,
            -3005,
            -3750,
            -1969,
            -2629,
            -971,
            -2113,
            -1183,
            -2481,
            -1396,
            -2293,
            -2080,
            -2660,
            -812,
            -903,
            -3471,
            -2338
        ],
        "steps_history": [
            5271,
            4220,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            3370,
            5290,
            4029,
            5290,
            5290,
            2621,
            5290,
            5189,
            5290,
            2086,
            5290,
            2380,
            5290,
            5290,
            4768,
            3782,
            2856,
            2299,
            2368,
            1447,
            957,
            3474,
            1352,
            2567,
            2992,
            3106,
            3851,
            2070,
            2730,
            1072,
            2214,
            1284,
            2582,
            1497,
            2394,
            2181,
            2761,
            913,
            1004,
            3572,
            2439
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "784/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.750043153762817,
        "final_policy_stability": 0.9810964083175804,
        "episodes_to_convergence": 43,
        "policy_stability_history": [
            0.0,
            0.8544423440453687,
            0.9149338374291115,
            0.8865784499054821,
            0.9130434782608695,
            0.8903591682419659,
            0.8393194706994329,
            0.8468809073724007,
            0.8468809073724007,
            0.9243856332703214,
            0.8638941398865785,
            0.8166351606805293,
            0.8620037807183365,
            0.9017013232514177,
            0.8506616257088847,
            0.8846880907372401,
            0.833648393194707,
            0.8771266540642723,
            0.8998109640831758,
            0.9376181474480151,
            0.941398865784499,
            0.8979206049149339,
            0.9489603024574669,
            0.9376181474480151,
            0.9546313799621928,
            0.9489603024574669,
            0.9187145557655955,
            0.9527410207939508,
            0.9262759924385633,
            0.9054820415879017,
            0.9886578449905482,
            0.945179584120983,
            0.9792060491493384,
            0.9206049149338374,
            0.9678638941398866,
            0.9640831758034026,
            0.945179584120983,
            0.9546313799621928,
            0.9319470699432892,
            0.9281663516068053,
            1.0,
            0.9792060491493384,
            0.994328922495274,
            0.9810964083175804
        ],
        "reward_history": [
            -2526,
            -5290,
            -5290,
            -5290,
            -5290,
            -3121,
            -5290,
            -4349,
            -5290,
            -1755,
            -5290,
            -5290,
            -3462,
            -5290,
            -4934,
            -5122,
            -5290,
            -5290,
            -2503,
            -1402,
            -2067,
            -5290,
            -1262,
            -2202,
            -1843,
            -1755,
            -4476,
            -1401,
            -2387,
            -5290,
            -794,
            -1576,
            -1235,
            -3060,
            -1055,
            -1444,
            -2261,
            -1967,
            -3198,
            -3312,
            -689,
            -1776,
            -622,
            -3351
        ],
        "steps_history": [
            2627,
            5290,
            5290,
            5290,
            5290,
            3222,
            5290,
            4450,
            5290,
            1856,
            5290,
            5290,
            3563,
            5290,
            5035,
            5223,
            5290,
            5290,
            2604,
            1503,
            2168,
            5290,
            1363,
            2303,
            1944,
            1856,
            4577,
            1502,
            2488,
            5290,
            895,
            1677,
            1336,
            3161,
            1156,
            1545,
            2362,
            2068,
            3299,
            3413,
            790,
            1877,
            723,
            3452
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "785/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.537299633026123,
        "final_policy_stability": 0.998109640831758,
        "episodes_to_convergence": 47,
        "policy_stability_history": [
            0.0,
            0.831758034026465,
            0.8506616257088847,
            0.8412098298676749,
            0.8431001890359168,
            0.8733459357277883,
            0.8733459357277883,
            0.8714555765595463,
            0.8468809073724007,
            0.8865784499054821,
            0.8695652173913043,
            0.8525519848771267,
            0.9262759924385633,
            0.8449905482041588,
            0.945179584120983,
            0.8147448015122873,
            0.8733459357277883,
            0.9300567107750473,
            0.8998109640831758,
            0.9357277882797732,
            0.941398865784499,
            0.8827977315689981,
            0.9092627599243857,
            0.9357277882797732,
            0.9659735349716446,
            0.9621928166351607,
            0.943289224952741,
            0.8771266540642723,
            0.8393194706994329,
            0.943289224952741,
            0.9659735349716446,
            0.9659735349716446,
            0.9017013232514177,
            0.9697542533081286,
            0.9716446124763705,
            0.9754253308128544,
            0.9300567107750473,
            0.9810964083175804,
            0.9810964083175804,
            0.994328922495274,
            0.9206049149338374,
            0.9640831758034026,
            0.9621928166351607,
            0.9697542533081286,
            0.9792060491493384,
            0.9376181474480151,
            0.994328922495274,
            0.998109640831758
        ],
        "reward_history": [
            -5290,
            -3389,
            -5290,
            -3688,
            -5290,
            -5290,
            -5290,
            -3512,
            -4927,
            -2838,
            -5083,
            -4930,
            -1608,
            -5290,
            -872,
            -5290,
            -5290,
            -2491,
            -1791,
            -1606,
            -1163,
            -3943,
            -5290,
            -1502,
            -699,
            -698,
            -1601,
            -5100,
            -5290,
            -2179,
            -1666,
            -1378,
            -4237,
            -1144,
            -1253,
            -853,
            -2431,
            -2231,
            -712,
            -491,
            -5290,
            -1321,
            -1184,
            -1759,
            -995,
            -3493,
            -595,
            -420
        ],
        "steps_history": [
            5290,
            3490,
            5290,
            3789,
            5290,
            5290,
            5290,
            3613,
            5028,
            2939,
            5184,
            5031,
            1709,
            5290,
            973,
            5290,
            5290,
            2592,
            1892,
            1707,
            1264,
            4044,
            5290,
            1603,
            800,
            799,
            1702,
            5201,
            5290,
            2280,
            1767,
            1479,
            4338,
            1245,
            1354,
            954,
            2532,
            2332,
            813,
            592,
            5290,
            1422,
            1285,
            1860,
            1096,
            3594,
            696,
            521
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "786/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.786763429641724,
        "final_policy_stability": 0.9886578449905482,
        "episodes_to_convergence": 58,
        "policy_stability_history": [
            0.0,
            0.8071833648393195,
            0.8374291115311909,
            0.8922495274102079,
            0.8166351606805293,
            0.9111531190926276,
            0.8733459357277883,
            0.8563327032136105,
            0.8790170132325141,
            0.9111531190926276,
            0.9073724007561437,
            0.8487712665406427,
            0.888468809073724,
            0.8355387523629489,
            0.9054820415879017,
            0.8676748582230623,
            0.8771266540642723,
            0.9149338374291115,
            0.9376181474480151,
            0.9017013232514177,
            0.8827977315689981,
            0.8827977315689981,
            0.8809073724007561,
            0.8544423440453687,
            0.9243856332703214,
            0.8638941398865785,
            0.9300567107750473,
            0.8903591682419659,
            0.9621928166351607,
            0.9376181474480151,
            0.947069943289225,
            0.9092627599243857,
            0.9584120982986768,
            0.9338374291115312,
            0.9905482041587902,
            0.9508506616257089,
            0.8676748582230623,
            0.9697542533081286,
            0.9810964083175804,
            0.9848771266540642,
            0.9810964083175804,
            0.9300567107750473,
            0.9678638941398866,
            0.9829867674858223,
            0.9829867674858223,
            0.9376181474480151,
            0.9357277882797732,
            0.9773156899810964,
            0.9678638941398866,
            0.996219281663516,
            0.9829867674858223,
            0.9754253308128544,
            0.9376181474480151,
            0.9489603024574669,
            0.998109640831758,
            0.9584120982986768,
            1.0,
            0.994328922495274,
            0.9886578449905482
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -1593,
            -5290,
            -2069,
            -3550,
            -5290,
            -5290,
            -2703,
            -1704,
            -5290,
            -5290,
            -5290,
            -1543,
            -3136,
            -2489,
            -3454,
            -799,
            -5290,
            -3209,
            -3598,
            -2348,
            -4425,
            -1326,
            -4952,
            -1546,
            -3878,
            -1228,
            -1662,
            -2140,
            -5290,
            -1028,
            -2006,
            -644,
            -1413,
            -5021,
            -1017,
            -1085,
            -933,
            -1065,
            -2413,
            -1487,
            -844,
            -1213,
            -2811,
            -3940,
            -1688,
            -1233,
            -429,
            -594,
            -800,
            -4656,
            -1682,
            -745,
            -2423,
            -945,
            -1153,
            -832
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            1694,
            5290,
            2170,
            3651,
            5290,
            5290,
            2804,
            1805,
            5290,
            5290,
            5290,
            1644,
            3237,
            2590,
            3555,
            900,
            5290,
            3310,
            3699,
            2449,
            4526,
            1427,
            5053,
            1647,
            3979,
            1329,
            1763,
            2241,
            5290,
            1129,
            2107,
            745,
            1514,
            5122,
            1118,
            1186,
            1034,
            1166,
            2514,
            1588,
            945,
            1314,
            2912,
            4041,
            1789,
            1334,
            530,
            695,
            901,
            4757,
            1783,
            846,
            2524,
            1046,
            1254,
            933
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "787/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.744945764541626,
        "final_policy_stability": 0.9697542533081286,
        "episodes_to_convergence": 57,
        "policy_stability_history": [
            0.0,
            0.8544423440453687,
            0.9017013232514177,
            0.8468809073724007,
            0.8827977315689981,
            0.8223062381852552,
            0.8506616257088847,
            0.8922495274102079,
            0.9281663516068053,
            0.9092627599243857,
            0.8449905482041588,
            0.8998109640831758,
            0.831758034026465,
            0.8809073724007561,
            0.9187145557655955,
            0.8620037807183365,
            0.945179584120983,
            0.8431001890359168,
            0.9149338374291115,
            0.9338374291115312,
            0.8809073724007561,
            0.9319470699432892,
            0.9357277882797732,
            0.8714555765595463,
            0.9603024574669187,
            0.9130434782608695,
            0.9357277882797732,
            0.9017013232514177,
            0.9546313799621928,
            0.9527410207939508,
            0.9149338374291115,
            0.8620037807183365,
            0.9810964083175804,
            0.9603024574669187,
            0.9905482041587902,
            0.8922495274102079,
            0.9621928166351607,
            0.9603024574669187,
            0.9508506616257089,
            0.8979206049149339,
            0.9584120982986768,
            0.9678638941398866,
            0.9338374291115312,
            0.9092627599243857,
            0.9754253308128544,
            0.9829867674858223,
            0.994328922495274,
            0.9848771266540642,
            0.9773156899810964,
            0.998109640831758,
            0.998109640831758,
            0.9848771266540642,
            0.9735349716446124,
            0.9810964083175804,
            0.998109640831758,
            0.998109640831758,
            1.0,
            0.9697542533081286
        ],
        "reward_history": [
            -5290,
            -3687,
            -5290,
            -4820,
            -5290,
            -5290,
            -3244,
            -5290,
            -5290,
            -1212,
            -5290,
            -1807,
            -5290,
            -5290,
            -1560,
            -5290,
            -910,
            -3699,
            -2967,
            -1294,
            -4183,
            -1712,
            -1281,
            -4601,
            -751,
            -1493,
            -2011,
            -3693,
            -1226,
            -819,
            -5290,
            -5290,
            -496,
            -2368,
            -388,
            -3994,
            -1436,
            -1221,
            -1199,
            -4303,
            -1360,
            -1072,
            -2827,
            -5290,
            -576,
            -745,
            -437,
            -784,
            -1107,
            -859,
            -390,
            -1028,
            -1304,
            -719,
            -571,
            -798,
            -802,
            -1066
        ],
        "steps_history": [
            5290,
            3788,
            5290,
            4921,
            5290,
            5290,
            3345,
            5290,
            5290,
            1313,
            5290,
            1908,
            5290,
            5290,
            1661,
            5290,
            1011,
            3800,
            3068,
            1395,
            4284,
            1813,
            1382,
            4702,
            852,
            1594,
            2112,
            3794,
            1327,
            920,
            5290,
            5290,
            597,
            2469,
            489,
            4095,
            1537,
            1322,
            1300,
            4404,
            1461,
            1173,
            2928,
            5290,
            677,
            846,
            538,
            885,
            1208,
            960,
            491,
            1129,
            1405,
            820,
            672,
            899,
            903,
            1167
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "788/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.249782800674438,
        "final_policy_stability": 0.9678638941398866,
        "episodes_to_convergence": 52,
        "policy_stability_history": [
            0.0,
            0.8601134215500945,
            0.8601134215500945,
            0.7996219281663516,
            0.8052930056710775,
            0.9017013232514177,
            0.8676748582230623,
            0.9300567107750473,
            0.8827977315689981,
            0.8827977315689981,
            0.9300567107750473,
            0.8752362948960303,
            0.8695652173913043,
            0.8922495274102079,
            0.8714555765595463,
            0.8525519848771267,
            0.8790170132325141,
            0.9338374291115312,
            0.9168241965973535,
            0.9168241965973535,
            0.8695652173913043,
            0.941398865784499,
            0.8865784499054821,
            0.9281663516068053,
            0.9621928166351607,
            0.941398865784499,
            0.9035916824196597,
            0.9111531190926276,
            0.8714555765595463,
            0.8695652173913043,
            0.9281663516068053,
            0.9640831758034026,
            0.9527410207939508,
            0.9300567107750473,
            0.9697542533081286,
            0.9338374291115312,
            0.9376181474480151,
            0.9376181474480151,
            0.9829867674858223,
            0.9187145557655955,
            0.9886578449905482,
            0.9621928166351607,
            0.9773156899810964,
            0.9886578449905482,
            0.9829867674858223,
            0.9924385633270322,
            0.943289224952741,
            1.0,
            0.9735349716446124,
            0.998109640831758,
            0.9735349716446124,
            0.9735349716446124,
            0.9678638941398866
        ],
        "reward_history": [
            -5290,
            -5290,
            -3579,
            -5290,
            -5290,
            -2417,
            -5290,
            -827,
            -5290,
            -5290,
            -1392,
            -5290,
            -2911,
            -1987,
            -5290,
            -5290,
            -5290,
            -968,
            -5290,
            -1653,
            -3277,
            -1243,
            -5290,
            -2298,
            -1183,
            -1643,
            -3777,
            -2393,
            -4448,
            -3892,
            -3059,
            -1224,
            -1656,
            -2199,
            -679,
            -2209,
            -2516,
            -3344,
            -1266,
            -2518,
            -284,
            -2051,
            -1268,
            -892,
            -1081,
            -1020,
            -3625,
            -1474,
            -1147,
            -1523,
            -988,
            -1707,
            -1960
        ],
        "steps_history": [
            5290,
            5290,
            3680,
            5290,
            5290,
            2518,
            5290,
            928,
            5290,
            5290,
            1493,
            5290,
            3012,
            2088,
            5290,
            5290,
            5290,
            1069,
            5290,
            1754,
            3378,
            1344,
            5290,
            2399,
            1284,
            1744,
            3878,
            2494,
            4549,
            3993,
            3160,
            1325,
            1757,
            2300,
            780,
            2310,
            2617,
            3445,
            1367,
            2619,
            385,
            2152,
            1369,
            993,
            1182,
            1121,
            3726,
            1575,
            1248,
            1624,
            1089,
            1808,
            2061
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "789/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.784951210021973,
        "final_policy_stability": 0.945179584120983,
        "episodes_to_convergence": 58,
        "policy_stability_history": [
            0.0,
            0.8714555765595463,
            0.8355387523629489,
            0.9073724007561437,
            0.8260869565217391,
            0.8865784499054821,
            0.8374291115311909,
            0.9073724007561437,
            0.9224952741020794,
            0.8771266540642723,
            0.8998109640831758,
            0.8846880907372401,
            0.9111531190926276,
            0.8487712665406427,
            0.9111531190926276,
            0.9092627599243857,
            0.9243856332703214,
            0.8393194706994329,
            0.9187145557655955,
            0.9073724007561437,
            0.8846880907372401,
            0.9149338374291115,
            0.9395085066162571,
            0.9527410207939508,
            0.9149338374291115,
            0.831758034026465,
            0.9319470699432892,
            0.943289224952741,
            0.8431001890359168,
            0.9508506616257089,
            0.9621928166351607,
            0.9187145557655955,
            0.9395085066162571,
            0.8979206049149339,
            0.9810964083175804,
            0.9376181474480151,
            0.9130434782608695,
            0.9848771266540642,
            0.9867674858223062,
            0.9640831758034026,
            0.9697542533081286,
            0.9754253308128544,
            0.9697542533081286,
            0.9357277882797732,
            0.9489603024574669,
            0.8865784499054821,
            0.9357277882797732,
            0.9224952741020794,
            0.9924385633270322,
            0.9829867674858223,
            0.994328922495274,
            0.998109640831758,
            0.9716446124763705,
            0.998109640831758,
            0.9867674858223062,
            0.996219281663516,
            1.0,
            0.9924385633270322,
            0.945179584120983
        ],
        "reward_history": [
            -5290,
            -2941,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -1721,
            -5290,
            -5290,
            -3748,
            -1614,
            -5290,
            -1993,
            -2088,
            -1523,
            -5290,
            -1269,
            -5290,
            -2324,
            -2191,
            -1347,
            -928,
            -3116,
            -5290,
            -1874,
            -1129,
            -5290,
            -663,
            -1465,
            -5290,
            -1708,
            -5290,
            -633,
            -1734,
            -3510,
            -785,
            -512,
            -1541,
            -514,
            -632,
            -1259,
            -3028,
            -1901,
            -4334,
            -1924,
            -2810,
            -1462,
            -1037,
            -752,
            -1062,
            -2977,
            -458,
            -1488,
            -571,
            -722,
            -762,
            -3644
        ],
        "steps_history": [
            5290,
            3042,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            1822,
            5290,
            5290,
            3849,
            1715,
            5290,
            2094,
            2189,
            1624,
            5290,
            1370,
            5290,
            2425,
            2292,
            1448,
            1029,
            3217,
            5290,
            1975,
            1230,
            5290,
            764,
            1566,
            5290,
            1809,
            5290,
            734,
            1835,
            3611,
            886,
            613,
            1642,
            615,
            733,
            1360,
            3129,
            2002,
            4435,
            2025,
            2911,
            1563,
            1138,
            853,
            1163,
            3078,
            559,
            1589,
            672,
            823,
            863,
            3745
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "790/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.563690185546875,
        "final_policy_stability": 0.9924385633270322,
        "episodes_to_convergence": 57,
        "policy_stability_history": [
            0.0,
            0.9017013232514177,
            0.8166351606805293,
            0.8752362948960303,
            0.833648393194707,
            0.8790170132325141,
            0.831758034026465,
            0.8449905482041588,
            0.8998109640831758,
            0.8582230623818525,
            0.8638941398865785,
            0.9092627599243857,
            0.8015122873345936,
            0.8563327032136105,
            0.9300567107750473,
            0.8676748582230623,
            0.8601134215500945,
            0.9224952741020794,
            0.8846880907372401,
            0.8185255198487713,
            0.8771266540642723,
            0.8998109640831758,
            0.947069943289225,
            0.9527410207939508,
            0.9224952741020794,
            0.9092627599243857,
            0.9527410207939508,
            0.9603024574669187,
            0.9395085066162571,
            0.9810964083175804,
            0.8752362948960303,
            0.9546313799621928,
            0.9640831758034026,
            0.8714555765595463,
            0.9659735349716446,
            0.9584120982986768,
            0.9886578449905482,
            0.9603024574669187,
            0.9262759924385633,
            0.9621928166351607,
            0.9565217391304348,
            0.9848771266540642,
            0.9810964083175804,
            0.9716446124763705,
            0.9773156899810964,
            0.9924385633270322,
            0.9924385633270322,
            0.9905482041587902,
            0.9886578449905482,
            0.9829867674858223,
            0.9678638941398866,
            0.9773156899810964,
            1.0,
            0.9584120982986768,
            0.9754253308128544,
            1.0,
            0.998109640831758,
            0.9924385633270322
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -3109,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5141,
            -1092,
            -5059,
            -2550,
            -1303,
            -5290,
            -5290,
            -1679,
            -4662,
            -5290,
            -3157,
            -2900,
            -999,
            -584,
            -1531,
            -2623,
            -1050,
            -952,
            -1475,
            -719,
            -4438,
            -1135,
            -857,
            -4488,
            -882,
            -875,
            -531,
            -1211,
            -4212,
            -718,
            -1315,
            -592,
            -727,
            -786,
            -929,
            -452,
            -551,
            -390,
            -499,
            -435,
            -1225,
            -1198,
            -589,
            -1722,
            -1475,
            -449,
            -482,
            -601
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            3210,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5242,
            1193,
            5160,
            2651,
            1404,
            5290,
            5290,
            1780,
            4763,
            5290,
            3258,
            3001,
            1100,
            685,
            1632,
            2724,
            1151,
            1053,
            1576,
            820,
            4539,
            1236,
            958,
            4589,
            983,
            976,
            632,
            1312,
            4313,
            819,
            1416,
            693,
            828,
            887,
            1030,
            553,
            652,
            491,
            600,
            536,
            1326,
            1299,
            690,
            1823,
            1576,
            550,
            583,
            702
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "791/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.5885910987854,
        "final_policy_stability": 0.9848771266540642,
        "episodes_to_convergence": 57,
        "policy_stability_history": [
            0.0,
            0.8544423440453687,
            0.8582230623818525,
            0.833648393194707,
            0.8223062381852552,
            0.8998109640831758,
            0.8544423440453687,
            0.8752362948960303,
            0.9281663516068053,
            0.8355387523629489,
            0.8468809073724007,
            0.8601134215500945,
            0.9319470699432892,
            0.8582230623818525,
            0.9092627599243857,
            0.8771266540642723,
            0.8809073724007561,
            0.8506616257088847,
            0.8638941398865785,
            0.943289224952741,
            0.9395085066162571,
            0.9489603024574669,
            0.9206049149338374,
            0.8525519848771267,
            0.8468809073724007,
            0.9640831758034026,
            0.8657844990548205,
            0.8827977315689981,
            0.9527410207939508,
            0.9773156899810964,
            0.9187145557655955,
            0.943289224952741,
            0.9848771266540642,
            0.9810964083175804,
            0.9754253308128544,
            0.9319470699432892,
            0.8714555765595463,
            0.9376181474480151,
            0.945179584120983,
            0.947069943289225,
            0.9735349716446124,
            0.9735349716446124,
            0.943289224952741,
            0.9754253308128544,
            0.9621928166351607,
            0.9810964083175804,
            0.9924385633270322,
            0.9886578449905482,
            0.994328922495274,
            0.994328922495274,
            0.994328922495274,
            0.998109640831758,
            0.9905482041587902,
            0.9924385633270322,
            0.9224952741020794,
            0.998109640831758,
            0.998109640831758,
            0.9848771266540642
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -4618,
            -2063,
            -5290,
            -2278,
            -5290,
            -4186,
            -5290,
            -5290,
            -548,
            -5290,
            -1450,
            -2982,
            -4670,
            -3619,
            -5290,
            -1065,
            -1297,
            -1167,
            -1315,
            -5090,
            -3977,
            -416,
            -3261,
            -3566,
            -1774,
            -774,
            -2906,
            -1413,
            -729,
            -818,
            -977,
            -1876,
            -4173,
            -1651,
            -2773,
            -1329,
            -1256,
            -650,
            -1944,
            -711,
            -1271,
            -776,
            -646,
            -672,
            -350,
            -544,
            -850,
            -513,
            -721,
            -744,
            -2571,
            -399,
            -600,
            -1045
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            4719,
            2164,
            5290,
            2379,
            5290,
            4287,
            5290,
            5290,
            649,
            5290,
            1551,
            3083,
            4771,
            3720,
            5290,
            1166,
            1398,
            1268,
            1416,
            5191,
            4078,
            517,
            3362,
            3667,
            1875,
            875,
            3007,
            1514,
            830,
            919,
            1078,
            1977,
            4274,
            1752,
            2874,
            1430,
            1357,
            751,
            2045,
            812,
            1372,
            877,
            747,
            773,
            451,
            645,
            951,
            614,
            822,
            845,
            2672,
            500,
            701,
            1146
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "792/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.73497986793518,
        "final_policy_stability": 0.9886578449905482,
        "episodes_to_convergence": 60,
        "policy_stability_history": [
            0.0,
            0.8752362948960303,
            0.7977315689981096,
            0.888468809073724,
            0.8657844990548205,
            0.8960302457466919,
            0.8563327032136105,
            0.8809073724007561,
            0.9054820415879017,
            0.8676748582230623,
            0.9130434782608695,
            0.9168241965973535,
            0.8260869565217391,
            0.8525519848771267,
            0.8998109640831758,
            0.9187145557655955,
            0.8393194706994329,
            0.888468809073724,
            0.8241965973534972,
            0.8809073724007561,
            0.8865784499054821,
            0.9054820415879017,
            0.9035916824196597,
            0.941398865784499,
            0.9489603024574669,
            0.9130434782608695,
            0.941398865784499,
            0.9111531190926276,
            0.8582230623818525,
            0.9376181474480151,
            0.8922495274102079,
            0.9527410207939508,
            0.8979206049149339,
            0.9527410207939508,
            0.9243856332703214,
            0.9886578449905482,
            0.9376181474480151,
            0.9773156899810964,
            0.9754253308128544,
            0.9735349716446124,
            0.9848771266540642,
            0.9867674858223062,
            0.9508506616257089,
            0.9754253308128544,
            0.9621928166351607,
            0.9603024574669187,
            0.9905482041587902,
            0.9640831758034026,
            0.9206049149338374,
            0.9867674858223062,
            0.994328922495274,
            0.8695652173913043,
            0.9489603024574669,
            0.9754253308128544,
            0.9773156899810964,
            0.996219281663516,
            0.998109640831758,
            0.9527410207939508,
            0.9886578449905482,
            1.0,
            0.9886578449905482
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2578,
            -3041,
            -1863,
            -5290,
            -1278,
            -1135,
            -5290,
            -3176,
            -1922,
            -1800,
            -5290,
            -1820,
            -5290,
            -2587,
            -2378,
            -2016,
            -1673,
            -981,
            -678,
            -1692,
            -1234,
            -2441,
            -4768,
            -1185,
            -3518,
            -742,
            -5290,
            -1162,
            -1618,
            -561,
            -1741,
            -837,
            -594,
            -1099,
            -650,
            -466,
            -1228,
            -1162,
            -1064,
            -1180,
            -935,
            -1318,
            -4646,
            -643,
            -359,
            -5029,
            -1510,
            -1057,
            -1312,
            -630,
            -670,
            -2238,
            -779,
            -373,
            -835
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            2679,
            3142,
            1964,
            5290,
            1379,
            1236,
            5290,
            3277,
            2023,
            1901,
            5290,
            1921,
            5290,
            2688,
            2479,
            2117,
            1774,
            1082,
            779,
            1793,
            1335,
            2542,
            4869,
            1286,
            3619,
            843,
            5290,
            1263,
            1719,
            662,
            1842,
            938,
            695,
            1200,
            751,
            567,
            1329,
            1263,
            1165,
            1281,
            1036,
            1419,
            4747,
            744,
            460,
            5130,
            1611,
            1158,
            1413,
            731,
            771,
            2339,
            880,
            474,
            936
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "793/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.83534550666809,
        "final_policy_stability": 0.9867674858223062,
        "episodes_to_convergence": 59,
        "policy_stability_history": [
            0.0,
            0.833648393194707,
            0.8260869565217391,
            0.8128544423440454,
            0.8298676748582231,
            0.8941398865784499,
            0.8506616257088847,
            0.8563327032136105,
            0.831758034026465,
            0.9224952741020794,
            0.9054820415879017,
            0.9300567107750473,
            0.9395085066162571,
            0.9262759924385633,
            0.8601134215500945,
            0.8487712665406427,
            0.941398865784499,
            0.8544423440453687,
            0.8809073724007561,
            0.9092627599243857,
            0.9527410207939508,
            0.9243856332703214,
            0.9565217391304348,
            0.9300567107750473,
            0.9035916824196597,
            0.943289224952741,
            0.9168241965973535,
            0.9054820415879017,
            0.9565217391304348,
            0.8260869565217391,
            0.9621928166351607,
            0.9489603024574669,
            0.8979206049149339,
            0.9810964083175804,
            0.9073724007561437,
            0.9073724007561437,
            0.8657844990548205,
            0.9792060491493384,
            0.9735349716446124,
            0.9886578449905482,
            0.9508506616257089,
            0.947069943289225,
            0.9508506616257089,
            0.9640831758034026,
            0.9867674858223062,
            0.9848771266540642,
            0.9867674858223062,
            0.9848771266540642,
            0.9905482041587902,
            0.9792060491493384,
            0.9659735349716446,
            0.994328922495274,
            0.996219281663516,
            0.9546313799621928,
            0.998109640831758,
            0.996219281663516,
            0.9659735349716446,
            1.0,
            0.9395085066162571,
            0.9867674858223062
        ],
        "reward_history": [
            -4059,
            -5290,
            -4237,
            -5290,
            -5290,
            -5290,
            -5290,
            -4107,
            -5290,
            -1046,
            -5290,
            -1486,
            -1090,
            -1426,
            -5290,
            -4296,
            -619,
            -5290,
            -2684,
            -5290,
            -1099,
            -2056,
            -1113,
            -1467,
            -2497,
            -1864,
            -2023,
            -5290,
            -662,
            -5290,
            -765,
            -1158,
            -3175,
            -542,
            -2981,
            -3624,
            -4436,
            -665,
            -429,
            -654,
            -1643,
            -1296,
            -1259,
            -1152,
            -367,
            -815,
            -610,
            -389,
            -617,
            -799,
            -1568,
            -623,
            -538,
            -2356,
            -630,
            -559,
            -1456,
            -478,
            -4332,
            -1491
        ],
        "steps_history": [
            4160,
            5290,
            4338,
            5290,
            5290,
            5290,
            5290,
            4208,
            5290,
            1147,
            5290,
            1587,
            1191,
            1527,
            5290,
            4397,
            720,
            5290,
            2785,
            5290,
            1200,
            2157,
            1214,
            1568,
            2598,
            1965,
            2124,
            5290,
            763,
            5290,
            866,
            1259,
            3276,
            643,
            3082,
            3725,
            4537,
            766,
            530,
            755,
            1744,
            1397,
            1360,
            1253,
            468,
            916,
            711,
            490,
            718,
            900,
            1669,
            724,
            639,
            2457,
            731,
            660,
            1557,
            579,
            4433,
            1592
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "794/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 12.521952629089355,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 63,
        "policy_stability_history": [
            0.0,
            0.8185255198487713,
            0.8431001890359168,
            0.8185255198487713,
            0.8431001890359168,
            0.8657844990548205,
            0.9035916824196597,
            0.8449905482041588,
            0.8714555765595463,
            0.8733459357277883,
            0.9243856332703214,
            0.8487712665406427,
            0.8657844990548205,
            0.8903591682419659,
            0.9111531190926276,
            0.8487712665406427,
            0.8676748582230623,
            0.9527410207939508,
            0.8431001890359168,
            0.8960302457466919,
            0.8771266540642723,
            0.9168241965973535,
            0.8941398865784499,
            0.8979206049149339,
            0.9243856332703214,
            0.9792060491493384,
            0.9603024574669187,
            0.943289224952741,
            0.9130434782608695,
            0.9697542533081286,
            0.9584120982986768,
            0.8846880907372401,
            0.9357277882797732,
            0.9621928166351607,
            0.8865784499054821,
            0.947069943289225,
            0.9754253308128544,
            0.9773156899810964,
            0.8903591682419659,
            0.9489603024574669,
            0.9546313799621928,
            0.9546313799621928,
            0.9754253308128544,
            0.9659735349716446,
            0.9168241965973535,
            0.9848771266540642,
            0.9206049149338374,
            0.9395085066162571,
            0.9792060491493384,
            0.9792060491493384,
            0.9905482041587902,
            0.9716446124763705,
            0.9848771266540642,
            0.9716446124763705,
            0.998109640831758,
            0.994328922495274,
            0.9565217391304348,
            1.0,
            0.9886578449905482,
            0.9678638941398866,
            0.9905482041587902,
            0.994328922495274,
            0.994328922495274,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -3163,
            -5290,
            -5290,
            -3745,
            -5290,
            -2876,
            -1067,
            -4599,
            -3538,
            -5290,
            -1334,
            -3821,
            -4475,
            -686,
            -4464,
            -2133,
            -3458,
            -2332,
            -3494,
            -2271,
            -1700,
            -371,
            -644,
            -2152,
            -2802,
            -614,
            -947,
            -4881,
            -2125,
            -750,
            -3330,
            -1515,
            -729,
            -605,
            -4999,
            -1379,
            -1187,
            -1427,
            -924,
            -823,
            -3277,
            -497,
            -2905,
            -1810,
            -565,
            -816,
            -793,
            -1079,
            -967,
            -1031,
            -395,
            -479,
            -1328,
            -749,
            -889,
            -1175,
            -1106,
            -807,
            -1316,
            -324
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            3264,
            5290,
            5290,
            3846,
            5290,
            2977,
            1168,
            4700,
            3639,
            5290,
            1435,
            3922,
            4576,
            787,
            4565,
            2234,
            3559,
            2433,
            3595,
            2372,
            1801,
            472,
            745,
            2253,
            2903,
            715,
            1048,
            4982,
            2226,
            851,
            3431,
            1616,
            830,
            706,
            5100,
            1480,
            1288,
            1528,
            1025,
            924,
            3378,
            598,
            3006,
            1911,
            666,
            917,
            894,
            1180,
            1068,
            1132,
            496,
            580,
            1429,
            850,
            990,
            1276,
            1207,
            908,
            1417,
            425
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "795/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.336103200912476,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 55,
        "policy_stability_history": [
            0.0,
            0.8903591682419659,
            0.8752362948960303,
            0.833648393194707,
            0.8676748582230623,
            0.9017013232514177,
            0.8468809073724007,
            0.8865784499054821,
            0.8846880907372401,
            0.8374291115311909,
            0.9224952741020794,
            0.8979206049149339,
            0.945179584120983,
            0.9527410207939508,
            0.8790170132325141,
            0.8468809073724007,
            0.8771266540642723,
            0.9168241965973535,
            0.947069943289225,
            0.9168241965973535,
            0.9092627599243857,
            0.8903591682419659,
            0.9187145557655955,
            0.9111531190926276,
            0.945179584120983,
            0.8790170132325141,
            0.9489603024574669,
            0.8941398865784499,
            0.9565217391304348,
            0.9130434782608695,
            0.9659735349716446,
            0.9659735349716446,
            0.9565217391304348,
            0.9603024574669187,
            0.9924385633270322,
            0.8790170132325141,
            0.9565217391304348,
            0.8752362948960303,
            0.9338374291115312,
            0.9697542533081286,
            0.8695652173913043,
            0.941398865784499,
            0.9621928166351607,
            0.9621928166351607,
            0.9829867674858223,
            0.9792060491493384,
            0.9640831758034026,
            0.9886578449905482,
            0.9376181474480151,
            0.9754253308128544,
            0.9867674858223062,
            0.996219281663516,
            0.994328922495274,
            0.9792060491493384,
            0.9905482041587902,
            1.0
        ],
        "reward_history": [
            -3253,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4427,
            -1575,
            -3094,
            -948,
            -1740,
            -5290,
            -5290,
            -5290,
            -2794,
            -1625,
            -2671,
            -3156,
            -5290,
            -4243,
            -5290,
            -2195,
            -4358,
            -1877,
            -3317,
            -1902,
            -2749,
            -1764,
            -1119,
            -1119,
            -2138,
            -806,
            -5290,
            -1516,
            -5290,
            -3546,
            -1171,
            -5290,
            -2744,
            -1314,
            -2608,
            -867,
            -3499,
            -2772,
            -1035,
            -5290,
            -1665,
            -1135,
            -1260,
            -1824,
            -2906,
            -2374,
            -397
        ],
        "steps_history": [
            3354,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            4528,
            1676,
            3195,
            1049,
            1841,
            5290,
            5290,
            5290,
            2895,
            1726,
            2772,
            3257,
            5290,
            4344,
            5290,
            2296,
            4459,
            1978,
            3418,
            2003,
            2850,
            1865,
            1220,
            1220,
            2239,
            907,
            5290,
            1617,
            5290,
            3647,
            1272,
            5290,
            2845,
            1415,
            2709,
            968,
            3600,
            2873,
            1136,
            5290,
            1766,
            1236,
            1361,
            1925,
            3007,
            2475,
            498
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "796/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.81908130645752,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 50,
        "policy_stability_history": [
            0.0,
            0.8922495274102079,
            0.8960302457466919,
            0.8771266540642723,
            0.8733459357277883,
            0.8922495274102079,
            0.888468809073724,
            0.8657844990548205,
            0.8827977315689981,
            0.8676748582230623,
            0.8771266540642723,
            0.8241965973534972,
            0.8733459357277883,
            0.8733459357277883,
            0.8752362948960303,
            0.8638941398865785,
            0.9300567107750473,
            0.9281663516068053,
            0.9754253308128544,
            0.888468809073724,
            0.8865784499054821,
            0.8922495274102079,
            0.9508506616257089,
            0.8846880907372401,
            0.9773156899810964,
            0.9546313799621928,
            0.9300567107750473,
            0.9565217391304348,
            0.9829867674858223,
            0.9659735349716446,
            0.9092627599243857,
            0.9640831758034026,
            0.9565217391304348,
            0.9678638941398866,
            0.9867674858223062,
            0.9886578449905482,
            0.9773156899810964,
            0.8979206049149339,
            0.9697542533081286,
            0.9357277882797732,
            0.945179584120983,
            0.9716446124763705,
            0.996219281663516,
            0.945179584120983,
            0.9867674858223062,
            0.9640831758034026,
            0.9773156899810964,
            0.9754253308128544,
            0.9905482041587902,
            0.9376181474480151,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -2286,
            -3658,
            -5290,
            -5290,
            -4457,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4803,
            -1691,
            -5290,
            -347,
            -3523,
            -5290,
            -2982,
            -1240,
            -5290,
            -819,
            -1684,
            -4472,
            -2232,
            -815,
            -1548,
            -3346,
            -989,
            -2460,
            -1041,
            -955,
            -1020,
            -776,
            -4849,
            -1863,
            -3245,
            -3792,
            -1813,
            -1559,
            -4515,
            -952,
            -2294,
            -3578,
            -3170,
            -1241,
            -5290,
            -1803
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            2387,
            3759,
            5290,
            5290,
            4558,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            4904,
            1792,
            5290,
            448,
            3624,
            5290,
            3083,
            1341,
            5290,
            920,
            1785,
            4573,
            2333,
            916,
            1649,
            3447,
            1090,
            2561,
            1142,
            1056,
            1121,
            877,
            4950,
            1964,
            3346,
            3893,
            1914,
            1660,
            4616,
            1053,
            2395,
            3679,
            3271,
            1342,
            5290,
            1904
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "797/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.773195505142212,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 48,
        "policy_stability_history": [
            0.0,
            0.8676748582230623,
            0.8714555765595463,
            0.9376181474480151,
            0.9243856332703214,
            0.8468809073724007,
            0.8903591682419659,
            0.8922495274102079,
            0.8714555765595463,
            0.8166351606805293,
            0.8752362948960303,
            0.8790170132325141,
            0.9187145557655955,
            0.8922495274102079,
            0.8563327032136105,
            0.8846880907372401,
            0.9149338374291115,
            0.9243856332703214,
            0.9243856332703214,
            0.9092627599243857,
            0.945179584120983,
            0.8733459357277883,
            0.9054820415879017,
            0.8827977315689981,
            0.8582230623818525,
            0.9659735349716446,
            0.8979206049149339,
            0.9130434782608695,
            0.9640831758034026,
            0.9678638941398866,
            0.9659735349716446,
            0.9338374291115312,
            0.9697542533081286,
            0.994328922495274,
            0.9905482041587902,
            0.8998109640831758,
            1.0,
            0.9376181474480151,
            1.0,
            0.9754253308128544,
            0.9867674858223062,
            0.9924385633270322,
            0.9924385633270322,
            0.9584120982986768,
            0.9754253308128544,
            0.996219281663516,
            0.994328922495274,
            1.0,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4009,
            -4982,
            -5290,
            -2862,
            -2246,
            -4262,
            -5290,
            -3012,
            -2077,
            -1595,
            -2274,
            -3565,
            -1694,
            -4408,
            -3292,
            -5290,
            -4872,
            -589,
            -3324,
            -5186,
            -1452,
            -1318,
            -1640,
            -3147,
            -2086,
            -771,
            -659,
            -4533,
            -544,
            -4431,
            -636,
            -1345,
            -696,
            -903,
            -1203,
            -3072,
            -2927,
            -1693,
            -1071,
            -568,
            -1955
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            4110,
            5083,
            5290,
            2963,
            2347,
            4363,
            5290,
            3113,
            2178,
            1696,
            2375,
            3666,
            1795,
            4509,
            3393,
            5290,
            4973,
            690,
            3425,
            5287,
            1553,
            1419,
            1741,
            3248,
            2187,
            872,
            760,
            4634,
            645,
            4532,
            737,
            1446,
            797,
            1004,
            1304,
            3173,
            3028,
            1794,
            1172,
            669,
            2056
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "798/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.685902833938599,
        "final_policy_stability": 0.996219281663516,
        "episodes_to_convergence": 45,
        "policy_stability_history": [
            0.0,
            0.8185255198487713,
            0.8714555765595463,
            0.8601134215500945,
            0.8393194706994329,
            0.8960302457466919,
            0.8412098298676749,
            0.8827977315689981,
            0.8279773156899811,
            0.8998109640831758,
            0.8865784499054821,
            0.9130434782608695,
            0.8620037807183365,
            0.8941398865784499,
            0.9206049149338374,
            0.8468809073724007,
            0.9092627599243857,
            0.8790170132325141,
            0.8846880907372401,
            0.9829867674858223,
            0.9243856332703214,
            0.9565217391304348,
            0.9149338374291115,
            0.9546313799621928,
            0.9508506616257089,
            0.8846880907372401,
            0.9395085066162571,
            0.8941398865784499,
            0.8846880907372401,
            0.943289224952741,
            0.9224952741020794,
            0.947069943289225,
            0.9149338374291115,
            0.9603024574669187,
            0.9659735349716446,
            0.9281663516068053,
            0.9168241965973535,
            0.9792060491493384,
            0.9867674858223062,
            0.9489603024574669,
            0.9489603024574669,
            0.9924385633270322,
            0.9697542533081286,
            0.9697542533081286,
            0.998109640831758,
            0.996219281663516
        ],
        "reward_history": [
            -5170,
            -4617,
            -5290,
            -5290,
            -4936,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2426,
            -5290,
            -1896,
            -5290,
            -5290,
            -624,
            -2212,
            -1194,
            -3247,
            -1411,
            -1670,
            -5290,
            -2591,
            -5290,
            -5290,
            -2799,
            -4568,
            -1959,
            -5290,
            -2328,
            -1401,
            -2248,
            -5064,
            -1182,
            -1537,
            -2235,
            -2926,
            -2293,
            -2080,
            -2028,
            -964,
            -1104
        ],
        "steps_history": [
            5271,
            4718,
            5290,
            5290,
            5037,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            2527,
            5290,
            1997,
            5290,
            5290,
            725,
            2313,
            1295,
            3348,
            1512,
            1771,
            5290,
            2692,
            5290,
            5290,
            2900,
            4669,
            2060,
            5290,
            2429,
            1502,
            2349,
            5165,
            1283,
            1638,
            2336,
            3027,
            2394,
            2181,
            2129,
            1065,
            1205
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "799/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.057030200958252,
        "final_policy_stability": 0.994328922495274,
        "episodes_to_convergence": 43,
        "policy_stability_history": [
            0.0,
            0.8544423440453687,
            0.9149338374291115,
            0.8846880907372401,
            0.831758034026465,
            0.8790170132325141,
            0.8487712665406427,
            0.8771266540642723,
            0.8638941398865785,
            0.8393194706994329,
            0.8733459357277883,
            0.8638941398865785,
            0.9017013232514177,
            0.8676748582230623,
            0.8487712665406427,
            0.9073724007561437,
            0.8941398865784499,
            0.8582230623818525,
            0.8733459357277883,
            0.9092627599243857,
            0.9338374291115312,
            0.9243856332703214,
            0.8733459357277883,
            0.8922495274102079,
            0.8695652173913043,
            0.9792060491493384,
            0.8979206049149339,
            0.9659735349716446,
            0.9243856332703214,
            0.9640831758034026,
            0.9243856332703214,
            0.8714555765595463,
            0.8998109640831758,
            0.9357277882797732,
            0.8998109640831758,
            0.9565217391304348,
            0.8941398865784499,
            0.9810964083175804,
            0.9773156899810964,
            0.9224952741020794,
            0.9848771266540642,
            0.9829867674858223,
            0.9810964083175804,
            0.994328922495274
        ],
        "reward_history": [
            -2526,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -4559,
            -2911,
            -5290,
            -4080,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2967,
            -5290,
            -5290,
            -5290,
            -4435,
            -2492,
            -3232,
            -5290,
            -5290,
            -5290,
            -1351,
            -5290,
            -1351,
            -2951,
            -1967,
            -3666,
            -5290,
            -4396,
            -3565,
            -2931,
            -3329,
            -5290,
            -2416,
            -2004,
            -3648,
            -3811,
            -1784,
            -2106,
            -1106
        ],
        "steps_history": [
            2627,
            5290,
            5290,
            5290,
            5290,
            5290,
            4660,
            3012,
            5290,
            4181,
            5290,
            5290,
            5290,
            5290,
            5290,
            3068,
            5290,
            5290,
            5290,
            4536,
            2593,
            3333,
            5290,
            5290,
            5290,
            1452,
            5290,
            1452,
            3052,
            2068,
            3767,
            5290,
            4497,
            3666,
            3032,
            3430,
            5290,
            2517,
            2105,
            3749,
            3912,
            1885,
            2207,
            1207
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "800/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.00819730758667,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 52,
        "policy_stability_history": [
            0.0,
            0.7996219281663516,
            0.8941398865784499,
            0.8979206049149339,
            0.8638941398865785,
            0.8903591682419659,
            0.8563327032136105,
            0.8620037807183365,
            0.8695652173913043,
            0.8223062381852552,
            0.7996219281663516,
            0.8544423440453687,
            0.8941398865784499,
            0.8827977315689981,
            0.9017013232514177,
            0.8695652173913043,
            0.9035916824196597,
            0.9035916824196597,
            0.8922495274102079,
            0.9149338374291115,
            0.9035916824196597,
            0.9319470699432892,
            0.9168241965973535,
            0.8979206049149339,
            0.9792060491493384,
            0.9338374291115312,
            0.8865784499054821,
            0.888468809073724,
            0.8998109640831758,
            0.9073724007561437,
            0.9035916824196597,
            0.943289224952741,
            0.8922495274102079,
            0.941398865784499,
            0.9565217391304348,
            0.9527410207939508,
            0.8998109640831758,
            0.9149338374291115,
            0.994328922495274,
            0.9621928166351607,
            0.9319470699432892,
            0.9924385633270322,
            0.9886578449905482,
            0.996219281663516,
            0.9640831758034026,
            0.941398865784499,
            0.9810964083175804,
            0.994328922495274,
            0.9357277882797732,
            0.9773156899810964,
            0.9924385633270322,
            0.9735349716446124,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -3513,
            -4104,
            -5290,
            -5290,
            -3120,
            -5036,
            -5290,
            -5290,
            -2004,
            -5290,
            -3189,
            -4588,
            -5290,
            -3375,
            -4905,
            -3468,
            -2747,
            -1929,
            -2527,
            -3574,
            -577,
            -1126,
            -4857,
            -4745,
            -2416,
            -2003,
            -5290,
            -1240,
            -5290,
            -1913,
            -1588,
            -1355,
            -4387,
            -3808,
            -1196,
            -1459,
            -3855,
            -869,
            -1430,
            -1021,
            -3102,
            -2004,
            -1101,
            -809,
            -4045,
            -1987,
            -1543,
            -1148,
            -489
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            3614,
            4205,
            5290,
            5290,
            3221,
            5137,
            5290,
            5290,
            2105,
            5290,
            3290,
            4689,
            5290,
            3476,
            5006,
            3569,
            2848,
            2030,
            2628,
            3675,
            678,
            1227,
            4958,
            4846,
            2517,
            2104,
            5290,
            1341,
            5290,
            2014,
            1689,
            1456,
            4488,
            3909,
            1297,
            1560,
            3956,
            970,
            1531,
            1122,
            3203,
            2105,
            1202,
            910,
            4146,
            2088,
            1644,
            1249,
            590
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "801/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.547819137573242,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 50,
        "policy_stability_history": [
            0.0,
            0.7731568998109641,
            0.8147448015122873,
            0.8298676748582231,
            0.9111531190926276,
            0.8695652173913043,
            0.8695652173913043,
            0.9054820415879017,
            0.8827977315689981,
            0.8128544423440454,
            0.9073724007561437,
            0.8865784499054821,
            0.8714555765595463,
            0.8487712665406427,
            0.8809073724007561,
            0.9111531190926276,
            0.8846880907372401,
            0.9035916824196597,
            0.9111531190926276,
            0.943289224952741,
            0.8960302457466919,
            0.8865784499054821,
            0.8468809073724007,
            0.9357277882797732,
            0.9716446124763705,
            0.8544423440453687,
            0.9338374291115312,
            0.9338374291115312,
            0.9716446124763705,
            0.8922495274102079,
            0.9187145557655955,
            0.9640831758034026,
            0.9338374291115312,
            0.9092627599243857,
            0.9546313799621928,
            0.9848771266540642,
            0.9130434782608695,
            0.9603024574669187,
            0.9678638941398866,
            0.9848771266540642,
            0.9565217391304348,
            0.9735349716446124,
            0.9621928166351607,
            0.9905482041587902,
            0.994328922495274,
            0.9376181474480151,
            0.9924385633270322,
            1.0,
            0.994328922495274,
            0.996219281663516,
            1.0
        ],
        "reward_history": [
            -5290,
            -5113,
            -5290,
            -4048,
            -5290,
            -5290,
            -5064,
            -2965,
            -3204,
            -5290,
            -5290,
            -3497,
            -5290,
            -5290,
            -4402,
            -3347,
            -2406,
            -1561,
            -3208,
            -1179,
            -3621,
            -3558,
            -5290,
            -1116,
            -1045,
            -5290,
            -1821,
            -2356,
            -909,
            -3628,
            -3830,
            -851,
            -2659,
            -4686,
            -1396,
            -622,
            -2175,
            -1303,
            -1487,
            -844,
            -1609,
            -952,
            -2810,
            -930,
            -1203,
            -2168,
            -1011,
            -429,
            -594,
            -1291,
            -1295
        ],
        "steps_history": [
            5290,
            5214,
            5290,
            4149,
            5290,
            5290,
            5165,
            3066,
            3305,
            5290,
            5290,
            3598,
            5290,
            5290,
            4503,
            3448,
            2507,
            1662,
            3309,
            1280,
            3722,
            3659,
            5290,
            1217,
            1146,
            5290,
            1922,
            2457,
            1010,
            3729,
            3931,
            952,
            2760,
            4787,
            1497,
            723,
            2276,
            1404,
            1588,
            945,
            1710,
            1053,
            2911,
            1031,
            1304,
            2269,
            1112,
            530,
            695,
            1392,
            1396
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "802/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.607108354568481,
        "final_policy_stability": 0.943289224952741,
        "episodes_to_convergence": 46,
        "policy_stability_history": [
            0.0,
            0.8544423440453687,
            0.9054820415879017,
            0.8166351606805293,
            0.8525519848771267,
            0.8279773156899811,
            0.8809073724007561,
            0.8544423440453687,
            0.8714555765595463,
            0.8449905482041588,
            0.9395085066162571,
            0.8865784499054821,
            0.8487712665406427,
            0.8620037807183365,
            0.9017013232514177,
            0.8846880907372401,
            0.9111531190926276,
            0.8941398865784499,
            0.9092627599243857,
            0.8449905482041588,
            0.9338374291115312,
            0.8582230623818525,
            0.9262759924385633,
            0.9168241965973535,
            0.9546313799621928,
            0.9527410207939508,
            0.941398865784499,
            0.9111531190926276,
            0.8676748582230623,
            0.8752362948960303,
            0.9754253308128544,
            0.8998109640831758,
            0.9603024574669187,
            0.9508506616257089,
            0.9905482041587902,
            0.9224952741020794,
            0.9659735349716446,
            0.9829867674858223,
            0.947069943289225,
            0.9395085066162571,
            0.9168241965973535,
            0.9867674858223062,
            0.9792060491493384,
            0.9697542533081286,
            0.998109640831758,
            0.9867674858223062,
            0.943289224952741
        ],
        "reward_history": [
            -5290,
            -3687,
            -5290,
            -4545,
            -4156,
            -5290,
            -5290,
            -4514,
            -5290,
            -5290,
            -650,
            -3450,
            -5001,
            -4021,
            -2485,
            -2870,
            -2393,
            -5290,
            -2157,
            -5290,
            -1234,
            -5290,
            -1956,
            -2325,
            -1179,
            -2901,
            -2018,
            -2444,
            -5290,
            -5290,
            -1420,
            -5290,
            -1103,
            -1782,
            -530,
            -4670,
            -1682,
            -493,
            -2372,
            -2360,
            -4789,
            -784,
            -1107,
            -1124,
            -662,
            -834,
            -2453
        ],
        "steps_history": [
            5290,
            3788,
            5290,
            4646,
            4257,
            5290,
            5290,
            4615,
            5290,
            5290,
            751,
            3551,
            5102,
            4122,
            2586,
            2971,
            2494,
            5290,
            2258,
            5290,
            1335,
            5290,
            2057,
            2426,
            1280,
            3002,
            2119,
            2545,
            5290,
            5290,
            1521,
            5290,
            1204,
            1883,
            631,
            4771,
            1783,
            594,
            2473,
            2461,
            4890,
            885,
            1208,
            1225,
            763,
            935,
            2554
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "803/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 9.752866268157959,
        "final_policy_stability": 0.9848771266540642,
        "episodes_to_convergence": 50,
        "policy_stability_history": [
            0.0,
            0.8279773156899811,
            0.8809073724007561,
            0.8865784499054821,
            0.9035916824196597,
            0.9206049149338374,
            0.8487712665406427,
            0.8922495274102079,
            0.8601134215500945,
            0.8695652173913043,
            0.8449905482041588,
            0.8279773156899811,
            0.9395085066162571,
            0.9300567107750473,
            0.8638941398865785,
            0.9017013232514177,
            0.8827977315689981,
            0.8771266540642723,
            0.943289224952741,
            0.8638941398865785,
            0.8846880907372401,
            0.943289224952741,
            0.8695652173913043,
            0.8941398865784499,
            0.8998109640831758,
            0.9621928166351607,
            0.9224952741020794,
            0.943289224952741,
            0.943289224952741,
            0.9243856332703214,
            0.9281663516068053,
            0.8355387523629489,
            0.947069943289225,
            0.8922495274102079,
            0.9187145557655955,
            0.9300567107750473,
            0.943289224952741,
            0.9603024574669187,
            0.9924385633270322,
            0.9792060491493384,
            0.994328922495274,
            0.9792060491493384,
            0.9773156899810964,
            0.9716446124763705,
            0.9829867674858223,
            0.996219281663516,
            0.9603024574669187,
            0.9565217391304348,
            0.9829867674858223,
            0.9867674858223062,
            0.9848771266540642
        ],
        "reward_history": [
            -5290,
            -5290,
            -4295,
            -5290,
            -1449,
            -5290,
            -5290,
            -5290,
            -3721,
            -3642,
            -3885,
            -4338,
            -1168,
            -1490,
            -5290,
            -2943,
            -5290,
            -5290,
            -2196,
            -5290,
            -5290,
            -1416,
            -4657,
            -3900,
            -2363,
            -779,
            -1784,
            -1144,
            -2054,
            -3256,
            -2126,
            -5290,
            -1362,
            -3187,
            -2489,
            -2357,
            -3369,
            -1213,
            -483,
            -787,
            -1079,
            -929,
            -1113,
            -1571,
            -774,
            -516,
            -2133,
            -2259,
            -1986,
            -988,
            -1707
        ],
        "steps_history": [
            5290,
            5290,
            4396,
            5290,
            1550,
            5290,
            5290,
            5290,
            3822,
            3743,
            3986,
            4439,
            1269,
            1591,
            5290,
            3044,
            5290,
            5290,
            2297,
            5290,
            5290,
            1517,
            4758,
            4001,
            2464,
            880,
            1885,
            1245,
            2155,
            3357,
            2227,
            5290,
            1463,
            3288,
            2590,
            2458,
            3470,
            1314,
            584,
            888,
            1180,
            1030,
            1214,
            1672,
            875,
            617,
            2234,
            2360,
            2087,
            1089,
            1808
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "804/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.220823287963867,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 55,
        "policy_stability_history": [
            0.0,
            0.8374291115311909,
            0.8582230623818525,
            0.8185255198487713,
            0.8412098298676749,
            0.8298676748582231,
            0.8676748582230623,
            0.8582230623818525,
            0.8827977315689981,
            0.8544423440453687,
            0.8752362948960303,
            0.9130434782608695,
            0.8582230623818525,
            0.9584120982986768,
            0.8827977315689981,
            0.8449905482041588,
            0.9224952741020794,
            0.8979206049149339,
            0.8979206049149339,
            0.9584120982986768,
            0.8903591682419659,
            0.8714555765595463,
            0.8620037807183365,
            0.9489603024574669,
            0.8790170132325141,
            0.8733459357277883,
            0.9149338374291115,
            0.9035916824196597,
            0.9035916824196597,
            0.9754253308128544,
            0.9206049149338374,
            0.9130434782608695,
            0.9716446124763705,
            0.9319470699432892,
            0.9527410207939508,
            0.9678638941398866,
            0.9792060491493384,
            0.8809073724007561,
            0.9735349716446124,
            0.9848771266540642,
            0.9489603024574669,
            0.9886578449905482,
            0.947069943289225,
            0.9527410207939508,
            0.9905482041587902,
            0.9395085066162571,
            0.9924385633270322,
            0.9867674858223062,
            0.9754253308128544,
            0.9810964083175804,
            0.9603024574669187,
            0.9886578449905482,
            0.9735349716446124,
            1.0,
            0.9905482041587902,
            1.0
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -2055,
            -5290,
            -5290,
            -5290,
            -5290,
            -5290,
            -3516,
            -395,
            -2628,
            -5290,
            -1741,
            -3888,
            -5290,
            -604,
            -3682,
            -4955,
            -5290,
            -1032,
            -3421,
            -4903,
            -2341,
            -5290,
            -3061,
            -1093,
            -2388,
            -5290,
            -890,
            -2475,
            -1745,
            -1998,
            -1015,
            -5007,
            -457,
            -764,
            -1428,
            -881,
            -2464,
            -1890,
            -1062,
            -4275,
            -408,
            -912,
            -2159,
            -1381,
            -2740,
            -1371,
            -1951,
            -872,
            -1277,
            -859
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5290,
            5290,
            5290,
            2156,
            5290,
            5290,
            5290,
            5290,
            5290,
            3617,
            496,
            2729,
            5290,
            1842,
            3989,
            5290,
            705,
            3783,
            5056,
            5290,
            1133,
            3522,
            5004,
            2442,
            5290,
            3162,
            1194,
            2489,
            5290,
            991,
            2576,
            1846,
            2099,
            1116,
            5108,
            558,
            865,
            1529,
            982,
            2565,
            1991,
            1163,
            4376,
            509,
            1013,
            2260,
            1482,
            2841,
            1472,
            2052,
            973,
            1378,
            960
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "805/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.56253957748413,
        "final_policy_stability": 0.9659735349716446,
        "episodes_to_convergence": 58,
        "policy_stability_history": [
            0.0,
            0.9017013232514177,
            0.8393194706994329,
            0.8052930056710775,
            0.8582230623818525,
            0.8676748582230623,
            0.8865784499054821,
            0.8071833648393195,
            0.9035916824196597,
            0.8223062381852552,
            0.8374291115311909,
            0.9130434782608695,
            0.8922495274102079,
            0.9073724007561437,
            0.8733459357277883,
            0.8260869565217391,
            0.9206049149338374,
            0.943289224952741,
            0.8714555765595463,
            0.8865784499054821,
            0.8676748582230623,
            0.9262759924385633,
            0.9300567107750473,
            0.9508506616257089,
            0.9621928166351607,
            0.8752362948960303,
            0.9111531190926276,
            0.9621928166351607,
            0.947069943289225,
            0.947069943289225,
            0.9281663516068053,
            0.9262759924385633,
            0.9111531190926276,
            0.9073724007561437,
            0.9735349716446124,
            0.9111531190926276,
            0.9867674858223062,
            0.9848771266540642,
            0.9867674858223062,
            1.0,
            0.9773156899810964,
            0.9754253308128544,
            0.9829867674858223,
            0.941398865784499,
            0.9886578449905482,
            0.9603024574669187,
            0.9697542533081286,
            0.9924385633270322,
            0.996219281663516,
            0.9395085066162571,
            0.9810964083175804,
            0.9905482041587902,
            0.996219281663516,
            0.9848771266540642,
            0.996219281663516,
            0.998109640831758,
            0.9716446124763705,
            0.998109640831758,
            0.9659735349716446
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -5147,
            -5290,
            -2093,
            -1916,
            -5290,
            -820,
            -5290,
            -5290,
            -5290,
            -2051,
            -1453,
            -5290,
            -5290,
            -1927,
            -562,
            -5290,
            -5290,
            -2622,
            -1108,
            -1687,
            -1287,
            -676,
            -4379,
            -1671,
            -647,
            -849,
            -734,
            -2533,
            -2566,
            -2734,
            -5290,
            -562,
            -3715,
            -514,
            -620,
            -467,
            -479,
            -771,
            -715,
            -691,
            -2192,
            -589,
            -803,
            -581,
            -955,
            -258,
            -3949,
            -1716,
            -551,
            -759,
            -666,
            -319,
            -471,
            -1532,
            -589,
            -1722
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            5248,
            5290,
            2194,
            2017,
            5290,
            921,
            5290,
            5290,
            5290,
            2152,
            1554,
            5290,
            5290,
            2028,
            663,
            5290,
            5290,
            2723,
            1209,
            1788,
            1388,
            777,
            4480,
            1772,
            748,
            950,
            835,
            2634,
            2667,
            2835,
            5290,
            663,
            3816,
            615,
            721,
            568,
            580,
            872,
            816,
            792,
            2293,
            690,
            904,
            682,
            1056,
            359,
            4050,
            1817,
            652,
            860,
            767,
            420,
            572,
            1633,
            690,
            1823
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "806/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.912626028060913,
        "final_policy_stability": 0.9716446124763705,
        "episodes_to_convergence": 58,
        "policy_stability_history": [
            0.0,
            0.8241965973534972,
            0.9130434782608695,
            0.8563327032136105,
            0.8166351606805293,
            0.8355387523629489,
            0.8695652173913043,
            0.8487712665406427,
            0.9281663516068053,
            0.8657844990548205,
            0.9130434782608695,
            0.8922495274102079,
            0.9092627599243857,
            0.9224952741020794,
            0.8979206049149339,
            0.8166351606805293,
            0.9168241965973535,
            0.9338374291115312,
            0.8563327032136105,
            0.9092627599243857,
            0.8714555765595463,
            0.9527410207939508,
            0.8468809073724007,
            0.8809073724007561,
            0.8903591682419659,
            0.8846880907372401,
            0.9395085066162571,
            0.9092627599243857,
            0.9376181474480151,
            0.947069943289225,
            0.9659735349716446,
            0.9395085066162571,
            0.8998109640831758,
            0.9262759924385633,
            0.9640831758034026,
            0.9584120982986768,
            0.8809073724007561,
            0.9905482041587902,
            0.9773156899810964,
            0.9508506616257089,
            0.8752362948960303,
            0.9848771266540642,
            0.9073724007561437,
            0.9168241965973535,
            0.9773156899810964,
            0.9735349716446124,
            0.9848771266540642,
            0.9848771266540642,
            0.9716446124763705,
            0.9867674858223062,
            0.9224952741020794,
            0.9754253308128544,
            0.9886578449905482,
            0.9697542533081286,
            0.9659735349716446,
            0.998109640831758,
            1.0,
            1.0,
            0.9716446124763705
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -3243,
            -5290,
            -3446,
            -3254,
            -4272,
            -1463,
            -5290,
            -1156,
            -5290,
            -2172,
            -5290,
            -3422,
            -4909,
            -1896,
            -1185,
            -5061,
            -1697,
            -5290,
            -479,
            -5290,
            -2641,
            -2328,
            -5290,
            -1378,
            -3117,
            -1519,
            -1822,
            -765,
            -1199,
            -3488,
            -1485,
            -947,
            -810,
            -4332,
            -576,
            -496,
            -2154,
            -3784,
            -372,
            -2782,
            -2668,
            -672,
            -1015,
            -830,
            -513,
            -721,
            -744,
            -2571,
            -1100,
            -1045,
            -1681,
            -1125,
            -520,
            -522,
            -788,
            -866
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            3344,
            5290,
            3547,
            3355,
            4373,
            1564,
            5290,
            1257,
            5290,
            2273,
            5290,
            3523,
            5010,
            1997,
            1286,
            5162,
            1798,
            5290,
            580,
            5290,
            2742,
            2429,
            5290,
            1479,
            3218,
            1620,
            1923,
            866,
            1300,
            3589,
            1586,
            1048,
            911,
            4433,
            677,
            597,
            2255,
            3885,
            473,
            2883,
            2769,
            773,
            1116,
            931,
            614,
            822,
            845,
            2672,
            1201,
            1146,
            1782,
            1226,
            621,
            623,
            889,
            967
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "807/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.082869052886963,
        "final_policy_stability": 0.9300567107750473,
        "episodes_to_convergence": 54,
        "policy_stability_history": [
            0.0,
            0.831758034026465,
            0.8166351606805293,
            0.8563327032136105,
            0.8166351606805293,
            0.8676748582230623,
            0.9149338374291115,
            0.8714555765595463,
            0.8487712665406427,
            0.9111531190926276,
            0.8960302457466919,
            0.782608695652174,
            0.8506616257088847,
            0.9206049149338374,
            0.9281663516068053,
            0.9054820415879017,
            0.8865784499054821,
            0.8393194706994329,
            0.8752362948960303,
            0.8620037807183365,
            0.8676748582230623,
            0.8771266540642723,
            0.8506616257088847,
            0.8449905482041588,
            0.941398865784499,
            0.9111531190926276,
            0.8960302457466919,
            0.9754253308128544,
            0.945179584120983,
            0.8733459357277883,
            0.9829867674858223,
            0.8960302457466919,
            0.9376181474480151,
            0.9546313799621928,
            0.9300567107750473,
            0.9508506616257089,
            0.9773156899810964,
            0.9697542533081286,
            0.9640831758034026,
            0.9773156899810964,
            0.9792060491493384,
            0.9697542533081286,
            0.9678638941398866,
            0.941398865784499,
            0.9905482041587902,
            0.9603024574669187,
            0.9697542533081286,
            0.9716446124763705,
            0.9603024574669187,
            0.9924385633270322,
            0.9924385633270322,
            0.996219281663516,
            0.9508506616257089,
            0.994328922495274,
            0.9300567107750473
        ],
        "reward_history": [
            -5290,
            -5290,
            -5290,
            -4333,
            -5290,
            -5290,
            -1350,
            -5290,
            -5290,
            -1442,
            -1761,
            -5290,
            -5290,
            -1201,
            -1140,
            -1827,
            -2462,
            -4393,
            -4933,
            -5085,
            -3813,
            -2651,
            -5290,
            -5290,
            -1576,
            -1956,
            -3118,
            -311,
            -1154,
            -3595,
            -641,
            -4184,
            -1021,
            -1205,
            -2049,
            -1144,
            -1000,
            -665,
            -847,
            -757,
            -1558,
            -965,
            -1765,
            -4640,
            -759,
            -1258,
            -1038,
            -1135,
            -2409,
            -608,
            -1020,
            -551,
            -3783,
            -856,
            -4060
        ],
        "steps_history": [
            5290,
            5290,
            5290,
            4434,
            5290,
            5290,
            1451,
            5290,
            5290,
            1543,
            1862,
            5290,
            5290,
            1302,
            1241,
            1928,
            2563,
            4494,
            5034,
            5186,
            3914,
            2752,
            5290,
            5290,
            1677,
            2057,
            3219,
            412,
            1255,
            3696,
            742,
            4285,
            1122,
            1306,
            2150,
            1245,
            1101,
            766,
            948,
            858,
            1659,
            1066,
            1866,
            4741,
            860,
            1359,
            1139,
            1236,
            2510,
            709,
            1121,
            652,
            3884,
            957,
            4161
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "808/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.180153369903564,
        "final_policy_stability": 0.9867674858223062,
        "episodes_to_convergence": 54,
        "policy_stability_history": [
            0.0,
            0.833648393194707,
            0.8582230623818525,
            0.8657844990548205,
            0.8827977315689981,
            0.833648393194707,
            0.888468809073724,
            0.8015122873345936,
            0.9054820415879017,
            0.8355387523629489,
            0.8771266540642723,
            0.8544423440453687,
            0.8468809073724007,
            0.8185255198487713,
            0.7939508506616257,
            0.8563327032136105,
            0.8676748582230623,
            0.9262759924385633,
            0.8922495274102079,
            0.8506616257088847,
            0.8941398865784499,
            0.8771266540642723,
            0.8412098298676749,
            0.9111531190926276,
            0.9338374291115312,
            0.9640831758034026,
            0.8676748582230623,
            0.8846880907372401,
            0.9640831758034026,
            0.8846880907372401,
            0.9206049149338374,
            0.941398865784499,
            0.9338374291115312,
            0.9111531190926276,
            0.9848771266540642,
            0.9262759924385633,
            0.9716446124763705,
            0.9489603024574669,
            0.9848771266540642,
            0.994328922495274,
            0.9754253308128544,
            0.9924385633270322,
            0.9716446124763705,
            0.8752362948960303,
            0.9659735349716446,
            0.9810964083175804,
            0.9621928166351607,
            0.9867674858223062,
            0.9735349716446124,
            0.9243856332703214,
            0.9924385633270322,
            0.9924385633270322,
            0.998109640831758,
            1.0,
            0.9867674858223062
        ],
        "reward_history": [
            -4059,
            -5290,
            -5290,
            -5290,
            -5290,
            -4154,
            -2055,
            -5290,
            -1843,
            -5290,
            -2146,
            -4658,
            -4742,
            -5290,
            -5290,
            -3379,
            -5290,
            -1404,
            -1833,
            -5290,
            -2433,
            -2928,
            -4807,
            -2134,
            -1257,
            -799,
            -5290,
            -5290,
            -774,
            -5290,
            -1796,
            -1534,
            -1758,
            -2376,
            -451,
            -4311,
            -881,
            -1504,
            -385,
            -627,
            -728,
            -739,
            -1001,
            -4008,
            -1200,
            -734,
            -1976,
            -512,
            -1230,
            -2224,
            -713,
            -567,
            -577,
            -939,
            -820
        ],
        "steps_history": [
            4160,
            5290,
            5290,
            5290,
            5290,
            4255,
            2156,
            5290,
            1944,
            5290,
            2247,
            4759,
            4843,
            5290,
            5290,
            3480,
            5290,
            1505,
            1934,
            5290,
            2534,
            3029,
            4908,
            2235,
            1358,
            900,
            5290,
            5290,
            875,
            5290,
            1897,
            1635,
            1859,
            2477,
            552,
            4412,
            982,
            1605,
            486,
            728,
            829,
            840,
            1102,
            4109,
            1301,
            835,
            2077,
            613,
            1331,
            2325,
            814,
            668,
            678,
            1040,
            921
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "809/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 11,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 60.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 11.777488946914673,
        "final_policy_stability": 0.9848771266540642,
        "episodes_to_convergence": 57,
        "policy_stability_history": [
            0.0,
            0.8393194706994329,
            0.831758034026465,
            0.8015122873345936,
            0.8620037807183365,
            0.7977315689981096,
            0.8393194706994329,
            0.8298676748582231,
            0.8090737240075614,
            0.8223062381852552,
            0.8771266540642723,
            0.9017013232514177,
            0.7939508506616257,
            0.8241965973534972,
            0.8752362948960303,
            0.9187145557655955,
            0.9017013232514177,
            0.8809073724007561,
            0.9300567107750473,
            0.9035916824196597,
            0.8752362948960303,
            0.947069943289225,
            0.9168241965973535,
            0.8752362948960303,
            0.8544423440453687,
            0.9508506616257089,
            0.8960302457466919,
            0.8638941398865785,
            0.9376181474480151,
            0.9905482041587902,
            0.9584120982986768,
            0.8733459357277883,
            0.9754253308128544,
            0.9603024574669187,
            0.8998109640831758,
            0.943289224952741,
            0.9810964083175804,
            0.9792060491493384,
            0.9035916824196597,
            0.9659735349716446,
            0.9792060491493384,
            0.9735349716446124,
            0.9565217391304348,
            0.9716446124763705,
            0.9867674858223062,
            0.941398865784499,
            0.9810964083175804,
            0.947069943289225,
            0.9848771266540642,
            0.9848771266540642,
            0.9886578449905482,
            0.9848771266540642,
            0.994328922495274,
            0.9716446124763705,
            1.0,
            0.9829867674858223,
            0.9886578449905482,
            0.9848771266540642
        ],
        "reward_history": [
            -5290,
            -5290,
            -4963,
            -5290,
            -2728,
            -4562,
            -5290,
            -5290,
            -5290,
            -5290,
            -4674,
            -3002,
            -5290,
            -5290,
            -3067,
            -1712,
            -2870,
            -2809,
            -2575,
            -3289,
            -2770,
            -1076,
            -1869,
            -5290,
            -5290,
            -974,
            -2498,
            -3968,
            -3091,
            -462,
            -1199,
            -3937,
            -550,
            -1147,
            -4961,
            -1381,
            -683,
            -724,
            -3372,
            -834,
            -730,
            -500,
            -1002,
            -967,
            -456,
            -3807,
            -1031,
            -1546,
            -682,
            -824,
            -889,
            -1086,
            -313,
            -3106,
            -324,
            -1372,
            -937,
            -1173
        ],
        "steps_history": [
            5290,
            5290,
            5064,
            5290,
            2829,
            4663,
            5290,
            5290,
            5290,
            5290,
            4775,
            3103,
            5290,
            5290,
            3168,
            1813,
            2971,
            2910,
            2676,
            3390,
            2871,
            1177,
            1970,
            5290,
            5290,
            1075,
            2599,
            4069,
            3192,
            563,
            1300,
            4038,
            651,
            1248,
            5062,
            1482,
            784,
            825,
            3473,
            935,
            831,
            601,
            1103,
            1068,
            557,
            3908,
            1132,
            1647,
            783,
            925,
            990,
            1187,
            414,
            3207,
            425,
            1473,
            1038,
            1274
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "810/810",
        "save_path": "experiments/20250131_160708/training_plots/size_11/lr0.4_df0.99_eps0.1_trial4"
    }
]