[
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.569745063781738,
        "final_policy_stability": 0.9695290858725761,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8254847645429363,
            0.8725761772853186,
            0.8725761772853186,
            0.8670360110803325,
            0.9113573407202216,
            0.8919667590027701,
            0.8476454293628809,
            0.8698060941828255,
            0.9362880886426593,
            0.8642659279778393,
            0.853185595567867,
            0.8698060941828255,
            0.9529085872576177,
            0.8919667590027701,
            0.9141274238227147,
            0.9861495844875346,
            0.9667590027700831,
            0.9501385041551247,
            0.9722991689750693,
            0.8393351800554016,
            0.8947368421052632,
            0.9778393351800554,
            0.9030470914127424,
            0.9362880886426593,
            0.9722991689750693,
            0.9806094182825484,
            0.9279778393351801,
            0.9750692520775623,
            1.0,
            0.9889196675900277,
            0.9833795013850416,
            0.8836565096952909,
            0.997229916897507,
            0.9695290858725761
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -3610,
            -2894,
            -1341,
            -2752,
            -3610,
            -2656,
            -1035,
            -3610,
            -3610,
            -3610,
            -623,
            -2690,
            -2033,
            -330,
            -518,
            -1036,
            -289,
            -3610,
            -3610,
            -242,
            -2653,
            -663,
            -714,
            -509,
            -1684,
            -791,
            -479,
            -363,
            -991,
            -3610,
            -261,
            -1670
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            3610,
            2995,
            1442,
            2853,
            3610,
            2757,
            1136,
            3610,
            3610,
            3610,
            724,
            2791,
            2134,
            431,
            619,
            1137,
            390,
            3610,
            3610,
            343,
            2754,
            764,
            815,
            610,
            1785,
            892,
            580,
            464,
            1092,
            3610,
            362,
            1771
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "541/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.482309103012085,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8448753462603878,
            0.8448753462603878,
            0.8559556786703602,
            0.9196675900277008,
            0.96398891966759,
            0.8614958448753463,
            0.8919667590027701,
            0.8781163434903048,
            0.8448753462603878,
            0.8781163434903048,
            0.9279778393351801,
            0.9722991689750693,
            0.9473684210526315,
            0.9806094182825484,
            0.9362880886426593,
            0.8808864265927978,
            0.9196675900277008,
            0.8919667590027701,
            0.961218836565097,
            0.9750692520775623,
            0.8975069252077562,
            0.8975069252077562,
            0.8919667590027701,
            0.9556786703601108,
            0.8781163434903048,
            0.9889196675900277,
            0.9002770083102493,
            0.8864265927977839,
            0.9861495844875346,
            0.9861495844875346,
            0.9889196675900277,
            0.9750692520775623,
            1.0,
            0.9889196675900277,
            0.8781163434903048,
            0.9916897506925207,
            0.9916897506925207,
            1.0,
            0.997229916897507,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -3610,
            -2374,
            -334,
            -3610,
            -3610,
            -2845,
            -3610,
            -3459,
            -2038,
            -325,
            -1321,
            -497,
            -2073,
            -3610,
            -2142,
            -3610,
            -1411,
            -797,
            -3610,
            -2627,
            -2918,
            -1544,
            -3610,
            -441,
            -3610,
            -3326,
            -713,
            -613,
            -1138,
            -883,
            -99,
            -749,
            -3610,
            -389,
            -352,
            -204,
            -281,
            -253
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            3610,
            2475,
            435,
            3610,
            3610,
            2946,
            3610,
            3560,
            2139,
            426,
            1422,
            598,
            2174,
            3610,
            2243,
            3610,
            1512,
            898,
            3610,
            2728,
            3019,
            1645,
            3610,
            542,
            3610,
            3427,
            814,
            714,
            1239,
            984,
            200,
            850,
            3610,
            490,
            453,
            305,
            382,
            354
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "542/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.472323417663574,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.8310249307479224,
            0.8947368421052632,
            0.8725761772853186,
            0.8836565096952909,
            0.8199445983379502,
            0.8808864265927978,
            0.9058171745152355,
            0.8698060941828255,
            0.9030470914127424,
            0.853185595567867,
            0.9778393351800554,
            0.9224376731301939,
            0.9556786703601108,
            0.9445983379501385,
            0.8476454293628809,
            0.9529085872576177,
            0.9501385041551247,
            0.889196675900277,
            0.9889196675900277,
            0.9584487534626038,
            0.8808864265927978,
            0.96398891966759,
            0.9390581717451524,
            0.9667590027700831,
            0.8725761772853186,
            0.9002770083102493,
            0.8975069252077562,
            0.9196675900277008,
            0.9778393351800554,
            0.9861495844875346,
            0.9861495844875346,
            0.9889196675900277,
            0.9501385041551247,
            1.0,
            0.96398891966759,
            1.0
        ],
        "reward_history": [
            -158,
            -3610,
            -3610,
            -3610,
            -1478,
            -3610,
            -3416,
            -2178,
            -3610,
            -2328,
            -3610,
            -206,
            -1912,
            -509,
            -1037,
            -3610,
            -1181,
            -1375,
            -3610,
            -343,
            -1042,
            -3610,
            -638,
            -1274,
            -973,
            -3610,
            -3610,
            -3610,
            -3610,
            -1196,
            -492,
            -580,
            -436,
            -1187,
            -304,
            -3364,
            -423
        ],
        "steps_history": [
            259,
            3610,
            3610,
            3610,
            1579,
            3610,
            3517,
            2279,
            3610,
            2429,
            3610,
            307,
            2013,
            610,
            1138,
            3610,
            1282,
            1476,
            3610,
            444,
            1143,
            3610,
            739,
            1375,
            1074,
            3610,
            3610,
            3610,
            3610,
            1297,
            593,
            681,
            537,
            1288,
            405,
            3465,
            524
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "543/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.4805662631988525,
        "final_policy_stability": 0.9806094182825484,
        "episodes_to_convergence": 38,
        "policy_stability_history": [
            0.0,
            0.853185595567867,
            0.9030470914127424,
            0.7811634349030471,
            0.814404432132964,
            0.8337950138504155,
            0.9168975069252078,
            0.8836565096952909,
            0.8559556786703602,
            0.889196675900277,
            0.8698060941828255,
            0.8753462603878116,
            0.850415512465374,
            0.850415512465374,
            0.925207756232687,
            0.9861495844875346,
            0.9806094182825484,
            0.9362880886426593,
            0.8753462603878116,
            0.9556786703601108,
            0.9196675900277008,
            0.997229916897507,
            0.9806094182825484,
            0.9861495844875346,
            0.9861495844875346,
            0.9833795013850416,
            0.9224376731301939,
            0.9916897506925207,
            0.8808864265927978,
            0.9861495844875346,
            0.9750692520775623,
            0.8698060941828255,
            0.9833795013850416,
            0.9861495844875346,
            0.9861495844875346,
            0.9916897506925207,
            0.9944598337950139,
            0.997229916897507,
            0.9806094182825484
        ],
        "reward_history": [
            -1707,
            -3610,
            -1132,
            -3610,
            -3610,
            -3424,
            -2116,
            -3610,
            -3610,
            -2602,
            -3610,
            -3610,
            -3610,
            -3610,
            -1151,
            -299,
            -720,
            -1339,
            -3610,
            -927,
            -2645,
            -90,
            -398,
            -723,
            -541,
            -228,
            -1850,
            -176,
            -3610,
            -801,
            -1218,
            -3610,
            -657,
            -321,
            -624,
            -216,
            -716,
            -694,
            -902
        ],
        "steps_history": [
            1808,
            3610,
            1233,
            3610,
            3610,
            3525,
            2217,
            3610,
            3610,
            2703,
            3610,
            3610,
            3610,
            3610,
            1252,
            400,
            821,
            1440,
            3610,
            1028,
            2746,
            191,
            499,
            824,
            642,
            329,
            1951,
            277,
            3610,
            902,
            1319,
            3610,
            758,
            422,
            725,
            317,
            817,
            795,
            1003
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "544/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.995323896408081,
        "final_policy_stability": 0.9473684210526315,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8448753462603878,
            0.8836565096952909,
            0.8587257617728532,
            0.8365650969529086,
            0.8947368421052632,
            0.9085872576177285,
            0.9556786703601108,
            0.9307479224376731,
            0.8781163434903048,
            0.8781163434903048,
            0.9224376731301939,
            0.9224376731301939,
            0.8808864265927978,
            0.9390581717451524,
            0.9667590027700831,
            0.9418282548476454,
            0.8670360110803325,
            0.9806094182825484,
            0.9778393351800554,
            0.96398891966759,
            0.9778393351800554,
            0.8559556786703602,
            0.9722991689750693,
            0.889196675900277,
            0.9667590027700831,
            0.9722991689750693,
            0.9695290858725761,
            0.9529085872576177,
            0.9916897506925207,
            0.9944598337950139,
            0.8698060941828255,
            0.9473684210526315
        ],
        "reward_history": [
            -803,
            -3137,
            -2458,
            -2754,
            -3610,
            -3610,
            -1744,
            -364,
            -873,
            -3610,
            -3610,
            -1575,
            -1204,
            -3610,
            -883,
            -493,
            -1114,
            -3610,
            -762,
            -367,
            -1007,
            -554,
            -3610,
            -1133,
            -3610,
            -805,
            -1025,
            -704,
            -1806,
            -323,
            -470,
            -3610,
            -1555
        ],
        "steps_history": [
            904,
            3238,
            2559,
            2855,
            3610,
            3610,
            1845,
            465,
            974,
            3610,
            3610,
            1676,
            1305,
            3610,
            984,
            594,
            1215,
            3610,
            863,
            468,
            1108,
            655,
            3610,
            1234,
            3610,
            906,
            1126,
            805,
            1907,
            424,
            571,
            3610,
            1656
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "545/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.09123420715332,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8476454293628809,
            0.8421052631578947,
            0.8919667590027701,
            0.8005540166204986,
            0.8393351800554016,
            0.8642659279778393,
            0.9279778393351801,
            0.9058171745152355,
            0.8614958448753463,
            0.8199445983379502,
            0.8421052631578947,
            0.9141274238227147,
            0.8559556786703602,
            0.9667590027700831,
            0.9695290858725761,
            0.9196675900277008,
            0.9833795013850416,
            0.8310249307479224,
            0.9667590027700831,
            0.9556786703601108,
            0.9889196675900277,
            0.9722991689750693,
            0.9833795013850416,
            0.9861495844875346,
            0.9667590027700831,
            0.997229916897507,
            0.8698060941828255,
            0.9778393351800554,
            0.9861495844875346,
            0.925207756232687,
            0.9889196675900277,
            0.997229916897507,
            0.9778393351800554,
            1.0,
            0.9889196675900277,
            1.0,
            1.0
        ],
        "reward_history": [
            -388,
            -2161,
            -2904,
            -1718,
            -3323,
            -3610,
            -1621,
            -989,
            -1071,
            -3610,
            -3610,
            -3610,
            -3347,
            -3610,
            -879,
            -1035,
            -1994,
            -284,
            -3610,
            -521,
            -706,
            -207,
            -840,
            -746,
            -292,
            -1230,
            -428,
            -3610,
            -649,
            -740,
            -3006,
            -545,
            -388,
            -1177,
            -706,
            -1038,
            -263,
            -731
        ],
        "steps_history": [
            489,
            2262,
            3005,
            1819,
            3424,
            3610,
            1722,
            1090,
            1172,
            3610,
            3610,
            3610,
            3448,
            3610,
            980,
            1136,
            2095,
            385,
            3610,
            622,
            807,
            308,
            941,
            847,
            393,
            1331,
            529,
            3610,
            750,
            841,
            3107,
            646,
            489,
            1278,
            807,
            1139,
            364,
            832
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "546/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.328986883163452,
        "final_policy_stability": 0.9861495844875346,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8337950138504155,
            0.7950138504155124,
            0.8199445983379502,
            0.9224376731301939,
            0.9113573407202216,
            0.96398891966759,
            0.8282548476454293,
            0.8587257617728532,
            0.8005540166204986,
            0.925207756232687,
            0.9445983379501385,
            0.9584487534626038,
            0.9113573407202216,
            0.850415512465374,
            0.8781163434903048,
            0.9473684210526315,
            0.925207756232687,
            0.8864265927977839,
            0.9529085872576177,
            0.8642659279778393,
            0.925207756232687,
            0.9806094182825484,
            0.8614958448753463,
            0.9390581717451524,
            0.9861495844875346,
            0.9861495844875346,
            0.961218836565097,
            0.9916897506925207,
            0.997229916897507,
            0.997229916897507,
            0.9861495844875346
        ],
        "reward_history": [
            -1936,
            -3610,
            -3610,
            -3610,
            -1477,
            -1675,
            -54,
            -3610,
            -3610,
            -3610,
            -827,
            -610,
            -522,
            -1394,
            -3610,
            -1961,
            -1470,
            -1667,
            -3610,
            -1003,
            -3610,
            -2049,
            -396,
            -3610,
            -1484,
            -199,
            -773,
            -1369,
            -916,
            -186,
            -332,
            -517
        ],
        "steps_history": [
            2037,
            3610,
            3610,
            3610,
            1578,
            1776,
            155,
            3610,
            3610,
            3610,
            928,
            711,
            623,
            1495,
            3610,
            2062,
            1571,
            1768,
            3610,
            1104,
            3610,
            2150,
            497,
            3610,
            1585,
            300,
            874,
            1470,
            1017,
            287,
            433,
            618
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "547/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.88043212890625,
        "final_policy_stability": 0.9944598337950139,
        "episodes_to_convergence": 45,
        "policy_stability_history": [
            0.0,
            0.8448753462603878,
            0.8559556786703602,
            0.8919667590027701,
            0.8393351800554016,
            0.8836565096952909,
            0.9196675900277008,
            0.8836565096952909,
            0.8559556786703602,
            0.9030470914127424,
            0.9695290858725761,
            0.9418282548476454,
            0.8587257617728532,
            0.8614958448753463,
            0.8587257617728532,
            0.9168975069252078,
            0.9806094182825484,
            0.9861495844875346,
            0.9667590027700831,
            0.8836565096952909,
            0.8614958448753463,
            0.8670360110803325,
            0.9335180055401662,
            0.9667590027700831,
            0.9556786703601108,
            0.9778393351800554,
            0.9501385041551247,
            0.9362880886426593,
            0.96398891966759,
            0.9806094182825484,
            0.9861495844875346,
            0.9889196675900277,
            0.9944598337950139,
            0.9916897506925207,
            0.9889196675900277,
            0.9667590027700831,
            0.9861495844875346,
            0.9833795013850416,
            0.9861495844875346,
            0.9085872576177285,
            0.997229916897507,
            0.9806094182825484,
            0.9944598337950139,
            0.9806094182825484,
            0.997229916897507,
            0.9944598337950139
        ],
        "reward_history": [
            -3610,
            -3610,
            -1430,
            -1433,
            -3610,
            -3610,
            -1462,
            -2137,
            -3610,
            -2305,
            -305,
            -775,
            -3610,
            -3610,
            -3610,
            -1906,
            -296,
            -287,
            -330,
            -3610,
            -3610,
            -3610,
            -1532,
            -546,
            -771,
            -377,
            -815,
            -1422,
            -372,
            -385,
            -163,
            -118,
            -179,
            -93,
            -595,
            -601,
            -517,
            -194,
            -638,
            -2248,
            -366,
            -982,
            -671,
            -1026,
            -578,
            -478
        ],
        "steps_history": [
            3610,
            3610,
            1531,
            1534,
            3610,
            3610,
            1563,
            2238,
            3610,
            2406,
            406,
            876,
            3610,
            3610,
            3610,
            2007,
            397,
            388,
            431,
            3610,
            3610,
            3610,
            1633,
            647,
            872,
            478,
            916,
            1523,
            473,
            486,
            264,
            219,
            280,
            194,
            696,
            702,
            618,
            295,
            739,
            2349,
            467,
            1083,
            772,
            1127,
            679,
            579
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "548/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.895479679107666,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 42,
        "policy_stability_history": [
            0.0,
            0.8365650969529086,
            0.8947368421052632,
            0.9196675900277008,
            0.814404432132964,
            0.853185595567867,
            0.9362880886426593,
            0.8448753462603878,
            0.9473684210526315,
            0.889196675900277,
            0.8614958448753463,
            0.8337950138504155,
            0.8781163434903048,
            0.9695290858725761,
            0.814404432132964,
            0.9224376731301939,
            0.8670360110803325,
            0.8559556786703602,
            0.96398891966759,
            0.8559556786703602,
            0.9445983379501385,
            0.9861495844875346,
            0.9806094182825484,
            0.9584487534626038,
            0.8670360110803325,
            0.9390581717451524,
            0.961218836565097,
            0.9445983379501385,
            0.9722991689750693,
            0.8421052631578947,
            0.9861495844875346,
            0.9916897506925207,
            0.9778393351800554,
            0.9695290858725761,
            0.9750692520775623,
            0.9861495844875346,
            0.9722991689750693,
            0.9833795013850416,
            0.997229916897507,
            1.0,
            0.9944598337950139,
            1.0,
            1.0
        ],
        "reward_history": [
            -2232,
            -2720,
            -2323,
            -1424,
            -3610,
            -3610,
            -905,
            -3335,
            -575,
            -1763,
            -3610,
            -3610,
            -3610,
            -477,
            -3610,
            -889,
            -3610,
            -2471,
            -620,
            -2521,
            -620,
            -135,
            -131,
            -625,
            -3610,
            -1195,
            -577,
            -747,
            -603,
            -3610,
            -648,
            -354,
            -467,
            -823,
            -648,
            -363,
            -855,
            -517,
            -386,
            -353,
            -660,
            -416,
            -666
        ],
        "steps_history": [
            2333,
            2821,
            2424,
            1525,
            3610,
            3610,
            1006,
            3436,
            676,
            1864,
            3610,
            3610,
            3610,
            578,
            3610,
            990,
            3610,
            2572,
            721,
            2622,
            721,
            236,
            232,
            726,
            3610,
            1296,
            678,
            848,
            704,
            3610,
            749,
            455,
            568,
            924,
            749,
            464,
            956,
            618,
            487,
            454,
            761,
            517,
            767
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "549/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.771304607391357,
        "final_policy_stability": 0.9750692520775623,
        "episodes_to_convergence": 42,
        "policy_stability_history": [
            0.0,
            0.8476454293628809,
            0.8337950138504155,
            0.925207756232687,
            0.8864265927977839,
            0.8975069252077562,
            0.8975069252077562,
            0.8781163434903048,
            0.9113573407202216,
            0.9529085872576177,
            0.9141274238227147,
            0.9418282548476454,
            0.8614958448753463,
            0.9889196675900277,
            0.8393351800554016,
            0.9362880886426593,
            0.9196675900277008,
            0.9667590027700831,
            0.9556786703601108,
            0.8476454293628809,
            0.9058171745152355,
            0.9750692520775623,
            0.9390581717451524,
            0.9695290858725761,
            0.8836565096952909,
            0.9833795013850416,
            0.8781163434903048,
            0.9889196675900277,
            0.997229916897507,
            0.9944598337950139,
            0.9861495844875346,
            0.9944598337950139,
            0.9916897506925207,
            0.853185595567867,
            0.9778393351800554,
            0.9944598337950139,
            0.9861495844875346,
            0.997229916897507,
            0.9916897506925207,
            0.9916897506925207,
            1.0,
            0.9916897506925207,
            0.9750692520775623
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -979,
            -2347,
            -1745,
            -3610,
            -3610,
            -1539,
            -872,
            -1705,
            -751,
            -3610,
            -253,
            -3610,
            -1151,
            -1944,
            -389,
            -1165,
            -3610,
            -3610,
            -699,
            -2134,
            -362,
            -3610,
            -915,
            -3610,
            -427,
            -101,
            -321,
            -449,
            -151,
            -432,
            -3610,
            -749,
            -727,
            -441,
            -207,
            -378,
            -706,
            -221,
            -397,
            -1792
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            1080,
            2448,
            1846,
            3610,
            3610,
            1640,
            973,
            1806,
            852,
            3610,
            354,
            3610,
            1252,
            2045,
            490,
            1266,
            3610,
            3610,
            800,
            2235,
            463,
            3610,
            1016,
            3610,
            528,
            202,
            422,
            550,
            252,
            533,
            3610,
            850,
            828,
            542,
            308,
            479,
            807,
            322,
            498,
            1893
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "550/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.864485740661621,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 45,
        "policy_stability_history": [
            0.0,
            0.8337950138504155,
            0.9002770083102493,
            0.7922437673130194,
            0.814404432132964,
            0.9085872576177285,
            0.8199445983379502,
            0.8393351800554016,
            0.8698060941828255,
            0.8310249307479224,
            0.817174515235457,
            0.9556786703601108,
            0.9279778393351801,
            0.8919667590027701,
            0.9196675900277008,
            0.9501385041551247,
            0.8864265927977839,
            0.9085872576177285,
            0.853185595567867,
            0.9695290858725761,
            0.9806094182825484,
            0.8642659279778393,
            0.9529085872576177,
            0.9916897506925207,
            0.9722991689750693,
            0.9750692520775623,
            0.9806094182825484,
            0.9695290858725761,
            0.9362880886426593,
            0.8725761772853186,
            0.9944598337950139,
            0.9833795013850416,
            0.9889196675900277,
            0.9916897506925207,
            0.9861495844875346,
            0.9889196675900277,
            0.9030470914127424,
            0.9944598337950139,
            0.9833795013850416,
            0.9916897506925207,
            0.9916897506925207,
            0.9861495844875346,
            0.9889196675900277,
            1.0,
            0.997229916897507,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -1060,
            -3610,
            -3478,
            -845,
            -3610,
            -2884,
            -1607,
            -3610,
            -3389,
            -300,
            -596,
            -1728,
            -908,
            -482,
            -2929,
            -1546,
            -3610,
            -287,
            -192,
            -3610,
            -867,
            -206,
            -562,
            -471,
            -230,
            -531,
            -1259,
            -3610,
            -197,
            -658,
            -886,
            -1218,
            -261,
            -259,
            -1880,
            -169,
            -839,
            -148,
            -397,
            -446,
            -703,
            -235,
            -229,
            -175
        ],
        "steps_history": [
            3610,
            3610,
            1161,
            3610,
            3579,
            946,
            3610,
            2985,
            1708,
            3610,
            3490,
            401,
            697,
            1829,
            1009,
            583,
            3030,
            1647,
            3610,
            388,
            293,
            3610,
            968,
            307,
            663,
            572,
            331,
            632,
            1360,
            3610,
            298,
            759,
            987,
            1319,
            362,
            360,
            1981,
            270,
            940,
            249,
            498,
            547,
            804,
            336,
            330,
            276
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "551/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.1192100048065186,
        "final_policy_stability": 0.9889196675900277,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8310249307479224,
            0.9196675900277008,
            0.853185595567867,
            0.9168975069252078,
            0.814404432132964,
            0.7867036011080333,
            0.8836565096952909,
            0.8698060941828255,
            0.9335180055401662,
            0.8282548476454293,
            0.9058171745152355,
            0.9501385041551247,
            0.9529085872576177,
            0.9002770083102493,
            0.9390581717451524,
            0.9501385041551247,
            0.8587257617728532,
            0.925207756232687,
            0.9750692520775623,
            0.9750692520775623,
            0.9722991689750693,
            0.9861495844875346,
            0.9750692520775623,
            0.9390581717451524,
            0.9695290858725761,
            0.9806094182825484,
            0.8836565096952909,
            1.0,
            0.9916897506925207,
            0.9861495844875346,
            0.9944598337950139,
            0.9750692520775623,
            0.9889196675900277,
            0.9889196675900277
        ],
        "reward_history": [
            -1271,
            -3610,
            -684,
            -3610,
            -761,
            -3610,
            -3610,
            -1362,
            -3610,
            -903,
            -3610,
            -1919,
            -1089,
            -439,
            -1407,
            -1357,
            -786,
            -3610,
            -1537,
            -366,
            -554,
            -420,
            -137,
            -210,
            -899,
            -513,
            -171,
            -3610,
            -66,
            -267,
            -820,
            -229,
            -1181,
            -405,
            -499
        ],
        "steps_history": [
            1372,
            3610,
            785,
            3610,
            862,
            3610,
            3610,
            1463,
            3610,
            1004,
            3610,
            2020,
            1190,
            540,
            1508,
            1458,
            887,
            3610,
            1638,
            467,
            655,
            521,
            238,
            311,
            1000,
            614,
            272,
            3610,
            167,
            368,
            921,
            330,
            1282,
            506,
            600
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "552/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.691587686538696,
        "final_policy_stability": 0.9750692520775623,
        "episodes_to_convergence": 48,
        "policy_stability_history": [
            0.0,
            0.817174515235457,
            0.8282548476454293,
            0.8781163434903048,
            0.8614958448753463,
            0.8975069252077562,
            0.8365650969529086,
            0.8199445983379502,
            0.850415512465374,
            0.8975069252077562,
            0.9224376731301939,
            0.8919667590027701,
            0.9058171745152355,
            0.925207756232687,
            0.9584487534626038,
            0.9556786703601108,
            0.9473684210526315,
            0.8587257617728532,
            0.9750692520775623,
            0.9307479224376731,
            0.9750692520775623,
            0.850415512465374,
            0.9722991689750693,
            0.853185595567867,
            0.9584487534626038,
            0.8698060941828255,
            0.9833795013850416,
            0.9030470914127424,
            0.9889196675900277,
            0.9833795013850416,
            0.9861495844875346,
            0.9778393351800554,
            0.9556786703601108,
            1.0,
            0.9944598337950139,
            0.9916897506925207,
            0.9279778393351801,
            0.9889196675900277,
            0.9833795013850416,
            0.997229916897507,
            0.96398891966759,
            0.9889196675900277,
            0.9861495844875346,
            0.9916897506925207,
            0.9889196675900277,
            1.0,
            0.9916897506925207,
            0.9944598337950139,
            0.9750692520775623
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -1620,
            -2867,
            -1806,
            -3610,
            -3610,
            -3610,
            -1240,
            -1117,
            -1146,
            -1855,
            -752,
            -1009,
            -509,
            -498,
            -3610,
            -909,
            -681,
            -397,
            -3610,
            -217,
            -3610,
            -651,
            -3610,
            -155,
            -2030,
            -109,
            -461,
            -159,
            -532,
            -1103,
            -129,
            -407,
            -147,
            -964,
            -243,
            -626,
            -439,
            -922,
            -212,
            -647,
            -718,
            -720,
            -280,
            -461,
            -291,
            -515
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            1721,
            2968,
            1907,
            3610,
            3610,
            3610,
            1341,
            1218,
            1247,
            1956,
            853,
            1110,
            610,
            599,
            3610,
            1010,
            782,
            498,
            3610,
            318,
            3610,
            752,
            3610,
            256,
            2131,
            210,
            562,
            260,
            633,
            1204,
            230,
            508,
            248,
            1065,
            344,
            727,
            540,
            1023,
            313,
            748,
            819,
            821,
            381,
            562,
            392,
            616
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "553/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.910606622695923,
        "final_policy_stability": 0.9944598337950139,
        "episodes_to_convergence": 42,
        "policy_stability_history": [
            0.0,
            0.8476454293628809,
            0.8254847645429363,
            0.8421052631578947,
            0.9529085872576177,
            0.8587257617728532,
            0.9085872576177285,
            0.8725761772853186,
            0.9473684210526315,
            0.8282548476454293,
            0.8476454293628809,
            0.8864265927977839,
            0.9335180055401662,
            0.8725761772853186,
            0.9529085872576177,
            0.9445983379501385,
            0.8781163434903048,
            0.8975069252077562,
            0.9058171745152355,
            0.9224376731301939,
            0.9501385041551247,
            0.9335180055401662,
            0.9806094182825484,
            0.9833795013850416,
            0.9806094182825484,
            0.9806094182825484,
            0.9861495844875346,
            0.9778393351800554,
            0.9667590027700831,
            0.9750692520775623,
            0.9695290858725761,
            0.9889196675900277,
            0.9722991689750693,
            0.9750692520775623,
            0.9335180055401662,
            0.9861495844875346,
            0.8781163434903048,
            0.9861495844875346,
            0.8698060941828255,
            1.0,
            0.997229916897507,
            0.997229916897507,
            0.9944598337950139
        ],
        "reward_history": [
            -3038,
            -3610,
            -3610,
            -3610,
            -459,
            -2603,
            -2255,
            -3610,
            -533,
            -3610,
            -3610,
            -3610,
            -600,
            -3610,
            -331,
            -981,
            -3187,
            -1480,
            -1568,
            -780,
            -867,
            -1061,
            -236,
            -225,
            -266,
            -221,
            -411,
            -503,
            -302,
            -265,
            -606,
            -163,
            -409,
            -222,
            -874,
            -439,
            -3610,
            -237,
            -3610,
            -392,
            -622,
            -253,
            -268
        ],
        "steps_history": [
            3139,
            3610,
            3610,
            3610,
            560,
            2704,
            2356,
            3610,
            634,
            3610,
            3610,
            3610,
            701,
            3610,
            432,
            1082,
            3288,
            1581,
            1669,
            881,
            968,
            1162,
            337,
            326,
            367,
            322,
            512,
            604,
            403,
            366,
            707,
            264,
            510,
            323,
            975,
            540,
            3610,
            338,
            3610,
            493,
            723,
            354,
            369
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "554/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.498218774795532,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 48,
        "policy_stability_history": [
            0.0,
            0.8476454293628809,
            0.9141274238227147,
            0.814404432132964,
            0.8614958448753463,
            0.9335180055401662,
            0.9141274238227147,
            0.9058171745152355,
            0.8919667590027701,
            0.8476454293628809,
            0.853185595567867,
            0.8836565096952909,
            0.8919667590027701,
            0.9529085872576177,
            0.9445983379501385,
            0.8337950138504155,
            0.9362880886426593,
            0.8559556786703602,
            0.961218836565097,
            0.9224376731301939,
            0.850415512465374,
            0.9778393351800554,
            0.8448753462603878,
            0.9584487534626038,
            0.9778393351800554,
            0.9584487534626038,
            0.961218836565097,
            0.9584487534626038,
            0.8199445983379502,
            0.9861495844875346,
            0.9861495844875346,
            0.9695290858725761,
            0.9889196675900277,
            0.9889196675900277,
            0.9722991689750693,
            0.9667590027700831,
            0.9778393351800554,
            0.9556786703601108,
            0.9861495844875346,
            0.9861495844875346,
            0.9861495844875346,
            0.9861495844875346,
            0.96398891966759,
            0.9889196675900277,
            0.9861495844875346,
            0.9916897506925207,
            0.997229916897507,
            1.0,
            0.997229916897507
        ],
        "reward_history": [
            -3610,
            -3506,
            -792,
            -3610,
            -1496,
            -866,
            -1150,
            -1196,
            -1581,
            -3610,
            -3610,
            -3610,
            -1787,
            -264,
            -506,
            -3610,
            -513,
            -3179,
            -423,
            -991,
            -3610,
            -77,
            -3610,
            -633,
            -174,
            -273,
            -574,
            -297,
            -3610,
            -218,
            -179,
            -452,
            -187,
            -275,
            -363,
            -598,
            -355,
            -632,
            -234,
            -474,
            -125,
            -407,
            -1071,
            -632,
            -368,
            -565,
            -483,
            -588,
            -606
        ],
        "steps_history": [
            3610,
            3607,
            893,
            3610,
            1597,
            967,
            1251,
            1297,
            1682,
            3610,
            3610,
            3610,
            1888,
            365,
            607,
            3610,
            614,
            3280,
            524,
            1092,
            3610,
            178,
            3610,
            734,
            275,
            374,
            675,
            398,
            3610,
            319,
            280,
            553,
            288,
            376,
            464,
            699,
            456,
            733,
            335,
            575,
            226,
            508,
            1172,
            733,
            469,
            666,
            584,
            689,
            707
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "555/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.2133629322052,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 38,
        "policy_stability_history": [
            0.0,
            0.8254847645429363,
            0.8725761772853186,
            0.8753462603878116,
            0.7977839335180056,
            0.8781163434903048,
            0.8670360110803325,
            0.8476454293628809,
            0.8698060941828255,
            0.9418282548476454,
            0.8698060941828255,
            0.9529085872576177,
            0.9362880886426593,
            0.9722991689750693,
            0.8670360110803325,
            0.9695290858725761,
            0.9030470914127424,
            0.9667590027700831,
            0.9390581717451524,
            0.9833795013850416,
            0.9445983379501385,
            0.8587257617728532,
            0.9030470914127424,
            0.9307479224376731,
            0.9529085872576177,
            0.9279778393351801,
            0.9141274238227147,
            0.9806094182825484,
            0.8919667590027701,
            0.9944598337950139,
            0.9418282548476454,
            0.997229916897507,
            0.9224376731301939,
            1.0,
            0.9916897506925207,
            1.0,
            1.0,
            0.9861495844875346,
            0.997229916897507
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -3610,
            -3610,
            -2152,
            -3610,
            -3610,
            -3610,
            -923,
            -3610,
            -1515,
            -969,
            -732,
            -3610,
            -210,
            -2175,
            -498,
            -1090,
            -382,
            -1945,
            -3610,
            -2339,
            -1703,
            -840,
            -2097,
            -3156,
            -363,
            -3610,
            -540,
            -2726,
            -150,
            -3610,
            -211,
            -856,
            -709,
            -188,
            -1780,
            -618
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            3610,
            3610,
            2253,
            3610,
            3610,
            3610,
            1024,
            3610,
            1616,
            1070,
            833,
            3610,
            311,
            2276,
            599,
            1191,
            483,
            2046,
            3610,
            2440,
            1804,
            941,
            2198,
            3257,
            464,
            3610,
            641,
            2827,
            251,
            3610,
            312,
            957,
            810,
            289,
            1881,
            719
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "556/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.087891101837158,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8448753462603878,
            0.8421052631578947,
            0.8310249307479224,
            0.8670360110803325,
            0.9307479224376731,
            0.8116343490304709,
            0.8254847645429363,
            0.8975069252077562,
            0.9141274238227147,
            0.8919667590027701,
            0.8836565096952909,
            0.9335180055401662,
            0.9390581717451524,
            0.8919667590027701,
            0.8614958448753463,
            0.9584487534626038,
            0.9778393351800554,
            0.8698060941828255,
            0.9556786703601108,
            0.9778393351800554,
            0.9667590027700831,
            0.961218836565097,
            0.9418282548476454,
            0.96398891966759,
            0.9806094182825484,
            0.997229916897507,
            0.9556786703601108,
            0.9806094182825484,
            0.889196675900277,
            0.9916897506925207,
            0.9944598337950139,
            0.9889196675900277,
            0.8947368421052632,
            0.9944598337950139,
            1.0,
            1.0,
            0.997229916897507
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -3610,
            -1706,
            -713,
            -3610,
            -3610,
            -2271,
            -1869,
            -3610,
            -3610,
            -1134,
            -1902,
            -2304,
            -3610,
            -1801,
            -482,
            -3503,
            -1291,
            -907,
            -1615,
            -886,
            -2621,
            -798,
            -707,
            -212,
            -2115,
            -973,
            -3610,
            -319,
            -275,
            -812,
            -3610,
            -784,
            -508,
            -268,
            -296
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            3610,
            1807,
            814,
            3610,
            3610,
            2372,
            1970,
            3610,
            3610,
            1235,
            2003,
            2405,
            3610,
            1902,
            583,
            3604,
            1392,
            1008,
            1716,
            987,
            2722,
            899,
            808,
            313,
            2216,
            1074,
            3610,
            420,
            376,
            913,
            3610,
            885,
            609,
            369,
            397
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "557/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.527486085891724,
        "final_policy_stability": 0.9916897506925207,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8310249307479224,
            0.8947368421052632,
            0.8698060941828255,
            0.8808864265927978,
            0.8254847645429363,
            0.8781163434903048,
            0.8365650969529086,
            0.9002770083102493,
            0.8559556786703602,
            0.96398891966759,
            0.9390581717451524,
            0.8559556786703602,
            0.9529085872576177,
            0.8698060941828255,
            0.8919667590027701,
            0.9556786703601108,
            0.9362880886426593,
            0.9750692520775623,
            0.961218836565097,
            0.9307479224376731,
            0.9667590027700831,
            0.9362880886426593,
            0.9861495844875346,
            0.9058171745152355,
            0.9141274238227147,
            0.9141274238227147,
            0.9390581717451524,
            0.9722991689750693,
            0.9889196675900277,
            0.9085872576177285,
            0.8919667590027701,
            0.8836565096952909,
            0.9556786703601108,
            0.9916897506925207
        ],
        "reward_history": [
            -158,
            -3610,
            -3610,
            -3610,
            -1478,
            -3610,
            -3610,
            -3610,
            -1508,
            -3610,
            -610,
            -1172,
            -3610,
            -1379,
            -3610,
            -3610,
            -798,
            -943,
            -865,
            -316,
            -1391,
            -890,
            -1631,
            -697,
            -3610,
            -2545,
            -2011,
            -2001,
            -561,
            -530,
            -3610,
            -3610,
            -3610,
            -1275,
            -1440
        ],
        "steps_history": [
            259,
            3610,
            3610,
            3610,
            1579,
            3610,
            3610,
            3610,
            1609,
            3610,
            711,
            1273,
            3610,
            1480,
            3610,
            3610,
            899,
            1044,
            966,
            417,
            1492,
            991,
            1732,
            798,
            3610,
            2646,
            2112,
            2102,
            662,
            631,
            3610,
            3610,
            3610,
            1376,
            1541
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "558/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.039009094238281,
        "final_policy_stability": 0.96398891966759,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8587257617728532,
            0.9030470914127424,
            0.7839335180055401,
            0.814404432132964,
            0.8476454293628809,
            0.9722991689750693,
            0.8310249307479224,
            0.8393351800554016,
            0.9058171745152355,
            0.850415512465374,
            0.961218836565097,
            0.8864265927977839,
            0.9529085872576177,
            0.9196675900277008,
            0.9584487534626038,
            0.8725761772853186,
            0.9750692520775623,
            0.8919667590027701,
            0.9390581717451524,
            0.9556786703601108,
            0.9916897506925207,
            0.889196675900277,
            0.9529085872576177,
            0.9722991689750693,
            0.9889196675900277,
            0.9473684210526315,
            0.9833795013850416,
            0.9889196675900277,
            0.9944598337950139,
            0.997229916897507,
            0.9085872576177285,
            0.961218836565097,
            0.997229916897507,
            0.9889196675900277,
            1.0,
            0.9833795013850416,
            0.96398891966759
        ],
        "reward_history": [
            -1707,
            -3610,
            -1132,
            -3610,
            -3610,
            -2978,
            -198,
            -3610,
            -3610,
            -1848,
            -3017,
            -272,
            -3610,
            -846,
            -1327,
            -756,
            -3610,
            -556,
            -3610,
            -1720,
            -756,
            -482,
            -3610,
            -1601,
            -528,
            -403,
            -755,
            -672,
            -723,
            -293,
            -338,
            -3610,
            -1614,
            -304,
            -729,
            -246,
            -1584,
            -2041
        ],
        "steps_history": [
            1808,
            3610,
            1233,
            3610,
            3610,
            3079,
            299,
            3610,
            3610,
            1949,
            3118,
            373,
            3610,
            947,
            1428,
            857,
            3610,
            657,
            3610,
            1821,
            857,
            583,
            3610,
            1702,
            629,
            504,
            856,
            773,
            824,
            394,
            439,
            3610,
            1715,
            405,
            830,
            347,
            1685,
            2142
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "559/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.96566367149353,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 44,
        "policy_stability_history": [
            0.0,
            0.8448753462603878,
            0.8836565096952909,
            0.8587257617728532,
            0.8365650969529086,
            0.8947368421052632,
            0.9473684210526315,
            0.8559556786703602,
            0.96398891966759,
            0.8476454293628809,
            0.8947368421052632,
            0.9722991689750693,
            0.9473684210526315,
            0.9390581717451524,
            0.889196675900277,
            0.9833795013850416,
            0.8698060941828255,
            0.9418282548476454,
            0.9584487534626038,
            0.9695290858725761,
            0.9418282548476454,
            0.8753462603878116,
            0.9750692520775623,
            0.96398891966759,
            0.9556786703601108,
            0.8947368421052632,
            0.850415512465374,
            0.961218836565097,
            0.9861495844875346,
            0.9889196675900277,
            0.9833795013850416,
            0.889196675900277,
            0.9750692520775623,
            0.9916897506925207,
            0.9861495844875346,
            0.9722991689750693,
            0.9889196675900277,
            0.9806094182825484,
            0.9418282548476454,
            0.9750692520775623,
            0.997229916897507,
            1.0,
            0.997229916897507,
            0.9695290858725761,
            0.997229916897507
        ],
        "reward_history": [
            -803,
            -3137,
            -2458,
            -2754,
            -3610,
            -3610,
            -1157,
            -3610,
            -401,
            -3610,
            -3610,
            -296,
            -640,
            -1551,
            -3610,
            -369,
            -3610,
            -1190,
            -865,
            -405,
            -1001,
            -3610,
            -790,
            -1023,
            -1411,
            -3610,
            -3610,
            -2175,
            -222,
            -788,
            -494,
            -3610,
            -935,
            -457,
            -540,
            -1449,
            -418,
            -365,
            -1861,
            -992,
            -598,
            -163,
            -521,
            -1424,
            -825
        ],
        "steps_history": [
            904,
            3238,
            2559,
            2855,
            3610,
            3610,
            1258,
            3610,
            502,
            3610,
            3610,
            397,
            741,
            1652,
            3610,
            470,
            3610,
            1291,
            966,
            506,
            1102,
            3610,
            891,
            1124,
            1512,
            3610,
            3610,
            2276,
            323,
            889,
            595,
            3610,
            1036,
            558,
            641,
            1550,
            519,
            466,
            1962,
            1093,
            699,
            264,
            622,
            1525,
            926
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "560/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.624854326248169,
        "final_policy_stability": 0.9944598337950139,
        "episodes_to_convergence": 41,
        "policy_stability_history": [
            0.0,
            0.8476454293628809,
            0.8421052631578947,
            0.8947368421052632,
            0.7950138504155124,
            0.850415512465374,
            0.8559556786703602,
            0.8975069252077562,
            0.9002770083102493,
            0.9141274238227147,
            0.8614958448753463,
            0.8448753462603878,
            0.8393351800554016,
            0.8365650969529086,
            0.9224376731301939,
            0.96398891966759,
            0.9058171745152355,
            0.8725761772853186,
            0.9335180055401662,
            0.9695290858725761,
            0.9695290858725761,
            0.9667590027700831,
            0.9833795013850416,
            0.9279778393351801,
            0.96398891966759,
            0.9944598337950139,
            0.9529085872576177,
            0.9695290858725761,
            0.9944598337950139,
            0.9778393351800554,
            0.9806094182825484,
            1.0,
            0.9889196675900277,
            0.997229916897507,
            0.9889196675900277,
            1.0,
            0.9916897506925207,
            0.9529085872576177,
            0.9833795013850416,
            0.997229916897507,
            1.0,
            0.9944598337950139
        ],
        "reward_history": [
            -388,
            -2161,
            -2904,
            -1718,
            -3323,
            -3610,
            -3610,
            -1641,
            -1697,
            -1224,
            -3610,
            -3610,
            -3610,
            -3610,
            -1022,
            -328,
            -681,
            -3610,
            -1233,
            -286,
            -505,
            -220,
            -173,
            -2003,
            -1254,
            -305,
            -1107,
            -648,
            -553,
            -463,
            -312,
            -213,
            -204,
            -613,
            -785,
            -79,
            -398,
            -1768,
            -369,
            -238,
            -388,
            -1177
        ],
        "steps_history": [
            489,
            2262,
            3005,
            1819,
            3424,
            3610,
            3610,
            1742,
            1798,
            1325,
            3610,
            3610,
            3610,
            3610,
            1123,
            429,
            782,
            3610,
            1334,
            387,
            606,
            321,
            274,
            2104,
            1355,
            406,
            1208,
            749,
            654,
            564,
            413,
            314,
            305,
            714,
            886,
            180,
            499,
            1869,
            470,
            339,
            489,
            1278
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "561/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.309712648391724,
        "final_policy_stability": 0.9944598337950139,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8337950138504155,
            0.7950138504155124,
            0.817174515235457,
            0.9224376731301939,
            0.9113573407202216,
            0.96398891966759,
            0.8254847645429363,
            0.8947368421052632,
            0.8947368421052632,
            0.9584487534626038,
            0.8559556786703602,
            0.9058171745152355,
            0.8587257617728532,
            0.9473684210526315,
            0.96398891966759,
            0.8670360110803325,
            0.9833795013850416,
            0.9556786703601108,
            0.9778393351800554,
            0.8587257617728532,
            0.9529085872576177,
            0.8836565096952909,
            0.9806094182825484,
            0.9806094182825484,
            0.9279778393351801,
            0.9667590027700831,
            0.9889196675900277,
            0.9861495844875346,
            0.9861495844875346,
            0.9750692520775623,
            0.9778393351800554,
            0.9944598337950139,
            0.9944598337950139
        ],
        "reward_history": [
            -1936,
            -3610,
            -3610,
            -3610,
            -1477,
            -1675,
            -54,
            -3610,
            -1458,
            -3610,
            -659,
            -3610,
            -2588,
            -3610,
            -710,
            -1090,
            -3610,
            -186,
            -1077,
            -348,
            -3610,
            -462,
            -3610,
            -370,
            -467,
            -1657,
            -920,
            -300,
            -420,
            -389,
            -466,
            -931,
            -721,
            -569
        ],
        "steps_history": [
            2037,
            3610,
            3610,
            3610,
            1578,
            1776,
            155,
            3610,
            1559,
            3610,
            760,
            3610,
            2689,
            3610,
            811,
            1191,
            3610,
            287,
            1178,
            449,
            3610,
            563,
            3610,
            471,
            568,
            1758,
            1021,
            401,
            521,
            490,
            567,
            1032,
            822,
            670
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "562/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.948524236679077,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 44,
        "policy_stability_history": [
            0.0,
            0.853185595567867,
            0.9168975069252078,
            0.8587257617728532,
            0.8476454293628809,
            0.8781163434903048,
            0.9224376731301939,
            0.8476454293628809,
            0.8725761772853186,
            0.9418282548476454,
            0.8476454293628809,
            0.925207756232687,
            0.925207756232687,
            0.8365650969529086,
            0.8365650969529086,
            0.9722991689750693,
            0.9473684210526315,
            0.9335180055401662,
            0.889196675900277,
            0.961218836565097,
            0.9529085872576177,
            0.925207756232687,
            0.9861495844875346,
            0.9889196675900277,
            0.9196675900277008,
            0.9833795013850416,
            0.9085872576177285,
            0.9473684210526315,
            0.889196675900277,
            0.9916897506925207,
            0.9778393351800554,
            0.8698060941828255,
            0.9695290858725761,
            1.0,
            0.9944598337950139,
            0.997229916897507,
            0.8836565096952909,
            0.9778393351800554,
            0.9667590027700831,
            0.997229916897507,
            0.9889196675900277,
            0.9944598337950139,
            0.9944598337950139,
            0.9916897506925207,
            1.0
        ],
        "reward_history": [
            -3610,
            -2804,
            -1258,
            -3610,
            -2746,
            -1951,
            -974,
            -3610,
            -3610,
            -545,
            -3610,
            -755,
            -1192,
            -2414,
            -3610,
            -314,
            -742,
            -1504,
            -3610,
            -554,
            -629,
            -566,
            -143,
            -309,
            -1391,
            -203,
            -1664,
            -1447,
            -3610,
            -95,
            -383,
            -3610,
            -588,
            -329,
            -247,
            -63,
            -3503,
            -967,
            -1071,
            -366,
            -982,
            -553,
            -481,
            -562,
            -578
        ],
        "steps_history": [
            3610,
            2905,
            1359,
            3610,
            2847,
            2052,
            1075,
            3610,
            3610,
            646,
            3610,
            856,
            1293,
            2515,
            3610,
            415,
            843,
            1605,
            3610,
            655,
            730,
            667,
            244,
            410,
            1492,
            304,
            1765,
            1548,
            3610,
            196,
            484,
            3610,
            689,
            430,
            348,
            164,
            3604,
            1068,
            1172,
            467,
            1083,
            654,
            582,
            663,
            679
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "563/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 8.202036142349243,
        "final_policy_stability": 0.9944598337950139,
        "episodes_to_convergence": 43,
        "policy_stability_history": [
            0.0,
            0.8365650969529086,
            0.8947368421052632,
            0.9196675900277008,
            0.8088642659279779,
            0.853185595567867,
            0.9390581717451524,
            0.8393351800554016,
            0.8421052631578947,
            0.8781163434903048,
            0.9750692520775623,
            0.9335180055401662,
            0.925207756232687,
            0.8698060941828255,
            0.9916897506925207,
            0.8587257617728532,
            0.9418282548476454,
            0.9750692520775623,
            0.9418282548476454,
            0.9002770083102493,
            0.9778393351800554,
            0.8670360110803325,
            0.9806094182825484,
            0.9750692520775623,
            0.961218836565097,
            0.8753462603878116,
            0.9335180055401662,
            0.9861495844875346,
            0.9750692520775623,
            0.9889196675900277,
            0.9833795013850416,
            0.9916897506925207,
            0.9944598337950139,
            0.9473684210526315,
            0.9529085872576177,
            0.9861495844875346,
            0.9889196675900277,
            0.9833795013850416,
            0.9861495844875346,
            0.9916897506925207,
            0.9916897506925207,
            0.9944598337950139,
            0.997229916897507,
            0.9944598337950139
        ],
        "reward_history": [
            -2232,
            -2720,
            -2323,
            -1424,
            -3610,
            -3610,
            -905,
            -3610,
            -3610,
            -3610,
            -311,
            -1237,
            -1977,
            -3610,
            -183,
            -3610,
            -645,
            -504,
            -658,
            -3062,
            -589,
            -3610,
            -132,
            -397,
            -762,
            -3610,
            -1290,
            -436,
            -888,
            -149,
            -323,
            -150,
            -1109,
            -1143,
            -2038,
            -467,
            -823,
            -648,
            -363,
            -855,
            -517,
            -386,
            -353,
            -660
        ],
        "steps_history": [
            2333,
            2821,
            2424,
            1525,
            3610,
            3610,
            1006,
            3610,
            3610,
            3610,
            412,
            1338,
            2078,
            3610,
            284,
            3610,
            746,
            605,
            759,
            3163,
            690,
            3610,
            233,
            498,
            863,
            3610,
            1391,
            537,
            989,
            250,
            424,
            251,
            1210,
            1244,
            2139,
            568,
            924,
            749,
            464,
            956,
            618,
            487,
            454,
            761
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "564/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.959972858428955,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 42,
        "policy_stability_history": [
            0.0,
            0.850415512465374,
            0.8337950138504155,
            0.9168975069252078,
            0.850415512465374,
            0.961218836565097,
            0.9113573407202216,
            0.8864265927977839,
            0.9390581717451524,
            0.9667590027700831,
            0.8808864265927978,
            0.9778393351800554,
            0.9473684210526315,
            0.9085872576177285,
            0.9806094182825484,
            0.9750692520775623,
            0.8725761772853186,
            0.8559556786703602,
            0.8725761772853186,
            0.9501385041551247,
            0.925207756232687,
            0.9806094182825484,
            0.9695290858725761,
            0.8670360110803325,
            0.9141274238227147,
            0.9695290858725761,
            0.8614958448753463,
            0.9833795013850416,
            0.8836565096952909,
            0.9916897506925207,
            0.8808864265927978,
            0.9778393351800554,
            0.997229916897507,
            0.9889196675900277,
            0.9833795013850416,
            0.9833795013850416,
            0.9833795013850416,
            0.9889196675900277,
            0.9889196675900277,
            0.9916897506925207,
            0.997229916897507,
            0.997229916897507,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -979,
            -3610,
            -451,
            -956,
            -3610,
            -1064,
            -181,
            -1908,
            -325,
            -720,
            -1939,
            -381,
            -252,
            -3610,
            -3610,
            -3610,
            -635,
            -1152,
            -251,
            -226,
            -3610,
            -1701,
            -846,
            -3610,
            -268,
            -3312,
            -999,
            -3610,
            -629,
            -470,
            -508,
            -476,
            -343,
            -1290,
            -1301,
            -407,
            -614,
            -727,
            -441,
            -207
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            1080,
            3610,
            552,
            1057,
            3610,
            1165,
            282,
            2009,
            426,
            821,
            2040,
            482,
            353,
            3610,
            3610,
            3610,
            736,
            1253,
            352,
            327,
            3610,
            1802,
            947,
            3610,
            369,
            3413,
            1100,
            3610,
            730,
            571,
            609,
            577,
            444,
            1391,
            1402,
            508,
            715,
            828,
            542,
            308
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "565/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.848918437957764,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 44,
        "policy_stability_history": [
            0.0,
            0.9224376731301939,
            0.8476454293628809,
            0.8587257617728532,
            0.8587257617728532,
            0.9362880886426593,
            0.8864265927977839,
            0.8670360110803325,
            0.925207756232687,
            0.9362880886426593,
            0.9418282548476454,
            0.8587257617728532,
            0.9335180055401662,
            0.9030470914127424,
            0.9279778393351801,
            0.9722991689750693,
            0.8559556786703602,
            0.9362880886426593,
            0.96398891966759,
            0.9168975069252078,
            0.9695290858725761,
            0.9667590027700831,
            0.9667590027700831,
            0.8448753462603878,
            0.9722991689750693,
            0.853185595567867,
            0.96398891966759,
            0.9335180055401662,
            0.9944598337950139,
            0.9833795013850416,
            0.9695290858725761,
            0.9722991689750693,
            0.9889196675900277,
            0.9833795013850416,
            0.9861495844875346,
            0.9778393351800554,
            0.8864265927977839,
            0.9806094182825484,
            0.9944598337950139,
            0.9861495844875346,
            0.8670360110803325,
            1.0,
            0.997229916897507,
            0.9889196675900277,
            1.0
        ],
        "reward_history": [
            -3610,
            -563,
            -3322,
            -3610,
            -3610,
            -805,
            -3610,
            -2512,
            -1504,
            -1444,
            -879,
            -3610,
            -878,
            -2151,
            -1017,
            -331,
            -3610,
            -957,
            -850,
            -1364,
            -474,
            -304,
            -673,
            -3610,
            -266,
            -2681,
            -429,
            -1073,
            -82,
            -337,
            -610,
            -347,
            -205,
            -422,
            -274,
            -776,
            -3610,
            -1382,
            -206,
            -172,
            -3610,
            -369,
            -446,
            -703,
            -235
        ],
        "steps_history": [
            3610,
            664,
            3423,
            3610,
            3610,
            906,
            3610,
            2613,
            1605,
            1545,
            980,
            3610,
            979,
            2252,
            1118,
            432,
            3610,
            1058,
            951,
            1465,
            575,
            405,
            774,
            3610,
            367,
            2782,
            530,
            1174,
            183,
            438,
            711,
            448,
            306,
            523,
            375,
            877,
            3610,
            1483,
            307,
            273,
            3610,
            470,
            547,
            804,
            336
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "566/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.8091747760772705,
        "final_policy_stability": 0.9944598337950139,
        "episodes_to_convergence": 41,
        "policy_stability_history": [
            0.0,
            0.8310249307479224,
            0.9196675900277008,
            0.8559556786703602,
            0.9168975069252078,
            0.814404432132964,
            0.9390581717451524,
            0.8587257617728532,
            0.8614958448753463,
            0.9556786703601108,
            0.9418282548476454,
            0.9141274238227147,
            0.9390581717451524,
            0.817174515235457,
            0.961218836565097,
            0.9473684210526315,
            0.9584487534626038,
            0.9722991689750693,
            0.8642659279778393,
            0.8282548476454293,
            0.8337950138504155,
            0.9833795013850416,
            0.9778393351800554,
            0.9889196675900277,
            0.961218836565097,
            0.9750692520775623,
            0.9030470914127424,
            0.8753462603878116,
            0.9861495844875346,
            0.9667590027700831,
            0.9861495844875346,
            0.9861495844875346,
            0.9944598337950139,
            0.9778393351800554,
            0.9833795013850416,
            0.997229916897507,
            0.9916897506925207,
            0.997229916897507,
            1.0,
            0.9916897506925207,
            0.9058171745152355,
            0.9944598337950139
        ],
        "reward_history": [
            -1271,
            -3610,
            -684,
            -3610,
            -761,
            -3610,
            -672,
            -1738,
            -2360,
            -242,
            -799,
            -1260,
            -639,
            -3610,
            -828,
            -936,
            -1079,
            -208,
            -3610,
            -3610,
            -3610,
            -121,
            -163,
            -159,
            -602,
            -224,
            -1264,
            -3610,
            -268,
            -472,
            -189,
            -691,
            -106,
            -398,
            -916,
            -355,
            -645,
            -373,
            -127,
            -418,
            -2103,
            -305
        ],
        "steps_history": [
            1372,
            3610,
            785,
            3610,
            862,
            3610,
            773,
            1839,
            2461,
            343,
            900,
            1361,
            740,
            3610,
            929,
            1037,
            1180,
            309,
            3610,
            3610,
            3610,
            222,
            264,
            260,
            703,
            325,
            1365,
            3610,
            369,
            573,
            290,
            792,
            207,
            499,
            1017,
            456,
            746,
            474,
            228,
            519,
            2204,
            406
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "567/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.84769344329834,
        "final_policy_stability": 0.9833795013850416,
        "episodes_to_convergence": 45,
        "policy_stability_history": [
            0.0,
            0.814404432132964,
            0.9473684210526315,
            0.8448753462603878,
            0.8448753462603878,
            0.889196675900277,
            0.8808864265927978,
            0.7922437673130194,
            0.8365650969529086,
            0.8947368421052632,
            0.9695290858725761,
            0.8919667590027701,
            0.925207756232687,
            0.9806094182825484,
            0.8725761772853186,
            0.8864265927977839,
            0.9501385041551247,
            0.9501385041551247,
            0.9556786703601108,
            0.8559556786703602,
            0.9695290858725761,
            0.9750692520775623,
            0.961218836565097,
            0.9390581717451524,
            0.9722991689750693,
            0.9861495844875346,
            0.9529085872576177,
            0.8725761772853186,
            0.9944598337950139,
            0.9806094182825484,
            0.9750692520775623,
            0.9833795013850416,
            0.9889196675900277,
            0.9833795013850416,
            0.9667590027700831,
            0.8559556786703602,
            0.9916897506925207,
            0.997229916897507,
            0.9861495844875346,
            0.9750692520775623,
            0.9889196675900277,
            0.9889196675900277,
            1.0,
            1.0,
            1.0,
            0.9833795013850416
        ],
        "reward_history": [
            -3610,
            -3610,
            -746,
            -3610,
            -3610,
            -2085,
            -1647,
            -3610,
            -3610,
            -1524,
            -184,
            -1145,
            -851,
            -223,
            -3610,
            -1768,
            -676,
            -669,
            -1282,
            -3610,
            -335,
            -370,
            -387,
            -1301,
            -556,
            -119,
            -823,
            -3610,
            -37,
            -196,
            -396,
            -364,
            -590,
            -111,
            -715,
            -3610,
            -222,
            -159,
            -532,
            -439,
            -307,
            -155,
            -129,
            -407,
            -147,
            -221
        ],
        "steps_history": [
            3610,
            3610,
            847,
            3610,
            3610,
            2186,
            1748,
            3610,
            3610,
            1625,
            285,
            1246,
            952,
            324,
            3610,
            1869,
            777,
            770,
            1383,
            3610,
            436,
            471,
            488,
            1402,
            657,
            220,
            924,
            3610,
            138,
            297,
            497,
            465,
            691,
            212,
            816,
            3610,
            323,
            260,
            633,
            540,
            408,
            256,
            230,
            508,
            248,
            322
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "568/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 10.02658724784851,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 53,
        "policy_stability_history": [
            0.0,
            0.850415512465374,
            0.8254847645429363,
            0.8448753462603878,
            0.9501385041551247,
            0.8587257617728532,
            0.9085872576177285,
            0.8698060941828255,
            0.9473684210526315,
            0.8282548476454293,
            0.9030470914127424,
            0.9667590027700831,
            0.9168975069252078,
            0.8448753462603878,
            0.8808864265927978,
            0.8559556786703602,
            0.9307479224376731,
            0.9778393351800554,
            0.9418282548476454,
            0.9695290858725761,
            0.9695290858725761,
            0.9584487534626038,
            0.961218836565097,
            0.9722991689750693,
            0.9722991689750693,
            0.8393351800554016,
            0.9806094182825484,
            0.9722991689750693,
            0.9806094182825484,
            0.8725761772853186,
            0.9307479224376731,
            0.9833795013850416,
            0.9806094182825484,
            0.9916897506925207,
            0.9778393351800554,
            0.9944598337950139,
            0.9722991689750693,
            0.9861495844875346,
            0.9944598337950139,
            0.9944598337950139,
            0.8808864265927978,
            0.997229916897507,
            0.9944598337950139,
            0.9944598337950139,
            0.9861495844875346,
            0.889196675900277,
            0.9889196675900277,
            0.9944598337950139,
            0.997229916897507,
            0.9944598337950139,
            0.997229916897507,
            0.8698060941828255,
            1.0,
            0.997229916897507
        ],
        "reward_history": [
            -3038,
            -3610,
            -3610,
            -3610,
            -459,
            -2603,
            -2255,
            -3610,
            -533,
            -3610,
            -2096,
            -177,
            -1028,
            -3610,
            -2058,
            -3610,
            -975,
            -389,
            -1076,
            -445,
            -285,
            -711,
            -435,
            -212,
            -280,
            -3610,
            -183,
            -236,
            -450,
            -3610,
            -1417,
            -439,
            -554,
            -115,
            -649,
            -189,
            -351,
            -56,
            -336,
            -252,
            -3610,
            -105,
            -421,
            -302,
            -622,
            -2559,
            -265,
            -534,
            -390,
            -544,
            -529,
            -3610,
            -116,
            -338
        ],
        "steps_history": [
            3139,
            3610,
            3610,
            3610,
            560,
            2704,
            2356,
            3610,
            634,
            3610,
            2197,
            278,
            1129,
            3610,
            2159,
            3610,
            1076,
            490,
            1177,
            546,
            386,
            812,
            536,
            313,
            381,
            3610,
            284,
            337,
            551,
            3610,
            1518,
            540,
            655,
            216,
            750,
            290,
            452,
            157,
            437,
            353,
            3610,
            206,
            522,
            403,
            723,
            2660,
            366,
            635,
            491,
            645,
            630,
            3610,
            217,
            439
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "569/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.2,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.707890033721924,
        "final_policy_stability": 0.9916897506925207,
        "episodes_to_convergence": 42,
        "policy_stability_history": [
            0.0,
            0.8476454293628809,
            0.8337950138504155,
            0.8864265927977839,
            0.9224376731301939,
            0.9806094182825484,
            0.8642659279778393,
            0.9196675900277008,
            0.8337950138504155,
            0.8781163434903048,
            0.9473684210526315,
            0.8448753462603878,
            0.96398891966759,
            0.9584487534626038,
            0.96398891966759,
            0.8393351800554016,
            0.96398891966759,
            0.9722991689750693,
            0.853185595567867,
            0.9501385041551247,
            0.9778393351800554,
            0.961218836565097,
            0.9362880886426593,
            0.9556786703601108,
            0.8476454293628809,
            0.9833795013850416,
            0.961218836565097,
            0.9750692520775623,
            0.9889196675900277,
            0.9889196675900277,
            0.9833795013850416,
            0.9307479224376731,
            0.9806094182825484,
            0.9806094182825484,
            0.9861495844875346,
            0.9944598337950139,
            0.9944598337950139,
            0.997229916897507,
            0.9944598337950139,
            0.9889196675900277,
            0.9861495844875346,
            0.9916897506925207,
            0.9916897506925207
        ],
        "reward_history": [
            -3610,
            -3506,
            -3298,
            -1813,
            -1491,
            -108,
            -3610,
            -613,
            -3610,
            -2773,
            -382,
            -3610,
            -310,
            -294,
            -239,
            -3610,
            -215,
            -331,
            -3610,
            -819,
            -196,
            -471,
            -732,
            -565,
            -3610,
            -286,
            -710,
            -259,
            -349,
            -230,
            -474,
            -1044,
            -327,
            -273,
            -235,
            -238,
            -125,
            -137,
            -231,
            -342,
            -300,
            -491,
            -190
        ],
        "steps_history": [
            3610,
            3607,
            3399,
            1914,
            1592,
            209,
            3610,
            714,
            3610,
            2874,
            483,
            3610,
            411,
            395,
            340,
            3610,
            316,
            432,
            3610,
            920,
            297,
            572,
            833,
            666,
            3610,
            387,
            811,
            360,
            450,
            331,
            575,
            1145,
            428,
            374,
            336,
            339,
            226,
            238,
            332,
            443,
            401,
            592,
            291
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "570/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.2_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.620975494384766,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8836565096952909,
            0.8753462603878116,
            0.9113573407202216,
            0.9418282548476454,
            0.8642659279778393,
            0.8365650969529086,
            0.8947368421052632,
            0.8670360110803325,
            0.9058171745152355,
            0.853185595567867,
            0.9335180055401662,
            0.9418282548476454,
            0.8781163434903048,
            0.8808864265927978,
            0.8698060941828255,
            0.9833795013850416,
            0.9390581717451524,
            0.9889196675900277,
            0.9806094182825484,
            0.9556786703601108,
            0.9778393351800554,
            0.9418282548476454,
            0.9944598337950139,
            0.9418282548476454,
            0.9806094182825484,
            0.9722991689750693,
            0.9418282548476454,
            1.0,
            0.997229916897507,
            0.997229916897507
        ],
        "reward_history": [
            -3610,
            -1025,
            -2795,
            -1988,
            -1506,
            -3610,
            -3610,
            -3369,
            -3610,
            -1045,
            -3610,
            -1553,
            -1001,
            -3610,
            -3610,
            -2472,
            -173,
            -1419,
            -382,
            -401,
            -828,
            -591,
            -2443,
            -160,
            -1113,
            -615,
            -1813,
            -1769,
            -584,
            -1055,
            -812
        ],
        "steps_history": [
            3610,
            1126,
            2896,
            2089,
            1607,
            3610,
            3610,
            3470,
            3610,
            1146,
            3610,
            1654,
            1102,
            3610,
            3610,
            2573,
            274,
            1520,
            483,
            502,
            929,
            692,
            2544,
            261,
            1214,
            716,
            1914,
            1870,
            685,
            1156,
            913
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "571/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.9585623741149902,
        "final_policy_stability": 0.889196675900277,
        "episodes_to_convergence": 19,
        "policy_stability_history": [
            0.0,
            0.8642659279778393,
            0.8614958448753463,
            0.8836565096952909,
            0.8587257617728532,
            0.9168975069252078,
            0.9168975069252078,
            0.9002770083102493,
            0.9196675900277008,
            0.9584487534626038,
            0.8864265927977839,
            0.9889196675900277,
            0.8670360110803325,
            0.9335180055401662,
            0.8753462603878116,
            0.997229916897507,
            0.9889196675900277,
            0.9916897506925207,
            0.9362880886426593,
            0.889196675900277
        ],
        "reward_history": [
            -3610,
            -3046,
            -3610,
            -1297,
            -3610,
            -1439,
            -1204,
            -1378,
            -2971,
            -332,
            -3610,
            -463,
            -3610,
            -2361,
            -3610,
            -230,
            -1059,
            -562,
            -2130,
            -3610
        ],
        "steps_history": [
            3610,
            3147,
            3610,
            1398,
            3610,
            1540,
            1305,
            1479,
            3072,
            433,
            3610,
            564,
            3610,
            2462,
            3610,
            331,
            1160,
            663,
            2231,
            3610
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "572/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.7143189907073975,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.8060941828254847,
            0.8476454293628809,
            0.8614958448753463,
            0.9196675900277008,
            0.8393351800554016,
            0.8725761772853186,
            0.8476454293628809,
            0.9418282548476454,
            0.889196675900277,
            0.9335180055401662,
            0.8614958448753463,
            0.8725761772853186,
            0.9085872576177285,
            0.8642659279778393,
            0.9667590027700831,
            0.925207756232687,
            0.9667590027700831,
            0.9750692520775623,
            0.8947368421052632,
            0.9445983379501385,
            0.9722991689750693,
            0.9695290858725761,
            0.9695290858725761,
            0.9889196675900277,
            0.9944598337950139,
            0.8725761772853186,
            0.997229916897507,
            0.9861495844875346,
            1.0
        ],
        "reward_history": [
            -158,
            -3610,
            -3610,
            -3610,
            -756,
            -3610,
            -3610,
            -3610,
            -650,
            -1935,
            -1169,
            -3610,
            -3610,
            -3610,
            -3610,
            -785,
            -1236,
            -979,
            -277,
            -2470,
            -2313,
            -1310,
            -1163,
            -1213,
            -766,
            -510,
            -3610,
            -601,
            -1092,
            -185
        ],
        "steps_history": [
            259,
            3610,
            3610,
            3610,
            857,
            3610,
            3610,
            3610,
            751,
            2036,
            1270,
            3610,
            3610,
            3610,
            3610,
            886,
            1337,
            1080,
            378,
            2571,
            2414,
            1411,
            1264,
            1314,
            867,
            611,
            3610,
            702,
            1193,
            286
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "573/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.427270174026489,
        "final_policy_stability": 0.889196675900277,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.9058171745152355,
            0.8559556786703602,
            0.9473684210526315,
            0.9113573407202216,
            0.9750692520775623,
            0.8947368421052632,
            0.8919667590027701,
            0.9473684210526315,
            0.8781163434903048,
            0.8642659279778393,
            0.8725761772853186,
            0.8698060941828255,
            0.8753462603878116,
            0.8448753462603878,
            0.9418282548476454,
            0.8670360110803325,
            0.961218836565097,
            0.9806094182825484,
            0.9667590027700831,
            0.9556786703601108,
            0.9916897506925207,
            0.8614958448753463,
            0.9390581717451524,
            0.9833795013850416,
            0.9916897506925207,
            0.9916897506925207,
            0.9944598337950139,
            0.997229916897507,
            0.997229916897507,
            0.9944598337950139,
            1.0,
            0.889196675900277
        ],
        "reward_history": [
            -3610,
            -763,
            -3333,
            -572,
            -1846,
            -362,
            -3610,
            -2248,
            -721,
            -3610,
            -3196,
            -3610,
            -3610,
            -3610,
            -3610,
            -1255,
            -3610,
            -671,
            -439,
            -951,
            -756,
            -482,
            -3610,
            -1397,
            -732,
            -403,
            -357,
            -446,
            -523,
            -723,
            -293,
            -338,
            -3610
        ],
        "steps_history": [
            3610,
            864,
            3434,
            673,
            1947,
            463,
            3610,
            2349,
            822,
            3610,
            3297,
            3610,
            3610,
            3610,
            3610,
            1356,
            3610,
            772,
            540,
            1052,
            857,
            583,
            3610,
            1498,
            833,
            504,
            458,
            547,
            624,
            824,
            394,
            439,
            3610
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "574/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.556856155395508,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.8005540166204986,
            0.8919667590027701,
            0.9224376731301939,
            0.889196675900277,
            0.7950138504155124,
            0.8421052631578947,
            0.814404432132964,
            0.8614958448753463,
            0.8476454293628809,
            0.9418282548476454,
            0.8919667590027701,
            0.9445983379501385,
            0.8725761772853186,
            0.8781163434903048,
            0.9418282548476454,
            0.9556786703601108,
            0.9806094182825484,
            0.9750692520775623,
            0.9584487534626038,
            0.8808864265927978,
            0.961218836565097,
            0.9944598337950139,
            0.997229916897507,
            0.9944598337950139,
            0.9916897506925207,
            0.997229916897507,
            0.9944598337950139,
            1.0
        ],
        "reward_history": [
            -1327,
            -3610,
            -2213,
            -762,
            -3610,
            -3610,
            -3610,
            -3610,
            -2522,
            -3610,
            -537,
            -2529,
            -1443,
            -3610,
            -3610,
            -900,
            -1275,
            -578,
            -618,
            -939,
            -3610,
            -1402,
            -1076,
            -1047,
            -580,
            -797,
            -835,
            -777,
            -894
        ],
        "steps_history": [
            1428,
            3610,
            2314,
            863,
            3610,
            3610,
            3610,
            3610,
            2623,
            3610,
            638,
            2630,
            1544,
            3610,
            3610,
            1001,
            1376,
            679,
            719,
            1040,
            3610,
            1503,
            1177,
            1148,
            681,
            898,
            936,
            878,
            995
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "575/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.616065740585327,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.814404432132964,
            0.9030470914127424,
            0.8864265927977839,
            0.8587257617728532,
            0.8670360110803325,
            0.8421052631578947,
            0.8836565096952909,
            0.8808864265927978,
            0.8725761772853186,
            0.9722991689750693,
            0.9695290858725761,
            0.9556786703601108,
            0.9390581717451524,
            0.9445983379501385,
            0.9889196675900277,
            0.9944598337950139,
            0.9944598337950139,
            0.8781163434903048,
            0.9889196675900277,
            0.9944598337950139,
            0.997229916897507,
            0.9667590027700831,
            0.9833795013850416,
            0.997229916897507
        ],
        "reward_history": [
            -388,
            -3610,
            -1025,
            -2249,
            -3610,
            -3610,
            -3610,
            -2673,
            -3610,
            -3610,
            -479,
            -396,
            -991,
            -1692,
            -902,
            -299,
            -252,
            -96,
            -2711,
            -234,
            -667,
            -399,
            -1028,
            -733,
            -284
        ],
        "steps_history": [
            489,
            3610,
            1126,
            2350,
            3610,
            3610,
            3610,
            2774,
            3610,
            3610,
            580,
            497,
            1092,
            1793,
            1003,
            400,
            353,
            197,
            2812,
            335,
            768,
            500,
            1129,
            834,
            385
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "576/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.48427152633667,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8282548476454293,
            0.8116343490304709,
            0.8587257617728532,
            0.8642659279778393,
            0.9058171745152355,
            0.9058171745152355,
            0.8337950138504155,
            0.925207756232687,
            0.8781163434903048,
            0.9307479224376731,
            0.9307479224376731,
            0.9445983379501385,
            0.9002770083102493,
            0.8642659279778393,
            0.8864265927977839,
            0.8808864265927978,
            0.9833795013850416,
            0.8642659279778393,
            0.9335180055401662,
            0.9584487534626038,
            0.9861495844875346,
            0.9916897506925207,
            0.9889196675900277,
            1.0,
            0.9889196675900277,
            1.0
        ],
        "reward_history": [
            -3610,
            -2862,
            -3610,
            -1928,
            -3610,
            -1485,
            -1985,
            -3610,
            -672,
            -3610,
            -735,
            -1044,
            -731,
            -1813,
            -3610,
            -3610,
            -3610,
            -442,
            -3610,
            -1878,
            -1171,
            -396,
            -504,
            -521,
            -422,
            -671,
            -451
        ],
        "steps_history": [
            3610,
            2963,
            3610,
            2029,
            3610,
            1586,
            2086,
            3610,
            773,
            3610,
            836,
            1145,
            832,
            1914,
            3610,
            3610,
            3610,
            543,
            3610,
            1979,
            1272,
            497,
            605,
            622,
            523,
            772,
            552
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "577/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.321629524230957,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.9030470914127424,
            0.8975069252077562,
            0.9224376731301939,
            0.9168975069252078,
            0.8587257617728532,
            0.8808864265927978,
            0.853185595567867,
            0.9030470914127424,
            0.925207756232687,
            0.96398891966759,
            0.9168975069252078,
            0.8476454293628809,
            0.96398891966759,
            0.9722991689750693,
            0.9889196675900277,
            0.8421052631578947,
            0.9916897506925207,
            0.9695290858725761,
            0.9916897506925207,
            0.9695290858725761,
            0.9085872576177285,
            0.9722991689750693,
            0.9944598337950139,
            0.997229916897507,
            1.0,
            1.0
        ],
        "reward_history": [
            -3610,
            -1207,
            -3026,
            -976,
            -907,
            -3610,
            -3610,
            -3610,
            -1296,
            -1244,
            -284,
            -1560,
            -3273,
            -567,
            -460,
            -265,
            -3409,
            -253,
            -969,
            -266,
            -707,
            -3320,
            -979,
            -410,
            -314,
            -150,
            -341
        ],
        "steps_history": [
            3610,
            1308,
            3127,
            1077,
            1008,
            3610,
            3610,
            3610,
            1397,
            1345,
            385,
            1661,
            3374,
            668,
            561,
            366,
            3510,
            354,
            1070,
            367,
            808,
            3421,
            1080,
            511,
            415,
            251,
            442
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "578/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.305068492889404,
        "final_policy_stability": 0.8864265927977839,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8337950138504155,
            0.9445983379501385,
            0.850415512465374,
            0.853185595567867,
            0.9529085872576177,
            0.8476454293628809,
            0.9584487534626038,
            0.9556786703601108,
            0.9390581717451524,
            0.9335180055401662,
            0.9002770083102493,
            0.9168975069252078,
            0.850415512465374,
            0.8947368421052632,
            0.9473684210526315,
            0.961218836565097,
            0.8559556786703602,
            0.9806094182825484,
            0.9778393351800554,
            0.9833795013850416,
            0.9113573407202216,
            0.9778393351800554,
            0.9778393351800554,
            0.9916897506925207,
            0.997229916897507,
            0.9944598337950139,
            0.9889196675900277,
            0.997229916897507,
            0.9916897506925207,
            0.9833795013850416,
            0.9889196675900277,
            1.0,
            0.997229916897507,
            0.8864265927977839
        ],
        "reward_history": [
            -3610,
            -3610,
            -671,
            -3610,
            -3610,
            -295,
            -3610,
            -631,
            -747,
            -760,
            -730,
            -3610,
            -1495,
            -3610,
            -3610,
            -735,
            -300,
            -3208,
            -431,
            -236,
            -271,
            -3610,
            -1040,
            -679,
            -336,
            -148,
            -553,
            -421,
            -266,
            -390,
            -242,
            -433,
            -95,
            -530,
            -3610
        ],
        "steps_history": [
            3610,
            3610,
            772,
            3610,
            3610,
            396,
            3610,
            732,
            848,
            861,
            831,
            3610,
            1596,
            3610,
            3610,
            836,
            401,
            3309,
            532,
            337,
            372,
            3610,
            1141,
            780,
            437,
            249,
            654,
            522,
            367,
            491,
            343,
            534,
            196,
            631,
            3610
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "579/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.067000865936279,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8365650969529086,
            0.8670360110803325,
            0.8559556786703602,
            0.9113573407202216,
            0.853185595567867,
            0.9473684210526315,
            0.8725761772853186,
            0.8476454293628809,
            0.9722991689750693,
            0.9279778393351801,
            0.853185595567867,
            0.9584487534626038,
            0.9196675900277008,
            0.9556786703601108,
            0.8919667590027701,
            0.9307479224376731,
            0.961218836565097,
            0.8587257617728532,
            0.9861495844875346,
            0.9806094182825484,
            0.9307479224376731,
            0.9916897506925207,
            0.9889196675900277,
            0.9806094182825484,
            0.9722991689750693,
            0.9833795013850416,
            0.9722991689750693,
            0.9916897506925207,
            0.9944598337950139,
            0.9916897506925207,
            0.9944598337950139,
            0.9806094182825484,
            0.997229916897507,
            0.997229916897507,
            0.889196675900277,
            0.9833795013850416,
            0.997229916897507
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -3610,
            -1034,
            -2276,
            -608,
            -3610,
            -2692,
            -173,
            -1003,
            -3610,
            -426,
            -1164,
            -576,
            -1763,
            -2206,
            -753,
            -3323,
            -251,
            -448,
            -1635,
            -420,
            -400,
            -278,
            -329,
            -670,
            -917,
            -546,
            -231,
            -641,
            -250,
            -656,
            -307,
            -279,
            -3610,
            -479,
            -657
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            3610,
            1135,
            2377,
            709,
            3610,
            2793,
            274,
            1104,
            3610,
            527,
            1265,
            677,
            1864,
            2307,
            854,
            3424,
            352,
            549,
            1736,
            521,
            501,
            379,
            430,
            771,
            1018,
            647,
            332,
            742,
            351,
            757,
            408,
            380,
            3610,
            580,
            758
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "580/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.107853412628174,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8448753462603878,
            0.9335180055401662,
            0.7894736842105263,
            0.8337950138504155,
            0.925207756232687,
            0.8421052631578947,
            0.8337950138504155,
            0.853185595567867,
            0.9224376731301939,
            0.961218836565097,
            0.9113573407202216,
            0.9307479224376731,
            0.9722991689750693,
            0.9750692520775623,
            0.9778393351800554,
            0.8725761772853186,
            0.9279778393351801,
            0.9501385041551247,
            0.8421052631578947,
            0.96398891966759,
            0.96398891966759,
            0.9778393351800554,
            0.9750692520775623,
            0.9778393351800554,
            0.9916897506925207,
            0.9833795013850416,
            0.9861495844875346,
            0.8587257617728532,
            0.9556786703601108,
            0.997229916897507,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -979,
            -3610,
            -3610,
            -977,
            -3610,
            -3610,
            -3610,
            -548,
            -200,
            -1225,
            -992,
            -161,
            -228,
            -430,
            -2003,
            -673,
            -957,
            -3126,
            -653,
            -665,
            -898,
            -335,
            -470,
            -696,
            -300,
            -910,
            -3610,
            -803,
            -310,
            -321
        ],
        "steps_history": [
            3610,
            3610,
            1080,
            3610,
            3610,
            1078,
            3610,
            3610,
            3610,
            649,
            301,
            1326,
            1093,
            262,
            329,
            531,
            2104,
            774,
            1058,
            3227,
            754,
            766,
            999,
            436,
            571,
            797,
            401,
            1011,
            3610,
            904,
            411,
            422
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "581/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.062180995941162,
        "final_policy_stability": 0.8919667590027701,
        "episodes_to_convergence": 39,
        "policy_stability_history": [
            0.0,
            0.8476454293628809,
            0.814404432132964,
            0.9141274238227147,
            0.8393351800554016,
            0.8975069252077562,
            0.850415512465374,
            0.9445983379501385,
            0.8781163434903048,
            0.889196675900277,
            0.9695290858725761,
            0.8614958448753463,
            0.9002770083102493,
            0.8919667590027701,
            0.961218836565097,
            0.9833795013850416,
            0.8642659279778393,
            0.8365650969529086,
            0.9529085872576177,
            0.9556786703601108,
            0.8282548476454293,
            0.9861495844875346,
            0.889196675900277,
            0.9556786703601108,
            0.9944598337950139,
            0.9833795013850416,
            0.9584487534626038,
            0.9833795013850416,
            0.9916897506925207,
            0.9861495844875346,
            0.9584487534626038,
            0.9806094182825484,
            1.0,
            0.96398891966759,
            0.9889196675900277,
            0.9806094182825484,
            1.0,
            1.0,
            0.9806094182825484,
            0.8919667590027701
        ],
        "reward_history": [
            -1271,
            -2133,
            -3610,
            -1082,
            -3610,
            -1592,
            -3610,
            -497,
            -1332,
            -1523,
            -89,
            -3610,
            -1937,
            -1378,
            -637,
            -208,
            -3610,
            -3610,
            -844,
            -477,
            -3610,
            -196,
            -2614,
            -1319,
            -95,
            -243,
            -522,
            -233,
            -120,
            -207,
            -898,
            -580,
            -153,
            -480,
            -355,
            -645,
            -373,
            -127,
            -418,
            -2780
        ],
        "steps_history": [
            1372,
            2234,
            3610,
            1183,
            3610,
            1693,
            3610,
            598,
            1433,
            1624,
            190,
            3610,
            2038,
            1479,
            738,
            309,
            3610,
            3610,
            945,
            578,
            3610,
            297,
            2715,
            1420,
            196,
            344,
            623,
            334,
            221,
            308,
            999,
            681,
            254,
            581,
            456,
            746,
            474,
            228,
            519,
            2881
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "582/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.8596062660217285,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 36,
        "policy_stability_history": [
            0.0,
            0.7839335180055401,
            0.8088642659279779,
            0.96398891966759,
            0.9141274238227147,
            0.8753462603878116,
            0.8587257617728532,
            0.925207756232687,
            0.9141274238227147,
            0.8282548476454293,
            0.8753462603878116,
            0.9224376731301939,
            0.8559556786703602,
            0.9667590027700831,
            0.8698060941828255,
            0.9529085872576177,
            0.9224376731301939,
            0.9722991689750693,
            0.9722991689750693,
            0.9778393351800554,
            0.9806094182825484,
            0.9168975069252078,
            0.9833795013850416,
            0.9750692520775623,
            0.9085872576177285,
            0.8725761772853186,
            0.9889196675900277,
            0.9944598337950139,
            0.8476454293628809,
            0.9806094182825484,
            0.997229916897507,
            1.0,
            0.9833795013850416,
            1.0,
            0.9916897506925207,
            0.9916897506925207,
            1.0
        ],
        "reward_history": [
            -2790,
            -3610,
            -3610,
            -260,
            -1335,
            -3610,
            -2087,
            -746,
            -962,
            -3610,
            -1994,
            -668,
            -1574,
            -242,
            -3610,
            -674,
            -840,
            -233,
            -224,
            -311,
            -220,
            -1380,
            -183,
            -374,
            -2115,
            -3221,
            -204,
            -158,
            -3610,
            -709,
            -160,
            -86,
            -371,
            -352,
            -1027,
            -548,
            -271
        ],
        "steps_history": [
            2891,
            3610,
            3610,
            361,
            1436,
            3610,
            2188,
            847,
            1063,
            3610,
            2095,
            769,
            1675,
            343,
            3610,
            775,
            941,
            334,
            325,
            412,
            321,
            1481,
            284,
            475,
            2216,
            3322,
            305,
            259,
            3610,
            810,
            261,
            187,
            472,
            453,
            1128,
            649,
            372
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "583/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.4890406131744385,
        "final_policy_stability": 0.9750692520775623,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8448753462603878,
            0.9030470914127424,
            0.9722991689750693,
            0.8227146814404432,
            0.8725761772853186,
            0.8559556786703602,
            0.8670360110803325,
            0.9307479224376731,
            0.9473684210526315,
            0.8421052631578947,
            0.9390581717451524,
            0.9418282548476454,
            0.9584487534626038,
            0.96398891966759,
            0.9861495844875346,
            0.9722991689750693,
            0.96398891966759,
            0.9695290858725761,
            0.9806094182825484,
            0.9806094182825484,
            0.9778393351800554,
            0.9916897506925207,
            0.8614958448753463,
            0.9944598337950139,
            0.997229916897507,
            0.8587257617728532,
            1.0,
            0.9833795013850416,
            0.9889196675900277,
            0.9750692520775623
        ],
        "reward_history": [
            -1612,
            -1960,
            -1118,
            -385,
            -3610,
            -3610,
            -3610,
            -3075,
            -779,
            -461,
            -3403,
            -813,
            -658,
            -288,
            -593,
            -136,
            -336,
            -449,
            -483,
            -292,
            -429,
            -240,
            -145,
            -3610,
            -664,
            -193,
            -3610,
            -446,
            -828,
            -674,
            -548
        ],
        "steps_history": [
            1713,
            2061,
            1219,
            486,
            3610,
            3610,
            3610,
            3176,
            880,
            562,
            3504,
            914,
            759,
            389,
            694,
            237,
            437,
            550,
            584,
            393,
            530,
            341,
            246,
            3610,
            765,
            294,
            3610,
            547,
            929,
            775,
            649
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "584/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.974919080734253,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 40,
        "policy_stability_history": [
            0.0,
            0.8864265927977839,
            0.7894736842105263,
            0.9390581717451524,
            0.9418282548476454,
            0.9224376731301939,
            0.8947368421052632,
            0.8476454293628809,
            0.8310249307479224,
            0.9141274238227147,
            0.9141274238227147,
            0.9196675900277008,
            0.8975069252077562,
            0.8227146814404432,
            0.9778393351800554,
            0.96398891966759,
            0.8614958448753463,
            0.8365650969529086,
            0.9556786703601108,
            0.8310249307479224,
            0.9861495844875346,
            0.96398891966759,
            0.9750692520775623,
            0.9778393351800554,
            0.9667590027700831,
            0.9861495844875346,
            0.9722991689750693,
            0.8947368421052632,
            0.9861495844875346,
            0.9722991689750693,
            0.9445983379501385,
            0.961218836565097,
            0.9833795013850416,
            0.997229916897507,
            0.9944598337950139,
            0.997229916897507,
            0.997229916897507,
            0.997229916897507,
            0.9916897506925207,
            1.0,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -3383,
            -363,
            -778,
            -655,
            -1522,
            -3610,
            -3610,
            -1180,
            -1097,
            -734,
            -1393,
            -3610,
            -55,
            -228,
            -1692,
            -3610,
            -661,
            -2625,
            -209,
            -284,
            -226,
            -405,
            -565,
            -585,
            -400,
            -1960,
            -129,
            -483,
            -1005,
            -671,
            -483,
            -283,
            -469,
            -243,
            -105,
            -342,
            -235,
            -238,
            -125
        ],
        "steps_history": [
            3610,
            3610,
            3484,
            464,
            879,
            756,
            1623,
            3610,
            3610,
            1281,
            1198,
            835,
            1494,
            3610,
            156,
            329,
            1793,
            3610,
            762,
            2726,
            310,
            385,
            327,
            506,
            666,
            686,
            501,
            2061,
            230,
            584,
            1106,
            772,
            584,
            384,
            570,
            344,
            206,
            443,
            336,
            339,
            226
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "585/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.237498760223389,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8836565096952909,
            0.8614958448753463,
            0.8393351800554016,
            0.8227146814404432,
            0.9390581717451524,
            0.9196675900277008,
            0.8725761772853186,
            0.8725761772853186,
            0.8670360110803325,
            0.9501385041551247,
            0.9002770083102493,
            0.9667590027700831,
            0.9501385041551247,
            0.9390581717451524,
            0.8698060941828255,
            0.9418282548476454,
            0.8448753462603878,
            0.8836565096952909,
            0.9695290858725761,
            0.9722991689750693,
            0.8975069252077562,
            0.9833795013850416,
            0.9889196675900277,
            0.9944598337950139,
            0.9695290858725761,
            0.9861495844875346,
            0.9861495844875346,
            0.9113573407202216,
            0.997229916897507,
            0.9944598337950139,
            1.0
        ],
        "reward_history": [
            -3610,
            -1025,
            -3610,
            -3610,
            -3342,
            -1174,
            -1060,
            -3610,
            -2728,
            -3610,
            -811,
            -3610,
            -633,
            -1548,
            -1161,
            -3610,
            -1513,
            -3610,
            -3610,
            -1457,
            -928,
            -3610,
            -740,
            -488,
            -332,
            -1000,
            -849,
            -663,
            -2421,
            -744,
            -445,
            -520
        ],
        "steps_history": [
            3610,
            1126,
            3610,
            3610,
            3443,
            1275,
            1161,
            3610,
            2829,
            3610,
            912,
            3610,
            734,
            1649,
            1262,
            3610,
            1614,
            3610,
            3610,
            1558,
            1029,
            3610,
            841,
            589,
            433,
            1101,
            950,
            764,
            2522,
            845,
            546,
            621
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "586/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.19063663482666,
        "final_policy_stability": 0.9695290858725761,
        "episodes_to_convergence": 32,
        "policy_stability_history": [
            0.0,
            0.8642659279778393,
            0.8614958448753463,
            0.8808864265927978,
            0.853185595567867,
            0.9667590027700831,
            0.8753462603878116,
            0.8919667590027701,
            0.925207756232687,
            0.8642659279778393,
            0.9002770083102493,
            0.9418282548476454,
            0.9695290858725761,
            0.9390581717451524,
            0.8808864265927978,
            0.8975069252077562,
            0.9501385041551247,
            0.8919667590027701,
            0.9944598337950139,
            0.997229916897507,
            0.9722991689750693,
            0.9916897506925207,
            0.8725761772853186,
            0.9916897506925207,
            0.9944598337950139,
            0.9944598337950139,
            0.9944598337950139,
            0.997229916897507,
            0.9889196675900277,
            0.9944598337950139,
            0.9889196675900277,
            1.0,
            0.9695290858725761
        ],
        "reward_history": [
            -3610,
            -3046,
            -3610,
            -1297,
            -3610,
            -390,
            -3610,
            -2331,
            -1195,
            -2903,
            -2474,
            -1840,
            -847,
            -1677,
            -3610,
            -3610,
            -847,
            -2797,
            -694,
            -599,
            -1186,
            -661,
            -3610,
            -649,
            -187,
            -445,
            -405,
            -715,
            -624,
            -1041,
            -697,
            -1320,
            -833
        ],
        "steps_history": [
            3610,
            3147,
            3610,
            1398,
            3610,
            491,
            3610,
            2432,
            1296,
            3004,
            2575,
            1941,
            948,
            1778,
            3610,
            3610,
            948,
            2898,
            795,
            700,
            1287,
            762,
            3610,
            750,
            288,
            546,
            506,
            816,
            725,
            1142,
            798,
            1421,
            934
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "587/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.741215705871582,
        "final_policy_stability": 0.9058171745152355,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.8421052631578947,
            0.9030470914127424,
            0.9030470914127424,
            0.889196675900277,
            0.8060941828254847,
            0.8254847645429363,
            0.8975069252077562,
            0.8642659279778393,
            0.8975069252077562,
            0.961218836565097,
            0.9473684210526315,
            0.8781163434903048,
            0.9584487534626038,
            0.850415512465374,
            0.9861495844875346,
            0.9833795013850416,
            0.9667590027700831,
            0.9556786703601108,
            0.9889196675900277,
            0.9889196675900277,
            0.8587257617728532,
            0.9889196675900277,
            1.0,
            0.8781163434903048,
            0.9058171745152355
        ],
        "reward_history": [
            -158,
            -3610,
            -3610,
            -1009,
            -3610,
            -3610,
            -3610,
            -1943,
            -3610,
            -1659,
            -522,
            -1042,
            -2001,
            -1083,
            -3610,
            -195,
            -165,
            -578,
            -1338,
            -705,
            -691,
            -3610,
            -884,
            -303,
            -3610,
            -2069
        ],
        "steps_history": [
            259,
            3610,
            3610,
            1110,
            3610,
            3610,
            3610,
            2044,
            3610,
            1760,
            623,
            1143,
            2102,
            1184,
            3610,
            296,
            266,
            679,
            1439,
            806,
            792,
            3610,
            985,
            404,
            3610,
            2170
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "588/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.430756330490112,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.9058171745152355,
            0.853185595567867,
            0.9279778393351801,
            0.8393351800554016,
            0.8587257617728532,
            0.9030470914127424,
            0.961218836565097,
            0.8919667590027701,
            0.8448753462603878,
            0.9335180055401662,
            0.9390581717451524,
            0.9695290858725761,
            0.8725761772853186,
            0.9279778393351801,
            0.8725761772853186,
            0.9473684210526315,
            0.8947368421052632,
            0.9722991689750693,
            0.9778393351800554,
            0.9667590027700831,
            0.8476454293628809,
            0.9556786703601108,
            0.9916897506925207,
            0.9916897506925207,
            0.9944598337950139,
            1.0,
            0.997229916897507
        ],
        "reward_history": [
            -3610,
            -763,
            -3610,
            -1508,
            -3610,
            -3610,
            -2315,
            -806,
            -3610,
            -3610,
            -1159,
            -843,
            -642,
            -3610,
            -2160,
            -3610,
            -1033,
            -2752,
            -1162,
            -1214,
            -877,
            -3610,
            -1469,
            -547,
            -474,
            -403,
            -357,
            -446
        ],
        "steps_history": [
            3610,
            864,
            3610,
            1609,
            3610,
            3610,
            2416,
            907,
            3610,
            3610,
            1260,
            944,
            743,
            3610,
            2261,
            3610,
            1134,
            2853,
            1263,
            1315,
            978,
            3610,
            1570,
            648,
            575,
            504,
            458,
            547
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "589/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.708967685699463,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.8005540166204986,
            0.889196675900277,
            0.9224376731301939,
            0.889196675900277,
            0.7950138504155124,
            0.8337950138504155,
            0.8005540166204986,
            0.8614958448753463,
            0.8254847645429363,
            0.9473684210526315,
            0.9529085872576177,
            0.9279778393351801,
            0.8587257617728532,
            0.9750692520775623,
            0.9362880886426593,
            0.8836565096952909,
            0.9778393351800554,
            0.9529085872576177,
            0.9861495844875346,
            0.96398891966759,
            0.9667590027700831,
            0.9750692520775623,
            0.9944598337950139,
            1.0,
            1.0
        ],
        "reward_history": [
            -1327,
            -3610,
            -2213,
            -762,
            -3610,
            -3610,
            -3610,
            -3610,
            -2522,
            -3610,
            -602,
            -672,
            -1835,
            -3610,
            -415,
            -2408,
            -2563,
            -1013,
            -1620,
            -160,
            -939,
            -2770,
            -2141,
            -1076,
            -1047,
            -580
        ],
        "steps_history": [
            1428,
            3610,
            2314,
            863,
            3610,
            3610,
            3610,
            3610,
            2623,
            3610,
            703,
            773,
            1936,
            3610,
            516,
            2509,
            2664,
            1114,
            1721,
            261,
            1040,
            2871,
            2242,
            1177,
            1148,
            681
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "590/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.331907272338867,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.814404432132964,
            0.9030470914127424,
            0.8947368421052632,
            0.8587257617728532,
            0.8670360110803325,
            0.8421052631578947,
            0.8781163434903048,
            0.8614958448753463,
            0.9501385041551247,
            0.8642659279778393,
            0.9501385041551247,
            0.8725761772853186,
            0.9556786703601108,
            0.9279778393351801,
            0.8670360110803325,
            0.9556786703601108,
            0.9833795013850416,
            0.9778393351800554,
            0.9861495844875346,
            0.9584487534626038,
            0.8421052631578947,
            0.9861495844875346,
            0.9722991689750693,
            0.853185595567867,
            0.9584487534626038,
            0.997229916897507,
            0.9889196675900277,
            0.9916897506925207,
            0.997229916897507,
            0.997229916897507,
            0.9916897506925207,
            1.0,
            0.997229916897507
        ],
        "reward_history": [
            -388,
            -3610,
            -1025,
            -2249,
            -3610,
            -3610,
            -3610,
            -2517,
            -3610,
            -199,
            -2587,
            -633,
            -3610,
            -295,
            -859,
            -2821,
            -782,
            -305,
            -596,
            -399,
            -1862,
            -3610,
            -399,
            -375,
            -3610,
            -641,
            -205,
            -218,
            -324,
            -262,
            -269,
            -698,
            -553,
            -463
        ],
        "steps_history": [
            489,
            3610,
            1126,
            2350,
            3610,
            3610,
            3610,
            2618,
            3610,
            300,
            2688,
            734,
            3610,
            396,
            960,
            2922,
            883,
            406,
            697,
            500,
            1963,
            3610,
            500,
            476,
            3610,
            742,
            306,
            319,
            425,
            363,
            370,
            799,
            654,
            564
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "591/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.1669793128967285,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8282548476454293,
            0.8088642659279779,
            0.8587257617728532,
            0.8642659279778393,
            0.8947368421052632,
            0.8060941828254847,
            0.8642659279778393,
            0.9584487534626038,
            0.925207756232687,
            0.961218836565097,
            0.853185595567867,
            0.8365650969529086,
            0.8448753462603878,
            0.8864265927977839,
            0.96398891966759,
            0.9889196675900277,
            0.8393351800554016,
            0.9224376731301939,
            0.9750692520775623,
            0.9833795013850416,
            0.997229916897507,
            0.9806094182825484,
            0.997229916897507,
            0.96398891966759,
            0.9916897506925207,
            0.9916897506925207,
            0.9916897506925207,
            1.0,
            0.9889196675900277,
            1.0,
            0.997229916897507
        ],
        "reward_history": [
            -3610,
            -2862,
            -3610,
            -1928,
            -3610,
            -1425,
            -3610,
            -3610,
            -874,
            -709,
            -583,
            -3610,
            -3610,
            -1833,
            -2408,
            -953,
            -214,
            -3610,
            -1843,
            -223,
            -1307,
            -67,
            -1230,
            -221,
            -921,
            -646,
            -504,
            -521,
            -422,
            -671,
            -451,
            -831
        ],
        "steps_history": [
            3610,
            2963,
            3610,
            2029,
            3610,
            1526,
            3610,
            3610,
            975,
            810,
            684,
            3610,
            3610,
            1934,
            2509,
            1054,
            315,
            3610,
            1944,
            324,
            1408,
            168,
            1331,
            322,
            1022,
            747,
            605,
            622,
            523,
            772,
            552,
            932
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "592/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.247816324234009,
        "final_policy_stability": 0.9833795013850416,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.9030470914127424,
            0.9002770083102493,
            0.9362880886426593,
            0.9168975069252078,
            0.8975069252077562,
            0.817174515235457,
            0.8864265927977839,
            0.9445983379501385,
            0.8698060941828255,
            0.9168975069252078,
            0.9695290858725761,
            0.853185595567867,
            0.9501385041551247,
            0.9833795013850416,
            0.9445983379501385,
            0.9778393351800554,
            0.9750692520775623,
            0.9944598337950139,
            0.9335180055401662,
            0.9750692520775623,
            0.9861495844875346,
            0.9861495844875346,
            0.9778393351800554,
            0.9833795013850416,
            0.961218836565097,
            0.9833795013850416
        ],
        "reward_history": [
            -3610,
            -1207,
            -3026,
            -976,
            -907,
            -2088,
            -3610,
            -2440,
            -567,
            -3610,
            -2475,
            -330,
            -3610,
            -515,
            -281,
            -619,
            -448,
            -308,
            -129,
            -1180,
            -389,
            -84,
            -253,
            -969,
            -266,
            -814,
            -432
        ],
        "steps_history": [
            3610,
            1308,
            3127,
            1077,
            1008,
            2189,
            3610,
            2541,
            668,
            3610,
            2576,
            431,
            3610,
            616,
            382,
            720,
            549,
            409,
            230,
            1281,
            490,
            185,
            354,
            1070,
            367,
            915,
            533
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "593/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.267392158508301,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.9030470914127424,
            0.7977839335180056,
            0.9556786703601108,
            0.853185595567867,
            0.9002770083102493,
            0.8337950138504155,
            0.9224376731301939,
            0.9279778393351801,
            0.9307479224376731,
            0.9279778393351801,
            0.8559556786703602,
            0.9362880886426593,
            0.8559556786703602,
            0.8642659279778393,
            0.9889196675900277,
            0.9390581717451524,
            0.9722991689750693,
            0.9307479224376731,
            0.9695290858725761,
            0.9889196675900277,
            0.9529085872576177,
            0.9362880886426593,
            0.9861495844875346,
            0.9667590027700831,
            0.9916897506925207,
            0.9722991689750693,
            0.9944598337950139,
            1.0,
            0.9916897506925207,
            0.9861495844875346,
            0.997229916897507
        ],
        "reward_history": [
            -3610,
            -1779,
            -3610,
            -225,
            -3610,
            -1252,
            -3610,
            -1715,
            -1743,
            -2104,
            -1024,
            -3610,
            -1052,
            -3610,
            -3610,
            -231,
            -1369,
            -438,
            -1428,
            -453,
            -104,
            -590,
            -1028,
            -632,
            -799,
            -299,
            -679,
            -336,
            -148,
            -553,
            -421,
            -266
        ],
        "steps_history": [
            3610,
            1880,
            3610,
            326,
            3610,
            1353,
            3610,
            1816,
            1844,
            2205,
            1125,
            3610,
            1153,
            3610,
            3610,
            332,
            1470,
            539,
            1529,
            554,
            205,
            691,
            1129,
            733,
            900,
            400,
            780,
            437,
            249,
            654,
            522,
            367
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "594/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.181844472885132,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.8365650969529086,
            0.8670360110803325,
            0.8559556786703602,
            0.9113573407202216,
            0.853185595567867,
            0.9473684210526315,
            0.8725761772853186,
            0.8476454293628809,
            0.9722991689750693,
            0.9279778393351801,
            0.8587257617728532,
            0.9584487534626038,
            0.9196675900277008,
            0.9529085872576177,
            0.8808864265927978,
            0.9390581717451524,
            0.9307479224376731,
            0.9529085872576177,
            0.9750692520775623,
            0.9362880886426593,
            0.8753462603878116,
            0.9806094182825484,
            0.997229916897507,
            0.9889196675900277,
            0.997229916897507,
            0.9833795013850416,
            1.0,
            0.9944598337950139,
            0.9806094182825484,
            1.0,
            1.0,
            0.8670360110803325,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -3610,
            -1034,
            -2276,
            -608,
            -3610,
            -2692,
            -173,
            -1003,
            -3610,
            -426,
            -1164,
            -576,
            -1763,
            -921,
            -1184,
            -1336,
            -1225,
            -2315,
            -3610,
            -627,
            -151,
            -665,
            -546,
            -231,
            -641,
            -224,
            -682,
            -307,
            -279,
            -3610,
            -434
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            3610,
            1135,
            2377,
            709,
            3610,
            2793,
            274,
            1104,
            3610,
            527,
            1265,
            677,
            1864,
            1022,
            1285,
            1437,
            1326,
            2416,
            3610,
            728,
            252,
            766,
            647,
            332,
            742,
            325,
            783,
            408,
            380,
            3610,
            535
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "595/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.333666563034058,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8559556786703602,
            0.9362880886426593,
            0.8254847645429363,
            0.8808864265927978,
            0.8088642659279779,
            0.8753462603878116,
            0.925207756232687,
            0.925207756232687,
            0.8448753462603878,
            0.9445983379501385,
            0.9335180055401662,
            0.9418282548476454,
            0.8587257617728532,
            0.9307479224376731,
            0.8753462603878116,
            0.9445983379501385,
            0.9778393351800554,
            0.9501385041551247,
            0.96398891966759,
            0.9584487534626038,
            0.8587257617728532,
            0.9833795013850416,
            0.8282548476454293,
            0.9806094182825484,
            0.9861495844875346,
            0.9889196675900277,
            0.9806094182825484,
            0.9916897506925207,
            0.9916897506925207,
            0.9833795013850416,
            0.9833795013850416,
            0.9833795013850416,
            0.9889196675900277,
            0.9944598337950139,
            0.9944598337950139,
            1.0,
            0.997229916897507
        ],
        "reward_history": [
            -3610,
            -3610,
            -881,
            -3610,
            -1710,
            -3610,
            -1210,
            -694,
            -784,
            -2858,
            -367,
            -1395,
            -859,
            -3610,
            -947,
            -3276,
            -554,
            -254,
            -602,
            -553,
            -789,
            -3610,
            -430,
            -3408,
            -486,
            -487,
            -17,
            -460,
            -112,
            -77,
            -543,
            -380,
            -316,
            -394,
            -310,
            -321,
            -392,
            -284
        ],
        "steps_history": [
            3610,
            3610,
            982,
            3610,
            1811,
            3610,
            1311,
            795,
            885,
            2959,
            468,
            1496,
            960,
            3610,
            1048,
            3377,
            655,
            355,
            703,
            654,
            890,
            3610,
            531,
            3509,
            587,
            588,
            118,
            561,
            213,
            178,
            644,
            481,
            417,
            495,
            411,
            422,
            493,
            385
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "596/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.152341365814209,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 31,
        "policy_stability_history": [
            0.0,
            0.8476454293628809,
            0.814404432132964,
            0.9168975069252078,
            0.8587257617728532,
            0.8836565096952909,
            0.9473684210526315,
            0.9307479224376731,
            0.8919667590027701,
            0.8559556786703602,
            0.8227146814404432,
            0.9030470914127424,
            0.8781163434903048,
            0.961218836565097,
            0.9750692520775623,
            0.8587257617728532,
            0.9556786703601108,
            0.9722991689750693,
            0.9806094182825484,
            0.961218836565097,
            0.8698060941828255,
            0.9778393351800554,
            0.9861495844875346,
            0.9833795013850416,
            0.9944598337950139,
            0.9722991689750693,
            0.997229916897507,
            0.9335180055401662,
            1.0,
            0.9944598337950139,
            1.0,
            0.997229916897507
        ],
        "reward_history": [
            -1271,
            -2133,
            -3610,
            -1082,
            -3610,
            -1592,
            -786,
            -604,
            -1066,
            -3610,
            -3610,
            -1289,
            -2670,
            -815,
            -317,
            -3610,
            -1024,
            -421,
            -215,
            -992,
            -3610,
            -526,
            -203,
            -159,
            -163,
            -632,
            -194,
            -1264,
            -180,
            -121,
            -129,
            -613
        ],
        "steps_history": [
            1372,
            2234,
            3610,
            1183,
            3610,
            1693,
            887,
            705,
            1167,
            3610,
            3610,
            1390,
            2771,
            916,
            418,
            3610,
            1125,
            522,
            316,
            1093,
            3610,
            627,
            304,
            260,
            264,
            733,
            295,
            1365,
            281,
            222,
            230,
            714
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "597/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.3709397315979,
        "final_policy_stability": 0.9916897506925207,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.7728531855955678,
            0.8642659279778393,
            0.9418282548476454,
            0.8448753462603878,
            0.8476454293628809,
            0.9085872576177285,
            0.9196675900277008,
            0.9445983379501385,
            0.9168975069252078,
            0.7950138504155124,
            0.9445983379501385,
            0.8448753462603878,
            0.9722991689750693,
            0.9279778393351801,
            0.8781163434903048,
            0.9750692520775623,
            0.9445983379501385,
            0.8476454293628809,
            0.9806094182825484,
            0.8476454293628809,
            0.9722991689750693,
            0.9750692520775623,
            0.9861495844875346,
            1.0,
            0.9750692520775623,
            1.0,
            0.9750692520775623,
            0.997229916897507,
            0.9916897506925207
        ],
        "reward_history": [
            -2790,
            -3610,
            -2298,
            -513,
            -3610,
            -3610,
            -1548,
            -721,
            -325,
            -1240,
            -3610,
            -1042,
            -3610,
            -478,
            -842,
            -2194,
            -481,
            -865,
            -2529,
            -276,
            -3610,
            -381,
            -415,
            -323,
            -250,
            -608,
            -105,
            -754,
            -78,
            -441
        ],
        "steps_history": [
            2891,
            3610,
            2399,
            614,
            3610,
            3610,
            1649,
            822,
            426,
            1341,
            3610,
            1143,
            3610,
            579,
            943,
            2295,
            582,
            966,
            2630,
            377,
            3610,
            482,
            516,
            424,
            351,
            709,
            206,
            855,
            179,
            542
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "598/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 7.102733850479126,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 37,
        "policy_stability_history": [
            0.0,
            0.8448753462603878,
            0.9030470914127424,
            0.9722991689750693,
            0.8254847645429363,
            0.850415512465374,
            0.9307479224376731,
            0.8310249307479224,
            0.8476454293628809,
            0.9501385041551247,
            0.9168975069252078,
            0.9418282548476454,
            0.9307479224376731,
            0.9501385041551247,
            0.9501385041551247,
            0.961218836565097,
            0.8808864265927978,
            0.9806094182825484,
            0.853185595567867,
            0.96398891966759,
            0.9944598337950139,
            0.961218836565097,
            0.9861495844875346,
            0.925207756232687,
            0.9861495844875346,
            0.9806094182825484,
            0.9861495844875346,
            0.9833795013850416,
            0.9806094182825484,
            0.9861495844875346,
            0.9362880886426593,
            0.997229916897507,
            0.9833795013850416,
            0.997229916897507,
            1.0,
            0.9722991689750693,
            0.997229916897507,
            1.0
        ],
        "reward_history": [
            -1612,
            -1960,
            -1118,
            -385,
            -3610,
            -3610,
            -1491,
            -3610,
            -3610,
            -621,
            -1587,
            -435,
            -862,
            -422,
            -261,
            -329,
            -3610,
            -59,
            -3610,
            -893,
            -154,
            -637,
            -222,
            -1457,
            -334,
            -247,
            -195,
            -284,
            -712,
            -674,
            -548,
            -240,
            -1589,
            -285,
            -711,
            -467,
            -147,
            -399
        ],
        "steps_history": [
            1713,
            2061,
            1219,
            486,
            3610,
            3610,
            1592,
            3610,
            3610,
            722,
            1688,
            536,
            963,
            523,
            362,
            430,
            3610,
            160,
            3610,
            994,
            255,
            738,
            323,
            1558,
            435,
            348,
            296,
            385,
            813,
            775,
            649,
            341,
            1690,
            386,
            812,
            568,
            248,
            500
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "599/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.3,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.1896421909332275,
        "final_policy_stability": 0.8836565096952909,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8614958448753463,
            0.7894736842105263,
            0.9390581717451524,
            0.9418282548476454,
            0.9224376731301939,
            0.8947368421052632,
            0.8476454293628809,
            0.850415512465374,
            0.9501385041551247,
            0.8947368421052632,
            0.9335180055401662,
            0.9445983379501385,
            0.8864265927977839,
            0.9058171745152355,
            0.9556786703601108,
            0.9695290858725761,
            0.9501385041551247,
            0.8559556786703602,
            0.9778393351800554,
            0.9529085872576177,
            0.9529085872576177,
            0.9695290858725761,
            0.9806094182825484,
            0.9916897506925207,
            0.9722991689750693,
            0.9750692520775623,
            0.9695290858725761,
            0.9944598337950139,
            0.8642659279778393,
            0.9916897506925207,
            1.0,
            0.9141274238227147,
            0.9944598337950139,
            0.8836565096952909
        ],
        "reward_history": [
            -3610,
            -3610,
            -3383,
            -363,
            -778,
            -655,
            -1522,
            -3610,
            -3610,
            -457,
            -1153,
            -606,
            -873,
            -1718,
            -1130,
            -477,
            -730,
            -575,
            -3610,
            -319,
            -331,
            -794,
            -154,
            -213,
            -139,
            -763,
            -984,
            -950,
            -167,
            -3610,
            -220,
            -279,
            -2411,
            -601,
            -2397
        ],
        "steps_history": [
            3610,
            3610,
            3484,
            464,
            879,
            756,
            1623,
            3610,
            3610,
            558,
            1254,
            707,
            974,
            1819,
            1231,
            578,
            831,
            676,
            3610,
            420,
            432,
            895,
            255,
            314,
            240,
            864,
            1085,
            1051,
            268,
            3610,
            321,
            380,
            2512,
            702,
            2498
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "600/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.3_df0.99_eps0.1_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.408058404922485,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.8642659279778393,
            0.9113573407202216,
            0.8421052631578947,
            0.8421052631578947,
            0.850415512465374,
            0.8310249307479224,
            0.9307479224376731,
            0.9529085872576177,
            0.9584487534626038,
            0.9279778393351801,
            0.9556786703601108,
            0.8254847645429363,
            0.889196675900277,
            0.9196675900277008,
            0.9418282548476454,
            0.9224376731301939,
            0.9916897506925207,
            0.9667590027700831,
            0.9667590027700831,
            0.9861495844875346,
            0.9944598337950139,
            1.0,
            0.9861495844875346,
            1.0,
            1.0,
            1.0,
            1.0
        ],
        "reward_history": [
            -3610,
            -1946,
            -1874,
            -3610,
            -3610,
            -3419,
            -3610,
            -849,
            -624,
            -516,
            -1342,
            -1205,
            -3610,
            -3610,
            -1290,
            -1091,
            -1526,
            -140,
            -1132,
            -1247,
            -674,
            -782,
            -1518,
            -830,
            -227,
            -729,
            -591,
            -322
        ],
        "steps_history": [
            3610,
            2047,
            1975,
            3610,
            3610,
            3520,
            3610,
            950,
            725,
            617,
            1443,
            1306,
            3610,
            3610,
            1391,
            1192,
            1627,
            241,
            1233,
            1348,
            775,
            883,
            1619,
            931,
            328,
            830,
            692,
            423
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "601/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.06_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 3.867781162261963,
        "final_policy_stability": 0.9833795013850416,
        "episodes_to_convergence": 17,
        "policy_stability_history": [
            0.0,
            0.8199445983379502,
            0.8559556786703602,
            0.9335180055401662,
            0.9141274238227147,
            0.889196675900277,
            0.9584487534626038,
            0.9113573407202216,
            0.8116343490304709,
            0.8421052631578947,
            0.8725761772853186,
            0.9473684210526315,
            0.8614958448753463,
            0.8670360110803325,
            0.9944598337950139,
            1.0,
            0.9085872576177285,
            0.9833795013850416
        ],
        "reward_history": [
            -3610,
            -3446,
            -3610,
            -596,
            -1147,
            -2342,
            -622,
            -985,
            -3109,
            -3610,
            -3493,
            -1552,
            -3610,
            -3610,
            -726,
            -212,
            -3610,
            -516
        ],
        "steps_history": [
            3610,
            3547,
            3610,
            697,
            1248,
            2443,
            723,
            1086,
            3210,
            3610,
            3594,
            1653,
            3610,
            3610,
            827,
            313,
            3610,
            617
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "602/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.06_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.632555723190308,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.8337950138504155,
            0.7977839335180056,
            0.814404432132964,
            0.8199445983379502,
            0.8808864265927978,
            0.9141274238227147,
            0.8781163434903048,
            0.8393351800554016,
            0.8947368421052632,
            0.9695290858725761,
            0.8975069252077562,
            0.9556786703601108,
            0.96398891966759,
            0.9529085872576177,
            0.850415512465374,
            0.850415512465374,
            0.9916897506925207,
            0.9916897506925207,
            1.0,
            0.9944598337950139,
            0.9916897506925207,
            1.0,
            0.9390581717451524,
            1.0,
            1.0
        ],
        "reward_history": [
            -158,
            -1737,
            -3610,
            -3610,
            -3610,
            -3610,
            -904,
            -2914,
            -3610,
            -3610,
            -721,
            -2107,
            -736,
            -699,
            -1182,
            -3610,
            -3610,
            -854,
            -337,
            -309,
            -303,
            -1113,
            -303,
            -1330,
            -68,
            -536
        ],
        "steps_history": [
            259,
            1838,
            3610,
            3610,
            3610,
            3610,
            1005,
            3015,
            3610,
            3610,
            822,
            2208,
            837,
            800,
            1283,
            3610,
            3610,
            955,
            438,
            410,
            404,
            1214,
            404,
            1431,
            169,
            637
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "603/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.06_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.74048376083374,
        "final_policy_stability": 0.9916897506925207,
        "episodes_to_convergence": 23,
        "policy_stability_history": [
            0.0,
            0.8670360110803325,
            0.8227146814404432,
            0.9529085872576177,
            0.8227146814404432,
            0.8698060941828255,
            0.8614958448753463,
            0.8587257617728532,
            0.9418282548476454,
            0.9390581717451524,
            0.8864265927977839,
            0.8808864265927978,
            0.9335180055401662,
            0.9722991689750693,
            0.9695290858725761,
            0.9778393351800554,
            0.8947368421052632,
            0.9889196675900277,
            0.9944598337950139,
            0.8975069252077562,
            0.9667590027700831,
            0.9833795013850416,
            0.997229916897507,
            0.9916897506925207
        ],
        "reward_history": [
            -3610,
            -1741,
            -3610,
            -530,
            -3610,
            -3610,
            -3610,
            -3056,
            -879,
            -412,
            -3610,
            -3610,
            -949,
            -534,
            -1236,
            -378,
            -3610,
            -289,
            -542,
            -3610,
            -886,
            -546,
            -567,
            -877
        ],
        "steps_history": [
            3610,
            1842,
            3610,
            631,
            3610,
            3610,
            3610,
            3157,
            980,
            513,
            3610,
            3610,
            1050,
            635,
            1337,
            479,
            3610,
            390,
            643,
            3610,
            987,
            647,
            668,
            978
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "604/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.06_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.6877360343933105,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.7839335180055401,
            0.8421052631578947,
            0.9501385041551247,
            0.8448753462603878,
            0.9279778393351801,
            0.8559556786703602,
            0.9667590027700831,
            0.96398891966759,
            0.96398891966759,
            0.9722991689750693,
            0.9418282548476454,
            0.925207756232687,
            0.9750692520775623,
            0.9390581717451524,
            0.9806094182825484,
            0.8725761772853186,
            0.9695290858725761,
            0.9889196675900277,
            0.9806094182825484,
            0.9833795013850416,
            0.997229916897507,
            1.0
        ],
        "reward_history": [
            -1352,
            -3610,
            -3610,
            -522,
            -3610,
            -1133,
            -3610,
            -264,
            -405,
            -364,
            -426,
            -1049,
            -902,
            -279,
            -1662,
            -446,
            -3610,
            -807,
            -451,
            -737,
            -1211,
            -523,
            -1299
        ],
        "steps_history": [
            1453,
            3610,
            3610,
            623,
            3610,
            1234,
            3610,
            365,
            506,
            465,
            527,
            1150,
            1003,
            380,
            1763,
            547,
            3610,
            908,
            552,
            838,
            1312,
            624,
            1400
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "605/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.06_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.766469240188599,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8642659279778393,
            0.817174515235457,
            0.8116343490304709,
            0.9002770083102493,
            0.8559556786703602,
            0.8781163434903048,
            0.8559556786703602,
            0.9141274238227147,
            0.9473684210526315,
            0.9168975069252078,
            0.96398891966759,
            0.9335180055401662,
            0.96398891966759,
            0.9584487534626038,
            0.9307479224376731,
            0.8614958448753463,
            0.9113573407202216,
            0.9806094182825484,
            0.9584487534626038,
            0.9833795013850416,
            1.0,
            0.8753462603878116,
            0.9861495844875346,
            1.0
        ],
        "reward_history": [
            -388,
            -2161,
            -2904,
            -3610,
            -3610,
            -3610,
            -3610,
            -3610,
            -1057,
            -738,
            -1307,
            -526,
            -1040,
            -769,
            -618,
            -1692,
            -3610,
            -2157,
            -399,
            -1862,
            -284,
            -139,
            -3171,
            -588,
            -255
        ],
        "steps_history": [
            489,
            2262,
            3005,
            3610,
            3610,
            3610,
            3610,
            3610,
            1158,
            839,
            1408,
            627,
            1141,
            870,
            719,
            1793,
            3610,
            2258,
            500,
            1963,
            385,
            240,
            3272,
            689,
            356
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "606/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.08_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.378115892410278,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.8448753462603878,
            0.8670360110803325,
            0.8919667590027701,
            0.8227146814404432,
            0.8698060941828255,
            0.9168975069252078,
            0.9168975069252078,
            0.817174515235457,
            0.8254847645429363,
            0.9418282548476454,
            0.9196675900277008,
            0.9473684210526315,
            0.8698060941828255,
            0.9445983379501385,
            0.9667590027700831,
            0.9584487534626038,
            0.9418282548476454,
            0.9722991689750693,
            0.9916897506925207,
            0.961218836565097,
            0.997229916897507,
            0.9695290858725761,
            0.9889196675900277,
            1.0,
            1.0,
            0.997229916897507,
            0.997229916897507
        ],
        "reward_history": [
            -3610,
            -3104,
            -1638,
            -2303,
            -3610,
            -3610,
            -1007,
            -741,
            -3163,
            -3610,
            -621,
            -1144,
            -1486,
            -3610,
            -592,
            -658,
            -520,
            -2360,
            -953,
            -331,
            -699,
            -244,
            -1098,
            -270,
            -348,
            -309,
            -627,
            -261
        ],
        "steps_history": [
            3610,
            3205,
            1739,
            2404,
            3610,
            3610,
            1108,
            842,
            3264,
            3610,
            722,
            1245,
            1587,
            3610,
            693,
            759,
            621,
            2461,
            1054,
            432,
            800,
            345,
            1199,
            371,
            449,
            410,
            728,
            362
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "607/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.08_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.635840654373169,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8421052631578947,
            0.817174515235457,
            0.8337950138504155,
            0.8670360110803325,
            0.8365650969529086,
            0.8476454293628809,
            0.9058171745152355,
            0.925207756232687,
            0.8614958448753463,
            0.9529085872576177,
            0.9473684210526315,
            0.9695290858725761,
            0.9695290858725761,
            0.9889196675900277,
            0.9806094182825484,
            0.9473684210526315,
            0.8947368421052632,
            0.889196675900277,
            0.9418282548476454,
            0.9722991689750693,
            0.9889196675900277,
            0.961218836565097,
            0.9778393351800554,
            0.9944598337950139,
            0.9916897506925207,
            0.961218836565097,
            0.9944598337950139,
            1.0,
            0.997229916897507,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -1636,
            -2947,
            -3610,
            -3327,
            -1582,
            -887,
            -2153,
            -783,
            -559,
            -577,
            -450,
            -265,
            -380,
            -825,
            -3610,
            -3610,
            -1181,
            -805,
            -205,
            -506,
            -400,
            -62,
            -347,
            -1060,
            -288,
            -172,
            -107,
            -288
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            1737,
            3048,
            3610,
            3428,
            1683,
            988,
            2254,
            884,
            660,
            678,
            551,
            366,
            481,
            926,
            3610,
            3610,
            1282,
            906,
            306,
            607,
            501,
            163,
            448,
            1161,
            389,
            273,
            208,
            389
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "608/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.08_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.658195495605469,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.8393351800554016,
            0.9362880886426593,
            0.8199445983379502,
            0.9224376731301939,
            0.9030470914127424,
            0.8448753462603878,
            0.9529085872576177,
            0.814404432132964,
            0.8282548476454293,
            0.8947368421052632,
            0.9224376731301939,
            0.9750692520775623,
            0.9362880886426593,
            0.961218836565097,
            0.9750692520775623,
            0.8864265927977839,
            0.9889196675900277,
            0.9335180055401662,
            0.8808864265927978,
            0.9889196675900277,
            0.961218836565097,
            1.0
        ],
        "reward_history": [
            -1485,
            -3610,
            -367,
            -3610,
            -1538,
            -2053,
            -3610,
            -1317,
            -3610,
            -3610,
            -2867,
            -1368,
            -328,
            -801,
            -803,
            -492,
            -3610,
            -100,
            -1711,
            -3610,
            -490,
            -2949,
            -679
        ],
        "steps_history": [
            1586,
            3610,
            468,
            3610,
            1639,
            2154,
            3610,
            1418,
            3610,
            3610,
            2968,
            1469,
            429,
            902,
            904,
            593,
            3610,
            201,
            1812,
            3610,
            591,
            3050,
            780
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "609/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.08_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.3319618701934814,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8836565096952909,
            0.8698060941828255,
            0.8476454293628809,
            0.9362880886426593,
            0.8725761772853186,
            0.9473684210526315,
            0.8781163434903048,
            0.9196675900277008,
            0.9196675900277008,
            0.8088642659279779,
            0.9473684210526315,
            0.9556786703601108,
            0.9002770083102493,
            0.961218836565097,
            0.8698060941828255,
            0.9584487534626038,
            0.9750692520775623,
            0.8587257617728532,
            0.9889196675900277,
            0.9861495844875346,
            0.9307479224376731,
            0.9833795013850416,
            0.9944598337950139,
            0.9944598337950139,
            0.997229916897507,
            0.9833795013850416,
            0.9944598337950139,
            0.997229916897507,
            0.997229916897507,
            1.0
        ],
        "reward_history": [
            -3158,
            -1000,
            -3266,
            -3610,
            -438,
            -3610,
            -494,
            -3610,
            -1109,
            -815,
            -3610,
            -741,
            -857,
            -2498,
            -539,
            -3610,
            -647,
            -482,
            -3610,
            -337,
            -1414,
            -1634,
            -758,
            -463,
            -400,
            -123,
            -378,
            -478,
            -275,
            -839,
            -546
        ],
        "steps_history": [
            3259,
            1101,
            3367,
            3610,
            539,
            3610,
            595,
            3610,
            1210,
            916,
            3610,
            842,
            958,
            2599,
            640,
            3610,
            748,
            583,
            3610,
            438,
            1515,
            1735,
            859,
            564,
            501,
            224,
            479,
            579,
            376,
            940,
            647
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "610/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.08_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.254527568817139,
        "final_policy_stability": 0.8725761772853186,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.817174515235457,
            0.925207756232687,
            0.8642659279778393,
            0.8476454293628809,
            0.9473684210526315,
            0.889196675900277,
            0.8836565096952909,
            0.9030470914127424,
            0.9002770083102493,
            0.8282548476454293,
            0.9418282548476454,
            0.9667590027700831,
            0.961218836565097,
            0.96398891966759,
            0.9695290858725761,
            0.8587257617728532,
            0.9861495844875346,
            0.96398891966759,
            0.9750692520775623,
            0.9889196675900277,
            0.9889196675900277,
            0.9141274238227147,
            1.0,
            0.997229916897507,
            0.9916897506925207,
            0.9695290858725761,
            0.997229916897507,
            0.9916897506925207,
            0.997229916897507,
            0.8725761772853186
        ],
        "reward_history": [
            -2683,
            -2141,
            -795,
            -1674,
            -3610,
            -382,
            -2517,
            -1104,
            -1076,
            -1493,
            -3610,
            -731,
            -765,
            -383,
            -469,
            -268,
            -3610,
            -298,
            -598,
            -841,
            -428,
            -461,
            -2206,
            -105,
            -97,
            -138,
            -418,
            -429,
            -553,
            -218,
            -3610
        ],
        "steps_history": [
            2784,
            2242,
            896,
            1775,
            3610,
            483,
            2618,
            1205,
            1177,
            1594,
            3610,
            832,
            866,
            484,
            570,
            369,
            3610,
            399,
            699,
            942,
            529,
            562,
            2307,
            206,
            198,
            239,
            519,
            530,
            654,
            319,
            3610
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "611/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.1_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.297389507293701,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.7950138504155124,
            0.8254847645429363,
            0.8559556786703602,
            0.8781163434903048,
            0.9168975069252078,
            0.8836565096952909,
            0.9584487534626038,
            0.925207756232687,
            0.889196675900277,
            0.8781163434903048,
            0.8421052631578947,
            0.9307479224376731,
            0.9473684210526315,
            0.9806094182825484,
            0.9279778393351801,
            0.9944598337950139,
            0.997229916897507,
            0.9722991689750693,
            0.9750692520775623,
            0.9833795013850416,
            0.9750692520775623,
            0.9944598337950139,
            0.9916897506925207,
            0.9085872576177285,
            0.9944598337950139,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -1719,
            -2799,
            -1645,
            -773,
            -1514,
            -584,
            -594,
            -1311,
            -3610,
            -3610,
            -1554,
            -815,
            -214,
            -1109,
            -357,
            -533,
            -615,
            -876,
            -263,
            -468,
            -331,
            -225,
            -1559,
            -976,
            -416
        ],
        "steps_history": [
            3610,
            3610,
            1820,
            2900,
            1746,
            874,
            1615,
            685,
            695,
            1412,
            3610,
            3610,
            1655,
            916,
            315,
            1210,
            458,
            634,
            716,
            977,
            364,
            569,
            432,
            326,
            1660,
            1077,
            517
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "612/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.1_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.639805555343628,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.8005540166204986,
            0.9307479224376731,
            0.8310249307479224,
            0.9529085872576177,
            0.8753462603878116,
            0.8614958448753463,
            0.8725761772853186,
            0.9695290858725761,
            0.889196675900277,
            0.9390581717451524,
            0.9279778393351801,
            0.9556786703601108,
            0.9889196675900277,
            0.8559556786703602,
            0.961218836565097,
            0.9833795013850416,
            0.9750692520775623,
            0.9695290858725761,
            0.9889196675900277,
            0.8642659279778393,
            0.9861495844875346,
            0.9833795013850416,
            0.9944598337950139,
            0.9141274238227147,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -441,
            -3610,
            -401,
            -3610,
            -2968,
            -3469,
            -199,
            -1707,
            -989,
            -1317,
            -851,
            -124,
            -3610,
            -542,
            -163,
            -879,
            -868,
            -343,
            -3341,
            -508,
            -594,
            -748,
            -2004,
            -247
        ],
        "steps_history": [
            3610,
            3610,
            542,
            3610,
            502,
            3610,
            3069,
            3570,
            300,
            1808,
            1090,
            1418,
            952,
            225,
            3610,
            643,
            264,
            980,
            969,
            444,
            3442,
            609,
            695,
            849,
            2105,
            348
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "613/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.1_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.328136682510376,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8005540166204986,
            0.8421052631578947,
            0.9279778393351801,
            0.8614958448753463,
            0.9362880886426593,
            0.9307479224376731,
            0.9224376731301939,
            0.850415512465374,
            0.8725761772853186,
            0.961218836565097,
            0.8393351800554016,
            0.9556786703601108,
            0.961218836565097,
            0.9778393351800554,
            0.9750692520775623,
            0.9778393351800554,
            0.9584487534626038,
            0.9584487534626038,
            0.9833795013850416,
            0.8642659279778393,
            0.9806094182825484,
            0.9778393351800554,
            0.9750692520775623,
            0.9833795013850416,
            0.9833795013850416,
            0.9445983379501385,
            0.9889196675900277,
            1.0,
            0.9722991689750693,
            1.0
        ],
        "reward_history": [
            -1612,
            -3610,
            -3610,
            -905,
            -2421,
            -411,
            -762,
            -1217,
            -3610,
            -1500,
            -291,
            -2155,
            -632,
            -584,
            -168,
            -155,
            -120,
            -219,
            -548,
            -316,
            -1884,
            -250,
            -388,
            -429,
            -259,
            -126,
            -978,
            -331,
            -340,
            -748,
            -158
        ],
        "steps_history": [
            1713,
            3610,
            3610,
            1006,
            2522,
            512,
            863,
            1318,
            3610,
            1601,
            392,
            2256,
            733,
            685,
            269,
            256,
            221,
            320,
            649,
            417,
            1985,
            351,
            489,
            530,
            360,
            227,
            1079,
            432,
            441,
            849,
            259
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "614/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.1_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 1.0,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.1140196323394775,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 35,
        "policy_stability_history": [
            0.0,
            0.853185595567867,
            0.7894736842105263,
            0.8199445983379502,
            0.8725761772853186,
            0.8393351800554016,
            0.853185595567867,
            0.817174515235457,
            0.9113573407202216,
            0.9556786703601108,
            0.9667590027700831,
            0.8421052631578947,
            0.9722991689750693,
            0.9556786703601108,
            0.9418282548476454,
            0.9861495844875346,
            0.961218836565097,
            0.9916897506925207,
            0.9861495844875346,
            0.9667590027700831,
            0.9778393351800554,
            0.9861495844875346,
            0.9833795013850416,
            0.9889196675900277,
            0.9916897506925207,
            0.9916897506925207,
            0.9889196675900277,
            0.9944598337950139,
            0.9806094182825484,
            0.9889196675900277,
            0.9944598337950139,
            0.9916897506925207,
            1.0,
            0.9861495844875346,
            0.997229916897507,
            1.0
        ],
        "reward_history": [
            -3610,
            -1577,
            -3610,
            -3610,
            -1594,
            -3610,
            -2356,
            -3610,
            -979,
            -417,
            -756,
            -3610,
            -228,
            -425,
            -630,
            -379,
            -373,
            -251,
            -210,
            -429,
            -361,
            -163,
            -231,
            -429,
            -298,
            -120,
            -203,
            -149,
            -763,
            -984,
            -395,
            -454,
            -167,
            -289,
            -263,
            -126
        ],
        "steps_history": [
            3610,
            1678,
            3610,
            3610,
            1695,
            3610,
            2457,
            3610,
            1080,
            518,
            857,
            3610,
            329,
            526,
            731,
            480,
            474,
            352,
            311,
            530,
            462,
            264,
            332,
            530,
            399,
            221,
            304,
            250,
            864,
            1085,
            496,
            555,
            268,
            390,
            364,
            227
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "615/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df1.0_eps0.1_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.621548175811768,
        "final_policy_stability": 0.9418282548476454,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8642659279778393,
            0.9113573407202216,
            0.8393351800554016,
            0.8476454293628809,
            0.853185595567867,
            0.8393351800554016,
            0.8753462603878116,
            0.8559556786703602,
            0.9113573407202216,
            0.9529085872576177,
            0.9806094182825484,
            0.961218836565097,
            0.9806094182825484,
            0.8587257617728532,
            0.9279778393351801,
            0.9501385041551247,
            0.9889196675900277,
            0.9778393351800554,
            0.9833795013850416,
            0.9113573407202216,
            0.9806094182825484,
            0.9833795013850416,
            0.9944598337950139,
            0.9916897506925207,
            1.0,
            0.9418282548476454
        ],
        "reward_history": [
            -3610,
            -1946,
            -1874,
            -3610,
            -3610,
            -3419,
            -3610,
            -3610,
            -3610,
            -2081,
            -1108,
            -446,
            -807,
            -391,
            -3610,
            -1344,
            -1384,
            -468,
            -629,
            -456,
            -3610,
            -686,
            -322,
            -736,
            -1183,
            -160,
            -875
        ],
        "steps_history": [
            3610,
            2047,
            1975,
            3610,
            3610,
            3520,
            3610,
            3610,
            3610,
            2182,
            1209,
            547,
            908,
            492,
            3610,
            1445,
            1485,
            569,
            730,
            557,
            3610,
            787,
            423,
            837,
            1284,
            261,
            976
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "616/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.06_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.721059083938599,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 21,
        "policy_stability_history": [
            0.0,
            0.8199445983379502,
            0.8559556786703602,
            0.9335180055401662,
            0.8725761772853186,
            0.9667590027700831,
            0.8836565096952909,
            0.8698060941828255,
            0.8670360110803325,
            0.9085872576177285,
            0.9141274238227147,
            0.9335180055401662,
            0.8476454293628809,
            0.9141274238227147,
            0.9916897506925207,
            0.9889196675900277,
            0.9722991689750693,
            0.997229916897507,
            0.9002770083102493,
            0.9833795013850416,
            0.9861495844875346,
            1.0
        ],
        "reward_history": [
            -3610,
            -3446,
            -3610,
            -596,
            -2364,
            -187,
            -2231,
            -2390,
            -3610,
            -3610,
            -1548,
            -1160,
            -3610,
            -3610,
            -587,
            -212,
            -1059,
            -562,
            -3610,
            -551,
            -671,
            -317
        ],
        "steps_history": [
            3610,
            3547,
            3610,
            697,
            2465,
            288,
            2332,
            2491,
            3610,
            3610,
            1649,
            1261,
            3610,
            3610,
            688,
            313,
            1160,
            663,
            3610,
            652,
            772,
            418
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "617/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.06_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.399473190307617,
        "final_policy_stability": 0.9279778393351801,
        "episodes_to_convergence": 26,
        "policy_stability_history": [
            0.0,
            0.8365650969529086,
            0.7977839335180056,
            0.814404432132964,
            0.817174515235457,
            0.8836565096952909,
            0.9279778393351801,
            0.9445983379501385,
            0.9141274238227147,
            0.9806094182825484,
            0.8725761772853186,
            0.8781163434903048,
            0.8421052631578947,
            0.9501385041551247,
            0.9806094182825484,
            0.9030470914127424,
            0.9778393351800554,
            0.8559556786703602,
            0.8919667590027701,
            0.9833795013850416,
            0.9861495844875346,
            0.9861495844875346,
            0.9889196675900277,
            0.9806094182825484,
            0.9944598337950139,
            0.9113573407202216,
            0.9279778393351801
        ],
        "reward_history": [
            -158,
            -1737,
            -3610,
            -3610,
            -3610,
            -3610,
            -904,
            -1351,
            -1902,
            -245,
            -3610,
            -3610,
            -2037,
            -1000,
            -220,
            -1564,
            -458,
            -3610,
            -3610,
            -234,
            -337,
            -309,
            -303,
            -1113,
            -303,
            -3610,
            -1632
        ],
        "steps_history": [
            259,
            1838,
            3610,
            3610,
            3610,
            3610,
            1005,
            1452,
            2003,
            346,
            3610,
            3610,
            2138,
            1101,
            321,
            1665,
            559,
            3610,
            3610,
            335,
            438,
            410,
            404,
            1214,
            404,
            3610,
            1733
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "618/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.06_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.585017204284668,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 27,
        "policy_stability_history": [
            0.0,
            0.9224376731301939,
            0.8254847645429363,
            0.8975069252077562,
            0.8587257617728532,
            0.8725761772853186,
            0.9501385041551247,
            0.9113573407202216,
            0.9141274238227147,
            0.850415512465374,
            0.8587257617728532,
            0.853185595567867,
            0.9556786703601108,
            0.889196675900277,
            0.8781163434903048,
            0.9529085872576177,
            0.9002770083102493,
            0.9168975069252078,
            0.8808864265927978,
            0.9473684210526315,
            0.9722991689750693,
            0.9806094182825484,
            0.9806094182825484,
            0.9806094182825484,
            1.0,
            0.997229916897507,
            0.9916897506925207,
            1.0
        ],
        "reward_history": [
            -3610,
            -753,
            -3610,
            -1414,
            -3610,
            -3610,
            -706,
            -2887,
            -1698,
            -3610,
            -3610,
            -3610,
            -744,
            -1885,
            -3610,
            -762,
            -3233,
            -3610,
            -3610,
            -1415,
            -547,
            -474,
            -403,
            -369,
            -434,
            -523,
            -723,
            -293
        ],
        "steps_history": [
            3610,
            854,
            3610,
            1515,
            3610,
            3610,
            807,
            2988,
            1799,
            3610,
            3610,
            3610,
            845,
            1986,
            3610,
            863,
            3334,
            3610,
            3610,
            1516,
            648,
            575,
            504,
            470,
            535,
            624,
            824,
            394
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "619/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.06_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.06,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.551295042037964,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 22,
        "policy_stability_history": [
            0.0,
            0.7839335180055401,
            0.8448753462603878,
            0.9501385041551247,
            0.8448753462603878,
            0.9279778393351801,
            0.8559556786703602,
            0.9667590027700831,
            0.96398891966759,
            0.96398891966759,
            0.9722991689750693,
            0.9390581717451524,
            0.9279778393351801,
            0.9750692520775623,
            0.9390581717451524,
            0.9806094182825484,
            0.8781163434903048,
            0.9695290858725761,
            0.9889196675900277,
            0.9806094182825484,
            0.9833795013850416,
            0.997229916897507,
            1.0
        ],
        "reward_history": [
            -1352,
            -3610,
            -3610,
            -522,
            -3610,
            -1133,
            -3610,
            -264,
            -405,
            -364,
            -426,
            -1049,
            -902,
            -279,
            -1662,
            -446,
            -3610,
            -807,
            -451,
            -737,
            -1211,
            -523,
            -1299
        ],
        "steps_history": [
            1453,
            3610,
            3610,
            623,
            3610,
            1234,
            3610,
            365,
            506,
            465,
            527,
            1150,
            1003,
            380,
            1763,
            547,
            3610,
            908,
            552,
            838,
            1312,
            624,
            1400
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "620/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.06_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.592392683029175,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 24,
        "policy_stability_history": [
            0.0,
            0.8642659279778393,
            0.8199445983379502,
            0.8116343490304709,
            0.9002770083102493,
            0.8642659279778393,
            0.8642659279778393,
            0.8421052631578947,
            0.8282548476454293,
            0.9307479224376731,
            0.9030470914127424,
            0.9722991689750693,
            0.9695290858725761,
            0.96398891966759,
            0.9889196675900277,
            0.8781163434903048,
            0.9889196675900277,
            0.9861495844875346,
            0.9944598337950139,
            0.9833795013850416,
            0.9916897506925207,
            0.9916897506925207,
            0.9806094182825484,
            0.997229916897507,
            1.0
        ],
        "reward_history": [
            -388,
            -2161,
            -2904,
            -3610,
            -3610,
            -3610,
            -3610,
            -3610,
            -3304,
            -968,
            -1661,
            -400,
            -740,
            -876,
            -75,
            -2957,
            -438,
            -154,
            -379,
            -491,
            -667,
            -399,
            -1862,
            -284,
            -139
        ],
        "steps_history": [
            489,
            2262,
            3005,
            3610,
            3610,
            3610,
            3610,
            3610,
            3405,
            1069,
            1762,
            501,
            841,
            977,
            176,
            3058,
            539,
            255,
            480,
            592,
            768,
            500,
            1963,
            385,
            240
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "621/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.08_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.484023571014404,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.8421052631578947,
            0.8614958448753463,
            0.8947368421052632,
            0.8282548476454293,
            0.8587257617728532,
            0.853185595567867,
            0.8559556786703602,
            0.9584487534626038,
            0.8559556786703602,
            0.9556786703601108,
            0.9529085872576177,
            0.8808864265927978,
            0.9279778393351801,
            0.961218836565097,
            0.9501385041551247,
            0.9861495844875346,
            0.8698060941828255,
            0.997229916897507,
            0.8753462603878116,
            0.9861495844875346,
            0.9889196675900277,
            0.9806094182825484,
            0.9944598337950139,
            0.9916897506925207,
            0.9944598337950139,
            0.9944598337950139,
            1.0,
            0.9778393351800554,
            0.9916897506925207,
            1.0
        ],
        "reward_history": [
            -3610,
            -3104,
            -1638,
            -1519,
            -3610,
            -2491,
            -2672,
            -3610,
            -532,
            -3610,
            -629,
            -1012,
            -3610,
            -2534,
            -805,
            -1317,
            -311,
            -3610,
            -115,
            -3610,
            -318,
            -248,
            -356,
            -472,
            -194,
            -67,
            -1230,
            -221,
            -921,
            -646,
            -504
        ],
        "steps_history": [
            3610,
            3205,
            1739,
            1620,
            3610,
            2592,
            2773,
            3610,
            633,
            3610,
            730,
            1113,
            3610,
            2635,
            906,
            1418,
            412,
            3610,
            216,
            3610,
            419,
            349,
            457,
            573,
            295,
            168,
            1331,
            322,
            1022,
            747,
            605
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "622/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.08_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.433313846588135,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 34,
        "policy_stability_history": [
            0.0,
            0.8421052631578947,
            0.817174515235457,
            0.8337950138504155,
            0.8670360110803325,
            0.8199445983379502,
            0.8753462603878116,
            0.9584487534626038,
            0.8642659279778393,
            0.9750692520775623,
            0.9667590027700831,
            0.8337950138504155,
            0.9556786703601108,
            0.9390581717451524,
            0.9695290858725761,
            0.9806094182825484,
            0.8476454293628809,
            0.9695290858725761,
            0.925207756232687,
            0.9889196675900277,
            0.8725761772853186,
            0.9944598337950139,
            0.9861495844875346,
            0.9806094182825484,
            0.9778393351800554,
            1.0,
            0.8919667590027701,
            0.997229916897507,
            0.9944598337950139,
            0.997229916897507,
            0.9944598337950139,
            0.9335180055401662,
            0.9944598337950139,
            1.0,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -1636,
            -2947,
            -2754,
            -2259,
            -516,
            -2154,
            -156,
            -377,
            -3610,
            -645,
            -896,
            -460,
            -265,
            -3610,
            -350,
            -2402,
            -176,
            -3610,
            -263,
            -205,
            -506,
            -400,
            -62,
            -3610,
            -417,
            -205,
            -88,
            -350,
            -1438,
            -591,
            -193,
            -299
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            1737,
            3048,
            2855,
            2360,
            617,
            2255,
            257,
            478,
            3610,
            746,
            997,
            561,
            366,
            3610,
            451,
            2503,
            277,
            3610,
            364,
            306,
            607,
            501,
            163,
            3610,
            518,
            306,
            189,
            451,
            1539,
            692,
            294,
            400
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "623/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.08_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.612690687179565,
        "final_policy_stability": 0.9916897506925207,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.853185595567867,
            0.9445983379501385,
            0.8421052631578947,
            0.8947368421052632,
            0.8781163434903048,
            0.9584487534626038,
            0.9307479224376731,
            0.8614958448753463,
            0.8476454293628809,
            0.853185595567867,
            0.961218836565097,
            0.889196675900277,
            0.9473684210526315,
            0.96398891966759,
            0.9141274238227147,
            0.9501385041551247,
            0.9667590027700831,
            0.8310249307479224,
            0.9806094182825484,
            0.9833795013850416,
            0.9889196675900277,
            0.9944598337950139,
            0.997229916897507,
            1.0,
            0.9916897506925207
        ],
        "reward_history": [
            -1485,
            -2111,
            -396,
            -3610,
            -1239,
            -3610,
            -579,
            -806,
            -2692,
            -2479,
            -3610,
            -379,
            -2032,
            -1002,
            -454,
            -1014,
            -840,
            -328,
            -3610,
            -595,
            -351,
            -463,
            -259,
            -313,
            -53,
            -487
        ],
        "steps_history": [
            1586,
            2212,
            497,
            3610,
            1340,
            3610,
            680,
            907,
            2793,
            2580,
            3610,
            480,
            2133,
            1103,
            555,
            1115,
            941,
            429,
            3610,
            696,
            452,
            564,
            360,
            414,
            154,
            588
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "624/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.08_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.08,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 4.732178688049316,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 25,
        "policy_stability_history": [
            0.0,
            0.8836565096952909,
            0.8698060941828255,
            0.8476454293628809,
            0.9362880886426593,
            0.8698060941828255,
            0.9501385041551247,
            0.8670360110803325,
            0.9085872576177285,
            0.9556786703601108,
            0.9889196675900277,
            0.889196675900277,
            0.9196675900277008,
            0.9861495844875346,
            0.8476454293628809,
            0.9722991689750693,
            0.96398891966759,
            0.9695290858725761,
            0.9944598337950139,
            0.9944598337950139,
            0.9806094182825484,
            0.9833795013850416,
            0.9944598337950139,
            0.9944598337950139,
            0.9944598337950139,
            1.0
        ],
        "reward_history": [
            -3158,
            -1000,
            -3266,
            -3610,
            -438,
            -3610,
            -612,
            -3610,
            -1259,
            -547,
            -175,
            -1876,
            -1457,
            -421,
            -3610,
            -718,
            -1296,
            -934,
            -215,
            -505,
            -589,
            -1203,
            -888,
            -1112,
            -1225,
            -1414
        ],
        "steps_history": [
            3259,
            1101,
            3367,
            3610,
            539,
            3610,
            713,
            3610,
            1360,
            648,
            276,
            1977,
            1558,
            522,
            3610,
            819,
            1397,
            1035,
            316,
            606,
            690,
            1304,
            989,
            1213,
            1326,
            1515
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "625/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.08_trial4"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 0,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.391826391220093,
        "final_policy_stability": 0.9944598337950139,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.8199445983379502,
            0.9224376731301939,
            0.8559556786703602,
            0.9279778393351801,
            0.8781163434903048,
            0.8365650969529086,
            0.8614958448753463,
            0.8808864265927978,
            0.8337950138504155,
            0.9307479224376731,
            0.8670360110803325,
            0.8919667590027701,
            0.925207756232687,
            0.9529085872576177,
            0.961218836565097,
            0.8725761772853186,
            0.9695290858725761,
            0.9778393351800554,
            0.8448753462603878,
            0.96398891966759,
            0.9667590027700831,
            0.9806094182825484,
            0.9833795013850416,
            0.9889196675900277,
            0.9944598337950139,
            0.9889196675900277,
            0.9944598337950139,
            1.0,
            0.9944598337950139
        ],
        "reward_history": [
            -2683,
            -2141,
            -795,
            -1674,
            -480,
            -1773,
            -3610,
            -3610,
            -1607,
            -3610,
            -279,
            -3610,
            -1745,
            -1526,
            -517,
            -422,
            -2374,
            -499,
            -97,
            -3610,
            -530,
            -939,
            -338,
            -1459,
            -189,
            -370,
            -447,
            -269,
            -179,
            -910
        ],
        "steps_history": [
            2784,
            2242,
            896,
            1775,
            581,
            1874,
            3610,
            3610,
            1708,
            3610,
            380,
            3610,
            1846,
            1627,
            618,
            523,
            2475,
            600,
            198,
            3610,
            631,
            1040,
            439,
            1560,
            290,
            471,
            548,
            370,
            280,
            1011
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "626/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.1_trial0"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 1,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.420681476593018,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 28,
        "policy_stability_history": [
            0.0,
            0.7950138504155124,
            0.8088642659279779,
            0.8947368421052632,
            0.850415512465374,
            0.9085872576177285,
            0.8448753462603878,
            0.8725761772853186,
            0.9556786703601108,
            0.850415512465374,
            0.9750692520775623,
            0.9445983379501385,
            0.925207756232687,
            0.9196675900277008,
            0.9778393351800554,
            0.9722991689750693,
            0.9362880886426593,
            0.9667590027700831,
            0.8365650969529086,
            0.9916897506925207,
            0.8975069252077562,
            0.8614958448753463,
            0.9861495844875346,
            1.0,
            0.9861495844875346,
            0.9944598337950139,
            0.9944598337950139,
            0.9944598337950139,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -1461,
            -2991,
            -1121,
            -3610,
            -2097,
            -921,
            -3610,
            -229,
            -740,
            -795,
            -963,
            -363,
            -413,
            -1048,
            -611,
            -3375,
            -803,
            -2346,
            -3610,
            -159,
            -90,
            -642,
            -117,
            -290,
            -659,
            -95
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            1562,
            3092,
            1222,
            3610,
            2198,
            1022,
            3610,
            330,
            841,
            896,
            1064,
            464,
            514,
            1149,
            712,
            3476,
            904,
            2447,
            3610,
            260,
            191,
            743,
            218,
            391,
            760,
            196
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "627/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.1_trial1"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 2,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.540302276611328,
        "final_policy_stability": 1.0,
        "episodes_to_convergence": 29,
        "policy_stability_history": [
            0.0,
            0.8005540166204986,
            0.9335180055401662,
            0.8282548476454293,
            0.9529085872576177,
            0.9473684210526315,
            0.8753462603878116,
            0.9473684210526315,
            0.9750692520775623,
            0.8781163434903048,
            0.9556786703601108,
            0.9667590027700831,
            0.8088642659279779,
            0.8919667590027701,
            0.9196675900277008,
            0.9168975069252078,
            0.9501385041551247,
            0.96398891966759,
            0.925207756232687,
            0.9722991689750693,
            0.9584487534626038,
            0.8670360110803325,
            0.9778393351800554,
            0.8393351800554016,
            0.8642659279778393,
            0.9944598337950139,
            1.0,
            0.997229916897507,
            0.9916897506925207,
            1.0
        ],
        "reward_history": [
            -3610,
            -3610,
            -441,
            -3610,
            -401,
            -480,
            -3610,
            -744,
            -298,
            -1818,
            -252,
            -512,
            -3610,
            -1459,
            -634,
            -990,
            -489,
            -227,
            -1388,
            -298,
            -735,
            -3610,
            -283,
            -3610,
            -3610,
            -379,
            -247,
            -193,
            -530,
            -105
        ],
        "steps_history": [
            3610,
            3610,
            542,
            3610,
            502,
            581,
            3610,
            845,
            399,
            1919,
            353,
            613,
            3610,
            1560,
            735,
            1091,
            590,
            328,
            1489,
            399,
            836,
            3610,
            384,
            3610,
            3610,
            480,
            348,
            294,
            631,
            206
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "628/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.1_trial2"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 3,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 6.021576166152954,
        "final_policy_stability": 0.997229916897507,
        "episodes_to_convergence": 33,
        "policy_stability_history": [
            0.0,
            0.814404432132964,
            0.8559556786703602,
            0.9113573407202216,
            0.8060941828254847,
            0.8808864265927978,
            0.9445983379501385,
            0.850415512465374,
            0.8725761772853186,
            0.9529085872576177,
            0.8448753462603878,
            0.9778393351800554,
            0.9529085872576177,
            0.9362880886426593,
            0.9667590027700831,
            0.9529085872576177,
            0.8254847645429363,
            0.96398891966759,
            0.9806094182825484,
            0.9833795013850416,
            0.9750692520775623,
            0.8864265927977839,
            0.9944598337950139,
            0.9833795013850416,
            0.9916897506925207,
            0.9806094182825484,
            0.9916897506925207,
            0.9861495844875346,
            0.9916897506925207,
            0.9916897506925207,
            0.997229916897507,
            1.0,
            1.0,
            0.997229916897507
        ],
        "reward_history": [
            -1612,
            -3610,
            -1975,
            -689,
            -3610,
            -1718,
            -572,
            -3610,
            -2164,
            -490,
            -2947,
            -181,
            -395,
            -493,
            -219,
            -345,
            -3418,
            -355,
            -240,
            -145,
            -364,
            -3610,
            -273,
            -685,
            -261,
            -222,
            -147,
            -489,
            -187,
            -640,
            -183,
            -356,
            -22,
            -190
        ],
        "steps_history": [
            1713,
            3610,
            2076,
            790,
            3610,
            1819,
            673,
            3610,
            2265,
            591,
            3048,
            282,
            496,
            594,
            320,
            446,
            3519,
            456,
            341,
            246,
            465,
            3610,
            374,
            786,
            362,
            323,
            248,
            590,
            288,
            741,
            284,
            457,
            123,
            291
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "629/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.1_trial3"
    },
    {
        "maze_size": 9,
        "maze_id": 42,
        "learning_rate": 0.4,
        "discount_factor": 0.99,
        "epsilon": 0.1,
        "num_episodes": 1000,
        "agent_seed": 4,
        "goal_reward": 100,
        "wall_penalty": -10,
        "step_penalty": -1,
        "optimal_path_convergence_window": 5,
        "success_rate": 1.0,
        "avg_steps": 32.0,
        "std_steps": 0.0,
        "avg_path_optimality": 1.0,
        "std_path_optimality": 0.0,
        "training_duration": 5.365376234054565,
        "final_policy_stability": 0.9944598337950139,
        "episodes_to_convergence": 30,
        "policy_stability_history": [
            0.0,
            0.7977839335180056,
            0.7867036011080333,
            0.8781163434903048,
            0.9058171745152355,
            0.9390581717451524,
            0.9473684210526315,
            0.9778393351800554,
            0.9279778393351801,
            0.9445983379501385,
            0.9667590027700831,
            0.9501385041551247,
            0.8642659279778393,
            0.9529085872576177,
            0.961218836565097,
            0.9722991689750693,
            0.8642659279778393,
            0.9806094182825484,
            0.9722991689750693,
            0.853185595567867,
            0.9916897506925207,
            0.961218836565097,
            0.9944598337950139,
            0.9833795013850416,
            0.9861495844875346,
            0.997229916897507,
            0.9861495844875346,
            0.9944598337950139,
            0.9944598337950139,
            0.9916897506925207,
            0.9944598337950139
        ],
        "reward_history": [
            -3610,
            -3610,
            -3610,
            -1527,
            -1725,
            -398,
            -651,
            -408,
            -618,
            -817,
            -430,
            -342,
            -2819,
            -381,
            -183,
            -141,
            -3610,
            -41,
            -416,
            -3610,
            -237,
            -898,
            -111,
            -240,
            -384,
            -174,
            -465,
            -361,
            -495,
            -429,
            -298
        ],
        "steps_history": [
            3610,
            3610,
            3610,
            1628,
            1826,
            499,
            752,
            509,
            719,
            918,
            531,
            443,
            2920,
            482,
            284,
            242,
            3610,
            142,
            517,
            3610,
            338,
            999,
            212,
            341,
            485,
            275,
            566,
            462,
            596,
            530,
            399
        ],
        "experiment_id": "20250131_160708",
        "iteration_count": "630/810",
        "save_path": "experiments/20250131_160708/training_plots/size_9/lr0.4_df0.99_eps0.1_trial4"
    }
]