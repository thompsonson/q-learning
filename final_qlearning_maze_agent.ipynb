{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environement set up details\n",
    "\n",
    "```\n",
    "pipx install uv\n",
    "uv venv\n",
    "uv pip install mazelib ipykernal ipython matplotlib pandas openpyxl\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from mazelib import Maze\n",
    "from mazelib.generate.Sidewinder import Sidewinder\n",
    "from mazelib.solve.BacktrackingSolver import BacktrackingSolver\n",
    "from pprint import pprint\n",
    "import openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for maze navigation system\"\"\"\n",
    "    # Maze parameters\n",
    "    maze_size: int = 5\n",
    "    maze_id: Optional[int] = None  # None for random generation, int for specific seed\n",
    "    \n",
    "    # Algorithm parameters \n",
    "    learning_rate: float = 0.1  # step size α ∈ (0, 1]\n",
    "    discount_factor: float = 0.9  # γ\n",
    "    epsilon: float = 0.1  # small ε > 0\n",
    "    num_episodes: int = 100\n",
    "\n",
    "    # Reward structure\n",
    "    goal_reward: float = 100\n",
    "    wall_penalty: float = -10\n",
    "    step_penalty: float = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent, Environment Control classes\n",
    "\n",
    "Go to the execution section to configure, train, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnvironment:\n",
    "    \"\"\"Handles maze generation, state management and visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.grid = None\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self._maze = None\n",
    "        self.seed = None\n",
    "        self.optimal_path = None\n",
    "        self.optimal_path_length = None\n",
    "        self.generate()\n",
    "\n",
    "        \n",
    "    def generate(self) -> None:\n",
    "        \"\"\"Creates new maze using Sidewinder algorithm\"\"\"\n",
    "        # Use config maze_id if provided, otherwise generate random seed\n",
    "        self.seed = self.config.maze_id if self.config.maze_id is not None else np.random.randint(1, 1000)\n",
    "   \n",
    "        self._maze = Maze(self.seed)\n",
    "        self._maze.generator = Sidewinder(self.config.maze_size, self.config.maze_size)\n",
    "        self._maze.generate()\n",
    "        self._maze.generate_entrances()\n",
    "        \n",
    "        self.grid = self._maze.grid\n",
    "\n",
    "        # Set the start to the first valid cell inside grid\n",
    "        self.start = (1, 1)  \n",
    "        # Set the end to the last valid cell inside grid\n",
    "        self.end = (self.grid.shape[0]-2, self.grid.shape[1]-2)\n",
    "\n",
    "        # After maze generation, calculate optimal path\n",
    "        self._calculate_optimal_path()\n",
    "\n",
    "    def _calculate_optimal_path(self) -> None:\n",
    "        \"\"\"Calculate optimal path using maze's solver\"\"\"\n",
    "        # Set up solver\n",
    "        self._maze.solver = BacktrackingSolver()\n",
    "        self._maze.start = self.start\n",
    "        self._maze.end = self.end\n",
    "        \n",
    "        # Solve\n",
    "        self._maze.solve()\n",
    "        \n",
    "        if self._maze.solutions:\n",
    "            self.optimal_path = self._maze.solutions[0]  # Store first solution\n",
    "            self.optimal_path_length = len(self.optimal_path) + 1\n",
    "        else:\n",
    "            # Handle case where no solution is found\n",
    "            self.optimal_path = None\n",
    "            self.optimal_path_length = None\n",
    "    \n",
    "    def get_minimum_steps(self) -> Optional[int]:\n",
    "        \"\"\"Returns the length of optimal path if it exists\"\"\"\n",
    "        return self.optimal_path_length\n",
    "        \n",
    "    def get_state(self, position: Tuple[int, int]) -> Tuple[int, int]:\n",
    "        \"\"\"Returns current state representation\"\"\"\n",
    "        # There is no uncertainity in this environment\n",
    "        # uncertainty could be added here to mimic a dirty sensor\n",
    "        # or to mimic external factors like wind\n",
    "        # this would be for a different problem (partial observability) \n",
    "        return position\n",
    "        \n",
    "    def get_reward(self, state: Tuple[int, int], next_state: Tuple[int, int]) -> float:\n",
    "        \"\"\"Calculates reward for a state transition\"\"\"\n",
    "        if not self.is_valid_move(next_state):\n",
    "            return self.config.wall_penalty\n",
    "        elif next_state == self.end:\n",
    "            return self.config.goal_reward\n",
    "        return self.config.step_penalty\n",
    "        \n",
    "    def is_valid_move(self, state: Tuple[int, int]) -> bool:\n",
    "        \"\"\"Checks if move is legal\"\"\"\n",
    "        row, col = state\n",
    "        # check if the move goes outside of the grid or into a wall (1)\n",
    "        if (row < 0 or row >= self.grid.shape[0] or \n",
    "            col < 0 or col >= self.grid.shape[1] or \n",
    "            self.grid[state] == 1):\n",
    "            return False\n",
    "        # move is valid\n",
    "        return True\n",
    "        \n",
    "    def visualize(self, path: Optional[List[Tuple[int, int]]] = None, show_optimal: bool = False) -> None:\n",
    "        \"\"\"Displays maze with optional path and optimal path\"\"\"\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        # Add title showing Maze ID and minimum steps\n",
    "        title = f\"Maze #{self.seed} - Min Steps: {self.get_minimum_steps()}\"\n",
    "        plt.title(title)\n",
    "        plt.imshow(self.grid, cmap='binary')\n",
    "        \n",
    "        # Plots the start (S) and goal (G)\n",
    "        plt.text(self.start[1], self.start[0], 'S', \n",
    "                ha='center', va='center', color='red', fontsize=20)\n",
    "        plt.text(self.end[1], self.end[0], 'G', \n",
    "                ha='center', va='center', color='green', fontsize=20)\n",
    "        \n",
    "        # Plot optimal path if requested\n",
    "        if show_optimal and self.optimal_path:\n",
    "            for pos in self.optimal_path:\n",
    "                plt.text(pos[1], pos[0], \"O\", \n",
    "                        ha='center', va='center', color='green', fontsize=15)\n",
    "        \n",
    "        # Plot current path\n",
    "        if path:\n",
    "            for pos in path:\n",
    "                plt.text(pos[1], pos[0], \"#\", \n",
    "                        ha='center', va='center', color='blue', fontsize=20)\n",
    "        \n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"Implements Q-learning algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, env: MazeEnvironment, config: Config):\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "        # Actions the agent can take: Up, Down, Left, Right. Each action is represented as a tuple of two values: (row_change, column_change)\n",
    "        self.actions = [\n",
    "            (-1, 0), # Up: Moving one step up, reducing the row index by 1\n",
    "            (1, 0),  # Down: Moving on step down, increasing the row index by 1\n",
    "            (0, -1), # Left: Moving one step to the left, reducing the column index by 1\n",
    "            (0, 1)   # Right: Moving one step to the right, increasing the column index by 1 \n",
    "        ]\n",
    "        maze_height, maze_width = env.grid.shape\n",
    "        self.q_table = np.zeros((maze_height, maze_width, 4))\n",
    "        self.exploration_rate = config.epsilon\n",
    "        \n",
    "    def get_action(self, state: Tuple[int, int], training: bool = True) -> int:\n",
    "        \"\"\"Selects action using ε-greedy policy\"\"\"\n",
    "        # When training, Choose A from S using policy derived from Q (e.g., ε-greedy)\n",
    "        if training and np.random.rand() < (1- self.exploration_rate):\n",
    "            # explore\n",
    "            return np.random.randint(4)\n",
    "        # exploit\n",
    "        return np.argmax(self.q_table[state])\n",
    "        \n",
    "    def update(self, state: Tuple[int, int], action: int, \n",
    "              reward: float, next_state: Tuple[int, int]) -> None:\n",
    "        \"\"\"Updates Q-value for state-action pair\"\"\"\n",
    "        # max_a Q(S', a)\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        # Q(S, A)\n",
    "        current_q = self.q_table[state][action]\n",
    "        # Q(S', a)\n",
    "        next_q = self.q_table[next_state][best_next_action]\n",
    "        # Q(S, A) ← Q(S, A) + α [R + γ max_a Q(S', a) - Q(S, A)]\n",
    "        new_q = current_q + self.config.learning_rate * (\n",
    "            reward + self.config.discount_factor * next_q - current_q)\n",
    "        # update the q_table\n",
    "        self.q_table[state][action] = new_q\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentControl:\n",
    "    \"\"\"Manages training, testing and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, env: MazeEnvironment, agent: QLearningAgent, config: Config):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.config = config\n",
    "        self.metrics = {\n",
    "            'rewards': [],\n",
    "            'steps': [],\n",
    "            'success_rate': [],\n",
    "            'episode_status': [],  # Track if episode reached goal\n",
    "            'training_start_time': None,\n",
    "            'training_duration': None,\n",
    "            'path_lengths': [],    # Track path length per episode\n",
    "            'final_qtable': None,  # Store final Q-table state\n",
    "            'policy_stability': [], # Track changes in policy\n",
    "            'reached_goal_test': [], # Track if the episode test reached the goal\n",
    "            'steps_test': [], # Track the episode test step count\n",
    "            'path_optimality_test': [], # Track the episode test path optimality\n",
    "        }\n",
    "        \n",
    "    def calculate_policy_stability(self) -> float:\n",
    "        \"\"\"Measures policy stability by comparing action choices across states\"\"\"\n",
    "        current_policy = {state: np.argmax(self.agent.q_table[state]) \n",
    "                         for state in np.ndindex(self.env.grid.shape)}\n",
    "        if not hasattr(self, '_last_policy'):\n",
    "            self._last_policy = current_policy\n",
    "            return 0.0\n",
    "        \n",
    "        matches = sum(1 for s in current_policy \n",
    "                     if current_policy[s] == self._last_policy[s])\n",
    "        stability = matches / len(current_policy)\n",
    "        self._last_policy = current_policy\n",
    "        return stability\n",
    "        \n",
    "    def run_episode(self, training: bool = True) -> Tuple[float, int, List[Tuple[int, int]], bool]:\n",
    "        \"\"\"Runs single episode with enhanced metrics\"\"\"\n",
    "        current_state = self.env.start\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        path = [current_state]\n",
    "        max_steps = self.env.grid.shape[0] * self.env.grid.shape[1] * 10\n",
    "        reached_goal = False\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            action = self.agent.get_action(current_state, training)\n",
    "            next_state = (\n",
    "                current_state[0] + self.agent.actions[action][0],\n",
    "                current_state[1] + self.agent.actions[action][1]\n",
    "            )\n",
    "            \n",
    "            if not self.env.is_valid_move(next_state):\n",
    "                next_state = current_state\n",
    "\n",
    "            reward = self.env.get_reward(current_state, next_state)\n",
    "            \n",
    "            if training:\n",
    "                self.agent.update(current_state, action, reward, next_state)\n",
    "                \n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            path.append(next_state)\n",
    "            \n",
    "            if next_state == self.env.end:\n",
    "                reached_goal = True\n",
    "                break\n",
    "                \n",
    "            current_state = next_state\n",
    "            \n",
    "        return episode_reward, steps, path, reached_goal\n",
    "        \n",
    "    def train(self, save_path: Optional[str] = None) -> None:\n",
    "        \"\"\"Runs training loop with enhanced metrics\"\"\"\n",
    "        plt.ion()\n",
    "        fig, (ax_reward, ax_steps, ax_stability, ax_optimality) = plt.subplots(1, 4, figsize=(15, 5))\n",
    "        \n",
    "        window_size = 20\n",
    "        moving_rewards = []\n",
    "        moving_steps = []\n",
    "        \n",
    "        self.metrics['training_start_time'] = time.time()\n",
    "        \n",
    "        for episode in range(self.config.num_episodes):\n",
    "            reward, steps, path, reached_goal = self.run_episode(training=True)\n",
    "            \n",
    "            # Update metrics\n",
    "            self.metrics['rewards'].append(reward)\n",
    "            self.metrics['steps'].append(steps)\n",
    "            self.metrics['path_lengths'].append(len(path))\n",
    "            self.metrics['episode_status'].append(reached_goal)\n",
    "            self.metrics['policy_stability'].append(self.calculate_policy_stability())\n",
    "            \n",
    "            # Calculate moving averages\n",
    "            if episode >= window_size:\n",
    "                avg_reward = np.mean(self.metrics['rewards'][-window_size:])\n",
    "                avg_steps = np.mean(self.metrics['steps'][-window_size:])\n",
    "            else:\n",
    "                avg_reward = np.mean(self.metrics['rewards'])\n",
    "                avg_steps = np.mean(self.metrics['steps'])\n",
    "                \n",
    "            moving_rewards.append(avg_reward)\n",
    "            moving_steps.append(avg_steps)\n",
    "\n",
    "            # run a test \n",
    "            _ , steps_test, path_test, reached_goal_test= self.run_episode(training=False)\n",
    "            \n",
    "            self.metrics['reached_goal_test'].append(reached_goal_test)\n",
    "            self.metrics['steps_test'].append(steps_test)\n",
    "            \n",
    "            if steps_test != 0 or self.env.optimal_path_length != 0:\n",
    "                path_optimality_test = self.env.optimal_path_length / steps_test\n",
    "            else:\n",
    "                path_optimality_test = 0\n",
    "            \n",
    "            self.metrics['path_optimality_test'].append(path_optimality_test)\n",
    "\n",
    "            # Update plots\n",
    "            self._update_training_plots(fig, ax_reward, ax_steps, ax_stability, ax_optimality,\n",
    "                                     moving_rewards, moving_steps)\n",
    "            \n",
    "        self.metrics['training_duration'] = time.time() - self.metrics['training_start_time']\n",
    "        self.metrics['final_qtable'] = self.agent.q_table.copy()\n",
    "\n",
    "        # One final plot update\n",
    "        self._update_training_plots(fig, ax_reward, ax_steps, ax_stability, ax_optimality,\n",
    "                                moving_rewards, moving_steps)\n",
    "\n",
    "        if save_path:\n",
    "            timestamp = int(time.time())\n",
    "            filename = (f\"{timestamp}_maze{self.config.maze_size}_\"\n",
    "                    f\"lr{self.config.learning_rate}_\"\n",
    "                    f\"df{self.config.discount_factor}_\"\n",
    "                    f\"eps{self.config.epsilon}.png\")\n",
    "            fig.savefig(f\"{save_path}/{filename}\", bbox_inches='tight')\n",
    "\n",
    "        plt.ioff()\n",
    "\n",
    "\n",
    "    def test(self, display: bool = False) -> None:\n",
    "        \"\"\"Evaluates agent performance\"\"\"\n",
    "        episode_reward, steps, path, reached_goal= self.run_episode(training=False)\n",
    "        print(f\"Test Results - Steps: {steps}, Reward: {episode_reward}, Successful: {reached_goal}\")\n",
    "        if display:\n",
    "            self.env.visualize(path)\n",
    "\n",
    "    def test_consistency(self, num_tests: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"Tests agent consistency across multiple runs\"\"\"\n",
    "        test_results = {\n",
    "            'success_rate': 0,\n",
    "            'avg_steps': 0,\n",
    "            'std_steps': 0\n",
    "        }\n",
    "        \n",
    "        steps_list = []\n",
    "        for _ in range(num_tests):\n",
    "            reward, steps, path, reached_goal = self.run_episode(training=False)\n",
    "            if reached_goal:\n",
    "                test_results['success_rate'] += 1\n",
    "            steps_list.append(steps)\n",
    "        \n",
    "        if steps_list:\n",
    "            test_results['success_rate'] /= num_tests\n",
    "            test_results['avg_steps'] = np.mean(steps_list)\n",
    "            test_results['std_steps'] = np.std(steps_list)\n",
    "            \n",
    "        return test_results\n",
    "\n",
    "    def _update_training_plots(self, fig, ax_reward, ax_steps, ax_stability, ax_optimality,\n",
    "                             moving_rewards, moving_steps) -> None:\n",
    "        \"\"\"Updates training visualization plots\"\"\"\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Add configuration header\n",
    "        elapsed_time = time.time() - self.metrics['training_start_time']\n",
    "        header = (f\"Maze Size: {self.config.maze_size}x{self.config.maze_size} | \"\n",
    "                f\"Maze ID: {self.env.seed} | \"\n",
    "                f\"Min Steps: {self.env.get_minimum_steps()} | \"\n",
    "                f\"Episodes: {self.config.num_episodes} | \"\n",
    "                f\"Learning Rate: {self.config.learning_rate} | \"\n",
    "                f\"Discount: {self.config.discount_factor} | \"\n",
    "                f\"Epsilon: {self.config.epsilon} | \"\n",
    "                f\"Training Time: {elapsed_time:.1f}s\")\n",
    "        fig.suptitle(header, wrap=True, y=1.05)\n",
    "        \n",
    "        # Reward plot\n",
    "        ax_reward.clear()\n",
    "        ax_reward.plot(self.metrics['rewards'], 'r-', alpha=0.3, label='Rewards')\n",
    "        ax_reward.plot(moving_rewards, 'r-', label='Moving Average')\n",
    "        ax_reward.set_xlabel('Episode')\n",
    "        ax_reward.set_ylabel('Reward')\n",
    "        ax_reward.legend()\n",
    "        ax_reward.grid(True)\n",
    "        \n",
    "        # Steps plot\n",
    "        ax_steps.clear()\n",
    "        ax_steps.plot(self.metrics['steps'], 'b-', alpha=0.3, label='Steps')\n",
    "        ax_steps.plot(moving_steps, 'b-', label='Moving Average')\n",
    "        ax_steps.set_xlabel('Episode')\n",
    "        ax_steps.set_ylabel('Steps')\n",
    "        ax_steps.legend()\n",
    "        ax_steps.grid(True)\n",
    "        \n",
    "        # Policy stability plot\n",
    "        ax_stability.clear()\n",
    "        ax_stability.plot(self.metrics['policy_stability'], 'g-', label='Policy Stability')\n",
    "        ax_stability.set_xlabel('Episode')\n",
    "        ax_stability.set_ylabel('Stability')\n",
    "        ax_stability.legend()\n",
    "        ax_stability.grid(True)\n",
    "\n",
    "        # Path optimality plot\n",
    "        ax_optimality.clear()\n",
    "        ax_optimality.plot(self.metrics['path_optimality_test'], 'g-', label='Path Optimality')\n",
    "        ax_optimality.set_xlabel('Episode')\n",
    "        ax_optimality.set_ylabel('Optimality')\n",
    "        ax_optimality.legend()\n",
    "        ax_optimality.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MazeEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a small maze and check its properties\n",
    "config = Config(maze_size=5, maze_id=42)  # Fixed seed for reproducibility\n",
    "env = MazeEnvironment(config)\n",
    "\n",
    "# Let's print basic information\n",
    "print(f\"Maze size: {env.grid.shape}\")\n",
    "print(f\"Start position: {env.start}\")\n",
    "print(f\"End position: {env.end}\")\n",
    "print(f\"Optimal path length: {env.optimal_path_length}\")\n",
    "print(f\"Optimal path: {env.optimal_path}\")\n",
    "\n",
    "# Visualize the maze with optimal path\n",
    "env.visualize(show_optimal=True)\n",
    "\n",
    "# Test reproducibility by creating another maze with same seed\n",
    "env2 = MazeEnvironment(config)\n",
    "print(\"\\nChecking reproducibility:\")\n",
    "print(f\"Same optimal path length? {env.optimal_path_length == env2.optimal_path_length}\")\n",
    "print(f\"Same optimal path? {env.optimal_path == env2.optimal_path}\")\n",
    "\n",
    "# Test with random seed\n",
    "config_random = Config(maze_size=5)  # No maze_id means random seed\n",
    "env_random = MazeEnvironment(config_random)\n",
    "print(f\"\\nRandom maze seed: {env_random.seed}\")\n",
    "print(f\"Random maze optimal path length: {env_random.optimal_path_length}\")\n",
    "\n",
    "# Visualize both current path and optimal path\n",
    "# Let's create a non-optimal path for demonstration\n",
    "test_path = [(1,1), (1,2), (1,3), (2,3), (3,3), (3,4), (3,5)]\n",
    "env_random.visualize(path=test_path, show_optimal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for training\n",
    "config = Config(maze_size=3, num_episodes=50)\n",
    "env = MazeEnvironment(config)\n",
    "agent = QLearningAgent(env, config)\n",
    "control = AgentControl(env, agent, config)\n",
    "# Train\n",
    "control.train(\"training_output\")\n",
    "# show number of steps, reward, and success of trained agent\n",
    "control.test()\n",
    "# show the best path\n",
    "control.test(True)\n",
    "# Test consistency\n",
    "test_results = control.test_consistency(num_tests=100)\n",
    "print(f\"Success rate: {test_results['success_rate']:.2%}\")\n",
    "print(f\"Average steps: {test_results['avg_steps']:.1f} ± {test_results['std_steps']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataclasses import asdict\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "class ExperimentRunner:\n",
    "    \"\"\"Manages automated testing and reporting\"\"\"\n",
    "    \n",
    "    def __init__(self, base_config: Config):\n",
    "        self.base_config = base_config\n",
    "        self.results = []\n",
    "        current_time = time.localtime()\n",
    "        date_prefix = time.strftime('%Y%m%d', current_time)\n",
    "        self.experiment_id = f\"{date_prefix}_{int(time.time())}\"\n",
    "        self.base_path = Path(f\"experiments/{self.experiment_id}\")\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def run_baseline_comparison(self, num_trials: int = 10) -> Dict:\n",
    "        \"\"\"Compare trained vs untrained agent performance\"\"\"\n",
    "        # create the training rsults folder\n",
    "        save_path = self.base_path / \"baseline\"\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # Untrained agent\n",
    "        untrained_results = self._run_untrained_tests(num_trials)\n",
    "        \n",
    "        # Trained agent\n",
    "        env = MazeEnvironment(self.base_config)\n",
    "        agent = QLearningAgent(env, self.base_config)\n",
    "        control = AgentControl(env, agent, self.base_config)\n",
    "        control.train(save_path=str(save_path))\n",
    "        trained_results = control.test_consistency(num_trials)\n",
    "\n",
    "        print(f\"untrained_results: {untrained_results}\") \n",
    "        \n",
    "        return {\n",
    "            'untrained': untrained_results,\n",
    "            'trained': trained_results\n",
    "        }\n",
    "        \n",
    "    def run_hyperparameter_sweep(self) -> List[Dict]:\n",
    "        \"\"\"Test different hyperparameter combinations\"\"\"\n",
    "        # create the training results folder\n",
    "        save_path = self.base_path / \"hyperparameter_sweep\"\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "\n",
    "        param_grid = {\n",
    "            'learning_rate': [0.1, 0.11],\n",
    "            'discount_factor': [1],\n",
    "            'epsilon': [0.31, 0.29, 0.3], \n",
    "            'maze_id': 42\n",
    "        }\n",
    "        \n",
    "        results = []\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for df in param_grid['discount_factor']:\n",
    "                for eps in param_grid['epsilon']:\n",
    "                    config = Config(\n",
    "                        maze_size=self.base_config.maze_size,\n",
    "                        maze_id=param_grid['maze_id'],\n",
    "                        learning_rate=lr,\n",
    "                        discount_factor=df,\n",
    "                        epsilon=eps\n",
    "                    )\n",
    "                    env = MazeEnvironment(config)\n",
    "                    agent = QLearningAgent(env, config)\n",
    "                    control = AgentControl(env, agent, config)\n",
    "                    \n",
    "                    # Train and test\n",
    "                    control.train(save_path=str(save_path))\n",
    "                    test_results = control.test_consistency(num_tests=10)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'params': asdict(config),\n",
    "                        'metrics': {\n",
    "                            **test_results,\n",
    "                            'training_duration': control.metrics['training_duration'],\n",
    "                            'final_policy_stability': control.metrics['policy_stability'][-1]\n",
    "                        }\n",
    "                    })\n",
    "        return results\n",
    "    \n",
    "    def run_scalability_test(self, sizes: List[int] = [5, 25, 200]) -> List[Dict]:\n",
    "        \"\"\"Test performance across different maze sizes\"\"\"\n",
    "        # create the training results folder\n",
    "        save_path = self.base_path / \"scalability\"\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "\n",
    "        results = []\n",
    "        for size in sizes:\n",
    "            # adust num_episodes for larger mazes\n",
    "            num_episodes = ( size+ 10 ) * size\n",
    "            config = Config(\n",
    "                maze_size=size, \n",
    "                num_episodes=num_episodes,\n",
    "                learning_rate=0.1,\n",
    "                discount_factor=0.99,\n",
    "                epsilon=0.3\n",
    "            )\n",
    "            env = MazeEnvironment(config)\n",
    "            agent = QLearningAgent(env, config)\n",
    "            control = AgentControl(env, agent, config)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            control.train(save_path=str(save_path))\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            test_results = control.test_consistency(num_tests=10)\n",
    "            results.append({\n",
    "                'maze_size': size,\n",
    "                'training_time': training_time,\n",
    "                **test_results\n",
    "            })\n",
    "        return results\n",
    "    \n",
    "    def _run_untrained_tests(self, num_trials: int) -> Dict:\n",
    "        \"\"\"Run tests with untrained agent\"\"\"\n",
    "        env = MazeEnvironment(self.base_config)\n",
    "        agent = QLearningAgent(env, self.base_config)\n",
    "        control = AgentControl(env, agent, self.base_config)\n",
    "        return control.test_consistency(num_trials)\n",
    "    \n",
    "    def save_report(self, experiment_results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Save experiment results to Excel report\"\"\"\n",
    "        output_path = self.base_path / \"experiment_report.xlsx\"\n",
    "        \n",
    "        with pd.ExcelWriter(output_path) as writer:\n",
    "            # Baseline comparison\n",
    "            if 'baseline' in experiment_results:\n",
    "                df_baseline = pd.DataFrame({\n",
    "                    'Metric': ['Success Rate', 'Average Steps', 'Std Steps'],\n",
    "                    'Untrained': [\n",
    "                        experiment_results['baseline']['untrained']['success_rate'],\n",
    "                        experiment_results['baseline']['untrained']['avg_steps'],\n",
    "                        experiment_results['baseline']['untrained']['std_steps']\n",
    "                    ],\n",
    "                    'Trained': [\n",
    "                        experiment_results['baseline']['trained']['success_rate'],\n",
    "                        experiment_results['baseline']['trained']['avg_steps'],\n",
    "                        experiment_results['baseline']['trained']['std_steps']\n",
    "                    ]\n",
    "                })\n",
    "                df_baseline.to_excel(writer, sheet_name='Baseline Comparison', index=False)\n",
    "            \n",
    "            # Hyperparameter results\n",
    "            if 'hyperparameters' in experiment_results:\n",
    "                df_hyper = pd.DataFrame(experiment_results['hyperparameters'])\n",
    "                df_hyper.to_excel(writer, sheet_name='Hyperparameter Study', index=False)\n",
    "            \n",
    "            # Scalability results\n",
    "            if 'scalability' in experiment_results:\n",
    "                df_scale = pd.DataFrame(experiment_results['scalability'])\n",
    "                df_scale.to_excel(writer, sheet_name='Scalability Study', index=False)\n",
    "\n",
    "def run_full_experiment_suite():\n",
    "    \"\"\"Run complete set of experiments and generate report\"\"\"\n",
    "    base_config = Config(maze_size=10, num_episodes=100)\n",
    "    runner = ExperimentRunner(base_config)\n",
    "    \n",
    "    results = {\n",
    "        'baseline': runner.run_baseline_comparison(),\n",
    "        'hyperparameters': runner.run_hyperparameter_sweep(),\n",
    "        'scalability': runner.run_scalability_test()\n",
    "    }\n",
    "    \n",
    "    runner.save_report(results, 'maze_experiments.xlsx')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run specific experiments\n",
    "runner = ExperimentRunner(\n",
    "    Config(\n",
    "        maze_size=5,\n",
    "        learning_rate=0.1,\n",
    "        discount_factor=0.99,\n",
    "        epsilon=0.3\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments and generate report\n",
    "# results = run_full_experiment_suite()\n",
    "\n",
    "# Baseline comparison\n",
    "baseline = runner.run_baseline_comparison(num_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter testing\n",
    "hyperparam_results = runner.run_hyperparameter_sweep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scalability testing\n",
    "scalability_results = runner.run_scalability_test([10,15,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "runner.save_report({\n",
    "    'baseline': baseline,\n",
    "    'hyperparameters': hyperparam_results,\n",
    "    'scalability': scalability_results\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
