{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environement set up details\n",
    "\n",
    "```\n",
    "pipx install uv\n",
    "uv venv\n",
    "uv pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from IPython.display import clear_output\n",
    "\n",
    "import openpyxl\n",
    "from pprint import pprint\n",
    "\n",
    "from mazelib import Maze\n",
    "from mazelib.generate.Sidewinder import Sidewinder\n",
    "from mazelib.solve.BacktrackingSolver import BacktrackingSolver\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for maze navigation system\"\"\"\n",
    "    # Maze parameters\n",
    "    maze_size: int = 5\n",
    "    maze_id: Optional[int] = None  # None for random generation, int for specific seed\n",
    "    \n",
    "    # Algorithm parameters \n",
    "    learning_rate: float = 0.1  # step size α ∈ (0, 1]\n",
    "    discount_factor: float = 0.9  # γ\n",
    "    epsilon: float = 0.1  # small ε > 0\n",
    "    num_episodes: int = 100\n",
    "    agent_seed: int = None\n",
    "\n",
    "    # Reward structure\n",
    "    goal_reward: float = 100\n",
    "    wall_penalty: float = -10\n",
    "    step_penalty: float = -1\n",
    "\n",
    "    # training parameter\n",
    "    optimal_path_convergence_window: int = 5 # number of optimal path occurence for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent, Environment Control classes\n",
    "\n",
    "Go to the execution section to configure, train, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnvironment:\n",
    "    \"\"\"Handles maze generation, state management and visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.grid = None\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self._maze = None\n",
    "        self.seed = None\n",
    "        self.optimal_path = None\n",
    "        self.optimal_path_length = None\n",
    "        self.generate()\n",
    "\n",
    "        \n",
    "    def generate(self) -> None:\n",
    "        \"\"\"Creates new maze using Sidewinder algorithm\"\"\"\n",
    "        # Use config maze_id if provided, otherwise generate random seed\n",
    "        self.seed = self.config.maze_id if self.config.maze_id is not None else np.random.randint(1, 1000)\n",
    "   \n",
    "        self._maze = Maze(self.seed)\n",
    "        self._maze.generator = Sidewinder(self.config.maze_size, self.config.maze_size)\n",
    "        self._maze.generate()\n",
    "        self._maze.generate_entrances()\n",
    "        \n",
    "        self.grid = self._maze.grid\n",
    "\n",
    "        # Set the start to the first valid cell inside grid\n",
    "        self.start = (1, 1)  \n",
    "        # Set the end to the last valid cell inside grid\n",
    "        self.end = (self.grid.shape[0]-2, self.grid.shape[1]-2)\n",
    "\n",
    "        # After maze generation, calculate optimal path\n",
    "        self._calculate_optimal_path()\n",
    "\n",
    "    def _calculate_optimal_path(self) -> None:\n",
    "        \"\"\"Calculate optimal path using maze's solver\"\"\"\n",
    "        # Set up solver\n",
    "        self._maze.solver = BacktrackingSolver()\n",
    "        self._maze.start = self.start\n",
    "        self._maze.end = self.end\n",
    "        \n",
    "        # Solve\n",
    "        self._maze.solve()\n",
    "        \n",
    "        if self._maze.solutions:\n",
    "            self.optimal_path = self._maze.solutions[0]  # Store first solution\n",
    "            self.optimal_path_length = len(self.optimal_path) + 1\n",
    "        else:\n",
    "            # Handle case where no solution is found\n",
    "            self.optimal_path = None\n",
    "            self.optimal_path_length = None\n",
    "    \n",
    "    def get_minimum_steps(self) -> Optional[int]:\n",
    "        \"\"\"Returns the length of optimal path if it exists\"\"\"\n",
    "        return self.optimal_path_length\n",
    "        \n",
    "    def get_state(self, position: Tuple[int, int]) -> Tuple[int, int]:\n",
    "        \"\"\"Returns current state representation\"\"\"\n",
    "        # There is no uncertainity in this environment\n",
    "        # uncertainty could be added here to mimic a dirty sensor\n",
    "        # or to mimic external factors like wind\n",
    "        # this would be for a different problem (partial observability) \n",
    "        return position\n",
    "        \n",
    "    def get_reward(self, state: Tuple[int, int], next_state: Tuple[int, int]) -> float:\n",
    "        \"\"\"Calculates reward for a state transition\"\"\"\n",
    "        if not self.is_valid_move(next_state):\n",
    "            return self.config.wall_penalty\n",
    "        elif next_state == self.end:\n",
    "            return self.config.goal_reward\n",
    "        return self.config.step_penalty\n",
    "        \n",
    "    def is_valid_move(self, state: Tuple[int, int]) -> bool:\n",
    "        \"\"\"Checks if move is legal\"\"\"\n",
    "        row, col = state\n",
    "        # check if the move goes outside of the grid or into a wall (1)\n",
    "        if (row < 0 or row >= self.grid.shape[0] or \n",
    "            col < 0 or col >= self.grid.shape[1] or \n",
    "            self.grid[state] == 1):\n",
    "            return False\n",
    "        # move is valid\n",
    "        return True\n",
    "\n",
    "    def visualize(self, path: Optional[List[Tuple[int, int]]] = None, \n",
    "                    show_optimal: bool = False,\n",
    "                    save_path: Optional[Path] = None) -> None:\n",
    "        \"\"\"Displays or saves maze visualization with optional path and optimal path\n",
    "        \n",
    "        Args:\n",
    "            path: Optional list of positions showing a solution path\n",
    "            show_optimal: Whether to display the optimal path\n",
    "            save_path: If provided, saves figure to this path instead of displaying\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        # Add title showing Maze ID and minimum steps\n",
    "        title = f\"Maze #{self.seed} - Min Steps: {self.get_minimum_steps()}\"\n",
    "        plt.title(title)\n",
    "        plt.imshow(self.grid, cmap='binary')\n",
    "        \n",
    "        # Plots the start (S) and goal (G)\n",
    "        plt.text(self.start[1], self.start[0], 'S', \n",
    "                ha='center', va='center', color='red', fontsize=20)\n",
    "        plt.text(self.end[1], self.end[0], 'G', \n",
    "                ha='center', va='center', color='green', fontsize=20)\n",
    "        \n",
    "        # Plot optimal path if requested\n",
    "        if show_optimal and self.optimal_path:\n",
    "            for pos in self.optimal_path:\n",
    "                plt.text(pos[1], pos[0], \"O\", \n",
    "                        ha='center', va='center', color='green', fontsize=15)\n",
    "        \n",
    "        # Plot current path\n",
    "        if path:\n",
    "            for pos in path:\n",
    "                plt.text(pos[1], pos[0], \"#\", \n",
    "                        ha='center', va='center', color='blue', fontsize=20)\n",
    "        \n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"Implements Q-learning algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, env: MazeEnvironment, config: Config):\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "        # Actions the agent can take: Up, Down, Left, Right. Each action is represented as a tuple of two values: (row_change, column_change)\n",
    "        self.actions = [\n",
    "            (-1, 0), # Up: Moving one step up, reducing the row index by 1\n",
    "            (1, 0),  # Down: Moving on step down, increasing the row index by 1\n",
    "            (0, -1), # Left: Moving one step to the left, reducing the column index by 1\n",
    "            (0, 1)   # Right: Moving one step to the right, increasing the column index by 1 \n",
    "        ]\n",
    "        maze_height, maze_width = env.grid.shape\n",
    "        self.q_table = np.zeros((maze_height, maze_width, 4))\n",
    "        self.exploration_rate = config.epsilon\n",
    "\n",
    "        # Set random seed for the agent\n",
    "        if self.config.agent_seed is not None:\n",
    "            np.random.seed(self.config.agent_seed)\n",
    "        \n",
    "    def get_action(self, state: Tuple[int, int], training: bool = True) -> int:\n",
    "        \"\"\"Selects action using ε-greedy policy\"\"\"\n",
    "        # When training, Choose A from S using policy derived from Q (e.g., ε-greedy)\n",
    "        if training and np.random.rand() < (1- self.exploration_rate):\n",
    "            # explore\n",
    "            return np.random.randint(4)\n",
    "        # exploit\n",
    "        return np.argmax(self.q_table[state])\n",
    "        \n",
    "    def update(self, state: Tuple[int, int], action: int, \n",
    "              reward: float, next_state: Tuple[int, int]) -> None:\n",
    "        \"\"\"Updates Q-value for state-action pair\"\"\"\n",
    "        # max_a Q(S', a)\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        # Q(S, A)\n",
    "        current_q = self.q_table[state][action]\n",
    "        # Q(S', a)\n",
    "        next_q = self.q_table[next_state][best_next_action]\n",
    "        # Q(S, A) ← Q(S, A) + α [R + γ max_a Q(S', a) - Q(S, A)]\n",
    "        new_q = current_q + self.config.learning_rate * (\n",
    "            reward + self.config.discount_factor * next_q - current_q)\n",
    "        # update the q_table\n",
    "        self.q_table[state][action] = new_q\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentControl:\n",
    "    \"\"\"Manages training, testing and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, env: MazeEnvironment, agent: QLearningAgent, config: Config):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.config = config\n",
    "        self.metrics = {\n",
    "            'rewards': [],\n",
    "            'steps': [],\n",
    "            'success_rate': [],\n",
    "            'episode_status': [],  # Track if episode reached goal\n",
    "            'training_start_time': None,\n",
    "            'training_duration': None,\n",
    "            'path_lengths': [],    # Track path length per episode\n",
    "            'final_qtable': None,  # Store final Q-table state\n",
    "            'policy_stability': [], # Track changes in policy\n",
    "            'reached_goal_test': [], # Track if the episode test reached the goal\n",
    "            'steps_test': [], # Track the episode test step count\n",
    "            'path_optimality_test': [], # Track the episode test path optimality\n",
    "            'steps_to_first_optimal': [], # Track the number of steps before the test agent uses optimal path\n",
    "            'episodes_to_convergence': None, # Track the number of steps before the test agent *always* uses optimal path\n",
    "        }\n",
    "        \n",
    "    def calculate_policy_stability(self) -> float:\n",
    "        \"\"\"Measures policy stability by comparing action choices across states\"\"\"\n",
    "        current_policy = {state: np.argmax(self.agent.q_table[state]) \n",
    "                         for state in np.ndindex(self.env.grid.shape)}\n",
    "        if not hasattr(self, '_last_policy'):\n",
    "            self._last_policy = current_policy\n",
    "            return 0.0\n",
    "        \n",
    "        matches = sum(1 for s in current_policy \n",
    "                     if current_policy[s] == self._last_policy[s])\n",
    "        stability = matches / len(current_policy)\n",
    "        self._last_policy = current_policy\n",
    "        return stability\n",
    "        \n",
    "    def run_episode(self, training: bool = True) -> Tuple[float, int, List[Tuple[int, int]], bool]:\n",
    "        \"\"\"Runs single episode with enhanced metrics\"\"\"\n",
    "        current_state = self.env.start\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        path = [current_state]\n",
    "        max_steps = self.env.grid.shape[0] * self.env.grid.shape[1] * 10\n",
    "        reached_goal = False\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            action = self.agent.get_action(current_state, training)\n",
    "            next_state = (\n",
    "                current_state[0] + self.agent.actions[action][0],\n",
    "                current_state[1] + self.agent.actions[action][1]\n",
    "            )\n",
    "            \n",
    "            if not self.env.is_valid_move(next_state):\n",
    "                next_state = current_state\n",
    "\n",
    "            reward = self.env.get_reward(current_state, next_state)\n",
    "            \n",
    "            if training:\n",
    "                self.agent.update(current_state, action, reward, next_state)\n",
    "                \n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            path.append(next_state)\n",
    "            \n",
    "            if next_state == self.env.end:\n",
    "                reached_goal = True\n",
    "                break\n",
    "                \n",
    "            current_state = next_state\n",
    "            \n",
    "        return episode_reward, steps, path, reached_goal\n",
    "        \n",
    "    def train(self, save_path: Optional[str] = None, experiment_id: Optional[str] = None, iteration_count: Optional[str] = None) -> None:\n",
    "        \"\"\"Runs training loop with enhanced metrics\"\"\"\n",
    "        plt.ion()\n",
    "        fig, (ax_reward, ax_steps, ax_stability, ax_optimality) = plt.subplots(1, 4, figsize=(15, 5))\n",
    "        \n",
    "        window_size = 20\n",
    "        moving_rewards = []\n",
    "        moving_steps = []\n",
    "        episodes_to_convergence = None\n",
    "        path_optimality_test_counter = 0\n",
    "\n",
    "        self.experiment_id = experiment_id if experiment_id else None\n",
    "        self.iteration_count = iteration_count if iteration_count else None\n",
    "        \n",
    "        self.metrics['training_start_time'] = time.time()\n",
    "\n",
    "        try:\n",
    "            for episode in range(self.config.num_episodes):\n",
    "                # Clear old plot data periodically to manage memory\n",
    "                if episode % 50 == 0:\n",
    "                    plt.close('all')\n",
    "                    fig, (ax_reward, ax_steps, ax_stability, ax_optimality) = plt.subplots(1, 4, figsize=(15, 5))\n",
    "                \n",
    "                \n",
    "                reward, steps, path, reached_goal = self.run_episode(training=True)\n",
    "                \n",
    "                # Update metrics\n",
    "                self.metrics['rewards'].append(reward)\n",
    "                self.metrics['steps'].append(steps)\n",
    "                self.metrics['episode_status'].append(reached_goal)\n",
    "                self.metrics['policy_stability'].append(self.calculate_policy_stability())\n",
    "                \n",
    "                # Calculate moving averages\n",
    "                if episode >= window_size:\n",
    "                    avg_reward = np.mean(self.metrics['rewards'][-window_size:])\n",
    "                    avg_steps = np.mean(self.metrics['steps'][-window_size:])\n",
    "                else:\n",
    "                    avg_reward = np.mean(self.metrics['rewards'])\n",
    "                    avg_steps = np.mean(self.metrics['steps'])\n",
    "                    \n",
    "                moving_rewards.append(avg_reward)\n",
    "                moving_steps.append(avg_steps)\n",
    "\n",
    "                # run a test \n",
    "                _ , steps_test, path_test, reached_goal_test= self.run_episode(training=False)\n",
    "                \n",
    "                self.metrics['reached_goal_test'].append(reached_goal_test)\n",
    "                self.metrics['steps_test'].append(steps_test)\n",
    "                \n",
    "                if steps_test != 0 or self.env.optimal_path_length != 0:\n",
    "                    path_optimality_test = self.env.optimal_path_length / steps_test\n",
    "                else:\n",
    "                    path_optimality_test = 0\n",
    "                \n",
    "                self.metrics['path_optimality_test'].append(path_optimality_test)\n",
    "\n",
    "                # check for convergence on the optimal path\n",
    "                if path_optimality_test == 1:\n",
    "                    path_optimality_test_counter += 1 \n",
    "                    if path_optimality_test_counter == self.config.optimal_path_convergence_window:\n",
    "                        episodes_to_convergence = episode\n",
    "                        break\n",
    "                else:\n",
    "                    path_optimality_test_counter = 0\n",
    "\n",
    "                # Update plots (with memory management)\n",
    "                if episode % 5 == 0:  # Update plots less frequently\n",
    "                    self._update_training_plots(fig, ax_reward, ax_steps, ax_stability, ax_optimality,\n",
    "                                            moving_rewards, moving_steps)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Training stopped due to error: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            print(self.config.optimal_path_convergence_window)    \n",
    "            self.metrics['training_duration'] = time.time() - self.metrics['training_start_time']\n",
    "            self.metrics['episodes_to_convergence'] = episodes_to_convergence\n",
    "\n",
    "            # One final plot update\n",
    "            self._update_training_plots(fig, ax_reward, ax_steps, ax_stability, ax_optimality,\n",
    "                                    moving_rewards, moving_steps)\n",
    "\n",
    "            if save_path:\n",
    "                timestamp = int(time.time())\n",
    "                filename = (f\"{timestamp}_maze{self.config.maze_size}_\"\n",
    "                        f\"lr{self.config.learning_rate}_\"\n",
    "                        f\"df{self.config.discount_factor}_\"\n",
    "                        f\"eps{self.config.epsilon}.png\")\n",
    "                fig.savefig(f\"{save_path}/{filename}\", bbox_inches='tight')\n",
    "\n",
    "            plt.close('all')  # Clean up all plots\n",
    "            plt.ioff()\n",
    "\n",
    "\n",
    "    def test(self, display: bool = False) -> None:\n",
    "        \"\"\"Evaluates agent performance\"\"\"\n",
    "        episode_reward, steps, path, reached_goal= self.run_episode(training=False)\n",
    "        print(f\"Test Results - Steps: {steps}, Reward: {episode_reward}, Successful: {reached_goal}\")\n",
    "        if display:\n",
    "            self.env.visualize(path)\n",
    "\n",
    "    def test_consistency(self, num_tests: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"Tests agent consistency across multiple runs\"\"\"\n",
    "        test_results = {\n",
    "            'success_rate': 0,\n",
    "            'avg_steps': 0,\n",
    "            'std_steps': 0,\n",
    "            'avg_path_optimality': 0,\n",
    "            'std_path_optimality': 0\n",
    "        }\n",
    "        \n",
    "        steps_list = []\n",
    "        optimality_list = []\n",
    "        for _ in range(num_tests):\n",
    "            reward, steps, path, reached_goal = self.run_episode(training=False)\n",
    "            if reached_goal:\n",
    "                test_results['success_rate'] += 1\n",
    "            if steps != 0 or self.env.optimal_path_length != 0:\n",
    "                optimality_list.append(self.env.optimal_path_length / steps)\n",
    "            else:\n",
    "                optimality_list.append(0)                \n",
    "            steps_list.append(steps)\n",
    "        \n",
    "        if steps_list:\n",
    "            test_results['success_rate'] /= num_tests\n",
    "            test_results['avg_steps'] = np.mean(steps_list)\n",
    "            test_results['std_steps'] = np.std(steps_list)\n",
    "            \n",
    "        if optimality_list:\n",
    "            test_results['avg_path_optimality'] = np.mean(optimality_list)\n",
    "            test_results['std_path_optimality'] = np.std(optimality_list)\n",
    "            \n",
    "        return test_results\n",
    "\n",
    "    def _update_training_plots(self, fig, ax_reward, ax_steps, ax_stability, ax_optimality,\n",
    "                             moving_rewards, moving_steps) -> None:\n",
    "        \"\"\"Updates training visualization plots\"\"\"\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Add configuration header\n",
    "        elapsed_time = time.time() - self.metrics['training_start_time']\n",
    "        header = (f\"Agent Seed: {self.config.agent_seed} | \"\n",
    "                f\"Maze Size: {self.config.maze_size}x{self.config.maze_size} | \"\n",
    "                f\"Maze ID: {self.env.seed} | \"\n",
    "                f\"Min Steps: {self.env.get_minimum_steps()} | \"\n",
    "                f\"Episodes: {self.config.num_episodes}\\n\"\n",
    "                f\"Learning Rate: {self.config.learning_rate} | \"\n",
    "                f\"Discount: {self.config.discount_factor} | \"\n",
    "                f\"Epsilon: {self.config.epsilon}\\n\"\n",
    "                f\"Episodes to Convergence: {self.metrics['episodes_to_convergence']} | \"\n",
    "                f\"Training Time: {elapsed_time:.1f}s\")\n",
    "        # Add experiment metadata as footer if available\n",
    "        if hasattr(self, 'experiment_id') and hasattr(self, 'iteration_count'):\n",
    "            plt.subplots_adjust()  \n",
    "            footer = f\"Experiment: {self.experiment_id} | Iteration: {self.iteration_count}\"\n",
    "            fig.text(0.5, 0, footer, ha='center', va='center', fontsize=10)\n",
    "\n",
    "        fig.suptitle(header, wrap=True, y=1.05)\n",
    "        \n",
    "        # Reward plot\n",
    "        ax_reward.clear()\n",
    "        ax_reward.plot(self.metrics['rewards'], 'r-', alpha=0.3, label='Rewards')\n",
    "        ax_reward.plot(moving_rewards, 'r-', label='Moving Average')\n",
    "        ax_reward.set_xlabel('Episode')\n",
    "        ax_reward.set_ylabel('Reward')\n",
    "        ax_reward.legend()\n",
    "        ax_reward.grid(True)\n",
    "        \n",
    "        # Steps plot\n",
    "        ax_steps.clear()\n",
    "        ax_steps.plot(self.metrics['steps'], 'b-', alpha=0.3, label='Steps')\n",
    "        ax_steps.plot(moving_steps, 'b-', label='Moving Average')\n",
    "        ax_steps.set_xlabel('Episode')\n",
    "        ax_steps.set_ylabel('Steps')\n",
    "        ax_steps.legend()\n",
    "        ax_steps.grid(True)\n",
    "        \n",
    "        # Policy stability plot\n",
    "        ax_stability.clear()\n",
    "        ax_stability.plot(self.metrics['policy_stability'], 'g-', label='Policy Stability')\n",
    "        ax_stability.set_xlabel('Episode')\n",
    "        ax_stability.set_ylabel('Stability')\n",
    "        ax_stability.legend()\n",
    "        ax_stability.grid(True)\n",
    "\n",
    "        # Path optimality plot\n",
    "        ax_optimality.clear()\n",
    "        ax_optimality.plot(self.metrics['path_optimality_test'], 'g-', label='Path Optimality')\n",
    "        ax_optimality.set_xlabel('Episode')\n",
    "        ax_optimality.set_ylabel('Optimality')\n",
    "        ax_optimality.legend()\n",
    "        ax_optimality.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataclasses import asdict\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "class ExperimentRunner:\n",
    "    \"\"\"Manages automated testing and reporting\"\"\"\n",
    "    \n",
    "    def __init__(self, base_config: Config):\n",
    "        self.base_config = base_config\n",
    "        self.results = []\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.experiment_id = experiment_id or timestamp\n",
    "        self.base_path = Path(f\"experiments/{self.experiment_id}\")\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _train_and_test_agent(self, config, num_tests, save_path, experiment_id=None, iteration_count=None):\n",
    "        env = MazeEnvironment(config)\n",
    "        agent = QLearningAgent(env, config)\n",
    "        control = AgentControl(env, agent, config)\n",
    "        \n",
    "        # Train and test\n",
    "        control.train(\n",
    "            save_path=str(save_path),\n",
    "            experiment_id=experiment_id,\n",
    "            iteration_count=iteration_count\n",
    "        )\n",
    "        test_results = control.test_consistency(num_tests=num_tests)\n",
    "        \n",
    "        # Flatten it like this:\n",
    "        flattened_result = {\n",
    "            # Unpack all parameters from config\n",
    "            **asdict(config),\n",
    "            # Unpack all test results\n",
    "            **test_results,\n",
    "            # Add the additional metrics\n",
    "            'training_duration': control.metrics['training_duration'],\n",
    "            'final_policy_stability': control.metrics['policy_stability'][-1],\n",
    "            'episodes_to_convergence': control.metrics['episodes_to_convergence'],\n",
    "            # Add experiment metadata\n",
    "            'experiment_id': experiment_id,\n",
    "            'iteration_count': iteration_count\n",
    "        }\n",
    "\n",
    "        return flattened_result\n",
    "        \n",
    "    def run_hyperparameter_sweep(self, param_grid: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Test different hyperparameter combinations\"\"\"\n",
    "        # create the training results folder\n",
    "        save_path = self.base_path / \"hyperparameter_sweep\"\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "\n",
    "        if param_grid == None:\n",
    "            param_grid = {\n",
    "                'learning_rate': [0.1, 0.11],\n",
    "                'discount_factor': [1, 0.9],\n",
    "                'epsilon': [0.25, 0.3], \n",
    "                'maze_id': 2,\n",
    "                'num_episodes': 1500,\n",
    "                'maze_size': 5,\n",
    "                'agent_seeds': [1, 2, 3, 4, 5], \n",
    "            } \n",
    "\n",
    "        # Calculate total number of iterations\n",
    "        total_iterations = (\n",
    "            len(param_grid['agent_seeds']) *\n",
    "            len(param_grid['learning_rate']) *\n",
    "            len(param_grid['discount_factor']) *\n",
    "            len(param_grid['epsilon'])\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        current_iteration = 0\n",
    "\n",
    "        for agent_seed in  param_grid['agent_seeds']:\n",
    "            for lr in param_grid['learning_rate']:\n",
    "                for df in param_grid['discount_factor']:\n",
    "                    for eps in param_grid['epsilon']:\n",
    "                        current_iteration += 1\n",
    "                        iteration_count = f\"{current_iteration}/{total_iterations}\"\n",
    "\n",
    "                        config = Config(\n",
    "                            maze_size=param_grid['maze_size'],\n",
    "                            maze_id=param_grid['maze_id'],\n",
    "                            num_episodes=param_grid['num_episodes'],\n",
    "                            learning_rate=lr,\n",
    "                            discount_factor=df,\n",
    "                            epsilon=eps,\n",
    "                            agent_seed=agent_seed\n",
    "                        )\n",
    "                        flattened_result = self._train_and_test_agent(\n",
    "                            config, \n",
    "                            num_tests=10, \n",
    "                            save_path=save_path,\n",
    "                            experiment_id=self.experiment_id,\n",
    "                            iteration_count=iteration_count\n",
    "                        )                        \n",
    "                        results.append(flattened_result)\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def _run_untrained_tests(self, num_trials: int) -> Dict:\n",
    "        \"\"\"Run tests with untrained agent\"\"\"\n",
    "        env = MazeEnvironment(self.base_config)\n",
    "        agent = QLearningAgent(env, self.base_config)\n",
    "        control = AgentControl(env, agent, self.base_config)\n",
    "        return control.test_consistency(num_trials)\n",
    "    \n",
    "    def save_report(self, experiment_results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Save experiment results to Excel report\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = self.base_path / f\"experiment_report_{timestamp}.xlsx\"\n",
    "        \n",
    "        if 'hyperparameters' in experiment_results:\n",
    "            output_path = self.base_path / f\"hyperparameter_results_{timestamp}.csv\"\n",
    "            df_hyper = pd.DataFrame(experiment_results['hyperparameters'])\n",
    "            df_hyper.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeExperiments:\n",
    "    \"\"\"Manages maze learning experiments with comprehensive tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = \"experiments\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.experiment_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "    def run_experiment(self, maze_sizes: List[int], param_grid: Dict, trials: int = 5):\n",
    "        \"\"\"Run experiments across maze sizes and parameters\"\"\"\n",
    "        results = {}\n",
    "        total_iterations = len(maze_sizes) * len(param_grid['learning_rate']) * \\\n",
    "                          len(param_grid['discount_factor']) * len(param_grid['epsilon']) * trials\n",
    "        current_iteration = 0\n",
    "        \n",
    "        # Create experiment directory structure\n",
    "        experiment_dir = self.base_path / self.experiment_id\n",
    "        experiment_dir.mkdir(exist_ok=True)\n",
    "        plots_dir = experiment_dir / 'training_plots'\n",
    "        plots_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for size in maze_sizes:\n",
    "            size_results = []\n",
    "            size_dir = plots_dir / f'size_{size}'\n",
    "            size_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            for lr in param_grid['learning_rate']:\n",
    "                for df in param_grid['discount_factor']:\n",
    "                    for eps in param_grid['epsilon']:\n",
    "                        for trial in range(trials):\n",
    "                            current_iteration += 1\n",
    "                            iteration_count = f\"{current_iteration}/{total_iterations}\"\n",
    "                            \n",
    "                            # Configure experiment\n",
    "                            config = Config(\n",
    "                                maze_size=size,\n",
    "                                learning_rate=lr,\n",
    "                                discount_factor=df,\n",
    "                                epsilon=eps,\n",
    "                                num_episodes=param_grid['num_episodes'],\n",
    "                                maze_id=param_grid.get('maze_id', None),\n",
    "                                agent_seed=trial\n",
    "                            )\n",
    "                            \n",
    "                            # Run single experiment\n",
    "                            save_path = size_dir / f\"lr{lr}_df{df}_eps{eps}_trial{trial}\"\n",
    "                            result = self._run_single_experiment(\n",
    "                                config=config,\n",
    "                                save_path=save_path,\n",
    "                                experiment_id=self.experiment_id,\n",
    "                                iteration_count=iteration_count\n",
    "                            )\n",
    "                            size_results.append(result)\n",
    "                            \n",
    "            results[size] = size_results\n",
    "            \n",
    "        # Save experiment metadata\n",
    "        metadata = {\n",
    "            'timestamp': self.experiment_id,\n",
    "            'param_grid': param_grid,\n",
    "            'maze_sizes': maze_sizes,\n",
    "            'trials': trials,\n",
    "            'total_iterations': total_iterations\n",
    "        }\n",
    "        with open(experiment_dir / 'metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "            \n",
    "        return self._save_results(results)\n",
    "    \n",
    "    def _save_maze_visualization(self, env: MazeEnvironment, save_path: Path):\n",
    "        \"\"\"Save visualization of maze with optimal path\"\"\"\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        # Add title showing Maze ID and minimum steps\n",
    "        title = f\"Maze #{env.seed} - Min Steps: {env.get_minimum_steps()}\"\n",
    "        plt.title(title)\n",
    "        plt.imshow(env.grid, cmap='binary')\n",
    "        \n",
    "        # Plots the start (S) and goal (G)\n",
    "        plt.text(env.start[1], env.start[0], 'S', \n",
    "                ha='center', va='center', color='red', fontsize=20)\n",
    "        plt.text(env.end[1], env.end[0], 'G', \n",
    "                ha='center', va='center', color='green', fontsize=20)\n",
    "        \n",
    "        # Plot optimal path\n",
    "        if env.optimal_path:\n",
    "            for pos in env.optimal_path:\n",
    "                plt.text(pos[1], pos[0], \"O\", \n",
    "                        ha='center', va='center', color='green', fontsize=15)\n",
    "        \n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.savefig(save_path / 'maze_optimal.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _run_single_experiment(self, config, save_path, experiment_id=None, iteration_count=None):\n",
    "        \"\"\"Execute single experiment with full metric collection\"\"\"\n",
    "        env = MazeEnvironment(config)\n",
    "        \n",
    "        # Create directory and save maze visualization\n",
    "        maze_path = Path(save_path)\n",
    "        maze_path.mkdir(parents=True, exist_ok=True)\n",
    "        self._save_maze_visualization(env, maze_path)\n",
    "        \n",
    "        # Save maze metadata\n",
    "        maze_metadata = {\n",
    "            'seed': env.seed,\n",
    "            'grid': env.grid.tolist(),  # Convert numpy array to list for JSON\n",
    "            'start': env.start,\n",
    "            'end': env.end,\n",
    "            'optimal_path': env.optimal_path,\n",
    "            'optimal_path_length': env.optimal_path_length\n",
    "        }\n",
    "        with open(maze_path / 'maze_metadata.json', 'w') as f:\n",
    "            json.dump(maze_metadata, f, indent=4)\n",
    "            \n",
    "        agent = QLearningAgent(env, config)\n",
    "        control = AgentControl(env, agent, config)\n",
    "        \n",
    "        # Train and save training plot\n",
    "        control.train(\n",
    "            save_path=str(save_path),\n",
    "            experiment_id=experiment_id,\n",
    "            iteration_count=iteration_count\n",
    "        )\n",
    "        \n",
    "        # Run consistency tests\n",
    "        test_results = control.test_consistency(num_tests=10)\n",
    "        \n",
    "        # Combine all results\n",
    "        flattened_result = {\n",
    "            # Configuration parameters\n",
    "            **asdict(config),\n",
    "            # Test results\n",
    "            **test_results,\n",
    "            # Additional metrics\n",
    "            'training_duration': control.metrics['training_duration'],\n",
    "            'final_policy_stability': control.metrics['policy_stability'][-1],\n",
    "            'episodes_to_convergence': control.metrics['episodes_to_convergence'],\n",
    "            'policy_stability_history': control.metrics['policy_stability'],\n",
    "            'reward_history': control.metrics['rewards'],\n",
    "            'steps_history': control.metrics['steps'],\n",
    "            # Experiment metadata\n",
    "            'experiment_id': experiment_id,\n",
    "            'iteration_count': iteration_count,\n",
    "            'save_path': str(save_path)\n",
    "        }\n",
    "        \n",
    "        return flattened_result\n",
    "    \n",
    "    def _save_results(self, results: Dict) -> str:\n",
    "        \"\"\"Save results with comprehensive structure\"\"\"\n",
    "        experiment_dir = self.base_path / self.experiment_id\n",
    "        results_dir = experiment_dir / 'results'\n",
    "        results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save each maze size results with full detail\n",
    "        for size, size_results in results.items():\n",
    "            with open(results_dir / f\"size_{size}.json\", 'w') as f:\n",
    "                json.dump(size_results, f, indent=4)\n",
    "        \n",
    "        return str(experiment_dir)\n",
    "    \n",
    "    def analyze_results(self, experiment_dir: Optional[str] = None):\n",
    "        \"\"\"Analyze and visualize experiment results\"\"\"\n",
    "        if experiment_dir is None:\n",
    "            experiment_dir = str(self.base_path / self.experiment_id)\n",
    "            \n",
    "        # Load results and metadata\n",
    "        exp_path = Path(experiment_dir)\n",
    "        with open(exp_path / 'metadata.json') as f:\n",
    "            metadata = json.load(f)\n",
    "            \n",
    "        results = {}\n",
    "        for size in metadata['maze_sizes']:\n",
    "            with open(exp_path / 'results' / f\"size_{size}.json\") as f:\n",
    "                results[size] = json.load(f)\n",
    "        \n",
    "        # Generate analysis\n",
    "        self._generate_analysis(results, metadata, exp_path)\n",
    "    \n",
    "    def _generate_analysis(self, results: Dict, metadata: Dict, save_path: Path):\n",
    "        \"\"\"Generate comprehensive analysis and visualizations\"\"\"\n",
    "        analysis_dir = save_path / 'analysis'\n",
    "        analysis_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Convert to DataFrame for analysis\n",
    "        dfs = []\n",
    "        for size, size_results in results.items():\n",
    "            df = pd.DataFrame(size_results)\n",
    "            dfs.append(df)\n",
    "        df_all = pd.concat(dfs)\n",
    "        \n",
    "        # Generate plots\n",
    "        self._plot_convergence_analysis(df_all, analysis_dir)\n",
    "        self._plot_parameter_impact(df_all, analysis_dir)\n",
    "        \n",
    "        # Generate summary report\n",
    "        self._generate_summary_report(df_all, metadata, analysis_dir)\n",
    "    \n",
    "    def _plot_convergence_analysis(self, df: pd.DataFrame, save_path: Path):\n",
    "        \"\"\"Plot convergence-related metrics\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Episodes to convergence\n",
    "        sns.boxplot(data=df, x='maze_size', y='episodes_to_convergence', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Episodes to Convergence by Maze Size')\n",
    "        axes[0, 0].set_ylabel('Episodes')\n",
    "        \n",
    "        # Training duration\n",
    "        sns.boxplot(data=df, x='maze_size', y='training_duration', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Training Duration by Maze Size')\n",
    "        axes[0, 1].set_ylabel('Duration (s)')\n",
    "        \n",
    "        # Policy stability\n",
    "        sns.boxplot(data=df, x='maze_size', y='final_policy_stability', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Final Policy Stability by Maze Size')\n",
    "        axes[1, 0].set_ylabel('Stability')\n",
    "        \n",
    "        # Success rate\n",
    "        sns.boxplot(data=df, x='maze_size', y='success_rate', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Success Rate by Maze Size')\n",
    "        axes[1, 1].set_ylabel('Success Rate')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path / 'convergence_analysis.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _plot_parameter_impact(self, df: pd.DataFrame, save_path: Path):\n",
    "        \"\"\"Plot parameter impact on performance\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Learning rate impact\n",
    "        sns.boxplot(\n",
    "            data=df, \n",
    "            x='learning_rate', \n",
    "            y='episodes_to_convergence', \n",
    "            hue='maze_size', \n",
    "            ax=axes[0, 0],\n",
    "            palette=\"pastel\"\n",
    "        )\n",
    "        axes[0, 0].set_title('Learning Rate Impact by Maze Size')\n",
    "        axes[0, 0].set_ylabel('Episodes to Convergence')\n",
    "        axes[0, 0].legend(title='Maze Size', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                \n",
    "        # Discount factor impact\n",
    "        sns.boxplot(\n",
    "            data=df, \n",
    "            x='discount_factor', \n",
    "            y='episodes_to_convergence',\n",
    "            hue='maze_size', \n",
    "            ax=axes[0, 1],\n",
    "            palette=\"pastel\"\n",
    "        )\n",
    "        axes[0, 1].set_title('Discount Factor Impact by Maze Size')\n",
    "        axes[0, 1].set_ylabel('Episodes to Convergence')\n",
    "        axes[0, 1].legend(title='Maze Size', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                \n",
    "        # Epsilon impact\n",
    "        sns.boxplot(\n",
    "            data=df, \n",
    "            x='epsilon', \n",
    "            y='episodes_to_convergence',\n",
    "            hue='maze_size', \n",
    "            ax=axes[1, 0],\n",
    "            palette=\"pastel\"\n",
    "        )\n",
    "        axes[1, 0].set_title('Epsilon Impact by Maze Size')\n",
    "        axes[1, 0].set_ylabel('Episodes to Convergence')\n",
    "        axes[1, 0].legend(title='Maze Size', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # Combined heatmap with per-maze-size normalization\n",
    "        pivot_table = df.groupby(['learning_rate', 'epsilon', 'maze_size'])['episodes_to_convergence'].mean().unstack()\n",
    "        \n",
    "        # Normalize each maze size column by its minimum value\n",
    "        normalized_pivot = pivot_table.copy()\n",
    "        for col in normalized_pivot.columns:\n",
    "            normalized_pivot[col] = normalized_pivot[col] / normalized_pivot[col].min()\n",
    "            \n",
    "        sns.heatmap(normalized_pivot, ax=axes[1, 1], \n",
    "                   cmap='YlOrRd',\n",
    "                   annot=True, \n",
    "                   fmt='.1f',\n",
    "                   cbar_kws={'label': 'Ratio to minimum episodes for maze size'})\n",
    "        axes[1, 1].set_title('Normalized Episodes to Convergence\\nby Learning Rate and Epsilon')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path / 'parameter_impact.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _generate_summary_report(self, df: pd.DataFrame, metadata: Dict, save_path: Path):\n",
    "        \"\"\"Generate detailed summary report\"\"\"\n",
    "        with open(save_path / 'summary_report.md', 'w') as f:\n",
    "            f.write(\"# Experiment Summary\\n\\n\")\n",
    "            f.write(f\"**Experiment ID**: {metadata['timestamp']}\\n\\n\")\n",
    "            f.write(f\"**Total Iterations**: {metadata['total_iterations']}\\n\\n\")\n",
    "\n",
    "            f.write(\"## Best Configurations by Maze Size\\n\")\n",
    "            for size in df['maze_size'].unique():\n",
    "                size_df = df[df['maze_size'] == size]\n",
    "                best_idx = size_df['episodes_to_convergence'].idxmin()\n",
    "                best_config = size_df.loc[best_idx]\n",
    "\n",
    "                f.write(f\"\\n### Maze Size {size}x{size}\\n\")\n",
    "                f.write(f\"- **Learning Rate**: {best_config['learning_rate']}\\n\")\n",
    "                f.write(f\"- **Discount Factor**: {best_config['discount_factor']}\\n\")\n",
    "                f.write(f\"- **Epsilon**: {best_config['epsilon']}\\n\")\n",
    "                f.write(f\"- **Episodes to Convergence**: {best_config['episodes_to_convergence']:.1f}\\n\")\n",
    "                f.write(f\"- **Success Rate**: {best_config['success_rate']:.1%}\\n\")\n",
    "                f.write(f\"- **Final Policy Stability**: {best_config['final_policy_stability']:.2f}\\n\")\n",
    "\n",
    "                # Add a link to the corresponding maze visualization\n",
    "                image_path = save_path / f\"training_plots/size_{size}/maze_optimal.png\"\n",
    "                if image_path.exists():\n",
    "                    relative_image_path = image_path.relative_to(save_path.parent)\n",
    "                    f.write(f\"![Maze Visualization](../{relative_image_path})\\n\\n\")\n",
    "                else:\n",
    "                    f.write(\"*Maze visualization not found.*\\n\\n\")\n",
    "                \n",
    "# Usage Example:\n",
    "if __name__ == \"__main__\":\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.2, 0.3, 0.4],\n",
    "        'discount_factor': [1.0, 0.99],\n",
    "        'epsilon': [0.06, 0.08, 0.1],\n",
    "        'num_episodes': 1000,\n",
    "        'maze_id': 42  # Optional: fixed maze for reproducibility\n",
    "    }\n",
    "    \n",
    "    experiments = MazeExperiments()\n",
    "    experiment_dir = experiments.run_experiment(\n",
    "        # maze_sizes=[3, 4, 5, 6, 7, 8],\n",
    "        maze_sizes=[3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "        param_grid=param_grid,\n",
    "        trials=5\n",
    "    )\n",
    "    experiments.analyze_results(experiment_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
